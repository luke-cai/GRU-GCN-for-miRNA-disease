{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mirna_mlp_exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "8ukuAdqcL6MQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ_Ci_QULpvN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "# os.environ['TORCH'] = torch.__version__\n",
        "# print(torch.__version__)\n",
        "\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "from collections.abc import Iterable\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "# import torch_geometric\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from torch.nn import Linear\n",
        "\n",
        "# from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool, BatchNorm\n",
        "# from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "# from torch_geometric.data import DataLoader as PYG_DataLoader\n",
        "# from torch_geometric.data import Data\n",
        "from itertools import cycle\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X4ZE_WPL8Cf",
        "outputId": "c741234a-a7fd-438e-81be-706e7bd8e024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "0QUkmt9o9YTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_path = \"/content/drive/MyDrive/miRNA/graph/edge.pkl\"\n",
        "edge_attr_path = \"/content/drive/MyDrive/miRNA/graph/edge_attr.pkl\"\n",
        "node_path = \"/content/drive/MyDrive/miRNA/graph/node.pkl\"\n",
        "mesh_emb_path = '/content/drive/MyDrive/miRNA/embedding/mesh_embs.npy'\n",
        "mesh_vocab_path = '/content/drive/MyDrive/miRNA/embedding/mesh_vocab.npy'"
      ],
      "metadata": {
        "id": "cBclIQPzM1sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_x_disease_path = '/content/drive/MyDrive/miRNA/last_data/cir_X_Disease_v3.npy'\n",
        "all_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/cir_X_RNA_v3.npy'\n",
        "all_pre_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/cir_pre_RNA_x_v3.npy'\n",
        "all_y_soft_path = '/content/drive/MyDrive/miRNA/last_data/cir_soft_y_v3.npy'\n",
        "all_y_hard_path = '/content/drive/MyDrive/miRNA/last_data/cir_hard_y_v3.npy'\n",
        "cir_hair_x_path = '/content/drive/MyDrive/miRNA/last_data/cir_pre_hair_X_v3.npy'\n",
        "snd_x_path = \"/content/drive/MyDrive/miRNA/last_data/cir_snd_X_v3.npy\""
      ],
      "metadata": {
        "id": "KBa_z9aE9drK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_orig_x_disease = np.load(all_x_disease_path, allow_pickle=True)\n",
        "all_orig_x_rna = np.load(all_x_rna_path, allow_pickle=True)\n",
        "all_orig_pre_x_rna = np.load(all_pre_x_rna_path, allow_pickle=True)\n",
        "\n",
        "all_hair_x = np.load(cir_hair_x_path, allow_pickle=True)\n",
        "snd_x = np.load(snd_x_path, allow_pickle=True)\n",
        "\n",
        "all_orig_Y_soft = np.load(all_y_soft_path, allow_pickle=True)\n",
        "all_orig_Y_hard = np.load(all_y_hard_path, allow_pickle=True)"
      ],
      "metadata": {
        "id": "5kU9U4TV9-R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(edge_path, 'rb') as f:\n",
        "    edge_d = pickle.load(f)\n",
        "\n",
        "with open(edge_attr_path, 'rb') as f:\n",
        "    edge_attr_d = pickle.load(f)\n",
        "\n",
        "with open(node_path, 'rb') as f:\n",
        "    node_d = pickle.load(f)"
      ],
      "metadata": {
        "id": "fQ7HEDk3Jl06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for regression\n",
        "x_disease=[]\n",
        "x_rna=[]\n",
        "hair_x=[]\n",
        "y_soft=[]\n",
        "# y_hard=[]\n",
        "max_len = 0\n",
        "for i in range(len(all_orig_x_disease)):\n",
        "  value = edge_d.get(all_orig_x_disease[i])\n",
        "  if value != None:\n",
        "    x_disease.append(all_orig_x_disease[i])\n",
        "    # x_rna.append(all_orig_pre_x_rna[i]+all_orig_x_rna[i])\n",
        "    x_rna.append(all_orig_x_rna[i])\n",
        "    if max_len < len(all_orig_x_rna[i]):max_len = len(all_orig_x_rna[i])\n",
        "    hair_x.append([*all_hair_x[i], *snd_x[i]])\n",
        "    y_soft.append(all_orig_Y_soft[i])\n",
        "    # # y_hard.append(all_orig_Y_hard[i])\n",
        "    # 'down', 'ns', 'up'\n",
        "    # print(i)\n",
        "\n",
        "x_disease=np.array(x_disease)\n",
        "x_rna=np.array(x_rna)\n",
        "hair_x=np.array(hair_x)\n",
        "y_soft=np.array(y_soft)\n",
        "# # y_hard=np.array(y_hard)"
      ],
      "metadata": {
        "id": "oXLQb85iZckY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy7CO87CsKtR",
        "outputId": "498e3aed-a3aa-4de4-ff67-25af0d65b406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_orig_x_disease))\n",
        "print(len(all_orig_x_rna))\n",
        "print(len(all_orig_Y_soft))\n",
        "print(len(all_orig_Y_hard))\n",
        "print(len(all_orig_pre_x_rna))\n",
        "print(len(all_hair_x))\n",
        "print(len(snd_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syhFxTkEDrdX",
        "outputId": "df2c0e70-1fab-475f-e58f-42a1043ddfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7835\n",
            "7835\n",
            "7835\n",
            "7835\n",
            "7835\n",
            "7835\n",
            "7835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_disease))\n",
        "print(len(x_rna))\n",
        "print(len(y_soft))\n",
        "# print(len(y_hard))\n",
        "print(len(hair_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr248eyIN1s7",
        "outputId": "ac2f50e8-9c06-462e-dab7-aa33e6250154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6587\n",
            "6587\n",
            "6587\n",
            "6587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = np.load(r'/content/drive/MyDrive/miRNA/embedding/vocab_npa.npy', allow_pickle=True)\n",
        "values = np.load(r'/content/drive/MyDrive/miRNA/embedding/embs_npa.npy', allow_pickle=True)\n",
        "word2vec = dict(zip(keys, values))"
      ],
      "metadata": {
        "id": "cMIk0-zrUoiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "BQJuYEDvMDhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(mer = {}, n = 1):\n",
        "  pointer = 4\n",
        "  if mer == {}:\n",
        "    for i in range(5):\n",
        "      mer[str(i)] = str(pointer)\n",
        "      pointer += 1\n",
        "  else:\n",
        "    old_dict = mer\n",
        "    new_dict = {}\n",
        "    for key in old_dict:\n",
        "      for i in range(5):\n",
        "        new_key = key + str(i)\n",
        "        if key[0] != '0' and key[-1] != '0':\n",
        "          new_dict[new_key] = str(pointer)\n",
        "          pointer += 1\n",
        "        if key[-1] == '0' and i == 0:\n",
        "          new_dict[new_key] = str(pointer)\n",
        "          pointer += 1\n",
        "    mer = new_dict\n",
        "  # print(mer)\n",
        "  if n != 0:\n",
        "    return make_dict(mer, n-1)\n",
        "  else:\n",
        "    return mer\n",
        "\n"
      ],
      "metadata": {
        "id": "hQLkZQz6ymvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=1)\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "new_mer_dict "
      ],
      "metadata": {
        "id": "sbdYG7uX0bpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446557cb-9352-4f1e-cf46-eefcbf396d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'00': 4,\n",
              " '10': 5,\n",
              " '11': 6,\n",
              " '12': 7,\n",
              " '13': 8,\n",
              " '14': 9,\n",
              " '20': 10,\n",
              " '21': 11,\n",
              " '22': 12,\n",
              " '23': 13,\n",
              " '24': 14,\n",
              " '30': 15,\n",
              " '31': 16,\n",
              " '32': 17,\n",
              " '33': 18,\n",
              " '34': 19,\n",
              " '40': 20,\n",
              " '41': 21,\n",
              " '42': 22,\n",
              " '43': 23,\n",
              " '44': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=0)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "twoMer_dict = {**a_dict, **new_mer_dict}\n",
        "twoMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGnFIvsWy4Af",
        "outputId": "aa40cf86-5a09-4032-b32b-c5b4f4462021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'00': 4,\n",
              " '10': 5,\n",
              " '11': 6,\n",
              " '12': 7,\n",
              " '13': 8,\n",
              " '14': 9,\n",
              " '20': 10,\n",
              " '21': 11,\n",
              " '22': 12,\n",
              " '23': 13,\n",
              " '24': 14,\n",
              " '30': 15,\n",
              " '31': 16,\n",
              " '32': 17,\n",
              " '33': 18,\n",
              " '34': 19,\n",
              " '40': 20,\n",
              " '41': 21,\n",
              " '42': 22,\n",
              " '43': 23,\n",
              " '44': 24,\n",
              " '[CLS]': 1,\n",
              " '[MASK]': 3,\n",
              " '[PAD]': 0,\n",
              " '[SEP]': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=1)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "twoMer_dict = {**a_dict, **new_mer_dict}\n",
        "twoMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vS_9KIJ0xOx",
        "outputId": "9a4a72cb-d7d6-4d88-8597-bb6049218e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'000': 4,\n",
              " '100': 5,\n",
              " '110': 6,\n",
              " '111': 7,\n",
              " '112': 8,\n",
              " '113': 9,\n",
              " '114': 10,\n",
              " '120': 11,\n",
              " '121': 12,\n",
              " '122': 13,\n",
              " '123': 14,\n",
              " '124': 15,\n",
              " '130': 16,\n",
              " '131': 17,\n",
              " '132': 18,\n",
              " '133': 19,\n",
              " '134': 20,\n",
              " '140': 21,\n",
              " '141': 22,\n",
              " '142': 23,\n",
              " '143': 24,\n",
              " '144': 25,\n",
              " '200': 26,\n",
              " '210': 27,\n",
              " '211': 28,\n",
              " '212': 29,\n",
              " '213': 30,\n",
              " '214': 31,\n",
              " '220': 32,\n",
              " '221': 33,\n",
              " '222': 34,\n",
              " '223': 35,\n",
              " '224': 36,\n",
              " '230': 37,\n",
              " '231': 38,\n",
              " '232': 39,\n",
              " '233': 40,\n",
              " '234': 41,\n",
              " '240': 42,\n",
              " '241': 43,\n",
              " '242': 44,\n",
              " '243': 45,\n",
              " '244': 46,\n",
              " '300': 47,\n",
              " '310': 48,\n",
              " '311': 49,\n",
              " '312': 50,\n",
              " '313': 51,\n",
              " '314': 52,\n",
              " '320': 53,\n",
              " '321': 54,\n",
              " '322': 55,\n",
              " '323': 56,\n",
              " '324': 57,\n",
              " '330': 58,\n",
              " '331': 59,\n",
              " '332': 60,\n",
              " '333': 61,\n",
              " '334': 62,\n",
              " '340': 63,\n",
              " '341': 64,\n",
              " '342': 65,\n",
              " '343': 66,\n",
              " '344': 67,\n",
              " '400': 68,\n",
              " '410': 69,\n",
              " '411': 70,\n",
              " '412': 71,\n",
              " '413': 72,\n",
              " '414': 73,\n",
              " '420': 74,\n",
              " '421': 75,\n",
              " '422': 76,\n",
              " '423': 77,\n",
              " '424': 78,\n",
              " '430': 79,\n",
              " '431': 80,\n",
              " '432': 81,\n",
              " '433': 82,\n",
              " '434': 83,\n",
              " '440': 84,\n",
              " '441': 85,\n",
              " '442': 86,\n",
              " '443': 87,\n",
              " '444': 88,\n",
              " '[CLS]': 1,\n",
              " '[MASK]': 3,\n",
              " '[PAD]': 0,\n",
              " '[SEP]': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularDataset(Dataset):\n",
        "  def __init__(self, orig_diseases, orig_mirna, orig_labels, hair_x, is_conv1d=False, is_mlp=False):\n",
        "    super(RegularDataset, self).__init__()\n",
        "    self.orig_diseases = orig_diseases\n",
        "    self.orig_mirna = orig_mirna\n",
        "    self.orig_labels = orig_labels\n",
        "    self.vocab_size = 5\n",
        "    self.MAX_LEN = 28\n",
        "    self.MAX_LEN_D = 6\n",
        "    self.hair_x = hair_x\n",
        "    self.is_conv1d = is_conv1d\n",
        "    self.is_mlp = is_mlp\n",
        "    self.letter2Number = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, \"A\": 4, \"C\": 5, \"G\": 6, \"U\": 7}\n",
        "\n",
        "  def convert_letter_to_number(self, lettter):\n",
        "    letter_to_number = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, \"A\": 4, \"C\": 5, \"G\": 6, \"U\": 7}\n",
        "\n",
        "    return letter_to_number[lettter]\n",
        "\n",
        "  def convert_letter_to_number_Conv1D(self, lettter):\n",
        "    letter_to_number = {'[PAD]': 0, \"A\": 1, \"C\": 2, \"G\": 3, \"U\": 4}\n",
        "\n",
        "    return letter_to_number[lettter]\n",
        "\n",
        "  def make_data_with_unified_length(self, token_list, max_len, covert_dict, is_conv1d=False):\n",
        "      max_len = max_len + 2  # add [CLS] and [SEP]\n",
        "      if is_conv1d:\n",
        "        token_list = token_list\n",
        "      else:\n",
        "        token_list = [covert_dict['[CLS]']] + token_list + [covert_dict['[SEP]']]\n",
        "      \n",
        "      n_pad = max_len - len(token_list)\n",
        "      token_list.extend([0] * n_pad)\n",
        "\n",
        "      return token_list\n",
        "\n",
        "  def mirna2list(self, data, is_conv1d=False):\n",
        "    out = []\n",
        "    if is_conv1d:\n",
        "      for j in range(len(data)):\n",
        "        coverted_rna = self.convert_letter_to_number_Conv1D(data[j])\n",
        "        out.append(coverted_rna)\n",
        "    else:\n",
        "      for j in range(len(data)):\n",
        "        coverted_rna = self.convert_letter_to_number(data[j])\n",
        "        out.append(coverted_rna)\n",
        "\n",
        "    data = self.make_data_with_unified_length(out, self.MAX_LEN, self.letter2Number, is_conv1d=is_conv1d)\n",
        "    \n",
        "    return np.array(data)\n",
        "\n",
        "  \n",
        "  def disease2list(self, disease):\n",
        "    disease = disease.replace(\",\", \"\")\n",
        "    disease = disease.replace(\"-\", \" \")\n",
        "    disease = disease.replace(\"[\", \"\")\n",
        "    disease = disease.replace(\"]\", \"\")\n",
        "    disease = disease.replace(\"Kideny\", \"Kidney\")\n",
        "    disease = disease.replace(\"Spondylarthritis\", \"Spondyloarthritis\")\n",
        "    disease_list = disease.split(\" \")\n",
        "    return disease_list\n",
        "\n",
        "\n",
        "  def disease2emb(self, disease):\n",
        "\n",
        "    disease_list = self.disease2list(disease)\n",
        "    vector = np.zeros(300)\n",
        "    for word in disease_list:\n",
        "      value = word2vec.get(word)\n",
        "      if value is not None:\n",
        "        vector += word2vec[word]\n",
        "      else:\n",
        "        vector = np.zeros(300)\n",
        "    out = vector/len(disease_list)\n",
        "    return out\n",
        "\n",
        "  def disease2token(self, disease):\n",
        "  \n",
        "    disease = disease.replace(\", \", \" \")\n",
        "    disease_list = disease.split(\" \")\n",
        "    \n",
        "    # print(disease_list)\n",
        "    tokenized=[self.disease_token_list[i] for i in disease_list]\n",
        "    max_len = self.MAX_LEN_D + 2\n",
        "    tokenized = [self.disease_token_list['[CLS]']] + tokenized + [self.disease_token_list['[SEP]']]\n",
        "    n_pad = max_len - len(tokenized)\n",
        "    tokenized.extend([0] * n_pad)\n",
        "    return np.array(tokenized)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      orig_disease = self.orig_diseases[index]\n",
        "      orig_mirna = self.orig_mirna[index]\n",
        "      label = self.orig_labels[index]+1\n",
        "      # mirna = self.mirna2list(orig_mirna)\n",
        "      # mirna = np.array([self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)])\n",
        "      if self.is_conv1d:\n",
        "        mirna = np.array([self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)])\n",
        "        mirna = torch.from_numpy(mirna).float()\n",
        "      else:\n",
        "        mirna = self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)\n",
        "        mirna = np.expand_dims(mirna, axis=0)\n",
        "      \n",
        "      if self.is_mlp:\n",
        "        pre_hair_x = torch.from_numpy(np.array([*self.hair_x[index], *self.mirna2list(orig_mirna)])).float()\n",
        "      else:\n",
        "        pre_hair_x = torch.from_numpy(np.array([*self.hair_x[index]])).float()\n",
        "\n",
        "      disease = self.disease2emb(orig_disease)\n",
        "      # disease = np.expand_dims(disease, axis=0)\n",
        "      return disease, mirna, label, pre_hair_x, orig_disease, orig_mirna\n",
        "      \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.orig_diseases)\n"
      ],
      "metadata": {
        "id": "8AIj5dZn-x0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  print('disease.shape ',disease.shape)\n",
        "  print('mirna ',mirna)\n",
        "  print('pre_hair_x.shape ',pre_hair_x.shape)\n",
        "  print('mirna.shape', mirna.shape)\n",
        "  print('label.shape ',label.shape)\n",
        "  print('label ',label)\n",
        "  print('orig_disease ',orig_disease)\n",
        "  print('orig_mirna ',orig_mirna)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ZCDUPP5f8i",
        "outputId": "df3481bd-25c3-47d6-8ed6-596a87b4763e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "disease.shape  torch.Size([2, 300])\n",
            "mirna  tensor([[[1, 7, 4, 6, 5, 4, 6, 5, 4, 5, 4, 7, 4, 4, 7, 6, 6, 7, 7, 7, 6, 7, 6,\n",
            "          2, 0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[1, 5, 4, 6, 6, 5, 5, 4, 7, 4, 7, 7, 6, 7, 6, 5, 7, 6, 5, 5, 7, 5, 4,\n",
            "          2, 0, 0, 0, 0, 0, 0]]])\n",
            "pre_hair_x.shape  torch.Size([2, 41])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "label.shape  torch.Size([2])\n",
            "label  tensor([0.6000, 0.6000], dtype=torch.float64)\n",
            "orig_disease  ('Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell')\n",
            "orig_mirna  ('UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x, is_conv1d=True)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  print('disease.shape ',disease.shape)\n",
        "  print('mirna ',mirna)\n",
        "  print('pre_hair_x.shape ',pre_hair_x.shape)\n",
        "  print('mirna.shape', mirna.shape)\n",
        "  print('label.shape ',label.shape)\n",
        "  print('label ',label)\n",
        "  print('orig_disease ',orig_disease)\n",
        "  print('orig_mirna ',orig_mirna)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjDuBMrMEgI9",
        "outputId": "e77eefde-d273-4e32-a42d-4cd415d4f3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "disease.shape  torch.Size([2, 300])\n",
            "mirna  tensor([[[4., 1., 3., 2., 1., 3., 2., 1., 2., 1., 4., 1., 1., 4., 3., 3., 4.,\n",
            "          4., 4., 3., 4., 3., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[2., 1., 3., 3., 2., 2., 1., 4., 1., 4., 4., 3., 4., 3., 2., 4., 3.,\n",
            "          2., 2., 4., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "pre_hair_x.shape  torch.Size([2, 41])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "label.shape  torch.Size([2])\n",
            "label  tensor([0.6000, 0.6000], dtype=torch.float64)\n",
            "orig_disease  ('Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell')\n",
            "orig_mirna  ('UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x, is_mlp=True)\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  print('disease.shape ',disease.shape)\n",
        "  print('mirna ',mirna)\n",
        "  print('pre_hair_x.shape ',pre_hair_x.shape)\n",
        "  print('mirna.shape', mirna.shape)\n",
        "  print('label.shape ',label.shape)\n",
        "  print('label ',label)\n",
        "  print('orig_disease ',orig_disease)\n",
        "  print('orig_mirna ',orig_mirna)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0hbqG-mEgBo",
        "outputId": "8624ccd6-7efe-47fb-d81f-0ce3de55752b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "disease.shape  torch.Size([2, 300])\n",
            "mirna  tensor([[[1, 7, 4, 6, 5, 4, 6, 5, 4, 5, 4, 7, 4, 4, 7, 6, 6, 7, 7, 7, 6, 7, 6,\n",
            "          2, 0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[1, 5, 4, 6, 6, 5, 5, 4, 7, 4, 7, 7, 6, 7, 6, 5, 7, 6, 5, 5, 7, 5, 4,\n",
            "          2, 0, 0, 0, 0, 0, 0]]])\n",
            "pre_hair_x.shape  torch.Size([2, 71])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "label.shape  torch.Size([2])\n",
            "label  tensor([0.6000, 0.6000], dtype=torch.float64)\n",
            "orig_disease  ('Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell')\n",
            "orig_mirna  ('UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Oa71gPQFWdna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "NuBnT63-We5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_module(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTM_module, self).__init__()\n",
        "    self.hidden_size = 64\n",
        "    self.embedding_dim = 30\n",
        "    self.num_layer = 2\n",
        "    self.bidirectional = True\n",
        "    self.bi_num = 2 if self.bidirectional else 1\n",
        "    self.dropout = 0.5\n",
        "    # input(seq_len, batch, input_size)\n",
        "    # h0(num_layers * num_directions, batch, hidden_size)\n",
        "    # c0(num_layers * num_directions, batch, hidden_size)\n",
        "    self.lstm1 = nn.LSTM(self.embedding_dim, self.hidden_size, self.num_layer, bidirectional=True, dropout=self.dropout)\n",
        "    # self.lstm2 = nn.LSTM(128, 64, self.num_layer, bidirectional=True, dropout=self.dropout)\n",
        "    self.FC = nn.Linear(128, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_0, c_0 = self.init_hidden_state(x.size(0))\n",
        "    # x = np.transpose(x, (1,0,2))\n",
        "    x = torch.swapaxes(x, 0, 1)\n",
        "    x = x.to(torch.float32)\n",
        "    # print('h_0', h_0.dtype)\n",
        "    # print('c_0', c_0.dtype)\n",
        "    # print('x', x.dtype)\n",
        "    x, (h_n, c_n) = self.lstm1(x, (h_0, c_0))\n",
        "    # print('x', x.shape)\n",
        "    # x, (h_n, c_n) = self.lstm2(x, (h_n, c_n))\n",
        "    # print('output', output)\n",
        "    # print('h_n', h_n.shape)\n",
        "    # print('c_n', c_n.shape)\n",
        "    out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=-1)\n",
        "    # print(out)\n",
        "    out = self.FC(out)\n",
        "    return out\n",
        "  \n",
        "  def init_hidden_state(self, batch_size):\n",
        "    h_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    c_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    return h_0, c_0"
      ],
      "metadata": {
        "id": "6NpK4iEgWehF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "      self.lstm_module = LSTM_module()\n",
        "      self.mlp_sec = nn.Sequential(\n",
        "          nn.Linear(41, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(428, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.4),\n",
        "          # nn.Linear(128, 64),\n",
        "          # nn.BatchNorm1d(64),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.Dropout(0.2),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1)\n",
        "      )\n",
        "      \n",
        "\n",
        "    def forward(self, x_d, x_mr, x_sec):\n",
        "      d_mr = self.lstm_module(x_mr)\n",
        "      d_sec = self.mlp_sec(x_sec)\n",
        "      # print('x_d.shape',x_d.shape)\n",
        "      # print('x_mr.shape',x_mr.shape)\n",
        "      # print('d_d.shape',d_d.shape)\n",
        "      # print('d_mr.shape',d_mr.shape)\n",
        "      x = torch.cat((d_mr, d_sec, x_d), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.mlp(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "PdE1Wu_Bb0C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-960, 960])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=960)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=500)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(mirna.shape)\n",
        "  mirna = mirna.cuda()\n",
        "  disease = disease.to(torch.float32).cuda()\n",
        "  secondary_feature = pre_hair_x.cuda()\n",
        "  print(disease.shape)\n",
        "  print(secondary_feature.shape)\n",
        "  output = model_class(disease, mirna, secondary_feature)\n",
        "  print('output.shape', output.shape)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOSZroa6b3hf",
        "outputId": "284196d0-f696-45ce-91dd-ea8bed9afcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 30])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 41])\n",
            "output.shape torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "sGuGXVHYWhbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU_module(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GRU_module, self).__init__()\n",
        "    self.hidden_size = 64\n",
        "    self.embedding_dim = 30\n",
        "    self.num_layer = 2\n",
        "    self.bidirectional = True\n",
        "    self.bi_num = 2 if self.bidirectional else 1\n",
        "    self.dropout = 0.5\n",
        "    # input(seq_len, batch, input_size)\n",
        "    # h0(num_layers * num_directions, batch, hidden_size)\n",
        "    # c0(num_layers * num_directions, batch, hidden_size)\n",
        "    self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layer, bidirectional=True, dropout=self.dropout)\n",
        "    # self.lstm2 = nn.LSTM(128, 64, self.num_layer, bidirectional=True, dropout=self.dropout)\n",
        "    self.FC = nn.Linear(128, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_0, c_0 = self.init_hidden_state(x.size(0))\n",
        "    # x = np.transpose(x, (1,0,2))\n",
        "    x = torch.swapaxes(x, 0, 1)\n",
        "    x = x.to(torch.float32)\n",
        "    # print('h_0', h_0.dtype)\n",
        "    # print('c_0', c_0.dtype)\n",
        "    # print('x', x.dtype)\n",
        "    x, h_n = self.gru(x, h_0)\n",
        "    # print('x', x.shape)\n",
        "    # x, (h_n, c_n) = self.lstm2(x, (h_n, c_n))\n",
        "    # print('output', output)\n",
        "    # print('h_n', h_n.shape)\n",
        "    # print('c_n', c_n.shape)\n",
        "    out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=-1)\n",
        "    # print(out)\n",
        "    out = self.FC(out)\n",
        "    return out\n",
        "  \n",
        "  def init_hidden_state(self, batch_size):\n",
        "    h_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    c_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    return h_0, c_0"
      ],
      "metadata": {
        "id": "smaSyHlLWiV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MzrMG3gsb52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "      self.gru_module = GRU_module()\n",
        "      self.mlp_sec = nn.Sequential(\n",
        "          nn.Linear(41, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(428, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(128, 64),\n",
        "          # nn.BatchNorm1d(64),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1)\n",
        "      )\n",
        "      \n",
        "\n",
        "    def forward(self, x_d, x_mr, x_sec):\n",
        "      d_mr = self.gru_module(x_mr)\n",
        "      d_sec = self.mlp_sec(x_sec)\n",
        "      # print('x_d.shape',x_d.shape)\n",
        "      # print('x_mr.shape',x_mr.shape)\n",
        "      # print('d_d.shape',d_d.shape)\n",
        "      # print('d_mr.shape',d_mr.shape)\n",
        "      x = torch.cat((d_mr, d_sec, x_d), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.mlp(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "vcdHMse9qnog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-960, 960])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=960)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=500)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(mirna.shape)\n",
        "  mirna = mirna.cuda()\n",
        "  disease = disease.to(torch.float32).cuda()\n",
        "  secondary_feature = pre_hair_x.cuda()\n",
        "  print(disease.shape)\n",
        "  print(secondary_feature.shape)\n",
        "  output = model_class(disease, mirna, secondary_feature)\n",
        "  print('output.shape', output.shape)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bee29c-37ea-4709-a67b-e85916740524",
        "id": "ODgu6T3Pqnog"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 30])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 41])\n",
            "output.shape torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "QpKqfaz9W0zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xiu1St_GalHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_config():\n",
        "    parse = argparse.ArgumentParser(description='iDNA_ABT train model')\n",
        "\n",
        "    # preoject setting\n",
        "    parse.add_argument('-learn-name', type=str, default='iDNA_ABT_train', help='learn name')\n",
        "    parse.add_argument('-save-best', type=bool, default=False, help='if save parameters of the current best model ')\n",
        "    parse.add_argument('-threshold', type=float, default=0.80, help='save threshold')\n",
        "    \n",
        "    # model parameters\n",
        "    parse.add_argument('-vocab_size', type=int, default=8, help='dictionary size of letter to token')\n",
        "    parse.add_argument('-max-len', type=int, default=30, help='max length of input sequences')\n",
        "    parse.add_argument('-num-layer', type=int, default=3, help='number of encoder blocks')  # 3\n",
        "    parse.add_argument('-num-head', type=int, default=8, help='number of head in multi-head attention')  # 8\n",
        "    parse.add_argument('-dim-embedding', type=int, default=64, help='residue embedding dimension')  # 64\n",
        "    parse.add_argument('-dim-feedforward', type=int, default=64, help='hidden layer dimension in feedforward layer')\n",
        "    parse.add_argument('-dim-k', type=int, default=32, help='embedding dimension of vector k or q')\n",
        "    parse.add_argument('-dim-v', type=int, default=32, help='embedding dimension of vector v')\n",
        "    parse.add_argument('-num-embedding', type=int, default=2, help='number of sense in multi-sense')\n",
        "    parse.add_argument('-k-mer', type=int, default=3, help='number of k(-mer) in multi-sccaled')\n",
        "    parse.add_argument('-embed-atten-size', type=int, default=8, help='size of soft attention')\n",
        "\n",
        "    # training parameters\n",
        "    parse.add_argument('-lr', type=float, default=0.0005, help='learning rate')\n",
        "    parse.add_argument('-reg', type=float, default=0.0025, help='weight lambda of regularization')\n",
        "    parse.add_argument('-batch-size', type=int, default=64, help='number of samples in a batch')\n",
        "    parse.add_argument('-epoch', type=int, default=100, help='number of iteration')  # 30\n",
        "    parse.add_argument('-k-fold', type=int, default=-1, help='k in cross validation,-1 represents train-test approach')\n",
        "    parse.add_argument('-num-class', type=int, default=3, help='number of classes')\n",
        "    parse.add_argument('-b', type=int, default=0.06, help='b')\n",
        "    # parse.add_argument('-cuda', type=bool, default=True, help='if use cuda')\n",
        "    parse.add_argument('-cuda', type=bool, default=False, help='if not use cuda')\n",
        "    parse.add_argument('-device', type=int, default=0, help='device id')\n",
        "    parse.add_argument('-interval-log', type=int, default=10,\n",
        "                       help='how many batches have gone through to record the training performance')\n",
        "    parse.add_argument('-interval-valid', type=int, default=5,\n",
        "                       help='how many epoches have gone through to record the validation performance')  # 20\n",
        "    parse.add_argument('-interval-test', type=int, default=1,\n",
        "                       help='how many epoches have gone through to record the test performance')\n",
        "    parse.add_argument('-alpha', type=float, default=0.1, help='information entropy')\n",
        "\n",
        "    config = parse.parse_args(args=[])\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "3m6A6N7QKhFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_freeze_by_names(model, layer_names, freeze=True):\n",
        "    if not isinstance(layer_names, Iterable):\n",
        "        layer_names = [layer_names]\n",
        "    for name, child in model.named_children():\n",
        "        if name not in layer_names:\n",
        "            continue\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "\n",
        "def freeze_by_names(model, layer_names):\n",
        "    set_freeze_by_names(model, layer_names, True)\n",
        "\n",
        "\n",
        "def unfreeze_by_names(model, layer_names):\n",
        "    set_freeze_by_names(model, layer_names, False)\n",
        "\n",
        "\n",
        "def set_freeze_by_idxs(model, idxs, freeze=True):\n",
        "    if not isinstance(idxs, Iterable):\n",
        "        idxs = [idxs]\n",
        "    num_child = len(list(model.children()))\n",
        "    idxs = tuple(map(lambda idx: num_child + idx if idx < 0 else idx, idxs))\n",
        "    for idx, child in enumerate(model.children()):\n",
        "        if idx not in idxs:\n",
        "            continue\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "\n",
        "def freeze_by_idxs(model, idxs):\n",
        "    set_freeze_by_idxs(model, idxs, True)\n",
        "\n",
        "\n",
        "def unfreeze_by_idxs(model, idxs):\n",
        "    set_freeze_by_idxs(model, idxs, False)"
      ],
      "metadata": {
        "id": "Z0dEiIvPKpkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_pad_mask(seq):\n",
        "    batch_size, seq_len = seq.size()\n",
        "    pad_attn_mask = seq.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
        "    pad_attn_mask_expand = pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
        "    return pad_attn_mask_expand\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding (look-up table) [64, 43, 64]\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)  # position embedding [64, 43, 64]\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)  # x: [batch_size, seq_len]\n",
        "        \n",
        "        pos = torch.arange(seq_len, device=device, dtype=torch.long)  # [seq_len]\n",
        "\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n",
        "        embedding = self.pos_embed(pos)\n",
        "        embedding = embedding + self.tok_embed(x)\n",
        "        embedding = self.norm(embedding)\n",
        "        return embedding\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)  # scores : [batch_size, n_head, seq_len, seq_len]\n",
        "        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)  # [batch_size, n_head, seq_len, seq_len]\n",
        "        context = torch.matmul(attn, V)  # [batch_size, n_head, seq_len, d_v]\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_head)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_head)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_head)\n",
        "\n",
        "        self.linear = nn.Linear(n_head * d_v, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_head, d_k).transpose(1, 2)  # q_s: [batch_size, n_head, seq_len, d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_head, d_k).transpose(1, 2)  # k_s: [batch_size, n_head, seq_len, d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_head, d_v).transpose(1, 2)  # v_s: [batch_size, n_head, seq_len, d_v]\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
        "        context, attention_map = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1,\n",
        "                                                            n_head * d_v)  # context: [batch_size, seq_len, n_head * d_v]\n",
        "        output = self.linear(context)\n",
        "        output = self.norm(output + residual)\n",
        "        return output, attention_map\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "        self.attention_map = None\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attention_map = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,\n",
        "                                                        enc_self_attn_mask)  # enc_inputs to same Q,K,V\n",
        "        self.attention_map = attention_map\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size, seq_len, d_model]\n",
        "        return enc_outputs\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        global max_len, n_layers, n_head, d_model, d_ff, d_k, d_v, vocab_size, device\n",
        "        max_len = config.max_len\n",
        "        n_layers = config.num_layer\n",
        "        n_head = config.num_head\n",
        "        d_model = config.dim_embedding\n",
        "        d_ff = config.dim_feedforward\n",
        "        d_k = config.dim_k\n",
        "        d_v = config.dim_v\n",
        "        vocab_size = config.vocab_size\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "        self.embedding = Embedding(config)\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc_task = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, 2),\n",
        "        )\n",
        "        self.classifier = nn.Linear(2, 2)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        output = self.embedding(input_ids)  # [bach_size, seq_len, d_model]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids)  # [batch_size, maxlen, maxlen]\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_self_attn_mask)\n",
        "            # output: [batch_size, max_len, d_model]\n",
        "\n",
        "        # classification\n",
        "        # only use [CLS]\n",
        "        representation = output[:, 0, :]\n",
        "        # print(representation.shape)\n",
        "        reduction_feature = self.fc_task(representation)\n",
        "        reduction_feature = reduction_feature.view(reduction_feature.size(0), -1)\n",
        "        logits_clsf = self.classifier(reduction_feature)\n",
        "        # representation = reduction_feature\n",
        "        return logits_clsf, representation"
      ],
      "metadata": {
        "id": "KqqrwIWEKq4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dy6Jl9DctCwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, config_mrna, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "      self.bert_module = BERT(config_mrna)\n",
        "      self.mlp_sec = nn.Sequential(\n",
        "          nn.Linear(41, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(428, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(128, 64),\n",
        "          # nn.BatchNorm1d(64),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1)\n",
        "      )\n",
        "      \n",
        "\n",
        "    def forward(self, x_d, x_mr, x_sec):\n",
        "      x_mr = x_mr.squeeze()\n",
        "      _, d_mr = self.bert_module(x_mr)\n",
        "      d_sec = self.mlp_sec(x_sec)\n",
        "      # print('x_d.shape',x_d.shape)\n",
        "      # print('x_mr.shape',x_mr.shape)\n",
        "      # print('d_d.shape',d_d.shape)\n",
        "      # print('d_mr.shape',d_mr.shape)\n",
        "      x = torch.cat((d_mr, d_sec, x_d), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.mlp(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "cWkL6hVntC8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_mrna = get_train_config()\n",
        "model_class = Classificaion_module(config_mrna).cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-960, 960])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=960)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=500)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(mirna.shape)\n",
        "  mirna = mirna.cuda()\n",
        "  disease = disease.to(torch.float32).cuda()\n",
        "  secondary_feature = pre_hair_x.cuda()\n",
        "  print(disease.shape)\n",
        "  print(secondary_feature.shape)\n",
        "  output = model_class(disease, mirna, secondary_feature)\n",
        "  print('output.shape', output.shape)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690f1284-4220-47b8-dee0-30d2d716d502",
        "id": "KX2ttpketC8G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 30])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 41])\n",
            "output.shape torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv1d"
      ],
      "metadata": {
        "id": "IfAsbFyJWjmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1d_module(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv1d_module, self).__init__()\n",
        "        self.conv1d_1 = nn.Conv1d(1, 1, 3, stride=1, padding=1)\n",
        "        self.batchnorm_1 = nn.BatchNorm1d(1)\n",
        "        self.conv1d_2 = nn.Conv1d(1, 2, 3, stride=1, padding=1)\n",
        "        self.batchnorm_2 = nn.BatchNorm1d(2)\n",
        "        self.conv1d_3 = nn.Conv1d(2, 4, 3, stride=1, padding=1)\n",
        "        self.batchnorm_3 = nn.BatchNorm1d(4)\n",
        "        self.linear_1 = nn.Linear(120, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # self.linear_2 = nn.Linear(320, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1d_1(x)\n",
        "        x = self.batchnorm_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1d_2(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1d_3(x)\n",
        "        x = self.batchnorm_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        # print(x.shape)\n",
        "        out = self.linear_1(x)\n",
        "        # out = self.linear_2(x)\n",
        "\n",
        "        \n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "BHnk04JeWk72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "      self.conv1d_module = Conv1d_module()\n",
        "      self.mlp_sec = nn.Sequential(\n",
        "          nn.Linear(41, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(428, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(128, 64),\n",
        "          # nn.BatchNorm1d(64),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1)\n",
        "      )\n",
        "      \n",
        "\n",
        "    def forward(self, x_d, x_mr, x_sec):\n",
        "\n",
        "      d_mr = self.conv1d_module(x_mr)\n",
        "      d_sec = self.mlp_sec(x_sec)\n",
        "      # print('x_d.shape',x_d.shape)\n",
        "      # print('x_mr.shape',x_mr.shape)\n",
        "      # print('d_d.shape',d_d.shape)\n",
        "      # print('d_mr.shape',d_mr.shape)\n",
        "      x = torch.cat((d_mr, d_sec, x_d), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.mlp(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "tjBCGOHvFF4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x, is_conv1d=True)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-960, 960])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=960)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=500)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(mirna.shape)\n",
        "  \n",
        "  mirna = mirna.cuda()\n",
        "\n",
        "  disease = disease.to(torch.float32).cuda()\n",
        "  secondary_feature = pre_hair_x.cuda()\n",
        "  print(disease.shape)\n",
        "  print(secondary_feature.shape)\n",
        "  output = model_class(disease, mirna, secondary_feature)\n",
        "  print('output.shape', output.shape)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4ab2fc-f103-4209-c19e-e2188e4e21ba",
        "id": "VV0RTvlBFF4G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 30])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 41])\n",
            "output.shape torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "BlSsmuWSWlzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.nn.modules.batchnorm import BatchNorm1d\n",
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "\n",
        "      # self.conv1d_module = Conv1d_module()\n",
        "      self.linear_1 = nn.Linear(71, 128)\n",
        "      self.bn_1 = nn.BatchNorm1d(128)\n",
        "      self.linear_2 = nn.Linear(128, 256)\n",
        "      self.bn_2 = nn.BatchNorm1d(256)\n",
        "      self.linear_3 = nn.Linear(256, 128)\n",
        "      self.bn_3 = nn.BatchNorm1d(128)\n",
        "      self.linear_4 = nn.Linear(128, 64)\n",
        "      self.bn_4 = nn.BatchNorm1d(64)\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "      self.RELU = nn.ReLU()\n",
        "      self.FC = nn.Sequential(\n",
        "          nn.Linear(364, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1),\n",
        "\n",
        "      )\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    def forward(self, x_d, pre_hair_x):\n",
        "      # print('x_d type',type(x_d))\n",
        "      # print('x_mr type',type(x_mr))\n",
        "      # d_mr = self.conv1d_module(data.mirna)\n",
        "      # print('data.pre_hair_x shape ', data.pre_hair_x.shape)\n",
        "      d_mr = self.linear_1(pre_hair_x)\n",
        "      d_mr = self.bn_1(d_mr)\n",
        "      d_mr = self.RELU(d_mr)\n",
        "      # d_mr = self.dropout(d_mr)\n",
        "      d_mr = self.linear_2(d_mr)\n",
        "      d_mr = self.bn_2(d_mr)\n",
        "      d_mr = self.RELU(d_mr)\n",
        "      d_mr = self.dropout(d_mr)\n",
        "      d_mr = self.linear_3(d_mr)\n",
        "      d_mr = self.bn_3(d_mr)\n",
        "      d_mr = self.RELU(d_mr)\n",
        "      # d_mr = self.dropout(d_mr)\n",
        "      d_mr = self.linear_4(d_mr)\n",
        "      d_mr = self.bn_4(d_mr)\n",
        "      d_mr = self.RELU(d_mr)\n",
        "\n",
        "      x = torch.cat((x_d, d_mr), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.FC(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "aA238m-qWvDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x, is_mlp=True)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-960, 960])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=960)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=500)\n",
        "\n",
        "for idx, (disease, mirna, label, pre_hair_x, orig_disease, orig_mirna) in enumerate(dataloader):\n",
        "  print(mirna.shape)\n",
        "  \n",
        "  mirna = mirna.cuda()\n",
        "\n",
        "  disease = disease.to(torch.float32).cuda()\n",
        "  secondary_feature = pre_hair_x.cuda()\n",
        "  print(disease.shape)\n",
        "  print(secondary_feature.shape)\n",
        "  output = model_class(disease, secondary_feature)\n",
        "  print('output.shape', output.shape)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-zfxvChJiCX",
        "outputId": "e123d80b-9dbe-4a39-dede-21eac4b32d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 30])\n",
            "torch.Size([32, 300])\n",
            "torch.Size([32, 71])\n",
            "output.shape torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "psnpVCMUamw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "3HAX9whXaock"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "11Wo51nTRYRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxQB2NBhRjoz",
        "outputId": "34fadd52-aec6-4a41-9b5a-0e5315b63e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.0008, weight_decay=0.0001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "7aUuc9cpRjmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuvrhN0ORk_U",
        "outputId": "86781b8e-4dfe-4db4-e594-18cddd9b7c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (lstm_module): LSTM_module(\n",
              "    (lstm1): LSTM(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=428, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (5): Dropout(p=0.2, inplace=False)\n",
              "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(test_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "c6ODKFG2Rmdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(val_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    label = label.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "L5GE-J46RtE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ajk4WmtRujT",
        "outputId": "cfa15bcd-35a0-4d89-a760-1555b1a022dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTraining Epoch[0] Loss:0.4059975612908602 | L1 Loss:0.4932502768933773 | R2:-0.14056239216215197 | ACC: 58.8000%(294/500)\n",
            "Testing Epoch[0] Loss:0.4300575256347656 | L1 Loss:0.5099345415830612 | R2:-0.1351200990163987 | ACC: 60.0000%(180/300)\n",
            "Training Epoch[1] Loss:0.38072561752051115 | L1 Loss:0.47631519474089146 | R2:-0.05111064050740481 | ACC: 61.6000%(308/500)\n",
            "Testing Epoch[1] Loss:0.4047554641962051 | L1 Loss:0.4902379661798477 | R2:-0.07592985481530609 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[2] Loss:0.3634476652368903 | L1 Loss:0.4574627336114645 | R2:-0.009777736680132106 | ACC: 62.4000%(312/500)\n",
            "Testing Epoch[2] Loss:0.37098280489444735 | L1 Loss:0.4677279472351074 | R2:0.01763742902653128 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[3] Loss:0.3339084777981043 | L1 Loss:0.43487480469048023 | R2:0.07926221890893953 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[3] Loss:0.3681647926568985 | L1 Loss:0.45198107063770293 | R2:0.023688621919907716 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[4] Loss:0.3443381516262889 | L1 Loss:0.44635735638439655 | R2:0.03558149175798286 | ACC: 64.2000%(321/500)\n",
            "Testing Epoch[4] Loss:0.3796837717294693 | L1 Loss:0.4614940881729126 | R2:-0.0019057181573451487 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[5] Loss:0.34566174261271954 | L1 Loss:0.44601002521812916 | R2:0.035529251533319005 | ACC: 62.6000%(313/500)\n",
            "Testing Epoch[5] Loss:0.4033647209405899 | L1 Loss:0.47688140273094176 | R2:-0.06581723887135363 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[6] Loss:0.3229166129603982 | L1 Loss:0.43997847475111485 | R2:0.09594721387972399 | ACC: 61.8000%(309/500)\n",
            "Testing Epoch[6] Loss:0.36664692163467405 | L1 Loss:0.4535883992910385 | R2:0.021131651304841938 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[7] Loss:0.325485285371542 | L1 Loss:0.4347684420645237 | R2:0.08576006420898591 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[7] Loss:0.37343535423278806 | L1 Loss:0.46645993292331694 | R2:0.013696590521951667 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[8] Loss:0.32745037507265806 | L1 Loss:0.4407219663262367 | R2:0.08326719435349086 | ACC: 62.4000%(312/500)\n",
            "Testing Epoch[8] Loss:0.3684997171163559 | L1 Loss:0.4577555149793625 | R2:0.02215236264204764 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[9] Loss:0.33259444311261177 | L1 Loss:0.44362205266952515 | R2:0.06329331185793213 | ACC: 62.8000%(314/500)\n",
            "Testing Epoch[9] Loss:0.3692652255296707 | L1 Loss:0.4508565992116928 | R2:0.026088585255810127 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[10] Loss:0.30265298392623663 | L1 Loss:0.4291117787361145 | R2:0.13890658692843827 | ACC: 62.0000%(310/500)\n",
            "Testing Epoch[10] Loss:0.3555886596441269 | L1 Loss:0.4450660586357117 | R2:0.05218169168226176 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[11] Loss:0.3310091318562627 | L1 Loss:0.4404414873570204 | R2:0.06067586156066981 | ACC: 62.8000%(314/500)\n",
            "Testing Epoch[11] Loss:0.3550202339887619 | L1 Loss:0.449628621339798 | R2:0.05206342726604031 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[12] Loss:0.300710404291749 | L1 Loss:0.41927962377667427 | R2:0.16152618260975704 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[12] Loss:0.3448892027139664 | L1 Loss:0.44253991544246674 | R2:0.0782548720185164 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[13] Loss:0.3031660933047533 | L1 Loss:0.4204963259398937 | R2:0.14241685988304473 | ACC: 63.8000%(319/500)\n",
            "Testing Epoch[13] Loss:0.3603153318166733 | L1 Loss:0.4514799028635025 | R2:0.04160966806757449 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[14] Loss:0.268484752625227 | L1 Loss:0.40177470073103905 | R2:0.25071805073527986 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[14] Loss:0.34510436058044436 | L1 Loss:0.451468625664711 | R2:0.08177414945212033 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[15] Loss:0.29398117307573557 | L1 Loss:0.42131192795932293 | R2:0.1687274375011366 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[15] Loss:0.35575189590454104 | L1 Loss:0.44567885994911194 | R2:0.05280228573156002 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[16] Loss:0.2970715034753084 | L1 Loss:0.42246420681476593 | R2:0.15563013371235113 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[16] Loss:0.36134570389986037 | L1 Loss:0.45829941928386686 | R2:0.03189532357187552 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[17] Loss:0.29705514572560787 | L1 Loss:0.4148907046765089 | R2:0.16549089910418036 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[17] Loss:0.3448440879583359 | L1 Loss:0.4346072405576706 | R2:0.0830014549414527 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[18] Loss:0.28519073873758316 | L1 Loss:0.41949096880853176 | R2:0.190122799538531 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[18] Loss:0.3300489395856857 | L1 Loss:0.425801146030426 | R2:0.12465098475918594 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[19] Loss:0.27717222180217505 | L1 Loss:0.3984427638351917 | R2:0.22820005175362035 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[19] Loss:0.3658821791410446 | L1 Loss:0.44894611835479736 | R2:0.020853595427404982 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[20] Loss:0.26576471980661154 | L1 Loss:0.39923030883073807 | R2:0.24771776861518136 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[20] Loss:0.3402045652270317 | L1 Loss:0.4359757602214813 | R2:0.09609930160612996 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[21] Loss:0.2783219534903765 | L1 Loss:0.4137058686465025 | R2:0.19732852071108603 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[21] Loss:0.3280925303697586 | L1 Loss:0.42734343111515044 | R2:0.1326670361323481 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[22] Loss:0.2758140005171299 | L1 Loss:0.4068526476621628 | R2:0.2203438889438477 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[22] Loss:0.327500993013382 | L1 Loss:0.42868658900260925 | R2:0.12904011755581007 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[23] Loss:0.26021711993962526 | L1 Loss:0.3924228921532631 | R2:0.2747161440028644 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[23] Loss:0.34180429875850676 | L1 Loss:0.4385268747806549 | R2:0.09505526264878364 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[24] Loss:0.2708880780264735 | L1 Loss:0.40188803896307945 | R2:0.23383051376342448 | ACC: 66.0000%(330/500)\n",
            "Testing Epoch[24] Loss:0.3593647927045822 | L1 Loss:0.44806287586689 | R2:0.047122797757915294 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[25] Loss:0.24982153996825218 | L1 Loss:0.3909111153334379 | R2:0.29582132603590894 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[25] Loss:0.36384337246417997 | L1 Loss:0.4581537008285522 | R2:0.02721299617025825 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[26] Loss:0.27313914336264133 | L1 Loss:0.4056662954390049 | R2:0.2135499161949449 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[26] Loss:0.3603686049580574 | L1 Loss:0.459106570482254 | R2:0.037033485039243884 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[27] Loss:0.2575833648443222 | L1 Loss:0.39825690910220146 | R2:0.272323872332575 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[27] Loss:0.33603211045265197 | L1 Loss:0.42867976427078247 | R2:0.11515401477091154 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[28] Loss:0.23610803671181202 | L1 Loss:0.3809557594358921 | R2:0.33359040797236644 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[28] Loss:0.3208184763789177 | L1 Loss:0.4266060501337051 | R2:0.15162950820447904 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[29] Loss:0.25941187236458063 | L1 Loss:0.39496135897934437 | R2:0.2559213994005093 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[29] Loss:0.34339759349822996 | L1 Loss:0.44255333840847016 | R2:0.08145146551889446 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[30] Loss:0.24877841025590897 | L1 Loss:0.3774791434407234 | R2:0.2977453299217639 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[30] Loss:0.33367673456668856 | L1 Loss:0.43174209594726565 | R2:0.11114364043570202 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[31] Loss:0.2603446515277028 | L1 Loss:0.39700067415833473 | R2:0.2636656160029234 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[31] Loss:0.3487647548317909 | L1 Loss:0.439601331949234 | R2:0.06635633810140533 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[32] Loss:0.24840123485773802 | L1 Loss:0.3877608831971884 | R2:0.29443741564065795 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[32] Loss:0.35924151837825774 | L1 Loss:0.44818117916584016 | R2:0.04773486353152434 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[33] Loss:0.252151170745492 | L1 Loss:0.38444603979587555 | R2:0.28712330624729293 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[33] Loss:0.32947388887405393 | L1 Loss:0.44130644500255584 | R2:0.12690319834723782 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[34] Loss:0.27268030028790236 | L1 Loss:0.40277706272900105 | R2:0.2205575257893392 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[34] Loss:0.32673153281211853 | L1 Loss:0.4307744711637497 | R2:0.13101014314081388 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[35] Loss:0.2502570580691099 | L1 Loss:0.3847948554903269 | R2:0.2938192386014902 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[35] Loss:0.3368510067462921 | L1 Loss:0.43491890728473664 | R2:0.10002316909979239 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[36] Loss:0.22369305789470673 | L1 Loss:0.364759124815464 | R2:0.36649285663984515 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[36] Loss:0.3391979217529297 | L1 Loss:0.4401248037815094 | R2:0.08742582304014956 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[37] Loss:0.2456599362194538 | L1 Loss:0.3800592552870512 | R2:0.2936406825289014 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[37] Loss:0.3377553358674049 | L1 Loss:0.43181344866752625 | R2:0.10061350068292704 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[38] Loss:0.23930861428380013 | L1 Loss:0.3744795210659504 | R2:0.32056346102205574 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[38] Loss:0.3156759053468704 | L1 Loss:0.43276804983615874 | R2:0.16198185330407378 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[39] Loss:0.25323573406785727 | L1 Loss:0.3779956791549921 | R2:0.27618294846101493 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[39] Loss:0.3086457833647728 | L1 Loss:0.4165963798761368 | R2:0.17520643440972308 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[40] Loss:0.23659573309123516 | L1 Loss:0.3723609670996666 | R2:0.3262036792696617 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[40] Loss:0.3313003689050674 | L1 Loss:0.43211101591587064 | R2:0.11842161287881395 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[41] Loss:0.24889406189322472 | L1 Loss:0.3817894719541073 | R2:0.29724087827727685 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[41] Loss:0.3403655901551247 | L1 Loss:0.43006307184696196 | R2:0.09519297305324559 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[42] Loss:0.24330531805753708 | L1 Loss:0.38149419240653515 | R2:0.29519987382353674 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[42] Loss:0.31912503093481065 | L1 Loss:0.42082800567150114 | R2:0.15155723282160796 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[43] Loss:0.24692911095917225 | L1 Loss:0.3825958836823702 | R2:0.3037678054075189 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[43] Loss:0.35223257541656494 | L1 Loss:0.44732686281204226 | R2:0.0509048695774645 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[44] Loss:0.24123864201828837 | L1 Loss:0.37289595790207386 | R2:0.31658781117175505 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[44] Loss:0.3107426643371582 | L1 Loss:0.42848922312259674 | R2:0.16836182366093427 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[45] Loss:0.24461880326271057 | L1 Loss:0.376937847584486 | R2:0.29996092867516466 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[45] Loss:0.33029305189847946 | L1 Loss:0.43178932964801786 | R2:0.1250318744771074 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[46] Loss:0.25279022101312876 | L1 Loss:0.37719156593084335 | R2:0.2846213979398727 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[46] Loss:0.32390997409820554 | L1 Loss:0.414730840921402 | R2:0.13274496861268564 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[47] Loss:0.23510658834129572 | L1 Loss:0.3731120638549328 | R2:0.31995247730813225 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[47] Loss:0.3364688754081726 | L1 Loss:0.4344599902629852 | R2:0.1065252175510187 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[48] Loss:0.2540075806900859 | L1 Loss:0.3900495432317257 | R2:0.2821488075947111 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[48] Loss:0.34150108098983767 | L1 Loss:0.43688057363033295 | R2:0.08229201101949228 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[49] Loss:0.24874413385987282 | L1 Loss:0.3865306358784437 | R2:0.2756608531673418 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[49] Loss:0.3064515575766563 | L1 Loss:0.4084921330213547 | R2:0.18446673633718438 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[50] Loss:0.2242993824183941 | L1 Loss:0.36878762394189835 | R2:0.35397204765501755 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[50] Loss:0.28349236100912095 | L1 Loss:0.40914363265037534 | R2:0.24212920629375265 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[51] Loss:0.25163643527776003 | L1 Loss:0.3864772003144026 | R2:0.2819130840821573 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[51] Loss:0.31160361468791964 | L1 Loss:0.412429079413414 | R2:0.18156355529263418 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[52] Loss:0.21810409519821405 | L1 Loss:0.357117623090744 | R2:0.3651251047807127 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[52] Loss:0.33065387457609174 | L1 Loss:0.4423991858959198 | R2:0.11999679569668284 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[53] Loss:0.23678318690508604 | L1 Loss:0.3741974364966154 | R2:0.32619187224523183 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[53] Loss:0.32113667875528334 | L1 Loss:0.4291865974664688 | R2:0.14482494475087968 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[54] Loss:0.22697741258889437 | L1 Loss:0.3707887642085552 | R2:0.34720466382442483 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[54] Loss:0.32269904017448425 | L1 Loss:0.4270941436290741 | R2:0.14795353420141727 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[55] Loss:0.2595174703747034 | L1 Loss:0.38966674730181694 | R2:0.24476096009557916 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[55] Loss:0.32029067128896715 | L1 Loss:0.42533980011940004 | R2:0.14599282072304587 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[56] Loss:0.2330780839547515 | L1 Loss:0.3694190848618746 | R2:0.3265092596160929 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[56] Loss:0.2969326063990593 | L1 Loss:0.43069884479045867 | R2:0.21313630528083194 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[57] Loss:0.23249183874577284 | L1 Loss:0.375356275588274 | R2:0.3383168254390774 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[57] Loss:0.3291507437825203 | L1 Loss:0.42918782234191893 | R2:0.11904885514462471 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[58] Loss:0.22687529399991035 | L1 Loss:0.3624349981546402 | R2:0.35578679674567043 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[58] Loss:0.29675387144088744 | L1 Loss:0.4200137287378311 | R2:0.21308156329650582 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[59] Loss:0.21949901711195707 | L1 Loss:0.3565691690891981 | R2:0.3733446078083971 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[59] Loss:0.29131121635437013 | L1 Loss:0.4151550382375717 | R2:0.22511463416521765 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[60] Loss:0.2254173019900918 | L1 Loss:0.36364073120057583 | R2:0.35375808829785044 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[60] Loss:0.3258464902639389 | L1 Loss:0.4333299368619919 | R2:0.1299250879129272 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[61] Loss:0.21709465887397528 | L1 Loss:0.35566182993352413 | R2:0.38911139750766505 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[61] Loss:0.2977720141410828 | L1 Loss:0.41855654418468474 | R2:0.20568180354817228 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[62] Loss:0.22625955659896135 | L1 Loss:0.3654520697891712 | R2:0.3472582153389704 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[62] Loss:0.2963008970022202 | L1 Loss:0.401769357919693 | R2:0.21124733131370696 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[63] Loss:0.21579154254868627 | L1 Loss:0.3602968826889992 | R2:0.3879490246385574 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[63] Loss:0.3355241134762764 | L1 Loss:0.4411776900291443 | R2:0.10492835329669052 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[64] Loss:0.21744744572788477 | L1 Loss:0.36134834587574005 | R2:0.3772618028750213 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[64] Loss:0.3063751131296158 | L1 Loss:0.4049886494874954 | R2:0.17808422872733454 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[65] Loss:0.2284719068557024 | L1 Loss:0.36856462992727757 | R2:0.3408779054337554 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[65] Loss:0.3254599541425705 | L1 Loss:0.428666490316391 | R2:0.12769960982702733 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[66] Loss:0.22599297668784857 | L1 Loss:0.36859107576310635 | R2:0.34763227982616546 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[66] Loss:0.3047894239425659 | L1 Loss:0.41448339223861697 | R2:0.18322431049498972 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[67] Loss:0.23668469116091728 | L1 Loss:0.37805626541376114 | R2:0.3223452814525951 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[67] Loss:0.30614001005887986 | L1 Loss:0.41077550053596495 | R2:0.18342312493670043 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[68] Loss:0.23113330453634262 | L1 Loss:0.37026924453675747 | R2:0.343215530263118 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[68] Loss:0.33145584762096403 | L1 Loss:0.4278351664543152 | R2:0.11548896278375771 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[69] Loss:0.21702389884740114 | L1 Loss:0.36262637190520763 | R2:0.37109145969084334 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[69] Loss:0.2968274548649788 | L1 Loss:0.41341268718242646 | R2:0.21285595508814711 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[70] Loss:0.21974847838282585 | L1 Loss:0.35863628797233105 | R2:0.36449824407035625 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[70] Loss:0.3103619977831841 | L1 Loss:0.4224498778581619 | R2:0.17516923050873207 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[71] Loss:0.2215609773993492 | L1 Loss:0.36068936064839363 | R2:0.367873380441181 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[71] Loss:0.2973769798874855 | L1 Loss:0.40995222330093384 | R2:0.20244955289091404 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[72] Loss:0.2078335816040635 | L1 Loss:0.35293263755738735 | R2:0.3924339335194037 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[72] Loss:0.30338034182786944 | L1 Loss:0.4161658674478531 | R2:0.19795372174089804 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[73] Loss:0.23539335187524557 | L1 Loss:0.36699594371020794 | R2:0.32227337059752864 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[73] Loss:0.31458799839019774 | L1 Loss:0.4260947495698929 | R2:0.16019577930019266 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[74] Loss:0.21956461481750011 | L1 Loss:0.35366210900247097 | R2:0.3681061790710254 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[74] Loss:0.27565682083368304 | L1 Loss:0.3977113366127014 | R2:0.2684957599343789 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[75] Loss:0.20967094600200653 | L1 Loss:0.3489072136580944 | R2:0.3872030667649167 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[75] Loss:0.29370965212583544 | L1 Loss:0.41791002452373505 | R2:0.21558408516204594 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[76] Loss:0.2373296618461609 | L1 Loss:0.37562582828104496 | R2:0.3233981984168583 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[76] Loss:0.27943452149629594 | L1 Loss:0.3929535001516342 | R2:0.25654465026584305 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[77] Loss:0.22975500486791134 | L1 Loss:0.3711363226175308 | R2:0.3526519227142374 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[77] Loss:0.2775424480438232 | L1 Loss:0.4036001205444336 | R2:0.2721852393201655 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[78] Loss:0.22562781255692244 | L1 Loss:0.37068815156817436 | R2:0.3301751505428161 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[78] Loss:0.3228987604379654 | L1 Loss:0.4232829362154007 | R2:0.13665500157317806 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[79] Loss:0.2094925194978714 | L1 Loss:0.356044365093112 | R2:0.3977637786635469 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[79] Loss:0.2823866784572601 | L1 Loss:0.398376590013504 | R2:0.24708299300318987 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[80] Loss:0.22102275397628546 | L1 Loss:0.3614155715331435 | R2:0.36541416857616815 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[80] Loss:0.31250868290662764 | L1 Loss:0.4198815584182739 | R2:0.16900726159163854 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[81] Loss:0.20664214249700308 | L1 Loss:0.3544252961874008 | R2:0.40619146177156207 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[81] Loss:0.31748650074005125 | L1 Loss:0.4172100991010666 | R2:0.14206349841455507 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[82] Loss:0.21812631888315082 | L1 Loss:0.3545575328171253 | R2:0.38510219614965435 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[82] Loss:0.2778463944792747 | L1 Loss:0.396217069029808 | R2:0.2681184836774085 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[83] Loss:0.2221277253702283 | L1 Loss:0.3649633564054966 | R2:0.36521927012851496 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[83] Loss:0.30460718721151353 | L1 Loss:0.4135701388120651 | R2:0.1903158004149566 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[84] Loss:0.21624505892395973 | L1 Loss:0.3629849571734667 | R2:0.38189360453801224 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[84] Loss:0.26701595783233645 | L1 Loss:0.3894104242324829 | R2:0.29394408269238176 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[85] Loss:0.2269945563748479 | L1 Loss:0.35808850917965174 | R2:0.3557712566128312 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[85] Loss:0.31151379346847535 | L1 Loss:0.4169438987970352 | R2:0.17727689574693326 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[86] Loss:0.21552795730531216 | L1 Loss:0.35490348376333714 | R2:0.37746239912148094 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[86] Loss:0.26853800415992735 | L1 Loss:0.38530808687210083 | R2:0.2829537878707783 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[87] Loss:0.22583218477666378 | L1 Loss:0.3690614067018032 | R2:0.3516799084246036 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[87] Loss:0.2993170961737633 | L1 Loss:0.4161086708307266 | R2:0.19815586107633792 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[88] Loss:0.2316099228337407 | L1 Loss:0.3673650063574314 | R2:0.3402645346428183 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[88] Loss:0.2976631224155426 | L1 Loss:0.40838549435138705 | R2:0.20218794787557953 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[89] Loss:0.22833278682082891 | L1 Loss:0.368194404989481 | R2:0.358808262874732 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[89] Loss:0.28574938923120496 | L1 Loss:0.40502783954143523 | R2:0.23982818198475497 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[90] Loss:0.2162611484527588 | L1 Loss:0.3579641040414572 | R2:0.3789005059924373 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[90] Loss:0.26194737702608106 | L1 Loss:0.3862973928451538 | R2:0.31191175394450293 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[91] Loss:0.2133930497802794 | L1 Loss:0.35796588100492954 | R2:0.38302862851339614 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[91] Loss:0.3227484583854675 | L1 Loss:0.43338045179843904 | R2:0.11769729653595389 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[92] Loss:0.21018441207706928 | L1 Loss:0.35773682966828346 | R2:0.39298866008735595 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[92] Loss:0.27916518449783323 | L1 Loss:0.4004025548696518 | R2:0.2519045309472048 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[93] Loss:0.23636102862656116 | L1 Loss:0.3678508624434471 | R2:0.3225098431697589 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[93] Loss:0.29406330436468125 | L1 Loss:0.41211006939411166 | R2:0.2112662023426485 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[94] Loss:0.19276521820575 | L1 Loss:0.3423087503761053 | R2:0.44208803211049097 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[94] Loss:0.31563928723335266 | L1 Loss:0.42676236033439635 | R2:0.15689027458172938 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[95] Loss:0.2145201377570629 | L1 Loss:0.36127973161637783 | R2:0.37684048746448406 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[95] Loss:0.2701934278011322 | L1 Loss:0.3916213259100914 | R2:0.28407250623131153 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[96] Loss:0.2320676427334547 | L1 Loss:0.3708480503410101 | R2:0.3331178179839148 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[96] Loss:0.269748555123806 | L1 Loss:0.39439123272895815 | R2:0.28600355748818956 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[97] Loss:0.22235062345862389 | L1 Loss:0.3587651401758194 | R2:0.3614509922888795 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[97] Loss:0.28011581748723985 | L1 Loss:0.4041264832019806 | R2:0.25650538437228276 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[98] Loss:0.21392096858471632 | L1 Loss:0.3588739912956953 | R2:0.37872793902799506 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[98] Loss:0.31381903737783434 | L1 Loss:0.41455394923686983 | R2:0.1604522147978273 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[99] Loss:0.21141809318214655 | L1 Loss:0.35172840394079685 | R2:0.39282144913318245 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[99] Loss:0.2943798840045929 | L1 Loss:0.4143892914056778 | R2:0.22018594811674955 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[100] Loss:0.20575260324403644 | L1 Loss:0.3428665120154619 | R2:0.4059852264269276 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[100] Loss:0.3055188685655594 | L1 Loss:0.3982974737882614 | R2:0.1971128169318075 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[101] Loss:0.2179329739883542 | L1 Loss:0.35529899969697 | R2:0.3662359152348858 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[101] Loss:0.3073874458670616 | L1 Loss:0.42664088010787965 | R2:0.18658382404601365 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[102] Loss:0.21700354386121035 | L1 Loss:0.3570729698985815 | R2:0.37724944336757404 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[102] Loss:0.31737361252307894 | L1 Loss:0.42254979014396665 | R2:0.1639929454496898 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[103] Loss:0.2156232576817274 | L1 Loss:0.3548517022281885 | R2:0.39304780643307735 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[103] Loss:0.299397249519825 | L1 Loss:0.4149791359901428 | R2:0.204222478044084 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[104] Loss:0.20669612661004066 | L1 Loss:0.3533390425145626 | R2:0.4176641914115389 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[104] Loss:0.26447166353464124 | L1 Loss:0.38917227685451505 | R2:0.3059131434004139 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[105] Loss:0.21443927381187677 | L1 Loss:0.3520021624863148 | R2:0.3865700740910155 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[105] Loss:0.28211640417575834 | L1 Loss:0.40576487183570864 | R2:0.25586375064339884 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[106] Loss:0.21377418749034405 | L1 Loss:0.3501628851518035 | R2:0.38662933337859456 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[106] Loss:0.2749870777130127 | L1 Loss:0.41570377349853516 | R2:0.272869460231768 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[107] Loss:0.19880677433684468 | L1 Loss:0.34968060441315174 | R2:0.42753541537845147 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[107] Loss:0.2632642835378647 | L1 Loss:0.4003479242324829 | R2:0.2919262493325711 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[108] Loss:0.23057714570313692 | L1 Loss:0.3749147988855839 | R2:0.33981757263637596 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[108] Loss:0.2894198313355446 | L1 Loss:0.40002153515815736 | R2:0.2259370352837799 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[109] Loss:0.2063506804406643 | L1 Loss:0.34356874600052834 | R2:0.40408278098328626 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[109] Loss:0.27923230528831483 | L1 Loss:0.4040486514568329 | R2:0.2592994683997397 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[110] Loss:0.21669625863432884 | L1 Loss:0.35743829794228077 | R2:0.3733487984914528 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[110] Loss:0.3143030345439911 | L1 Loss:0.4395455628633499 | R2:0.16529493706988269 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[111] Loss:0.23518484737724066 | L1 Loss:0.37254784628748894 | R2:0.3144121734652885 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[111] Loss:0.3010985553264618 | L1 Loss:0.4166377991437912 | R2:0.20164454920583547 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[112] Loss:0.21606254391372204 | L1 Loss:0.3493506610393524 | R2:0.36947365868660104 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[112] Loss:0.30733855068683624 | L1 Loss:0.412762513756752 | R2:0.18022300672720504 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[113] Loss:0.2207014299929142 | L1 Loss:0.36836570873856544 | R2:0.36169127389575206 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[113] Loss:0.30741700530052185 | L1 Loss:0.40658433735370636 | R2:0.1810471374595473 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[114] Loss:0.21832409221678972 | L1 Loss:0.35473497584462166 | R2:0.36836737236840084 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[114] Loss:0.2728292942047119 | L1 Loss:0.39038019478321073 | R2:0.26984775210668566 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[115] Loss:0.20866261050105095 | L1 Loss:0.34825434535741806 | R2:0.38907035742536045 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[115] Loss:0.2974648535251617 | L1 Loss:0.4192608237266541 | R2:0.21397168677851472 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[116] Loss:0.19380169361829758 | L1 Loss:0.334591512568295 | R2:0.44859209256529775 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[116] Loss:0.3226202830672264 | L1 Loss:0.42406322062015533 | R2:0.13282469161659138 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[117] Loss:0.22318989224731922 | L1 Loss:0.3588712755590677 | R2:0.3640979614131344 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[117] Loss:0.2810634672641754 | L1 Loss:0.40646237432956694 | R2:0.25145849241369855 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[118] Loss:0.2131733698770404 | L1 Loss:0.36184811033308506 | R2:0.3937033550810122 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[118] Loss:0.3100757122039795 | L1 Loss:0.42570342123508453 | R2:0.17059320747856588 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[119] Loss:0.2160725835710764 | L1 Loss:0.3519766815006733 | R2:0.3882653881580557 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[119] Loss:0.310806530714035 | L1 Loss:0.41478086113929746 | R2:0.16356186664104336 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[120] Loss:0.22698417492210865 | L1 Loss:0.3668884113430977 | R2:0.3434501887471889 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[120] Loss:0.3140451282262802 | L1 Loss:0.4125214904546738 | R2:0.1609914752436345 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[121] Loss:0.2055462533608079 | L1 Loss:0.3456684108823538 | R2:0.41806926762573143 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[121] Loss:0.27994768917560575 | L1 Loss:0.40373041927814485 | R2:0.26080982131761504 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[122] Loss:0.21389292925596237 | L1 Loss:0.35365778394043446 | R2:0.37082082186386506 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[122] Loss:0.26276770830154417 | L1 Loss:0.39431110620498655 | R2:0.3013430176917592 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[123] Loss:0.21476517990231514 | L1 Loss:0.3585652559995651 | R2:0.3807005447827614 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[123] Loss:0.303631928563118 | L1 Loss:0.41041146218776703 | R2:0.19977469956979643 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[124] Loss:0.21262142155319452 | L1 Loss:0.3549985084682703 | R2:0.37605462207702123 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[124] Loss:0.29049713015556333 | L1 Loss:0.4045511484146118 | R2:0.23112472922520197 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[125] Loss:0.21058800630271435 | L1 Loss:0.3412601426243782 | R2:0.3928385968843098 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[125] Loss:0.2812431111931801 | L1 Loss:0.3882261037826538 | R2:0.2534174663944171 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[126] Loss:0.20216836174950004 | L1 Loss:0.34181092493236065 | R2:0.41337414968803377 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[126] Loss:0.2788252457976341 | L1 Loss:0.39730171859264374 | R2:0.2708705793643979 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[127] Loss:0.2070960197597742 | L1 Loss:0.3471481930464506 | R2:0.3949462429692079 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[127] Loss:0.2910434380173683 | L1 Loss:0.4113678425550461 | R2:0.2202950199859739 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[128] Loss:0.20198059920221567 | L1 Loss:0.33903862349689007 | R2:0.4159823124220874 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[128] Loss:0.29254592061042783 | L1 Loss:0.40533445179462435 | R2:0.21338348634245205 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[129] Loss:0.22948304750025272 | L1 Loss:0.37252615205943584 | R2:0.3437808555345243 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[129] Loss:0.27712018638849256 | L1 Loss:0.4125867187976837 | R2:0.25960690774109835 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[130] Loss:0.2095466279424727 | L1 Loss:0.345742417499423 | R2:0.39838054499980097 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[130] Loss:0.2790900319814682 | L1 Loss:0.39946727454662323 | R2:0.24653709015439684 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[131] Loss:0.21732025314122438 | L1 Loss:0.3596300110220909 | R2:0.3783681497335827 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[131] Loss:0.28596502989530564 | L1 Loss:0.402115198969841 | R2:0.23883565064599627 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[132] Loss:0.22341439221054316 | L1 Loss:0.3606708273291588 | R2:0.35644230711385816 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[132] Loss:0.3320304945111275 | L1 Loss:0.4285361349582672 | R2:0.12236739985346885 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[133] Loss:0.20659935101866722 | L1 Loss:0.3501783199608326 | R2:0.412104151624546 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[133] Loss:0.29662981182336806 | L1 Loss:0.40296036601066587 | R2:0.21433730091442343 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[134] Loss:0.2040241309441626 | L1 Loss:0.3362992312759161 | R2:0.41686408071175146 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[134] Loss:0.2930758774280548 | L1 Loss:0.40983609557151796 | R2:0.23016800229643683 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[135] Loss:0.21812351094558835 | L1 Loss:0.35535488836467266 | R2:0.3795321969801748 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[135] Loss:0.27532654255628586 | L1 Loss:0.4085599660873413 | R2:0.2662583410645899 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[136] Loss:0.22927173972129822 | L1 Loss:0.36632293462753296 | R2:0.3397102379003708 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[136] Loss:0.31866748481988905 | L1 Loss:0.419799929857254 | R2:0.14141865078860805 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[137] Loss:0.2217155834659934 | L1 Loss:0.3537437692284584 | R2:0.3579417929167201 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[137] Loss:0.26867211908102034 | L1 Loss:0.3905430167913437 | R2:0.29655633211495097 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[138] Loss:0.2118840841576457 | L1 Loss:0.3452093433588743 | R2:0.3997095681211372 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[138] Loss:0.28063069730997087 | L1 Loss:0.39442754089832305 | R2:0.2531694362046902 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[139] Loss:0.23207822442054749 | L1 Loss:0.3604612909257412 | R2:0.3353254163918473 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[139] Loss:0.28963584154844285 | L1 Loss:0.4042969971895218 | R2:0.23358882450805427 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[140] Loss:0.19849011860787868 | L1 Loss:0.338260043412447 | R2:0.4299410414815616 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[140] Loss:0.3017499178647995 | L1 Loss:0.4056032806634903 | R2:0.19897548947271124 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[141] Loss:0.21906007546931505 | L1 Loss:0.36918584257364273 | R2:0.36395442897820796 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[141] Loss:0.2917880222201347 | L1 Loss:0.41209558248519895 | R2:0.2235223609155951 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[142] Loss:0.20448097307235003 | L1 Loss:0.3428960535675287 | R2:0.42598841133583343 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[142] Loss:0.29903433173894883 | L1 Loss:0.4083485573530197 | R2:0.19888445604825158 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[143] Loss:0.21633219486102462 | L1 Loss:0.3533787727355957 | R2:0.3754411353141544 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[143] Loss:0.2946310698986053 | L1 Loss:0.4145252227783203 | R2:0.21379580061569933 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[144] Loss:0.20416723750531673 | L1 Loss:0.33713740296661854 | R2:0.41346144631276094 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[144] Loss:0.23597124963998795 | L1 Loss:0.36098951995372774 | R2:0.36779682975069206 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[145] Loss:0.20232194103300571 | L1 Loss:0.34037399757653475 | R2:0.42583196825882497 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[145] Loss:0.31620400995016096 | L1 Loss:0.40904754400253296 | R2:0.162719010467161 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[146] Loss:0.20229456992819905 | L1 Loss:0.3425947427749634 | R2:0.42287452043398355 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[146] Loss:0.2907161772251129 | L1 Loss:0.4076652258634567 | R2:0.22190822881872174 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[147] Loss:0.2143072746694088 | L1 Loss:0.35491225495934486 | R2:0.3805507633028993 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[147] Loss:0.25816311985254287 | L1 Loss:0.37678736448287964 | R2:0.3193089474857553 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[148] Loss:0.2060064859688282 | L1 Loss:0.34510167222470045 | R2:0.41775392034747993 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[148] Loss:0.2715704932808876 | L1 Loss:0.39317994713783266 | R2:0.27803264173453257 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[149] Loss:0.20898858550935984 | L1 Loss:0.347718495875597 | R2:0.40105395833318974 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[149] Loss:0.26535687744617464 | L1 Loss:0.3798553764820099 | R2:0.29872339352450583 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[150] Loss:0.20210494473576546 | L1 Loss:0.342945771291852 | R2:0.41220817200981014 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[150] Loss:0.2616510346531868 | L1 Loss:0.3796157270669937 | R2:0.30253535110500646 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[151] Loss:0.1908559650182724 | L1 Loss:0.33901602402329445 | R2:0.4468931619986067 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[151] Loss:0.2818328753113747 | L1 Loss:0.3893329292535782 | R2:0.2508957304761883 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[152] Loss:0.1921094572171569 | L1 Loss:0.3373674638569355 | R2:0.43892628681592827 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[152] Loss:0.2976031482219696 | L1 Loss:0.41129967868328093 | R2:0.20921025959386458 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[153] Loss:0.20210787001997232 | L1 Loss:0.34633368998765945 | R2:0.4139843811575184 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[153] Loss:0.27671062350273135 | L1 Loss:0.39543938636779785 | R2:0.2724612640416331 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[154] Loss:0.20704766176640987 | L1 Loss:0.3456491222605109 | R2:0.38112678008787193 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[154] Loss:0.28753585964441297 | L1 Loss:0.4075946718454361 | R2:0.22763995340928028 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[155] Loss:0.20673107635229826 | L1 Loss:0.350495845079422 | R2:0.40190741822591725 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[155] Loss:0.2941679865121841 | L1 Loss:0.41027121245861053 | R2:0.20751184585552201 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[156] Loss:0.21807416435331106 | L1 Loss:0.35832214541733265 | R2:0.3723360197396952 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[156] Loss:0.2804853364825249 | L1 Loss:0.4057735025882721 | R2:0.2510618515390236 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[157] Loss:0.21091989101842046 | L1 Loss:0.3523668386042118 | R2:0.38680188852203845 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[157] Loss:0.26150350123643873 | L1 Loss:0.37793585658073425 | R2:0.30522043826784795 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[158] Loss:0.21109624858945608 | L1 Loss:0.35134813003242016 | R2:0.39202143085039726 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[158] Loss:0.2935656398534775 | L1 Loss:0.3966206818819046 | R2:0.21430347207689274 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[159] Loss:0.2156837061047554 | L1 Loss:0.357777182944119 | R2:0.3888815384338353 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[159] Loss:0.3107943758368492 | L1 Loss:0.42147264182567595 | R2:0.16617461722062027 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[160] Loss:0.20573169644922018 | L1 Loss:0.3414008114486933 | R2:0.40891636932714887 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[160] Loss:0.25836853235960006 | L1 Loss:0.38953793942928316 | R2:0.3105702019894431 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[161] Loss:0.2135203331708908 | L1 Loss:0.3504395969212055 | R2:0.38288426401816905 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[161] Loss:0.2706348389387131 | L1 Loss:0.38081667721271517 | R2:0.2866115648753213 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[162] Loss:0.2146433563902974 | L1 Loss:0.35536734014749527 | R2:0.38238257080239946 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[162] Loss:0.31654897183179853 | L1 Loss:0.41031392812728884 | R2:0.15566704201571674 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[163] Loss:0.21356585575267673 | L1 Loss:0.3472489211708307 | R2:0.37344897139439254 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[163] Loss:0.28671945035457613 | L1 Loss:0.4008166819810867 | R2:0.24362557916404787 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[164] Loss:0.21581394970417023 | L1 Loss:0.3593306876718998 | R2:0.37375192758708203 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[164] Loss:0.24721529185771943 | L1 Loss:0.3769132137298584 | R2:0.3467676288883289 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[165] Loss:0.20203167665749788 | L1 Loss:0.34017684683203697 | R2:0.41929097690762224 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[165] Loss:0.27371634244918824 | L1 Loss:0.3931012362241745 | R2:0.2677284580612448 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[166] Loss:0.2176510514691472 | L1 Loss:0.35434961318969727 | R2:0.35259354818458105 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[166] Loss:0.27883186340332033 | L1 Loss:0.38985084891319277 | R2:0.2541813639464213 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[167] Loss:0.21429808111861348 | L1 Loss:0.3623777702450752 | R2:0.3810276923283271 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[167] Loss:0.2982819601893425 | L1 Loss:0.39271525740623475 | R2:0.19402895187606076 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[168] Loss:0.20455577969551086 | L1 Loss:0.34984649904072285 | R2:0.4141102479527037 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[168] Loss:0.3006295025348663 | L1 Loss:0.3984848946332932 | R2:0.19065370524060635 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[169] Loss:0.21366713056340814 | L1 Loss:0.35136323794722557 | R2:0.3904890388192481 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[169] Loss:0.27737042903900144 | L1 Loss:0.393956658244133 | R2:0.2588800454247257 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[170] Loss:0.21187967713922262 | L1 Loss:0.3507829364389181 | R2:0.3942865623220279 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[170] Loss:0.25835743099451064 | L1 Loss:0.3741666853427887 | R2:0.3130590839389845 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[171] Loss:0.21455147955566645 | L1 Loss:0.3572890777140856 | R2:0.3764355612735434 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[171] Loss:0.28937337547540665 | L1 Loss:0.39853264689445494 | R2:0.22911144648234746 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[172] Loss:0.2267057034187019 | L1 Loss:0.36074173264205456 | R2:0.3501901076438279 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[172] Loss:0.27703467160463335 | L1 Loss:0.38813979029655454 | R2:0.25224968695271127 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[173] Loss:0.2040377203375101 | L1 Loss:0.3522730004042387 | R2:0.4042113676085233 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[173] Loss:0.2533184766769409 | L1 Loss:0.3821034044027328 | R2:0.31788379082928836 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[174] Loss:0.19529116991907358 | L1 Loss:0.3382738381624222 | R2:0.4313480571941231 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[174] Loss:0.27525571137666704 | L1 Loss:0.3910682052373886 | R2:0.26719412524315855 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[175] Loss:0.2023285860195756 | L1 Loss:0.33671396039426327 | R2:0.41943603070013874 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[175] Loss:0.27694147974252703 | L1 Loss:0.39230890572071075 | R2:0.2574886400864724 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[176] Loss:0.18529346678406 | L1 Loss:0.33464865013957024 | R2:0.46058984734043884 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[176] Loss:0.32172509133815763 | L1 Loss:0.4104627311229706 | R2:0.12754683955755902 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[177] Loss:0.2051203311420977 | L1 Loss:0.33792148903012276 | R2:0.4067715713824476 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[177] Loss:0.2952103137969971 | L1 Loss:0.40558384358882904 | R2:0.21291891413277614 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[178] Loss:0.19775689113885164 | L1 Loss:0.34227242693305016 | R2:0.4274113600123848 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[178] Loss:0.3011784091591835 | L1 Loss:0.4030732661485672 | R2:0.18507112633762007 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[179] Loss:0.2078427653759718 | L1 Loss:0.3476650696247816 | R2:0.3965847199530063 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[179] Loss:0.2900686919689178 | L1 Loss:0.40522177517414093 | R2:0.22526113147972016 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[180] Loss:0.20779727771878242 | L1 Loss:0.3537297211587429 | R2:0.39315332892084126 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[180] Loss:0.2688153266906738 | L1 Loss:0.3763188898563385 | R2:0.2779311780778947 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[181] Loss:0.2148558311164379 | L1 Loss:0.3547007981687784 | R2:0.38014700218887193 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[181] Loss:0.3127321988344193 | L1 Loss:0.4292960464954376 | R2:0.1445097804413571 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[182] Loss:0.21272466238588095 | L1 Loss:0.3548442367464304 | R2:0.38963332350328506 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[182] Loss:0.25987964421510695 | L1 Loss:0.3880111396312714 | R2:0.305780799525456 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[183] Loss:0.20563662983477116 | L1 Loss:0.3405346628278494 | R2:0.3997419278112544 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[183] Loss:0.2769004702568054 | L1 Loss:0.39272654950618746 | R2:0.2576641118513014 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[184] Loss:0.20258999057114124 | L1 Loss:0.34291765466332436 | R2:0.4192354206825817 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[184] Loss:0.30161258280277253 | L1 Loss:0.41725996434688567 | R2:0.20169365293585387 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[185] Loss:0.20197475235909224 | L1 Loss:0.3415311146527529 | R2:0.4237094064058432 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[185] Loss:0.26374704092741014 | L1 Loss:0.3941171854734421 | R2:0.2997690506775755 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[186] Loss:0.20042075077071786 | L1 Loss:0.33679643645882607 | R2:0.42338131423476244 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[186] Loss:0.2718774378299713 | L1 Loss:0.38098000884056094 | R2:0.2738193980252167 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[187] Loss:0.20961180236190557 | L1 Loss:0.3468658048659563 | R2:0.38058859001609235 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[187] Loss:0.2587343767285347 | L1 Loss:0.37686967849731445 | R2:0.3193879321706878 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[188] Loss:0.21330170473083854 | L1 Loss:0.34596714936196804 | R2:0.3879747909031364 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[188] Loss:0.27124674171209334 | L1 Loss:0.39252989292144774 | R2:0.2722563538888038 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[189] Loss:0.21295076701790094 | L1 Loss:0.3466258682310581 | R2:0.3779105575955204 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[189] Loss:0.26847093403339384 | L1 Loss:0.39192808270454405 | R2:0.285308672545619 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[190] Loss:0.2145371944643557 | L1 Loss:0.34966418519616127 | R2:0.38129557319618673 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[190] Loss:0.2956364810466766 | L1 Loss:0.41174752116203306 | R2:0.20285320910998594 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[191] Loss:0.21664062421768904 | L1 Loss:0.34702524170279503 | R2:0.3755161578308275 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[191] Loss:0.27019222229719164 | L1 Loss:0.3953933745622635 | R2:0.2784421369917566 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[192] Loss:0.2161907311528921 | L1 Loss:0.3579086363315582 | R2:0.37446967392861524 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[192] Loss:0.3223867163062096 | L1 Loss:0.4060391277074814 | R2:0.13432539114623143 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[193] Loss:0.23261196725070477 | L1 Loss:0.3640519678592682 | R2:0.3334183911841009 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[193] Loss:0.28500929921865464 | L1 Loss:0.39368414878845215 | R2:0.23992068402913755 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[194] Loss:0.194756125099957 | L1 Loss:0.3367462642490864 | R2:0.4312496136813058 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[194] Loss:0.31254290491342546 | L1 Loss:0.4157481074333191 | R2:0.17057270559288068 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[195] Loss:0.20959569606930017 | L1 Loss:0.3520367071032524 | R2:0.38279591254480394 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[195] Loss:0.2991253137588501 | L1 Loss:0.420073339343071 | R2:0.19712985683166623 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[196] Loss:0.2091653565876186 | L1 Loss:0.3438763339072466 | R2:0.4004695398332943 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[196] Loss:0.2736843079328537 | L1 Loss:0.39117967486381533 | R2:0.266248578429113 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[197] Loss:0.2162011032924056 | L1 Loss:0.35186092741787434 | R2:0.37347214939014606 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[197] Loss:0.26088011264801025 | L1 Loss:0.39144042432308196 | R2:0.3040522936017983 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[198] Loss:0.18872157484292984 | L1 Loss:0.32847551815211773 | R2:0.4541365738501964 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[198] Loss:0.2562600657343864 | L1 Loss:0.3838890612125397 | R2:0.31901113043447615 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[199] Loss:0.21766583900898695 | L1 Loss:0.3592611774802208 | R2:0.3716516549042579 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[199] Loss:0.2878235518932343 | L1 Loss:0.40413396060466766 | R2:0.23276017416193623 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[200] Loss:0.20585836097598076 | L1 Loss:0.3435928300023079 | R2:0.3974915755293964 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[200] Loss:0.24960755556821823 | L1 Loss:0.37782018482685087 | R2:0.32982936223532905 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[201] Loss:0.21355049312114716 | L1 Loss:0.3539111576974392 | R2:0.3830427336781044 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[201] Loss:0.31273424476385114 | L1 Loss:0.41427388191223147 | R2:0.16949018894658713 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[202] Loss:0.21316203102469444 | L1 Loss:0.3530631475150585 | R2:0.37415630668442834 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[202] Loss:0.26552731841802596 | L1 Loss:0.38213956654071807 | R2:0.27830061716868437 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[203] Loss:0.2090868349187076 | L1 Loss:0.3507990185171366 | R2:0.38740826094311526 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[203] Loss:0.28105855733156204 | L1 Loss:0.39646518528461455 | R2:0.24625004715734217 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[204] Loss:0.22267361730337143 | L1 Loss:0.3474534619599581 | R2:0.35277129560838383 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[204] Loss:0.24808110147714615 | L1 Loss:0.3733297765254974 | R2:0.3456084773140217 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[205] Loss:0.21805193647742271 | L1 Loss:0.3516363352537155 | R2:0.3706398226129971 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[205] Loss:0.2806501999497414 | L1 Loss:0.39594976902008056 | R2:0.2601758464260744 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[206] Loss:0.20845768554136157 | L1 Loss:0.3495145160704851 | R2:0.40019053007006156 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[206] Loss:0.2712446734309196 | L1 Loss:0.39186373054981233 | R2:0.27266742330630295 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[207] Loss:0.21003002300858498 | L1 Loss:0.3424185737967491 | R2:0.3946075956504954 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[207] Loss:0.3279801234602928 | L1 Loss:0.4208832740783691 | R2:0.12157336274569645 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[208] Loss:0.21223509777337313 | L1 Loss:0.34724826738238335 | R2:0.37746635693333264 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[208] Loss:0.2915737673640251 | L1 Loss:0.40551879107952116 | R2:0.22331165701625033 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[209] Loss:0.20685708802193403 | L1 Loss:0.3437079209834337 | R2:0.403087411440604 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[209] Loss:0.27423946782946584 | L1 Loss:0.3882256805896759 | R2:0.27849333330563697 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[210] Loss:0.21807808708399534 | L1 Loss:0.3554403893649578 | R2:0.3678683371335753 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[210] Loss:0.29900428652763367 | L1 Loss:0.41191646456718445 | R2:0.20802167169524308 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[211] Loss:0.20569290965795517 | L1 Loss:0.3461141362786293 | R2:0.4157436676484444 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[211] Loss:0.2841377675533295 | L1 Loss:0.39857444167137146 | R2:0.24422437483277243 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[212] Loss:0.2110009342432022 | L1 Loss:0.3486419841647148 | R2:0.40200748911592293 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[212] Loss:0.2720513015985489 | L1 Loss:0.3929228514432907 | R2:0.269937946808516 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[213] Loss:0.2062255349010229 | L1 Loss:0.3477871045470238 | R2:0.4071030030064961 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[213] Loss:0.2719689145684242 | L1 Loss:0.3890092194080353 | R2:0.27390965809075196 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[214] Loss:0.20388291496783495 | L1 Loss:0.3354264609515667 | R2:0.41600727993842584 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[214] Loss:0.2712863892316818 | L1 Loss:0.39194533228874207 | R2:0.27645118158690557 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[215] Loss:0.2043846007436514 | L1 Loss:0.33960435539484024 | R2:0.4036762752582165 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[215] Loss:0.26852704435586927 | L1 Loss:0.3883448392152786 | R2:0.2928760000377212 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[216] Loss:0.2151662167161703 | L1 Loss:0.34373512864112854 | R2:0.374813508680081 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[216] Loss:0.25499117821455003 | L1 Loss:0.3860398977994919 | R2:0.3280773235055055 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[217] Loss:0.2074112524278462 | L1 Loss:0.34296666644513607 | R2:0.400998360473257 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[217] Loss:0.2919590979814529 | L1 Loss:0.4073906123638153 | R2:0.2157982236002835 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[218] Loss:0.2040877277031541 | L1 Loss:0.3414753954857588 | R2:0.39703053780601977 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[218] Loss:0.27294940054416655 | L1 Loss:0.39063429832458496 | R2:0.2812219978656647 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[219] Loss:0.2011663275770843 | L1 Loss:0.3366944380104542 | R2:0.4162569597460003 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[219] Loss:0.29494075030088424 | L1 Loss:0.40854371190071104 | R2:0.21099406342895746 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[220] Loss:0.19855812983587384 | L1 Loss:0.3313169702887535 | R2:0.4238149263666973 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[220] Loss:0.24855868518352509 | L1 Loss:0.3647356957197189 | R2:0.34002459923347406 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[221] Loss:0.22845895774662495 | L1 Loss:0.36587560176849365 | R2:0.3462160540949189 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[221] Loss:0.3045203611254692 | L1 Loss:0.4055769950151443 | R2:0.182405074387396 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[222] Loss:0.19248981727287173 | L1 Loss:0.32565772347152233 | R2:0.448071409317633 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[222] Loss:0.29250016063451767 | L1 Loss:0.4071038752794266 | R2:0.22022415102784582 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[223] Loss:0.21480085793882608 | L1 Loss:0.3420420680195093 | R2:0.3782187890858993 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[223] Loss:0.26059712618589403 | L1 Loss:0.38061278462409975 | R2:0.30298762679238744 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[224] Loss:0.19335745833814144 | L1 Loss:0.33357606269419193 | R2:0.4456296862598843 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[224] Loss:0.29268967136740687 | L1 Loss:0.39927965998649595 | R2:0.22162493674175493 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[225] Loss:0.20077252108603716 | L1 Loss:0.3414004500955343 | R2:0.43396064241785126 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[225] Loss:0.28225433379411696 | L1 Loss:0.3858522891998291 | R2:0.23954284959199548 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[226] Loss:0.19950219802558422 | L1 Loss:0.3385595306754112 | R2:0.4228718713361107 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[226] Loss:0.2762448087334633 | L1 Loss:0.3874416440725327 | R2:0.27159585796130203 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[227] Loss:0.21174650732427835 | L1 Loss:0.34683084674179554 | R2:0.39526099802783865 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[227] Loss:0.2969679445028305 | L1 Loss:0.40830734074115754 | R2:0.21854895165265012 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[228] Loss:0.22336105722934008 | L1 Loss:0.3508513979613781 | R2:0.36410391690253546 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[228] Loss:0.3354029685258865 | L1 Loss:0.43050076961517336 | R2:0.09370176137052018 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[229] Loss:0.21455183625221252 | L1 Loss:0.3489240165799856 | R2:0.3886540284461013 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[229] Loss:0.30951598584651946 | L1 Loss:0.4118756324052811 | R2:0.18054444492137037 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[230] Loss:0.22049380419775844 | L1 Loss:0.3519511744379997 | R2:0.3511852268852782 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[230] Loss:0.27891357988119125 | L1 Loss:0.3957718551158905 | R2:0.2577380024229935 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[231] Loss:0.20591858215630054 | L1 Loss:0.3464471325278282 | R2:0.39996781183634117 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[231] Loss:0.27330147475004196 | L1 Loss:0.3940726757049561 | R2:0.26570312293888987 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[232] Loss:0.20889711054041982 | L1 Loss:0.34464971255511045 | R2:0.3808165905936788 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[232] Loss:0.28057117462158204 | L1 Loss:0.3898647576570511 | R2:0.2552852628508232 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[233] Loss:0.1947986874729395 | L1 Loss:0.3346302956342697 | R2:0.4374846082234871 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[233] Loss:0.24743699729442598 | L1 Loss:0.3790088355541229 | R2:0.34675423374899417 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[234] Loss:0.1906286897137761 | L1 Loss:0.3287901971489191 | R2:0.4528427624201611 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[234] Loss:0.30027957260608673 | L1 Loss:0.4051635205745697 | R2:0.19543777110990193 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[235] Loss:0.1975755551829934 | L1 Loss:0.3395452778786421 | R2:0.42128895788207055 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[235] Loss:0.3226371109485626 | L1 Loss:0.4204583138227463 | R2:0.13857306565839111 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[236] Loss:0.20985701493918896 | L1 Loss:0.34722222574055195 | R2:0.3999251831740221 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[236] Loss:0.28329277634620664 | L1 Loss:0.4043731153011322 | R2:0.23966585582597505 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[237] Loss:0.2007080432958901 | L1 Loss:0.33732056245207787 | R2:0.4231524568142443 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[237] Loss:0.24852494150400162 | L1 Loss:0.3776322215795517 | R2:0.3285595591642123 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[238] Loss:0.22544742561876774 | L1 Loss:0.3633033875375986 | R2:0.3446725588183379 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[238] Loss:0.25520033836364747 | L1 Loss:0.3863309293985367 | R2:0.3217385448479976 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[239] Loss:0.23184254299849272 | L1 Loss:0.3501947857439518 | R2:0.3316041079247087 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[239] Loss:0.3004864752292633 | L1 Loss:0.4251322090625763 | R2:0.19448159658313702 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[240] Loss:0.2136249104514718 | L1 Loss:0.35374014265835285 | R2:0.3892760707195335 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[240] Loss:0.30500068217515947 | L1 Loss:0.41571538150310516 | R2:0.18090772218812412 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[241] Loss:0.2239000704139471 | L1 Loss:0.3500834312289953 | R2:0.3443240321577385 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[241] Loss:0.2799746975302696 | L1 Loss:0.3931117862462997 | R2:0.24844959236627623 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[242] Loss:0.2268963512033224 | L1 Loss:0.35725746117532253 | R2:0.3617819477328057 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[242] Loss:0.282563753426075 | L1 Loss:0.4079592555761337 | R2:0.2310673522050152 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[243] Loss:0.20525101944804192 | L1 Loss:0.3431244734674692 | R2:0.41078743465514417 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[243] Loss:0.28655080795288085 | L1 Loss:0.3935405343770981 | R2:0.22673316793236803 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[244] Loss:0.21294056437909603 | L1 Loss:0.3443101141601801 | R2:0.37573226270794335 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[244] Loss:0.3063972979784012 | L1 Loss:0.41437438130378723 | R2:0.16772711055066586 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[245] Loss:0.20838202396407723 | L1 Loss:0.3359309285879135 | R2:0.38963581325694013 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[245] Loss:0.2786525011062622 | L1 Loss:0.39245234727859496 | R2:0.25914835215698034 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[246] Loss:0.2023122701793909 | L1 Loss:0.3411255422979593 | R2:0.40770005029870193 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[246] Loss:0.30120557695627215 | L1 Loss:0.40770233869552613 | R2:0.20028278071612596 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[247] Loss:0.21397306211292744 | L1 Loss:0.34524596855044365 | R2:0.3880959011711153 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[247] Loss:0.2793246641755104 | L1 Loss:0.40773345828056334 | R2:0.24762485261412842 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[248] Loss:0.22405231790617108 | L1 Loss:0.35523834731429815 | R2:0.358566183719082 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[248] Loss:0.24734797179698945 | L1 Loss:0.37125410735607145 | R2:0.3369594667685166 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[249] Loss:0.21148953959345818 | L1 Loss:0.34638506919145584 | R2:0.38489082016142173 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[249] Loss:0.3069383680820465 | L1 Loss:0.40639123022556306 | R2:0.17953212126557413 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[250] Loss:0.23185257893055677 | L1 Loss:0.35888082161545753 | R2:0.3397054023316821 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[250] Loss:0.27459100931882857 | L1 Loss:0.39204342663288116 | R2:0.26798311292611277 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[251] Loss:0.21350920153781772 | L1 Loss:0.3408438842743635 | R2:0.37339771499867086 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[251] Loss:0.2662817046046257 | L1 Loss:0.3728881299495697 | R2:0.28489446794809287 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[252] Loss:0.2075287066400051 | L1 Loss:0.345750879496336 | R2:0.40300487939707924 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[252] Loss:0.28488181680440905 | L1 Loss:0.3971155047416687 | R2:0.24416553058578608 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[253] Loss:0.20765792950987816 | L1 Loss:0.34422276355326176 | R2:0.39940418391677324 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[253] Loss:0.28774949312210085 | L1 Loss:0.4009232401847839 | R2:0.23033036881252705 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[254] Loss:0.20929964818060398 | L1 Loss:0.33823070116341114 | R2:0.4140857430712986 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[254] Loss:0.2770238399505615 | L1 Loss:0.3939456671476364 | R2:0.26209598438502946 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[255] Loss:0.19993632100522518 | L1 Loss:0.32517223060131073 | R2:0.4214032202040044 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[255] Loss:0.2769556507468224 | L1 Loss:0.3952988386154175 | R2:0.24851791105596216 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[256] Loss:0.21449532452970743 | L1 Loss:0.3508445229381323 | R2:0.39017201510454214 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[256] Loss:0.2661991655826569 | L1 Loss:0.3861104935407639 | R2:0.29055970303316514 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[257] Loss:0.21190920379012823 | L1 Loss:0.3455603774636984 | R2:0.38038370958107126 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[257] Loss:0.27509367614984515 | L1 Loss:0.3896193027496338 | R2:0.25758330905003535 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[258] Loss:0.22509245201945305 | L1 Loss:0.3471620474010706 | R2:0.355515296505901 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[258] Loss:0.2861793011426926 | L1 Loss:0.3992204189300537 | R2:0.23846394176456243 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[259] Loss:0.21413214690983295 | L1 Loss:0.35284979455173016 | R2:0.37645807110142077 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[259] Loss:0.26850846856832505 | L1 Loss:0.38368949890136717 | R2:0.2866109705987131 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[260] Loss:0.2234921371564269 | L1 Loss:0.35684538073837757 | R2:0.3485136566527554 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[260] Loss:0.2949624449014664 | L1 Loss:0.41956951916217805 | R2:0.21397083785996437 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[261] Loss:0.20559313148260117 | L1 Loss:0.33826877921819687 | R2:0.4111727661484673 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[261] Loss:0.32529000490903853 | L1 Loss:0.4194101393222809 | R2:0.12046933516064925 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[262] Loss:0.22714488487690687 | L1 Loss:0.35343390703201294 | R2:0.34908467764845524 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[262] Loss:0.25817982107400894 | L1 Loss:0.3827031135559082 | R2:0.3115091561464329 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[263] Loss:0.20607111789286137 | L1 Loss:0.34075467474758625 | R2:0.4011661306158387 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[263] Loss:0.3067592665553093 | L1 Loss:0.41557397544384 | R2:0.1706060246938039 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[264] Loss:0.1996128740720451 | L1 Loss:0.3374389288946986 | R2:0.436262019670664 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[264] Loss:0.302318400144577 | L1 Loss:0.39985457360744475 | R2:0.1765690900576185 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[265] Loss:0.21990969451144338 | L1 Loss:0.34331079944968224 | R2:0.36719859367839397 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[265] Loss:0.2932154506444931 | L1 Loss:0.41199281215667727 | R2:0.2214494679837677 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[266] Loss:0.21821431163698435 | L1 Loss:0.34873963333666325 | R2:0.37064794303347237 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[266] Loss:0.30530878305435183 | L1 Loss:0.42358394861221316 | R2:0.18424164803677204 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[267] Loss:0.20168905891478062 | L1 Loss:0.3476059418171644 | R2:0.42769688733119876 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[267] Loss:0.27955079525709153 | L1 Loss:0.3886766850948334 | R2:0.24327837350145512 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[268] Loss:0.19463408924639225 | L1 Loss:0.337385730817914 | R2:0.4451563308853586 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[268] Loss:0.278554730117321 | L1 Loss:0.39439196288585665 | R2:0.25621196376051136 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[269] Loss:0.2201931094750762 | L1 Loss:0.34871397353708744 | R2:0.36550502706668003 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[269] Loss:0.2713012367486954 | L1 Loss:0.38623934984207153 | R2:0.27984198946164296 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[270] Loss:0.20472701359540224 | L1 Loss:0.33842338249087334 | R2:0.39031246364476224 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[270] Loss:0.29298116713762284 | L1 Loss:0.3977357357740402 | R2:0.21341003041770543 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[271] Loss:0.19929124601185322 | L1 Loss:0.33675071969628334 | R2:0.42457384387738983 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[271] Loss:0.2948285549879074 | L1 Loss:0.4125966876745224 | R2:0.2091909392827585 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[272] Loss:0.21344727277755737 | L1 Loss:0.3489848617464304 | R2:0.39187583067315224 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[272] Loss:0.28166340440511706 | L1 Loss:0.404467898607254 | R2:0.23755905711460326 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[273] Loss:0.21147579699754715 | L1 Loss:0.3561381511390209 | R2:0.38699620968976156 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[273] Loss:0.2703587755560875 | L1 Loss:0.3875316113233566 | R2:0.27763598905976317 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[274] Loss:0.22667451668530703 | L1 Loss:0.3518786281347275 | R2:0.3446587226196005 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[274] Loss:0.26633433550596236 | L1 Loss:0.3886120647192001 | R2:0.28949785312874476 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[275] Loss:0.22724172566086054 | L1 Loss:0.35562643222510815 | R2:0.34755296852185036 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[275] Loss:0.282697494328022 | L1 Loss:0.40685672163963316 | R2:0.2496498656963088 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[276] Loss:0.22521993471309543 | L1 Loss:0.35885935835540295 | R2:0.35532571612811314 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[276] Loss:0.28122961819171904 | L1 Loss:0.3852335691452026 | R2:0.24745804181121933 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[277] Loss:0.2131085703149438 | L1 Loss:0.35271022096276283 | R2:0.37796486684029146 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[277] Loss:0.2777118802070618 | L1 Loss:0.39195486903190613 | R2:0.2502885437389769 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[278] Loss:0.20170934591442347 | L1 Loss:0.3373899608850479 | R2:0.4063695534289646 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[278] Loss:0.24196254462003708 | L1 Loss:0.364272466301918 | R2:0.36418491893002897 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[279] Loss:0.19001996144652367 | L1 Loss:0.3345482014119625 | R2:0.4507479361389304 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[279] Loss:0.2970146656036377 | L1 Loss:0.40638130307197573 | R2:0.20217185618737762 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[280] Loss:0.20842684200033545 | L1 Loss:0.33953549712896347 | R2:0.4001540349698135 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[280] Loss:0.29961218386888505 | L1 Loss:0.4082676887512207 | R2:0.20296013301960883 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[281] Loss:0.2235255241394043 | L1 Loss:0.3638627976179123 | R2:0.34756000245134333 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[281] Loss:0.28437796533107756 | L1 Loss:0.39326768815517427 | R2:0.23980795518429007 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[282] Loss:0.22220869548618793 | L1 Loss:0.36217859387397766 | R2:0.37041277609832135 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[282] Loss:0.2905694365501404 | L1 Loss:0.41078294813632965 | R2:0.21545009398025047 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[283] Loss:0.20306961610913277 | L1 Loss:0.3379764389246702 | R2:0.4160921129280553 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[283] Loss:0.2879986077547073 | L1 Loss:0.39972551465034484 | R2:0.23547244531294878 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[284] Loss:0.22774085123091936 | L1 Loss:0.3658934812992811 | R2:0.3435465746491911 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[284] Loss:0.267598582804203 | L1 Loss:0.39563133120536803 | R2:0.2850500836138838 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[285] Loss:0.21378258429467678 | L1 Loss:0.3424910455942154 | R2:0.374297462916211 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[285] Loss:0.257692988216877 | L1 Loss:0.3701504111289978 | R2:0.3148539692384519 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[286] Loss:0.21400465816259384 | L1 Loss:0.3467244990170002 | R2:0.3677765601496642 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[286] Loss:0.30015123188495635 | L1 Loss:0.4145142674446106 | R2:0.19594364587251398 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[287] Loss:0.20709267677739263 | L1 Loss:0.3420234713703394 | R2:0.40745440629820406 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[287] Loss:0.2757610633969307 | L1 Loss:0.38697993755340576 | R2:0.257497627437396 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[288] Loss:0.20182048808783293 | L1 Loss:0.34193623438477516 | R2:0.42633580307790425 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[288] Loss:0.30050918757915496 | L1 Loss:0.40492798388004303 | R2:0.20701467145570723 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[289] Loss:0.20597229152917862 | L1 Loss:0.3450743453577161 | R2:0.4092858854725464 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[289] Loss:0.29345903247594834 | L1 Loss:0.40340847671031954 | R2:0.22143542476471345 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[290] Loss:0.21159907896071672 | L1 Loss:0.3463104972615838 | R2:0.38227716075955587 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[290] Loss:0.2731552332639694 | L1 Loss:0.385904523730278 | R2:0.27745123917291536 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[291] Loss:0.1915993159636855 | L1 Loss:0.3311972878873348 | R2:0.436349068279191 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[291] Loss:0.287542550265789 | L1 Loss:0.4100069135427475 | R2:0.228675220048145 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[292] Loss:0.21434809640049934 | L1 Loss:0.34962647780776024 | R2:0.3915702055263755 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[292] Loss:0.3001041978597641 | L1 Loss:0.40162295401096343 | R2:0.1946109241327348 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[293] Loss:0.21120620518922806 | L1 Loss:0.3491446375846863 | R2:0.38782724486183917 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[293] Loss:0.285459204018116 | L1 Loss:0.40184311866760253 | R2:0.2435658588418616 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[294] Loss:0.1968700187280774 | L1 Loss:0.3279918320477009 | R2:0.42629132337845027 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[294] Loss:0.275545009970665 | L1 Loss:0.3944914937019348 | R2:0.2579756287974204 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[295] Loss:0.20042722672224045 | L1 Loss:0.33957136049866676 | R2:0.4107973160203867 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[295] Loss:0.3119559735059738 | L1 Loss:0.41378271877765654 | R2:0.15972123124675952 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[296] Loss:0.20303518883883953 | L1 Loss:0.34364581294357777 | R2:0.4144669749424769 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[296] Loss:0.2831288978457451 | L1 Loss:0.3972358494997025 | R2:0.23751508306833405 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[297] Loss:0.21604560781270266 | L1 Loss:0.3461654204875231 | R2:0.36673028038678773 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[297] Loss:0.24483286142349242 | L1 Loss:0.3802594840526581 | R2:0.3458835738484659 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[298] Loss:0.20101897791028023 | L1 Loss:0.34195351973176 | R2:0.42766433601657794 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[298] Loss:0.297964783012867 | L1 Loss:0.40731425285339357 | R2:0.20538187811247055 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[299] Loss:0.1927291303873062 | L1 Loss:0.33482917211949825 | R2:0.44864322892172187 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[299] Loss:0.2888765349984169 | L1 Loss:0.4026269197463989 | R2:0.22339516155374622 | ACC: 70.6667%(212/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "id": "VQ8L05UrVCpf",
        "outputId": "db6982f7-9925-49b6-b40c-1e97e20e4f65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 300 epochs-----\n",
            "max test_r2_record  0.36779682975069206\n",
            "min test_loss_l1_record  0.36098951995372774\n",
            "min test_loss_record  0.23597124963998795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3692b6-b3c9-475a-be0c-5042a665bb77",
        "id": "2fC5jH4JqWIJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 300 epochs-----\n",
            "max test_r2_record  0.4539764606567404\n",
            "min test_loss_l1_record  0.34505664110183715\n",
            "min test_loss_record  0.19740368351340293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901b9eec-a9f9-4515-be58-ef826f8f9add",
        "id": "UxMvX2LcqWIJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 100 epochs-----\n",
            "max test_r2_record  0.40889370284073223\n",
            "min test_loss_l1_record  0.34584062695503237\n",
            "min test_loss_record  0.21329353600740433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "265e3b63-2684-419d-ebb3-9e6e9abac938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4QpP7QDqWIK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 50 epochs-----\n",
            "max test_r2_record  0.33985270740833773\n",
            "min test_loss_l1_record  0.37808668315410615\n",
            "min test_loss_record  0.23791675567626952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4bKQJUBxqV40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxswWOUKtSv3",
        "outputId": "d04b8091-1a11-4970-dfbe-30868797fd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 300 epochs-----\n",
            "max test_r2_record  0.55865865548055\n",
            "min test_loss_l1_record  0.298935204744339\n",
            "min test_loss_record  0.18107207119464874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqM4DJGajK7-",
        "outputId": "e1d418d1-153a-442a-adb5-8174f3c51a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 100 epochs-----\n",
            "max test_r2_record  0.5205038869404067\n",
            "min test_loss_l1_record  0.31958115100860596\n",
            "min test_loss_record  0.19672611355781555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbDy3khIjUH-",
        "outputId": "7f573744-81a7-40ca-8cf6-e6fe20c35e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 200 epochs-----\n",
            "max test_r2_record  0.552302451566165\n",
            "min test_loss_l1_record  0.3034108579158783\n",
            "min test_loss_record  0.18367987871170044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----LSTM+ MLP 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "id": "NhPzd34mkHK_",
        "outputId": "feb53e6b-137b-41a8-e6c7-1bda561df4ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----LSTM+ MLP 50 epochs-----\n",
            "max test_r2_record  0.45373308581377814\n",
            "min test_loss_l1_record  0.3382137417793274\n",
            "min test_loss_record  0.2241206020116806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "ll1Bhiq3ap5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RuWPQbhDaq6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "_8H4CoiaqzHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a83b00-f4a4-4277-d735-d0b2e660bbce",
        "id": "EvqV7tJaqzHI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.0008, weight_decay=0.0001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "GpNlVVxVqzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26a2a36-0d53-441d-984b-1b6e9222c7a7",
        "id": "Lcj2KO77qzHJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=428, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(test_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "Og92KpLhqzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(val_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    label = label.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "yQ--rb-1qzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(200)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cef94e-6467-4032-e266-69be7cb51b61",
        "id": "42BCuA6zqzHJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTesting Epoch[0] Loss:0.3957033306360245 | L1 Loss:0.47571220993995667 | R2:-0.04122904508350231 | ACC: 59.0000%(295/500)\n",
            "Testing Epoch[0] Loss:0.3858307167887688 | L1 Loss:0.46802532970905303 | R2:-0.012582346996903781 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[1] Loss:0.35906716994941235 | L1 Loss:0.45285539887845516 | R2:0.060582040947702157 | ACC: 61.8000%(309/500)\n",
            "Testing Epoch[1] Loss:0.35153576731681824 | L1 Loss:0.44123417139053345 | R2:0.0765907289610338 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[2] Loss:0.3298210445791483 | L1 Loss:0.436803026124835 | R2:0.14093842215103608 | ACC: 62.0000%(310/500)\n",
            "Testing Epoch[2] Loss:0.34147956073284147 | L1 Loss:0.4347683846950531 | R2:0.09923356579213696 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[3] Loss:0.31075404305011034 | L1 Loss:0.42227941006422043 | R2:0.1893496411900744 | ACC: 63.6000%(318/500)\n",
            "Testing Epoch[3] Loss:0.335654978454113 | L1 Loss:0.4304762721061707 | R2:0.11969857233735767 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[4] Loss:0.29080779012292624 | L1 Loss:0.40709899738430977 | R2:0.240712639500739 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[4] Loss:0.3208051547408104 | L1 Loss:0.42162376940250396 | R2:0.1537701439109304 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[5] Loss:0.2742041451856494 | L1 Loss:0.39616294018924236 | R2:0.2827192300684977 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[5] Loss:0.3230074867606163 | L1 Loss:0.42057461142539976 | R2:0.14001465926393378 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[6] Loss:0.2662925021722913 | L1 Loss:0.3911803252995014 | R2:0.3009819733557954 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[6] Loss:0.30830159932374956 | L1 Loss:0.41224308013916017 | R2:0.1846073547691343 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[7] Loss:0.2527229804545641 | L1 Loss:0.38392578065395355 | R2:0.3374283934107961 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[7] Loss:0.28340289890766146 | L1 Loss:0.3994921773672104 | R2:0.2469322470187604 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[8] Loss:0.23601988330483437 | L1 Loss:0.3726344648748636 | R2:0.3774042905426397 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[8] Loss:0.2860839903354645 | L1 Loss:0.400054007768631 | R2:0.24020319998311726 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[9] Loss:0.23568456806242466 | L1 Loss:0.3723270781338215 | R2:0.3789099390773113 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[9] Loss:0.2871128782629967 | L1 Loss:0.40667971074581144 | R2:0.23678842435954764 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[10] Loss:0.22646242007613182 | L1 Loss:0.3618664834648371 | R2:0.39869192013526633 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[10] Loss:0.2877556040883064 | L1 Loss:0.4055988848209381 | R2:0.23332585570191516 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[11] Loss:0.22312147077172995 | L1 Loss:0.3648574147373438 | R2:0.40466624368462834 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[11] Loss:0.2894080325961113 | L1 Loss:0.40583181381225586 | R2:0.2340845760526813 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[12] Loss:0.22165833692997694 | L1 Loss:0.36216181702911854 | R2:0.4103798023453936 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[12] Loss:0.29231572300195696 | L1 Loss:0.41269670724868773 | R2:0.21544338328745685 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[13] Loss:0.20877501275390387 | L1 Loss:0.3557417690753937 | R2:0.44520032945000687 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[13] Loss:0.2806484535336494 | L1 Loss:0.3983699381351471 | R2:0.25707275147324116 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[14] Loss:0.214263373054564 | L1 Loss:0.35851751267910004 | R2:0.42942722432501057 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[14] Loss:0.28689094483852384 | L1 Loss:0.41276765465736387 | R2:0.2419140176861529 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[15] Loss:0.2170540108345449 | L1 Loss:0.36411393247544765 | R2:0.4248905060068848 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[15] Loss:0.29435839504003525 | L1 Loss:0.4072914093732834 | R2:0.22381901884675787 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[16] Loss:0.21720768883824348 | L1 Loss:0.3622245267033577 | R2:0.41782184521443905 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[16] Loss:0.3022446021437645 | L1 Loss:0.4186155557632446 | R2:0.1932500728281832 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[17] Loss:0.21391370985656977 | L1 Loss:0.3620215654373169 | R2:0.4281933436852813 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[17] Loss:0.2914558410644531 | L1 Loss:0.40869115889072416 | R2:0.22165084279023847 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[18] Loss:0.22060925140976906 | L1 Loss:0.3702116049826145 | R2:0.4089874279358239 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[18] Loss:0.29754662960767747 | L1 Loss:0.4126353621482849 | R2:0.20704960185631754 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[19] Loss:0.22016198746860027 | L1 Loss:0.364491967484355 | R2:0.4105204097730185 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[19] Loss:0.29099246561527253 | L1 Loss:0.4123048961162567 | R2:0.2193879776646283 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[20] Loss:0.22290394082665443 | L1 Loss:0.3680750476196408 | R2:0.4030889447118212 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[20] Loss:0.2925115585327148 | L1 Loss:0.4108556300401688 | R2:0.2220403371143095 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[21] Loss:0.20872483914718032 | L1 Loss:0.3595756608992815 | R2:0.44497923683027973 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[21] Loss:0.28624583333730697 | L1 Loss:0.40990203619003296 | R2:0.2471677957288322 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[22] Loss:0.21711974125355482 | L1 Loss:0.36983561888337135 | R2:0.4220976620915821 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[22] Loss:0.28993095457553864 | L1 Loss:0.4135104030370712 | R2:0.22976933836213737 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[23] Loss:0.20980667788535357 | L1 Loss:0.359353294596076 | R2:0.4417254347792042 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[23] Loss:0.2760865390300751 | L1 Loss:0.3983915627002716 | R2:0.2604531882558465 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[24] Loss:0.20367523655295372 | L1 Loss:0.3542721029371023 | R2:0.4550166603505156 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[24] Loss:0.2810827255249023 | L1 Loss:0.400320765376091 | R2:0.24551621976025456 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[25] Loss:0.20951287355273962 | L1 Loss:0.3577550817281008 | R2:0.4437280342939099 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[25] Loss:0.2794500559568405 | L1 Loss:0.40008665025234225 | R2:0.2565151647049909 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[26] Loss:0.2025528671219945 | L1 Loss:0.3460184410214424 | R2:0.45931459688706017 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[26] Loss:0.2765760481357574 | L1 Loss:0.40178983509540556 | R2:0.2630764384602037 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[27] Loss:0.20692996261641383 | L1 Loss:0.34106922522187233 | R2:0.4441851000695837 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[27] Loss:0.29731187373399737 | L1 Loss:0.4185117810964584 | R2:0.20681970122811336 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[28] Loss:0.20471554761752486 | L1 Loss:0.3469322994351387 | R2:0.4456466581273906 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[28] Loss:0.29975536167621614 | L1 Loss:0.4051995724439621 | R2:0.1935149556438658 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[29] Loss:0.20376947615295649 | L1 Loss:0.3482413850724697 | R2:0.455578612884238 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[29] Loss:0.309943263232708 | L1 Loss:0.41171320974826814 | R2:0.15762012649118975 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[30] Loss:0.20438104774802923 | L1 Loss:0.34974220767617226 | R2:0.45390079191577504 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[30] Loss:0.28955768793821335 | L1 Loss:0.40210764706134794 | R2:0.21433992153472406 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[31] Loss:0.20884344773367047 | L1 Loss:0.34631830640137196 | R2:0.4350499293156225 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[31] Loss:0.3021991431713104 | L1 Loss:0.412758880853653 | R2:0.18292438455173704 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[32] Loss:0.21274104434996843 | L1 Loss:0.35362412966787815 | R2:0.4316559698043711 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[32] Loss:0.2977863520383835 | L1 Loss:0.40678246021270753 | R2:0.18826367921707757 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[33] Loss:0.22191578801721334 | L1 Loss:0.3539626765996218 | R2:0.4077394276027886 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[33] Loss:0.292141056060791 | L1 Loss:0.4042427271604538 | R2:0.2102468689130938 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[34] Loss:0.2123818527907133 | L1 Loss:0.34900111705064774 | R2:0.4292635174502615 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[34] Loss:0.29713236540555954 | L1 Loss:0.4115282386541367 | R2:0.19377362102875012 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[35] Loss:0.21646886318922043 | L1 Loss:0.35395204834640026 | R2:0.41373753058543006 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[35] Loss:0.2964399129152298 | L1 Loss:0.40475982427597046 | R2:0.20202093953105718 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[36] Loss:0.20889211678877473 | L1 Loss:0.3526079338043928 | R2:0.4362647072157604 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[36] Loss:0.2903583198785782 | L1 Loss:0.41180731654167174 | R2:0.21551898870646102 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[37] Loss:0.21855256985872984 | L1 Loss:0.35487317480146885 | R2:0.41065514693014016 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[37] Loss:0.29422797858715055 | L1 Loss:0.40961515307426455 | R2:0.20918837245098337 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[38] Loss:0.21765316743403673 | L1 Loss:0.3611808065325022 | R2:0.4174556407648248 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[38] Loss:0.2925488233566284 | L1 Loss:0.4165153056383133 | R2:0.21603665289515356 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[39] Loss:0.2142658894881606 | L1 Loss:0.35389873944222927 | R2:0.4260119508604046 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[39] Loss:0.28036647886037824 | L1 Loss:0.4062217861413956 | R2:0.2468468593660738 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[40] Loss:0.22196158301085234 | L1 Loss:0.3636838477104902 | R2:0.406035122518387 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[40] Loss:0.2906988710165024 | L1 Loss:0.41287382543087003 | R2:0.21160934015384045 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[41] Loss:0.206243091262877 | L1 Loss:0.3474863190203905 | R2:0.44636055616694925 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[41] Loss:0.2813957080245018 | L1 Loss:0.3963723212480545 | R2:0.23462812467361016 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[42] Loss:0.2094798102043569 | L1 Loss:0.35085607320070267 | R2:0.44057265690484976 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[42] Loss:0.2684503376483917 | L1 Loss:0.39754167199134827 | R2:0.28181404451967007 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[43] Loss:0.19605881348252296 | L1 Loss:0.3401452638208866 | R2:0.4754255133181335 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[43] Loss:0.272290301322937 | L1 Loss:0.3921519875526428 | R2:0.2764814829504915 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[44] Loss:0.19657830614596605 | L1 Loss:0.34150839410722256 | R2:0.4729895611497019 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[44] Loss:0.2741158060729504 | L1 Loss:0.3914389729499817 | R2:0.260537826139981 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[45] Loss:0.20593294315040112 | L1 Loss:0.3501684293150902 | R2:0.4481374786046384 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[45] Loss:0.27957853823900225 | L1 Loss:0.3960459977388382 | R2:0.247542167153527 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[46] Loss:0.2027347693219781 | L1 Loss:0.34642948396503925 | R2:0.45892964508288075 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[46] Loss:0.28729812055826187 | L1 Loss:0.39450740814208984 | R2:0.22476464043669003 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[47] Loss:0.20700450241565704 | L1 Loss:0.35029030218720436 | R2:0.4484409725184907 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[47] Loss:0.2936011373996735 | L1 Loss:0.4009875476360321 | R2:0.21033324447380783 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[48] Loss:0.20266441022977233 | L1 Loss:0.3450768366456032 | R2:0.4611236535361875 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[48] Loss:0.2978896528482437 | L1 Loss:0.40782133042812346 | R2:0.19882494899568443 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[49] Loss:0.2016392876394093 | L1 Loss:0.34647458139806986 | R2:0.46396021567567014 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[49] Loss:0.30621763616800307 | L1 Loss:0.41700572371482847 | R2:0.16912157577494105 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[50] Loss:0.20084715774282813 | L1 Loss:0.339530436322093 | R2:0.4614156249797585 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[50] Loss:0.28952581286430357 | L1 Loss:0.40967343747615814 | R2:0.21716440931054343 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[51] Loss:0.19788148254156113 | L1 Loss:0.34152743965387344 | R2:0.4696136092974268 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[51] Loss:0.29059896916151046 | L1 Loss:0.3980556964874268 | R2:0.21043265003746203 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[52] Loss:0.1967031266540289 | L1 Loss:0.33955565467476845 | R2:0.47208060146073155 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[52] Loss:0.29039211124181746 | L1 Loss:0.40525768995285033 | R2:0.20607004173688992 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[53] Loss:0.19801046745851636 | L1 Loss:0.34034937247633934 | R2:0.4709634176226363 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[53] Loss:0.2951781988143921 | L1 Loss:0.40603946447372435 | R2:0.20294485432381607 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[54] Loss:0.1932332538999617 | L1 Loss:0.3419529441744089 | R2:0.48398421182153484 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[54] Loss:0.28510656356811526 | L1 Loss:0.4068231612443924 | R2:0.22353247327072184 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[55] Loss:0.18322778027504683 | L1 Loss:0.33044211752712727 | R2:0.5127604125962812 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[55] Loss:0.2916007250547409 | L1 Loss:0.40314207375049593 | R2:0.20580648694257367 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[56] Loss:0.18989473581314087 | L1 Loss:0.33946303091943264 | R2:0.49316568164364993 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[56] Loss:0.29800090938806534 | L1 Loss:0.4092698484659195 | R2:0.18745783503477037 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[57] Loss:0.18491125758737326 | L1 Loss:0.3354959413409233 | R2:0.5064635222682289 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[57] Loss:0.2897254556417465 | L1 Loss:0.3993644744157791 | R2:0.21015174036739417 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[58] Loss:0.1867319862358272 | L1 Loss:0.3339250748977065 | R2:0.5034481904065845 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[58] Loss:0.2782153904438019 | L1 Loss:0.39456346333026887 | R2:0.2430409943927198 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[59] Loss:0.18772172555327415 | L1 Loss:0.3326324848458171 | R2:0.5005994650618334 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[59] Loss:0.2917750760912895 | L1 Loss:0.4098446547985077 | R2:0.2125997994114802 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[60] Loss:0.18518614256754518 | L1 Loss:0.3349954979494214 | R2:0.5042823702585655 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[60] Loss:0.27949406802654264 | L1 Loss:0.3996924489736557 | R2:0.24592066265183235 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[61] Loss:0.19346938049420714 | L1 Loss:0.3381638936698437 | R2:0.47783464274865084 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[61] Loss:0.2867168769240379 | L1 Loss:0.4105120420455933 | R2:0.21782282893190708 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[62] Loss:0.1905891802161932 | L1 Loss:0.3382860738784075 | R2:0.49026436059304773 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[62] Loss:0.2706721305847168 | L1 Loss:0.3965909481048584 | R2:0.2706855902480796 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[63] Loss:0.19079325534403324 | L1 Loss:0.3362151551991701 | R2:0.4904820940692525 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[63] Loss:0.25999551713466645 | L1 Loss:0.39034029841423035 | R2:0.3016229279258906 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[64] Loss:0.19156521186232567 | L1 Loss:0.34033721312880516 | R2:0.4895497784951126 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[64] Loss:0.2698821038007736 | L1 Loss:0.3995988994836807 | R2:0.2701798942745991 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[65] Loss:0.18864165153354406 | L1 Loss:0.3383911922574043 | R2:0.4955206574675712 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[65] Loss:0.27593123316764834 | L1 Loss:0.39686817228794097 | R2:0.2606274612767351 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[66] Loss:0.18622532859444618 | L1 Loss:0.32990034110844135 | R2:0.5041134890372821 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[66] Loss:0.2786928631365299 | L1 Loss:0.39806956350803374 | R2:0.25419929072856173 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[67] Loss:0.19028814788907766 | L1 Loss:0.33696084935218096 | R2:0.4926668725633771 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[67] Loss:0.28449449092149737 | L1 Loss:0.4019157916307449 | R2:0.23647937601129695 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[68] Loss:0.18675808096304536 | L1 Loss:0.3361819637939334 | R2:0.5044698512386218 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[68] Loss:0.27880371958017347 | L1 Loss:0.39292125701904296 | R2:0.2609934601178165 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[69] Loss:0.19345625722780824 | L1 Loss:0.3435499360784888 | R2:0.48704532118066324 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[69] Loss:0.29743355959653855 | L1 Loss:0.4028469264507294 | R2:0.21262242756394006 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[70] Loss:0.183182867243886 | L1 Loss:0.33501527830958366 | R2:0.5132516980056575 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[70] Loss:0.29893181324005125 | L1 Loss:0.40067865550518034 | R2:0.21049666509360482 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[71] Loss:0.18768201395869255 | L1 Loss:0.335698451846838 | R2:0.5031681580099562 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[71] Loss:0.30383590832352636 | L1 Loss:0.40925572514534 | R2:0.20221304605790005 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[72] Loss:0.18578177876770496 | L1 Loss:0.33025006391108036 | R2:0.5046880260894366 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[72] Loss:0.2921775564551353 | L1 Loss:0.4054004818201065 | R2:0.22678484022504786 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[73] Loss:0.18448027782142162 | L1 Loss:0.33183618262410164 | R2:0.5094724309100848 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[73] Loss:0.29179039150476455 | L1 Loss:0.4067630499601364 | R2:0.22697554726824976 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[74] Loss:0.18667263397946954 | L1 Loss:0.32790764421224594 | R2:0.5036063965225968 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[74] Loss:0.28474780917167664 | L1 Loss:0.4028526097536087 | R2:0.23755241563790216 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[75] Loss:0.17727882182225585 | L1 Loss:0.3243939336389303 | R2:0.5237718095016691 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[75] Loss:0.2786546558141708 | L1 Loss:0.39736263155937196 | R2:0.2490673645133422 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[76] Loss:0.18281547026708722 | L1 Loss:0.33061780221760273 | R2:0.512508668602557 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[76] Loss:0.27133816480636597 | L1 Loss:0.39859052896499636 | R2:0.2656681142412346 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[77] Loss:0.17990572517737746 | L1 Loss:0.3234888333827257 | R2:0.5184885882526418 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[77] Loss:0.27453267127275466 | L1 Loss:0.3994369089603424 | R2:0.2533261327565485 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[78] Loss:0.17789288563653827 | L1 Loss:0.3218523692339659 | R2:0.5228498767821077 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[78] Loss:0.27201199233531953 | L1 Loss:0.4005205392837524 | R2:0.2584794988742051 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[79] Loss:0.18000588240101933 | L1 Loss:0.3271129671484232 | R2:0.5199709374888098 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[79] Loss:0.2783698499202728 | L1 Loss:0.4005959749221802 | R2:0.23387243394461005 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[80] Loss:0.1801405414007604 | L1 Loss:0.3266853652894497 | R2:0.5192206282574016 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[80] Loss:0.28611830323934556 | L1 Loss:0.40429732799530027 | R2:0.2149357465133468 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[81] Loss:0.1779452716000378 | L1 Loss:0.32196451537311077 | R2:0.5264909165642542 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[81] Loss:0.27959179878234863 | L1 Loss:0.3954620391130447 | R2:0.236313926199041 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[82] Loss:0.1781206582672894 | L1 Loss:0.324259196408093 | R2:0.5246109367702 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[82] Loss:0.27122621387243273 | L1 Loss:0.3835774093866348 | R2:0.2592721234332034 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[83] Loss:0.16860247030854225 | L1 Loss:0.3171015214174986 | R2:0.5501196648284153 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[83] Loss:0.28062415570020677 | L1 Loss:0.38866407573223116 | R2:0.23876793952452244 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[84] Loss:0.1711936960928142 | L1 Loss:0.31687892507761717 | R2:0.5437614870201918 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[84] Loss:0.2744514554738998 | L1 Loss:0.3843634635210037 | R2:0.26217619328976893 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[85] Loss:0.1739527522586286 | L1 Loss:0.32610404305160046 | R2:0.5329739073605084 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[85] Loss:0.2846326991915703 | L1 Loss:0.39233432412147523 | R2:0.22737986676857366 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[86] Loss:0.17839989578351378 | L1 Loss:0.3265975136309862 | R2:0.5213818329024689 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[86] Loss:0.27817104011774063 | L1 Loss:0.38984889090061187 | R2:0.24597078065784822 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[87] Loss:0.17942754132673144 | L1 Loss:0.324806559830904 | R2:0.5163902266439071 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[87] Loss:0.27885679304599764 | L1 Loss:0.3902951776981354 | R2:0.24355280717441677 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[88] Loss:0.1806349684484303 | L1 Loss:0.3257621666416526 | R2:0.5108017060776507 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[88] Loss:0.27489938735961916 | L1 Loss:0.37593736946582795 | R2:0.25918643049148804 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[89] Loss:0.18568683182820678 | L1 Loss:0.32962788827717304 | R2:0.49889871151482207 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[89] Loss:0.29322781711816787 | L1 Loss:0.3987839728593826 | R2:0.2090187037701549 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[90] Loss:0.1815253347158432 | L1 Loss:0.3262051111087203 | R2:0.5087747934769243 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[90] Loss:0.29440427422523496 | L1 Loss:0.3951463669538498 | R2:0.20670277032586587 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[91] Loss:0.18115470511838794 | L1 Loss:0.3244250752031803 | R2:0.5091133270180856 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[91] Loss:0.2732485607266426 | L1 Loss:0.38613504767417905 | R2:0.262136217170189 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[92] Loss:0.1859026001766324 | L1 Loss:0.3309543179348111 | R2:0.5007433310987065 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[92] Loss:0.26603866964578626 | L1 Loss:0.3779112219810486 | R2:0.29159786760554046 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[93] Loss:0.18957715295255184 | L1 Loss:0.33419214002788067 | R2:0.49132450652891896 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[93] Loss:0.2564698293805122 | L1 Loss:0.36992483735084536 | R2:0.3202273865438901 | ACC: 76.3333%(229/300)\n",
            "Testing Epoch[94] Loss:0.18324689008295536 | L1 Loss:0.3275418123230338 | R2:0.5083008130545421 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[94] Loss:0.26702012270689013 | L1 Loss:0.3776060789823532 | R2:0.289854491122827 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[95] Loss:0.18171486258506775 | L1 Loss:0.33002154529094696 | R2:0.5156137090458999 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[95] Loss:0.27137487679719924 | L1 Loss:0.3793902724981308 | R2:0.2697159273688873 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[96] Loss:0.18188328063115478 | L1 Loss:0.3292461549863219 | R2:0.5176434187040151 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[96] Loss:0.2784917399287224 | L1 Loss:0.3808035433292389 | R2:0.2528541087254865 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[97] Loss:0.1789745558053255 | L1 Loss:0.32809562608599663 | R2:0.5261724376169375 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[97] Loss:0.27933437526226046 | L1 Loss:0.38577049076557157 | R2:0.2510260522593636 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[98] Loss:0.17353337444365025 | L1 Loss:0.3229977563023567 | R2:0.540774373491587 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[98] Loss:0.27737431079149244 | L1 Loss:0.3861061900854111 | R2:0.25608358773584994 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[99] Loss:0.1792010348290205 | L1 Loss:0.3292487282305956 | R2:0.5271473198533208 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[99] Loss:0.26741005554795266 | L1 Loss:0.37473139464855193 | R2:0.2849928966331802 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[100] Loss:0.17800696613267064 | L1 Loss:0.3256207536906004 | R2:0.525328261167859 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[100] Loss:0.2805421143770218 | L1 Loss:0.3795920521020889 | R2:0.2407061337238941 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[101] Loss:0.17103327484801412 | L1 Loss:0.3186832405626774 | R2:0.5464699009304641 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[101] Loss:0.26405312567949296 | L1 Loss:0.37179113924503326 | R2:0.2894268852468257 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[102] Loss:0.17442354978993535 | L1 Loss:0.321332355029881 | R2:0.5373936285621541 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[102] Loss:0.26629378348588945 | L1 Loss:0.37487149238586426 | R2:0.2873155631772459 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[103] Loss:0.16439158376306295 | L1 Loss:0.31144311837852 | R2:0.5628522080658857 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[103] Loss:0.26833576858043673 | L1 Loss:0.3781968206167221 | R2:0.28096819120941496 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[104] Loss:0.16816135589033365 | L1 Loss:0.31997035816311836 | R2:0.5551289767951451 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[104] Loss:0.285492954403162 | L1 Loss:0.39296131730079653 | R2:0.23774690598128023 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[105] Loss:0.1810912759974599 | L1 Loss:0.3339279852807522 | R2:0.5201065469667233 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[105] Loss:0.28728270158171654 | L1 Loss:0.3897942900657654 | R2:0.23358383977077798 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[106] Loss:0.17957597644999623 | L1 Loss:0.3308620322495699 | R2:0.5241290934871646 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[106] Loss:0.29352831095457077 | L1 Loss:0.3959547311067581 | R2:0.21757737904654323 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[107] Loss:0.18209503218531609 | L1 Loss:0.3343285471200943 | R2:0.5170599234345381 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[107] Loss:0.29738122746348383 | L1 Loss:0.40288565754890443 | R2:0.21466689690345087 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[108] Loss:0.1791504267603159 | L1 Loss:0.3293134970590472 | R2:0.5267275638777689 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[108] Loss:0.2979869730770588 | L1 Loss:0.396879243850708 | R2:0.21134465092295013 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[109] Loss:0.17794024432078004 | L1 Loss:0.3259527161717415 | R2:0.5306566516569403 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[109] Loss:0.28318726643919945 | L1 Loss:0.38145933151245115 | R2:0.25989775674196874 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[110] Loss:0.16928427061066031 | L1 Loss:0.31831477768719196 | R2:0.5536820707416064 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[110] Loss:0.28502247035503386 | L1 Loss:0.38381019830703733 | R2:0.24765095802270115 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[111] Loss:0.16658097179606557 | L1 Loss:0.3175821229815483 | R2:0.5576406090589651 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[111] Loss:0.2792679563164711 | L1 Loss:0.3779606088995934 | R2:0.2624354973083541 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[112] Loss:0.1692556794732809 | L1 Loss:0.3164481669664383 | R2:0.5513661267642285 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[112] Loss:0.275334857404232 | L1 Loss:0.38252643793821334 | R2:0.27226345471803926 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[113] Loss:0.1667231321334839 | L1 Loss:0.3147199936211109 | R2:0.5587480960503293 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[113] Loss:0.27935381457209585 | L1 Loss:0.3859222918748856 | R2:0.2554508152131827 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[114] Loss:0.16127626271918416 | L1 Loss:0.30973041616380215 | R2:0.5713139488954342 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[114] Loss:0.2695887327194214 | L1 Loss:0.38460909724235537 | R2:0.2773968793752094 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[115] Loss:0.16457254951819777 | L1 Loss:0.3089870512485504 | R2:0.5634018251022364 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[115] Loss:0.2631531126797199 | L1 Loss:0.3778782576322556 | R2:0.3005241907289621 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[116] Loss:0.17318002646788955 | L1 Loss:0.3139093993231654 | R2:0.541202381707454 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[116] Loss:0.26764742732048036 | L1 Loss:0.38289308547973633 | R2:0.28493073193887597 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[117] Loss:0.1742166792973876 | L1 Loss:0.31835347414016724 | R2:0.5365915381173807 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[117] Loss:0.2704464480280876 | L1 Loss:0.38430105745792387 | R2:0.2826121728523426 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[118] Loss:0.16952631808817387 | L1 Loss:0.31281943898648024 | R2:0.5496701734123411 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[118] Loss:0.26695394366979597 | L1 Loss:0.38010183572769163 | R2:0.28943672754154426 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[119] Loss:0.17361616529524326 | L1 Loss:0.31863964907824993 | R2:0.5386374373908147 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[119] Loss:0.2708734944462776 | L1 Loss:0.37998483777046205 | R2:0.27580610753228474 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[120] Loss:0.17083503864705563 | L1 Loss:0.31030701845884323 | R2:0.5467891757058179 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[120] Loss:0.2667253702878952 | L1 Loss:0.37757387161254885 | R2:0.2853967858151752 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[121] Loss:0.16889436170458794 | L1 Loss:0.3111867979168892 | R2:0.5530053210826626 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[121] Loss:0.2607438266277313 | L1 Loss:0.3754713088274002 | R2:0.3047544461198318 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[122] Loss:0.1693407353013754 | L1 Loss:0.31730266101658344 | R2:0.550302654749322 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[122] Loss:0.25264607444405557 | L1 Loss:0.3720827639102936 | R2:0.3266484032046545 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[123] Loss:0.16467912774533033 | L1 Loss:0.31035482697188854 | R2:0.5637202577159156 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[123] Loss:0.25590750202536583 | L1 Loss:0.37423247694969175 | R2:0.31574070050227576 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[124] Loss:0.15826802793890238 | L1 Loss:0.30364668276160955 | R2:0.5798057008890256 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[124] Loss:0.2510884694755077 | L1 Loss:0.3681897819042206 | R2:0.33057401320491175 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[125] Loss:0.1567541155964136 | L1 Loss:0.30456642247736454 | R2:0.5850025198890778 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[125] Loss:0.24458863213658333 | L1 Loss:0.3618777930736542 | R2:0.3467241702122653 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[126] Loss:0.1550497622229159 | L1 Loss:0.3040005434304476 | R2:0.5887099083491888 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[126] Loss:0.24512959718704225 | L1 Loss:0.36357631981372834 | R2:0.34520307513188997 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[127] Loss:0.16152300778776407 | L1 Loss:0.3112597241997719 | R2:0.5732119706189053 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[127] Loss:0.25041954666376115 | L1 Loss:0.3612635344266891 | R2:0.33204054963194146 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[128] Loss:0.1652905596420169 | L1 Loss:0.31179343443363905 | R2:0.5638990926946185 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[128] Loss:0.2571909919381142 | L1 Loss:0.36804172992706297 | R2:0.31558733600502187 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[129] Loss:0.17106612073257565 | L1 Loss:0.314781004562974 | R2:0.5503628850686292 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[129] Loss:0.264896023273468 | L1 Loss:0.3741609513759613 | R2:0.29294166473925065 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[130] Loss:0.1706503126770258 | L1 Loss:0.31453662738204 | R2:0.5506493279325243 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[130] Loss:0.2596725508570671 | L1 Loss:0.3756801873445511 | R2:0.3059352453201291 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[131] Loss:0.17158767068758607 | L1 Loss:0.31247473508119583 | R2:0.5439537156054702 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[131] Loss:0.2745569750666618 | L1 Loss:0.3855575233697891 | R2:0.2617317900024309 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[132] Loss:0.165934638120234 | L1 Loss:0.3098549069836736 | R2:0.5576933302374846 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[132] Loss:0.2548666253685951 | L1 Loss:0.3784358769655228 | R2:0.31312658946186556 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[133] Loss:0.16172877047210932 | L1 Loss:0.30575140286237 | R2:0.568866752478668 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[133] Loss:0.2684425085783005 | L1 Loss:0.38645538985729216 | R2:0.2755050281640114 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[134] Loss:0.16041703801602125 | L1 Loss:0.3079478591680527 | R2:0.5755435594729722 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[134] Loss:0.26860916167497634 | L1 Loss:0.38614471852779386 | R2:0.27257381439203243 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[135] Loss:0.16667072987183928 | L1 Loss:0.3113703588023782 | R2:0.5590826030510646 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[135] Loss:0.2722201064229012 | L1 Loss:0.3934206157922745 | R2:0.2624270381602316 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[136] Loss:0.16398990666493773 | L1 Loss:0.31099823117256165 | R2:0.5638716929343984 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[136] Loss:0.27456399947404864 | L1 Loss:0.38537209331989286 | R2:0.2527028081125092 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[137] Loss:0.17019572528079152 | L1 Loss:0.31822044122964144 | R2:0.5458870457431481 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[137] Loss:0.26803482621908187 | L1 Loss:0.3851206123828888 | R2:0.26764992814433664 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[138] Loss:0.1668753381818533 | L1 Loss:0.3129579247906804 | R2:0.55569915297538 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[138] Loss:0.2738129526376724 | L1 Loss:0.38810553550720217 | R2:0.25097998658312376 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[139] Loss:0.1704031890258193 | L1 Loss:0.3206832092255354 | R2:0.545897532696201 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[139] Loss:0.2705690324306488 | L1 Loss:0.38853547275066375 | R2:0.26455388108432715 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[140] Loss:0.16176119586452842 | L1 Loss:0.31220033671706915 | R2:0.568384885206896 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[140] Loss:0.26295123249292374 | L1 Loss:0.3823993384838104 | R2:0.29020048768723145 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[141] Loss:0.166326398961246 | L1 Loss:0.3159449100494385 | R2:0.5571518630986899 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[141] Loss:0.2598414048552513 | L1 Loss:0.38135755360126494 | R2:0.3047687037992606 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[142] Loss:0.1612718952819705 | L1 Loss:0.3089589821174741 | R2:0.5688094730197485 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[142] Loss:0.2585065171122551 | L1 Loss:0.375354528427124 | R2:0.3098774742712648 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[143] Loss:0.1629061778075993 | L1 Loss:0.3111816830933094 | R2:0.5643527111547141 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[143] Loss:0.2571296453475952 | L1 Loss:0.3710283100605011 | R2:0.3112955322945027 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[144] Loss:0.1674237037077546 | L1 Loss:0.31106201000511646 | R2:0.5495525749285324 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[144] Loss:0.25157723426818845 | L1 Loss:0.3696435302495956 | R2:0.32800742551792805 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[145] Loss:0.16356067778542638 | L1 Loss:0.31123354099690914 | R2:0.5642494353363735 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[145] Loss:0.2539061039686203 | L1 Loss:0.37286634743213654 | R2:0.32109234782908863 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[146] Loss:0.16271509090438485 | L1 Loss:0.3102718573063612 | R2:0.5647132173888427 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[146] Loss:0.24398487508296968 | L1 Loss:0.36027117669582365 | R2:0.3488956959732975 | ACC: 76.3333%(229/300)\n",
            "Testing Epoch[147] Loss:0.16158187529072165 | L1 Loss:0.3104205746203661 | R2:0.5676756658375725 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[147] Loss:0.2496900163590908 | L1 Loss:0.3619355082511902 | R2:0.331865970780114 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[148] Loss:0.16007392015308142 | L1 Loss:0.3055611364543438 | R2:0.5744221693576156 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[148] Loss:0.2532361827790737 | L1 Loss:0.36104726344347 | R2:0.3212713954560664 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[149] Loss:0.15592836076393723 | L1 Loss:0.3004494709894061 | R2:0.5827076714539569 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[149] Loss:0.2531923718750477 | L1 Loss:0.36723365932703017 | R2:0.32132981487581413 | ACC: 76.6667%(230/300)\n",
            "Testing Epoch[150] Loss:0.16073190234601498 | L1 Loss:0.3057423671707511 | R2:0.5717284075129517 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[150] Loss:0.2624260619282722 | L1 Loss:0.3715937063097954 | R2:0.29440463550738255 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[151] Loss:0.15749001363292336 | L1 Loss:0.301834587007761 | R2:0.5825118485088429 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[151] Loss:0.2547814019024372 | L1 Loss:0.36551313400268554 | R2:0.317895828451768 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[152] Loss:0.15538062946870923 | L1 Loss:0.2989693954586983 | R2:0.5878464594899386 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[152] Loss:0.2539144270122051 | L1 Loss:0.3628264755010605 | R2:0.31739460198399144 | ACC: 77.6667%(233/300)\n",
            "Testing Epoch[153] Loss:0.157341661863029 | L1 Loss:0.30057983845472336 | R2:0.5826369734122563 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[153] Loss:0.2575874462723732 | L1 Loss:0.36314853429794314 | R2:0.3069264678696427 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[154] Loss:0.16023545386269689 | L1 Loss:0.3038444919511676 | R2:0.5740106926265649 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[154] Loss:0.26423062309622763 | L1 Loss:0.36589479446411133 | R2:0.28992726700052074 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[155] Loss:0.1585513772442937 | L1 Loss:0.3013666458427906 | R2:0.5778639797145709 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[155] Loss:0.2621514931321144 | L1 Loss:0.3660132259130478 | R2:0.2998054234745463 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[156] Loss:0.15443168533965945 | L1 Loss:0.3015254493802786 | R2:0.5883762339050082 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[156] Loss:0.2576143443584442 | L1 Loss:0.36372763216495513 | R2:0.31308118405632407 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[157] Loss:0.15746457781642675 | L1 Loss:0.3035583021119237 | R2:0.5811722945536921 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[157] Loss:0.2538101188838482 | L1 Loss:0.3663525372743607 | R2:0.324109109653646 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[158] Loss:0.16035550460219383 | L1 Loss:0.30403360445052385 | R2:0.5741502761663715 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[158] Loss:0.2465386398136616 | L1 Loss:0.3583170175552368 | R2:0.34278453547565096 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[159] Loss:0.16060137748718262 | L1 Loss:0.3028447534888983 | R2:0.5743227627288817 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[159] Loss:0.25014936923980713 | L1 Loss:0.35851886570453645 | R2:0.3291298989371253 | ACC: 78.0000%(234/300)\n",
            "Testing Epoch[160] Loss:0.16242783842608333 | L1 Loss:0.3056669859215617 | R2:0.5686390144019989 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[160] Loss:0.24773310124874115 | L1 Loss:0.3610367625951767 | R2:0.3349061020993681 | ACC: 78.0000%(234/300)\n",
            "Testing Epoch[161] Loss:0.16106315469369292 | L1 Loss:0.3024043757468462 | R2:0.5745007166608024 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[161] Loss:0.24780912101268768 | L1 Loss:0.36484878659248354 | R2:0.333659185199377 | ACC: 77.6667%(233/300)\n",
            "Testing Epoch[162] Loss:0.16352044697850943 | L1 Loss:0.30740836542099714 | R2:0.5692132089828682 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[162] Loss:0.25342310070991514 | L1 Loss:0.36830762028694153 | R2:0.3177857936096688 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[163] Loss:0.16594980377703905 | L1 Loss:0.3132445979863405 | R2:0.5626437374140832 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[163] Loss:0.24857092797756195 | L1 Loss:0.36639911234378814 | R2:0.3325822549598151 | ACC: 76.6667%(230/300)\n",
            "Testing Epoch[164] Loss:0.16649278532713652 | L1 Loss:0.31199169903993607 | R2:0.5630734103252009 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[164] Loss:0.25295019894838333 | L1 Loss:0.3648653358221054 | R2:0.31794664039085474 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[165] Loss:0.16374616418033838 | L1 Loss:0.3089306326583028 | R2:0.5690593139277129 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[165] Loss:0.2517938204109669 | L1 Loss:0.3661424875259399 | R2:0.32142072117136894 | ACC: 76.3333%(229/300)\n",
            "Testing Epoch[166] Loss:0.16115066828206182 | L1 Loss:0.30671493895351887 | R2:0.5752763976352057 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[166] Loss:0.25517728328704836 | L1 Loss:0.364037948846817 | R2:0.31036311176584513 | ACC: 76.3333%(229/300)\n",
            "Testing Epoch[167] Loss:0.1598580228164792 | L1 Loss:0.3031465718522668 | R2:0.5791934477043115 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[167] Loss:0.254671335965395 | L1 Loss:0.3636949211359024 | R2:0.31373676563782726 | ACC: 76.6667%(230/300)\n",
            "Testing Epoch[168] Loss:0.15089724911376834 | L1 Loss:0.2960817152634263 | R2:0.6025906292709705 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[168] Loss:0.25543620586395266 | L1 Loss:0.3601986676454544 | R2:0.3102953631719339 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[169] Loss:0.1527908630669117 | L1 Loss:0.29774793330579996 | R2:0.5957077880483959 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[169] Loss:0.2497623346745968 | L1 Loss:0.3586466759443283 | R2:0.32388020162620795 | ACC: 76.6667%(230/300)\n",
            "Testing Epoch[170] Loss:0.14996413653716445 | L1 Loss:0.2965600276365876 | R2:0.6036924892543519 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[170] Loss:0.25352141112089155 | L1 Loss:0.36453446447849275 | R2:0.31601194793316756 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[171] Loss:0.1549948751926422 | L1 Loss:0.3027636185288429 | R2:0.5906236758967481 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[171] Loss:0.25175224170088767 | L1 Loss:0.36724194288253786 | R2:0.3217483509410871 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[172] Loss:0.16084477584809065 | L1 Loss:0.3098838496953249 | R2:0.5768184423090049 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[172] Loss:0.25334628373384477 | L1 Loss:0.3690358489751816 | R2:0.3202614028166753 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[173] Loss:0.16211933828890324 | L1 Loss:0.3079151213169098 | R2:0.5717507642149035 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[173] Loss:0.2576713070273399 | L1 Loss:0.3705294758081436 | R2:0.30672446009877563 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[174] Loss:0.16166316904127598 | L1 Loss:0.3067860808223486 | R2:0.5734360227494809 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[174] Loss:0.25796937942504883 | L1 Loss:0.36969434320926664 | R2:0.30665515847523817 | ACC: 76.6667%(230/300)\n",
            "Testing Epoch[175] Loss:0.16580301290377975 | L1 Loss:0.31076830066740513 | R2:0.5586165965931659 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[175] Loss:0.26313558369874956 | L1 Loss:0.37605064511299136 | R2:0.2907375505407194 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[176] Loss:0.16120976768434048 | L1 Loss:0.3083406165242195 | R2:0.5722472321793359 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[176] Loss:0.2625472828745842 | L1 Loss:0.371711465716362 | R2:0.29165356580406065 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[177] Loss:0.15835058875381947 | L1 Loss:0.3034912571310997 | R2:0.5757587285375849 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[177] Loss:0.2574248656630516 | L1 Loss:0.37170951813459396 | R2:0.30108937286833426 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[178] Loss:0.16288265492767096 | L1 Loss:0.30746558867394924 | R2:0.564565042718817 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[178] Loss:0.25651753023266793 | L1 Loss:0.36952199041843414 | R2:0.30671537190933507 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[179] Loss:0.16062585776671767 | L1 Loss:0.30531248450279236 | R2:0.5693882216794914 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[179] Loss:0.25761508867144584 | L1 Loss:0.3676967442035675 | R2:0.30064991694657 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[180] Loss:0.1563649051822722 | L1 Loss:0.30266569647938013 | R2:0.5814170534608946 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[180] Loss:0.2547840401530266 | L1 Loss:0.36829314976930616 | R2:0.3109809015482403 | ACC: 76.0000%(228/300)\n",
            "Testing Epoch[181] Loss:0.15897296695038676 | L1 Loss:0.30586488731205463 | R2:0.5738669336243778 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[181] Loss:0.259007154405117 | L1 Loss:0.3755908489227295 | R2:0.2994245836595771 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[182] Loss:0.1572927380912006 | L1 Loss:0.3056331779807806 | R2:0.5792469519942606 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[182] Loss:0.25869191735982894 | L1 Loss:0.3762276336550713 | R2:0.30170295146559994 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[183] Loss:0.15809187293052673 | L1 Loss:0.3051829021424055 | R2:0.5767068167006368 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[183] Loss:0.2594695597887039 | L1 Loss:0.3738943487405777 | R2:0.2921559110734601 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[184] Loss:0.15542173199355602 | L1 Loss:0.29967211466282606 | R2:0.5823964416134784 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[184] Loss:0.25123782008886336 | L1 Loss:0.367087784409523 | R2:0.3159677891442695 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[185] Loss:0.15670005790889263 | L1 Loss:0.29980051703751087 | R2:0.5810416706040862 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[185] Loss:0.24682193472981453 | L1 Loss:0.3596415907144547 | R2:0.3290422745641563 | ACC: 76.6667%(230/300)\n",
            "Testing Epoch[186] Loss:0.15632615331560373 | L1 Loss:0.30126896034926176 | R2:0.5832360978705363 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[186] Loss:0.24051513746380807 | L1 Loss:0.35475748479366304 | R2:0.3489497023260543 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[187] Loss:0.15531062660738826 | L1 Loss:0.30002269614487886 | R2:0.5870695686126997 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[187] Loss:0.24019850939512252 | L1 Loss:0.34821377247571944 | R2:0.34833881981349346 | ACC: 77.6667%(233/300)\n",
            "Testing Epoch[188] Loss:0.15640672761946917 | L1 Loss:0.30269419588148594 | R2:0.5839175952998434 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[188] Loss:0.2403782032430172 | L1 Loss:0.34806563556194303 | R2:0.3521524378501228 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[189] Loss:0.15570327313616872 | L1 Loss:0.30414558481425047 | R2:0.5857506654283671 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[189] Loss:0.2414986625313759 | L1 Loss:0.3502989232540131 | R2:0.34280055316414837 | ACC: 77.0000%(231/300)\n",
            "Testing Epoch[190] Loss:0.15499640442430973 | L1 Loss:0.3027602694928646 | R2:0.5889896143286238 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[190] Loss:0.2453891821205616 | L1 Loss:0.35039910674095154 | R2:0.3333882834428355 | ACC: 76.3333%(229/300)\n",
            "Testing Epoch[191] Loss:0.1544450968503952 | L1 Loss:0.30025115981698036 | R2:0.5902805378623567 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[191] Loss:0.2525050714612007 | L1 Loss:0.35779794454574587 | R2:0.3144310919517377 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[192] Loss:0.1544944867491722 | L1 Loss:0.3008609376847744 | R2:0.5899006991683278 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[192] Loss:0.24363560676574708 | L1 Loss:0.3518443524837494 | R2:0.33914657735327036 | ACC: 78.3333%(235/300)\n",
            "Testing Epoch[193] Loss:0.15810326905921102 | L1 Loss:0.3046928271651268 | R2:0.5815748845649228 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[193] Loss:0.23851460441946984 | L1 Loss:0.3495765835046768 | R2:0.3527981892988015 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[194] Loss:0.15299099870026112 | L1 Loss:0.2991057885810733 | R2:0.5960312811463255 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[194] Loss:0.23189863041043282 | L1 Loss:0.3450680568814278 | R2:0.37302114988344404 | ACC: 78.0000%(234/300)\n",
            "Testing Epoch[195] Loss:0.15452631656080484 | L1 Loss:0.29930806905031204 | R2:0.5917729607400006 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[195] Loss:0.23336877077817916 | L1 Loss:0.34599277526140215 | R2:0.36900761978530605 | ACC: 77.6667%(233/300)\n",
            "Testing Epoch[196] Loss:0.162660070694983 | L1 Loss:0.307219005189836 | R2:0.5710595163709721 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[196] Loss:0.23884050846099852 | L1 Loss:0.34895000159740447 | R2:0.35907916699236 | ACC: 77.3333%(232/300)\n",
            "Testing Epoch[197] Loss:0.16026141215115786 | L1 Loss:0.3053696369752288 | R2:0.5776998886745303 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[197] Loss:0.23928044289350509 | L1 Loss:0.34893073737621305 | R2:0.3600317936121703 | ACC: 76.3333%(229/300)\n",
            "Testing Epoch[198] Loss:0.158765172585845 | L1 Loss:0.3069727635011077 | R2:0.5819211521048316 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[198] Loss:0.24510103315114976 | L1 Loss:0.354375559091568 | R2:0.34124442170964403 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[199] Loss:0.15932296658866107 | L1 Loss:0.3091841693967581 | R2:0.5786266660130062 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[199] Loss:0.24227252155542373 | L1 Loss:0.35352844148874285 | R2:0.3505766827778981 | ACC: 75.6667%(227/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ MLP 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b16182-3cde-44fc-8026-ac15424bc2cc",
        "id": "qlErKyv6qzHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ MLP 200 epochs-----\n",
            "max test_r2_record  0.37302114988344404\n",
            "min test_loss_l1_record  0.3450680568814278\n",
            "min test_loss_record  0.23189863041043282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ MLP 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466def5a-98de-42e3-83e1-2c499d9a4da9",
        "id": "hkwfhoAaqzHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ MLP 100 epochs-----\n",
            "max test_r2_record  0.3202273865438901\n",
            "min test_loss_l1_record  0.36992483735084536\n",
            "min test_loss_record  0.2564698293805122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ MLP 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "fbe1fc2f-b7ac-4fa7-ae5d-84dc4787c74e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guHmxwXFqzHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ MLP 50 epochs-----\n",
            "max test_r2_record  0.28181404451967007\n",
            "min test_loss_l1_record  0.3914389729499817\n",
            "min test_loss_record  0.2684503376483917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "KlitWbPYasC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_mrna = get_train_config()\n",
        "model_class = Classificaion_module(config_mrna).cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "h4AGJICuuQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483e3c32-68e7-499d-96f8-034ec42504a4",
        "id": "JiVr_2fWuQ47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.0008, weight_decay=0.0001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "15s84bXZuQ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92561e6-4ab6-4cce-98cb-cd9ff2dfeebd",
        "id": "nT8vxDwVuQ47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (bert_module): BERT(\n",
              "    (embedding): Embedding(\n",
              "      (tok_embed): Embedding(8, 64)\n",
              "      (pos_embed): Embedding(30, 64)\n",
              "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (enc_self_attn): MultiHeadAttention(\n",
              "          (W_Q): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (W_K): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (W_V): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (linear): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (pos_ffn): PoswiseFeedForwardNet(\n",
              "          (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (enc_self_attn): MultiHeadAttention(\n",
              "          (W_Q): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (W_K): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (W_V): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (linear): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (pos_ffn): PoswiseFeedForwardNet(\n",
              "          (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (enc_self_attn): MultiHeadAttention(\n",
              "          (W_Q): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (W_K): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (W_V): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (linear): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (pos_ffn): PoswiseFeedForwardNet(\n",
              "          (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fc_task): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=32, out_features=2, bias=True)\n",
              "    )\n",
              "    (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=428, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(test_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "mqdv8YRuuQ48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(val_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    label = label.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "hLBCKsViuQ48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(200)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a1aab2-0160-4da0-a75a-13c4757ad0cd",
        "id": "MjTRq1IRuQ48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTesting Epoch[0] Loss:0.3247112249955535 | L1 Loss:0.42460986971855164 | R2:0.1437690896913646 | ACC: 62.6000%(313/500)\n",
            "Testing Epoch[0] Loss:0.4063527524471283 | L1 Loss:0.48019022941589357 | R2:0.011191200061615825 | ACC: 58.3333%(175/300)\n",
            "Testing Epoch[1] Loss:0.2979937558993697 | L1 Loss:0.408096881583333 | R2:0.21273461537958555 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[1] Loss:0.3881159335374832 | L1 Loss:0.4692078113555908 | R2:0.05238336934526059 | ACC: 59.6667%(179/300)\n",
            "Testing Epoch[2] Loss:0.27653005812317133 | L1 Loss:0.3954276368021965 | R2:0.27357330531968815 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[2] Loss:0.3704243928194046 | L1 Loss:0.46286865174770353 | R2:0.09767300242000274 | ACC: 61.3333%(184/300)\n",
            "Testing Epoch[3] Loss:0.2585213938727975 | L1 Loss:0.38693030923604965 | R2:0.32119173199035594 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[3] Loss:0.3516015112400055 | L1 Loss:0.450244602560997 | R2:0.1434514523989164 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[4] Loss:0.2423085719347 | L1 Loss:0.3749153483659029 | R2:0.36191624697735086 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[4] Loss:0.34644297063350676 | L1 Loss:0.44937529861927034 | R2:0.15670865567608988 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[5] Loss:0.2265360662713647 | L1 Loss:0.3617638535797596 | R2:0.40181622842292414 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[5] Loss:0.3422141820192337 | L1 Loss:0.4433571547269821 | R2:0.1697212081566349 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[6] Loss:0.22145477961748838 | L1 Loss:0.36035509407520294 | R2:0.4127391655343167 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[6] Loss:0.33806327283382415 | L1 Loss:0.4396438479423523 | R2:0.18117413341391164 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[7] Loss:0.21173239639028907 | L1 Loss:0.35059430077672005 | R2:0.4410943380701146 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[7] Loss:0.33065906167030334 | L1 Loss:0.43294153809547425 | R2:0.19950979501552585 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[8] Loss:0.21189683163538575 | L1 Loss:0.3505565505474806 | R2:0.43881405569822285 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[8] Loss:0.3297909885644913 | L1 Loss:0.43356287479400635 | R2:0.19882098548334318 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[9] Loss:0.2098889914341271 | L1 Loss:0.345911780372262 | R2:0.4446605713054034 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[9] Loss:0.3221971571445465 | L1 Loss:0.43539691269397734 | R2:0.2137530551018792 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[10] Loss:0.2025602082721889 | L1 Loss:0.3400063905864954 | R2:0.46207251974805186 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[10] Loss:0.30769931972026826 | L1 Loss:0.4246756911277771 | R2:0.2533110436241882 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[11] Loss:0.20190729154273868 | L1 Loss:0.3445829562842846 | R2:0.45903884587472177 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[11] Loss:0.31380248367786406 | L1 Loss:0.4301708906888962 | R2:0.2362855898001678 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[12] Loss:0.2004771106876433 | L1 Loss:0.340751463547349 | R2:0.46638716612276276 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[12] Loss:0.3141336217522621 | L1 Loss:0.43351989090442655 | R2:0.24307457945974295 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[13] Loss:0.21749723237007856 | L1 Loss:0.3637292180210352 | R2:0.41560059989608517 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[13] Loss:0.31801335513591766 | L1 Loss:0.4374089241027832 | R2:0.2303624175178057 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[14] Loss:0.19755050214007497 | L1 Loss:0.3433432187885046 | R2:0.4633678512333806 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[14] Loss:0.28493933826684953 | L1 Loss:0.41192711889743805 | R2:0.31487517236170304 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[15] Loss:0.20419476088136435 | L1 Loss:0.3447896707803011 | R2:0.44417228624309046 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[15] Loss:0.28592823147773744 | L1 Loss:0.4162687212228775 | R2:0.3107514353142206 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[16] Loss:0.1934991590678692 | L1 Loss:0.33901660330593586 | R2:0.4813778026160309 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[16] Loss:0.2678287461400032 | L1 Loss:0.39517955482006073 | R2:0.35527207132922234 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[17] Loss:0.19819522323086858 | L1 Loss:0.34419603273272514 | R2:0.47074186093948595 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[17] Loss:0.28010826259851457 | L1 Loss:0.4038008153438568 | R2:0.32494179575346704 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[18] Loss:0.20218330528587103 | L1 Loss:0.34401447512209415 | R2:0.45211140731695587 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[18] Loss:0.2815573126077652 | L1 Loss:0.40403841733932494 | R2:0.32504459553290166 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[19] Loss:0.20487275160849094 | L1 Loss:0.3491244884207845 | R2:0.45107633375456285 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[19] Loss:0.2791174963116646 | L1 Loss:0.4018307596445084 | R2:0.32539313050609253 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[20] Loss:0.19820121442899108 | L1 Loss:0.3408900685608387 | R2:0.47099573969559955 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[20] Loss:0.26455709636211394 | L1 Loss:0.3920943856239319 | R2:0.36623230131394 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[21] Loss:0.19481511041522026 | L1 Loss:0.34023331571370363 | R2:0.480316963947143 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[21] Loss:0.2660326585173607 | L1 Loss:0.39651053547859194 | R2:0.3645913877381832 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[22] Loss:0.20214100554585457 | L1 Loss:0.353191502392292 | R2:0.4585097681981416 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[22] Loss:0.2763258844614029 | L1 Loss:0.4053595751523972 | R2:0.33948730826286655 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[23] Loss:0.1931566596031189 | L1 Loss:0.3426075140014291 | R2:0.47855395066983664 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[23] Loss:0.2748491778969765 | L1 Loss:0.4018933743238449 | R2:0.3410052229174595 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[24] Loss:0.19612114829942584 | L1 Loss:0.344920726493001 | R2:0.4693635968581562 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[24] Loss:0.26637873202562334 | L1 Loss:0.40002231001853944 | R2:0.36036888176890897 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[25] Loss:0.1869258456863463 | L1 Loss:0.33526332676410675 | R2:0.48973326395344974 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[25] Loss:0.27841856628656386 | L1 Loss:0.41045875251293185 | R2:0.33124102930463384 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[26] Loss:0.18643014319241047 | L1 Loss:0.33848980627954006 | R2:0.4914950327613962 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[26] Loss:0.2887775033712387 | L1 Loss:0.42054588794708253 | R2:0.3035557224676157 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[27] Loss:0.18589343782514334 | L1 Loss:0.3371437918394804 | R2:0.4948075038105866 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[27] Loss:0.27643679827451706 | L1 Loss:0.4115417867898941 | R2:0.335122294129368 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[28] Loss:0.18557940097525716 | L1 Loss:0.33318230137228966 | R2:0.4955752207938056 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[28] Loss:0.2860488459467888 | L1 Loss:0.4138182520866394 | R2:0.3105316314029069 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[29] Loss:0.1928630224429071 | L1 Loss:0.33566589653491974 | R2:0.4831899827093572 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[29] Loss:0.28139554262161254 | L1 Loss:0.4057825535535812 | R2:0.3274307780489506 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[30] Loss:0.1866050846874714 | L1 Loss:0.3328925287351012 | R2:0.49781225107044774 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[30] Loss:0.27621638774871826 | L1 Loss:0.40005687773227694 | R2:0.33847419909149334 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[31] Loss:0.18086268659681082 | L1 Loss:0.32955154217779636 | R2:0.5137784303383556 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[31] Loss:0.273862124979496 | L1 Loss:0.39747189581394193 | R2:0.34618825552968635 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[32] Loss:0.18643773719668388 | L1 Loss:0.3380888570100069 | R2:0.4946088524443028 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[32] Loss:0.28073952794075013 | L1 Loss:0.40384580492973327 | R2:0.33386660439155735 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[33] Loss:0.19487140094861388 | L1 Loss:0.34847794845700264 | R2:0.4675837900325263 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[33] Loss:0.2807935237884521 | L1 Loss:0.4086450845003128 | R2:0.337268872910058 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[34] Loss:0.20362023683264852 | L1 Loss:0.3536997679620981 | R2:0.44510197503011134 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[34] Loss:0.3011299714446068 | L1 Loss:0.42290525138378143 | R2:0.28923260406271883 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[35] Loss:0.1921428730711341 | L1 Loss:0.3437064727768302 | R2:0.4800821323988284 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[35] Loss:0.2958793193101883 | L1 Loss:0.4200215309858322 | R2:0.2990965817529563 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[36] Loss:0.18936959700658917 | L1 Loss:0.33816512674093246 | R2:0.4920423408426409 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[36] Loss:0.2705102160573006 | L1 Loss:0.4022369772195816 | R2:0.35829386232293126 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[37] Loss:0.19573219260200858 | L1 Loss:0.3465641839429736 | R2:0.4733596770755244 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[37] Loss:0.27636844515800474 | L1 Loss:0.4036464780569077 | R2:0.34432906953988696 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[38] Loss:0.1861576410010457 | L1 Loss:0.341109374538064 | R2:0.49996652859495544 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[38] Loss:0.2710925072431564 | L1 Loss:0.3961304485797882 | R2:0.3606167903744877 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[39] Loss:0.19089196808636189 | L1 Loss:0.34402214363217354 | R2:0.48788843854633124 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[39] Loss:0.27413649410009383 | L1 Loss:0.3948596179485321 | R2:0.35129372286973565 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[40] Loss:0.19250531401485205 | L1 Loss:0.34295762330293655 | R2:0.47949572790290435 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[40] Loss:0.2889625996351242 | L1 Loss:0.4080854505300522 | R2:0.31582995339701964 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[41] Loss:0.19015583442524076 | L1 Loss:0.3424487393349409 | R2:0.49123926347696234 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[41] Loss:0.2927845552563667 | L1 Loss:0.40987012088298796 | R2:0.30926644304094125 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[42] Loss:0.1896034935489297 | L1 Loss:0.34031540993601084 | R2:0.4893397232151566 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[42] Loss:0.30087257027626035 | L1 Loss:0.4136600524187088 | R2:0.2932947748695201 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[43] Loss:0.17482179636135697 | L1 Loss:0.32558768428862095 | R2:0.5272110480283417 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[43] Loss:0.29464105516672134 | L1 Loss:0.41338457763195036 | R2:0.3052160791422862 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[44] Loss:0.17670825961977243 | L1 Loss:0.32901918701827526 | R2:0.5182183803755258 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[44] Loss:0.3028911933302879 | L1 Loss:0.4211418807506561 | R2:0.28809501506026064 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[45] Loss:0.17151349782943726 | L1 Loss:0.3261903878301382 | R2:0.5359303228369765 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[45] Loss:0.287737238407135 | L1 Loss:0.405423304438591 | R2:0.3182535427752175 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[46] Loss:0.17411460541188717 | L1 Loss:0.32770004123449326 | R2:0.5244838399438404 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[46] Loss:0.28313642293214797 | L1 Loss:0.4034177154302597 | R2:0.3299234360782557 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[47] Loss:0.17307967320084572 | L1 Loss:0.3287558052688837 | R2:0.5282961148679082 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[47] Loss:0.2777588039636612 | L1 Loss:0.39912590086460115 | R2:0.3395515936025527 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[48] Loss:0.17687259148806334 | L1 Loss:0.3297309810295701 | R2:0.5158958004917858 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[48] Loss:0.2756974071264267 | L1 Loss:0.40602787435054777 | R2:0.34467296610054543 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[49] Loss:0.17465939978137612 | L1 Loss:0.32546188961714506 | R2:0.5225708474412524 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[49] Loss:0.27943848073482513 | L1 Loss:0.40721487402915957 | R2:0.33827324841141077 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[50] Loss:0.1786958472803235 | L1 Loss:0.3303399085998535 | R2:0.5099164530491673 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[50] Loss:0.2781151279807091 | L1 Loss:0.3997424691915512 | R2:0.3400699411959518 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[51] Loss:0.18452078569680452 | L1 Loss:0.32991402223706245 | R2:0.4946428609202118 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[51] Loss:0.2642382010817528 | L1 Loss:0.3908034205436707 | R2:0.3665149173943201 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[52] Loss:0.1894816500134766 | L1 Loss:0.33312778547406197 | R2:0.47852182168668234 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[52] Loss:0.26332771629095075 | L1 Loss:0.39058192670345304 | R2:0.37104756708071607 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[53] Loss:0.18841037014499307 | L1 Loss:0.3340517934411764 | R2:0.48895231509668824 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[53] Loss:0.2785008057951927 | L1 Loss:0.40270455479621886 | R2:0.3317956337255993 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[54] Loss:0.19068156369030476 | L1 Loss:0.33903524093329906 | R2:0.48441206519311275 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[54] Loss:0.2828424945473671 | L1 Loss:0.40414755046367645 | R2:0.32115402455663666 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[55] Loss:0.18558008642867208 | L1 Loss:0.3300981633365154 | R2:0.501487642605043 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[55] Loss:0.27877349257469175 | L1 Loss:0.40035033226013184 | R2:0.33249506873898005 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[56] Loss:0.18759648967534304 | L1 Loss:0.3333037383854389 | R2:0.4944858919819931 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[56] Loss:0.29521456062793733 | L1 Loss:0.40787544250488283 | R2:0.2904062677691458 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[57] Loss:0.19285974139347672 | L1 Loss:0.336505513638258 | R2:0.48172004276101105 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[57] Loss:0.29735576659440993 | L1 Loss:0.4178679078817368 | R2:0.28817623860462616 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[58] Loss:0.1951650744304061 | L1 Loss:0.33971554040908813 | R2:0.47214940245593146 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[58] Loss:0.29642085283994674 | L1 Loss:0.410495924949646 | R2:0.29060821469028697 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[59] Loss:0.19001116650179029 | L1 Loss:0.3386399317532778 | R2:0.48757836711954927 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[59] Loss:0.29786172658205035 | L1 Loss:0.4138605952262878 | R2:0.2899013589812182 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[60] Loss:0.20233154157176614 | L1 Loss:0.3447435349225998 | R2:0.45332859991156554 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[60] Loss:0.3207951545715332 | L1 Loss:0.4254339009523392 | R2:0.2332313549147312 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[61] Loss:0.1970759043470025 | L1 Loss:0.3406206611543894 | R2:0.4678344922716735 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[61] Loss:0.33229083120822905 | L1 Loss:0.4326703757047653 | R2:0.20703625628406944 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[62] Loss:0.2018598159775138 | L1 Loss:0.34648515470325947 | R2:0.45308427455083494 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[62] Loss:0.3258041933178902 | L1 Loss:0.4330761551856995 | R2:0.21818639286228936 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[63] Loss:0.19689912721514702 | L1 Loss:0.33915906585752964 | R2:0.4663720995637098 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[63] Loss:0.31879938542842867 | L1 Loss:0.43031384646892545 | R2:0.23594180882833177 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[64] Loss:0.182073594070971 | L1 Loss:0.3314100354909897 | R2:0.506248540612772 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[64] Loss:0.30980631411075593 | L1 Loss:0.42085844576358794 | R2:0.2594610286445935 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[65] Loss:0.17523105535656214 | L1 Loss:0.3236493244767189 | R2:0.5230618840869091 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[65] Loss:0.29680963754653933 | L1 Loss:0.4191239714622498 | R2:0.2935235433493549 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[66] Loss:0.17269090469926596 | L1 Loss:0.3255424704402685 | R2:0.5305915446462175 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[66] Loss:0.3020989432930946 | L1 Loss:0.4221967548131943 | R2:0.27532436963027407 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[67] Loss:0.17726635886356235 | L1 Loss:0.32752474769949913 | R2:0.5182167712148332 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[67] Loss:0.2914225235581398 | L1 Loss:0.4182885855436325 | R2:0.3025762769950414 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[68] Loss:0.1842176360078156 | L1 Loss:0.33219053130596876 | R2:0.4998287344223432 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[68] Loss:0.2993877664208412 | L1 Loss:0.42417425811290743 | R2:0.2758046382801877 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[69] Loss:0.182488517370075 | L1 Loss:0.3302757078781724 | R2:0.5034076784335542 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[69] Loss:0.2907601878046989 | L1 Loss:0.41344507932662966 | R2:0.29632218504589486 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[70] Loss:0.17581504117697477 | L1 Loss:0.3202336262911558 | R2:0.5190985562900536 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[70] Loss:0.28807403445243834 | L1 Loss:0.40741102397441864 | R2:0.3067309684284576 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[71] Loss:0.17342084040865302 | L1 Loss:0.3188134990632534 | R2:0.5250182864342932 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[71] Loss:0.28904184252023696 | L1 Loss:0.40971109867095945 | R2:0.3015970268378989 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[72] Loss:0.16849750885739923 | L1 Loss:0.31392418686300516 | R2:0.5433661049194535 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[72] Loss:0.2878704383969307 | L1 Loss:0.4080642342567444 | R2:0.3023914948991482 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[73] Loss:0.16557284956797957 | L1 Loss:0.31151197105646133 | R2:0.5522084596328731 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[73] Loss:0.2876446947455406 | L1 Loss:0.409677192568779 | R2:0.3073917962067588 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[74] Loss:0.1695303930900991 | L1 Loss:0.3131223423406482 | R2:0.541578603430441 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[74] Loss:0.29554322361946106 | L1 Loss:0.42062513530254364 | R2:0.2910184173696996 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[75] Loss:0.16590646747499704 | L1 Loss:0.3093262407928705 | R2:0.5512219990209863 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[75] Loss:0.2963886171579361 | L1 Loss:0.4217565208673477 | R2:0.28884740022858824 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[76] Loss:0.1640256061218679 | L1 Loss:0.3053878927603364 | R2:0.5542914865387575 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[76] Loss:0.2904991865158081 | L1 Loss:0.4108726292848587 | R2:0.3080525204418746 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[77] Loss:0.1678654090501368 | L1 Loss:0.312138675712049 | R2:0.5408624774674939 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[77] Loss:0.28943642377853396 | L1 Loss:0.4065611869096756 | R2:0.31037293766248436 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[78] Loss:0.17277570068836212 | L1 Loss:0.3182562766596675 | R2:0.5280000211960226 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[78] Loss:0.28768145442008974 | L1 Loss:0.41048915684223175 | R2:0.31457390269671115 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[79] Loss:0.16814249521121383 | L1 Loss:0.31786667834967375 | R2:0.5391991515290372 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[79] Loss:0.270188544690609 | L1 Loss:0.39826522171497347 | R2:0.35278132296981773 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[80] Loss:0.1605022414587438 | L1 Loss:0.3117477633059025 | R2:0.5593241229169612 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[80] Loss:0.265872023999691 | L1 Loss:0.395512068271637 | R2:0.36324523813243625 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[81] Loss:0.15687325224280357 | L1 Loss:0.30709507782012224 | R2:0.5725344033164578 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[81] Loss:0.2600714057683945 | L1 Loss:0.3935728698968887 | R2:0.3766886576266254 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[82] Loss:0.16206176578998566 | L1 Loss:0.3104473929852247 | R2:0.5587769005346119 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[82] Loss:0.2795079559087753 | L1 Loss:0.4020834892988205 | R2:0.32725025878574787 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[83] Loss:0.16010501980781555 | L1 Loss:0.3118136180564761 | R2:0.5628660482564438 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[83] Loss:0.2755819171667099 | L1 Loss:0.4104864507913589 | R2:0.336625830935942 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[84] Loss:0.16422161739319563 | L1 Loss:0.3117393655702472 | R2:0.5533528610546344 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[84] Loss:0.2746333688497543 | L1 Loss:0.40471288859844207 | R2:0.3396748201151386 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[85] Loss:0.16599226742982864 | L1 Loss:0.31515844259411097 | R2:0.5505847210014799 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[85] Loss:0.27132391929626465 | L1 Loss:0.4061092913150787 | R2:0.3464416346859211 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[86] Loss:0.16859097545966506 | L1 Loss:0.31906276661902666 | R2:0.5454167378427202 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[86] Loss:0.2762632265686989 | L1 Loss:0.41083653569221495 | R2:0.335375823150741 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[87] Loss:0.17652980890125036 | L1 Loss:0.32575737312436104 | R2:0.5239583798147935 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[87] Loss:0.26850106865167617 | L1 Loss:0.40693407952785493 | R2:0.3582967650736218 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[88] Loss:0.17009023670107126 | L1 Loss:0.31715745106339455 | R2:0.5401166685776441 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[88] Loss:0.26362604945898055 | L1 Loss:0.40508137047290804 | R2:0.3686846521749392 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[89] Loss:0.16675010649487376 | L1 Loss:0.31414742581546307 | R2:0.5490895166661411 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[89] Loss:0.2630601808428764 | L1 Loss:0.3931183457374573 | R2:0.36983542176985146 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[90] Loss:0.17210792982950807 | L1 Loss:0.3199232239276171 | R2:0.5328427956295474 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[90] Loss:0.24860855489969252 | L1 Loss:0.3853272259235382 | R2:0.4034281074029506 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[91] Loss:0.17355642979964614 | L1 Loss:0.3207501955330372 | R2:0.5298051666675931 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[91] Loss:0.2643336459994316 | L1 Loss:0.38991917073726656 | R2:0.3706609276084745 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[92] Loss:0.16683495370671153 | L1 Loss:0.3141458071768284 | R2:0.5498364167933404 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[92] Loss:0.2595093309879303 | L1 Loss:0.3837520211935043 | R2:0.3797749480278733 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[93] Loss:0.17515895189717412 | L1 Loss:0.31772819347679615 | R2:0.5237233130331855 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[93] Loss:0.2540779411792755 | L1 Loss:0.3847478270530701 | R2:0.39096413558126586 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[94] Loss:0.17556656198576093 | L1 Loss:0.3202531021088362 | R2:0.5252582697890973 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[94] Loss:0.2671637088060379 | L1 Loss:0.3924321740865707 | R2:0.36411176377672344 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[95] Loss:0.167672922834754 | L1 Loss:0.3138285093009472 | R2:0.5474764479456194 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[95] Loss:0.2696709156036377 | L1 Loss:0.3983117938041687 | R2:0.3580256467713605 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[96] Loss:0.17221601773053408 | L1 Loss:0.3179737403988838 | R2:0.5326948808130372 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[96] Loss:0.27004922926425934 | L1 Loss:0.39851455986499784 | R2:0.35822708248757384 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[97] Loss:0.18163547897711396 | L1 Loss:0.32596957217901945 | R2:0.5086377641718609 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[97] Loss:0.26680504679679873 | L1 Loss:0.4017289370298386 | R2:0.3627431469228983 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[98] Loss:0.17892711237072945 | L1 Loss:0.32649231143295765 | R2:0.5137951480254177 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[98] Loss:0.26031963974237443 | L1 Loss:0.39413862228393554 | R2:0.3758480601947751 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[99] Loss:0.1752519365400076 | L1 Loss:0.3247188162058592 | R2:0.5232373083983608 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[99] Loss:0.2540583834052086 | L1 Loss:0.3893075793981552 | R2:0.3920195885088602 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[100] Loss:0.1769213927909732 | L1 Loss:0.3233516626060009 | R2:0.5157734219663744 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[100] Loss:0.2483073115348816 | L1 Loss:0.3851917952299118 | R2:0.40550241022456135 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[101] Loss:0.18406084878370166 | L1 Loss:0.334614765830338 | R2:0.5008791767462312 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[101] Loss:0.2615065395832062 | L1 Loss:0.3984606057405472 | R2:0.3733324841519613 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[102] Loss:0.18309092475101352 | L1 Loss:0.3312578210607171 | R2:0.503950795707988 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[102] Loss:0.2588359206914902 | L1 Loss:0.3948772221803665 | R2:0.3812008274997611 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[103] Loss:0.1740990150719881 | L1 Loss:0.3268508380278945 | R2:0.5265204014892048 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[103] Loss:0.25937420725822447 | L1 Loss:0.392631271481514 | R2:0.3796661936487404 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[104] Loss:0.17279547126963735 | L1 Loss:0.32088343519717455 | R2:0.5312766284873724 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[104] Loss:0.2607779070734978 | L1 Loss:0.39424569308757784 | R2:0.37894805771578477 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[105] Loss:0.16866328194737434 | L1 Loss:0.31276117265224457 | R2:0.542469720077882 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[105] Loss:0.2620865821838379 | L1 Loss:0.397844460606575 | R2:0.3740370556928596 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[106] Loss:0.16417214460670948 | L1 Loss:0.30909003876149654 | R2:0.5576374789129882 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[106] Loss:0.2548894599080086 | L1 Loss:0.394220769405365 | R2:0.3905495050926564 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[107] Loss:0.15996875939890742 | L1 Loss:0.30372462794184685 | R2:0.5686098667865399 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[107] Loss:0.26164150387048724 | L1 Loss:0.3983727812767029 | R2:0.37780050952734917 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[108] Loss:0.16228076769039035 | L1 Loss:0.30671353451907635 | R2:0.5678542547960215 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[108] Loss:0.25335632264614105 | L1 Loss:0.3903604418039322 | R2:0.39526371537531546 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[109] Loss:0.15954139176756144 | L1 Loss:0.30381389800459146 | R2:0.5758013119996608 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[109] Loss:0.2572632521390915 | L1 Loss:0.3911621242761612 | R2:0.38737117880773564 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[110] Loss:0.161833212710917 | L1 Loss:0.3055019285529852 | R2:0.5671860884908293 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[110] Loss:0.2606398046016693 | L1 Loss:0.3923762798309326 | R2:0.3751034971481838 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[111] Loss:0.1646207761950791 | L1 Loss:0.310948827303946 | R2:0.5568167235553176 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[111] Loss:0.2501203820109367 | L1 Loss:0.38871288001537324 | R2:0.40064398767951737 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[112] Loss:0.1681411974132061 | L1 Loss:0.31454612873494625 | R2:0.5475517140180801 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[112] Loss:0.24500779956579208 | L1 Loss:0.3868507266044617 | R2:0.41063914497516985 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[113] Loss:0.16222452465444803 | L1 Loss:0.30784427281469107 | R2:0.5605198684551319 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[113] Loss:0.24186953157186508 | L1 Loss:0.3811329364776611 | R2:0.4180822578842963 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[114] Loss:0.1653441651724279 | L1 Loss:0.3098765518516302 | R2:0.5501472968806412 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[114] Loss:0.2485886737704277 | L1 Loss:0.3862460911273956 | R2:0.4030003430403973 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[115] Loss:0.164305308368057 | L1 Loss:0.3073461502790451 | R2:0.5520358031399887 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[115] Loss:0.24686513394117354 | L1 Loss:0.3797292858362198 | R2:0.4063005760358619 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[116] Loss:0.163569123018533 | L1 Loss:0.3099590092897415 | R2:0.5552214959994204 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[116] Loss:0.25902068614959717 | L1 Loss:0.3878178298473358 | R2:0.38105578875666496 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[117] Loss:0.1633042050525546 | L1 Loss:0.30898938328027725 | R2:0.5561654955899914 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[117] Loss:0.25651139616966245 | L1 Loss:0.38426522314548495 | R2:0.3863287471086813 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[118] Loss:0.162757970392704 | L1 Loss:0.3079941626638174 | R2:0.5595514238941375 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[118] Loss:0.2673364594578743 | L1 Loss:0.39488792419433594 | R2:0.3610002154542472 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[119] Loss:0.16123975347727537 | L1 Loss:0.30459754168987274 | R2:0.564035981748698 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[119] Loss:0.26727758049964906 | L1 Loss:0.395378053188324 | R2:0.36236226007936784 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[120] Loss:0.15949383936822414 | L1 Loss:0.30451399832963943 | R2:0.5663463436401094 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[120] Loss:0.2612928092479706 | L1 Loss:0.39175678193569186 | R2:0.37319899706057946 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[121] Loss:0.15410742163658142 | L1 Loss:0.2997534638270736 | R2:0.5811423770054862 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[121] Loss:0.2618714481592178 | L1 Loss:0.3871040463447571 | R2:0.3721273333607868 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[122] Loss:0.15458628255873919 | L1 Loss:0.30121371801942587 | R2:0.5775611203280321 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[122] Loss:0.2598757669329643 | L1 Loss:0.3882899880409241 | R2:0.380107069107756 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[123] Loss:0.15138620929792523 | L1 Loss:0.2968474365770817 | R2:0.5853928982429865 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[123] Loss:0.250227390229702 | L1 Loss:0.38024917542934417 | R2:0.3999943194625841 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[124] Loss:0.1534124012105167 | L1 Loss:0.29982748068869114 | R2:0.5805228764011147 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[124] Loss:0.24282853454351425 | L1 Loss:0.37801688313484194 | R2:0.4159736143406782 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[125] Loss:0.15425631729885936 | L1 Loss:0.30034313537180424 | R2:0.5762547197309049 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[125] Loss:0.2433549255132675 | L1 Loss:0.37868604362010955 | R2:0.415248413320468 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[126] Loss:0.1526875039562583 | L1 Loss:0.29413220193237066 | R2:0.5836924568341207 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[126] Loss:0.24532085210084914 | L1 Loss:0.38219315409660337 | R2:0.41175534775480094 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[127] Loss:0.14746312098577619 | L1 Loss:0.28866517450660467 | R2:0.5986014199298828 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[127] Loss:0.23553833663463591 | L1 Loss:0.3722775161266327 | R2:0.4344730945723527 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[128] Loss:0.14787127682939172 | L1 Loss:0.2909560715779662 | R2:0.5989205434645385 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[128] Loss:0.2428130954504013 | L1 Loss:0.3812839508056641 | R2:0.4186640774117304 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[129] Loss:0.14932892797514796 | L1 Loss:0.2931558908894658 | R2:0.5957134388353758 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[129] Loss:0.2481316566467285 | L1 Loss:0.3898041516542435 | R2:0.40422074449196055 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[130] Loss:0.14971732581034303 | L1 Loss:0.2932968521490693 | R2:0.5958478518971639 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[130] Loss:0.24114074259996415 | L1 Loss:0.38194177150726316 | R2:0.4202811972094541 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[131] Loss:0.15437841415405273 | L1 Loss:0.2945170272141695 | R2:0.5823983462300643 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[131] Loss:0.2511545941233635 | L1 Loss:0.3826062262058258 | R2:0.3922494992790952 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[132] Loss:0.15585998445749283 | L1 Loss:0.2972607100382447 | R2:0.5763961928610826 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[132] Loss:0.2517727047204971 | L1 Loss:0.38929945826530454 | R2:0.39281857805100207 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[133] Loss:0.15755203645676374 | L1 Loss:0.300892049446702 | R2:0.5713215217671386 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[133] Loss:0.25811831951141356 | L1 Loss:0.3917666167020798 | R2:0.3776025343903443 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[134] Loss:0.16185431741178036 | L1 Loss:0.30642594024538994 | R2:0.5596976445564742 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[134] Loss:0.26965595632791517 | L1 Loss:0.4020535409450531 | R2:0.35224370940100613 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[135] Loss:0.15959939267486334 | L1 Loss:0.3038339652121067 | R2:0.5649190707652455 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[135] Loss:0.2619791850447655 | L1 Loss:0.3901266485452652 | R2:0.3687363128666682 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[136] Loss:0.15909138042479753 | L1 Loss:0.30358657613396645 | R2:0.5655527609682165 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[136] Loss:0.2562463119626045 | L1 Loss:0.38898937702178954 | R2:0.3794033569600336 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[137] Loss:0.16162459272891283 | L1 Loss:0.3074481263756752 | R2:0.5581858488318157 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[137] Loss:0.2531172677874565 | L1 Loss:0.384115469455719 | R2:0.38767982652271515 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[138] Loss:0.1604158589616418 | L1 Loss:0.30345164611935616 | R2:0.561040469755307 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[138] Loss:0.24616202861070632 | L1 Loss:0.37866889834403994 | R2:0.4047468662655659 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[139] Loss:0.15790088288486004 | L1 Loss:0.3033499214798212 | R2:0.5678446488297091 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[139] Loss:0.24189606457948684 | L1 Loss:0.37385779321193696 | R2:0.41317159715238405 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[140] Loss:0.1556718247011304 | L1 Loss:0.2989545725286007 | R2:0.5762193407235952 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[140] Loss:0.24860625118017196 | L1 Loss:0.38139899969100954 | R2:0.3969399415808026 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[141] Loss:0.1543147023767233 | L1 Loss:0.29586715064942837 | R2:0.5759333344566712 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[141] Loss:0.24521488696336746 | L1 Loss:0.3767098098993301 | R2:0.4047607606433692 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[142] Loss:0.15315910009667277 | L1 Loss:0.2955924002453685 | R2:0.5801990758083725 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[142] Loss:0.2522980898618698 | L1 Loss:0.38016181290149687 | R2:0.3903800195337663 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[143] Loss:0.15218896977603436 | L1 Loss:0.2937631784006953 | R2:0.5840582702929885 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[143] Loss:0.24280868470668793 | L1 Loss:0.3721492201089859 | R2:0.411526472043239 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[144] Loss:0.1488251001574099 | L1 Loss:0.2876789905130863 | R2:0.5978288044657325 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[144] Loss:0.24238789528608323 | L1 Loss:0.3728474587202072 | R2:0.4161452626316735 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[145] Loss:0.15512413531541824 | L1 Loss:0.2976678218692541 | R2:0.5815188321353973 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[145] Loss:0.23865057528018951 | L1 Loss:0.3709591209888458 | R2:0.42438244656058705 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[146] Loss:0.15014899661764503 | L1 Loss:0.29271616227924824 | R2:0.595738064972271 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[146] Loss:0.2390281930565834 | L1 Loss:0.36907343864440917 | R2:0.4256996709647991 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[147] Loss:0.1506823431700468 | L1 Loss:0.2933480041101575 | R2:0.5921348898459878 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[147] Loss:0.23726461231708526 | L1 Loss:0.369599711894989 | R2:0.4307725503486039 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[148] Loss:0.14430289156734943 | L1 Loss:0.28461274690926075 | R2:0.6102351032312326 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[148] Loss:0.24831332862377167 | L1 Loss:0.3782904654741287 | R2:0.4033488856720321 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[149] Loss:0.14907019026577473 | L1 Loss:0.2917135553434491 | R2:0.5979817625650959 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[149] Loss:0.2580911234021187 | L1 Loss:0.38559689521789553 | R2:0.37962775454683484 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[150] Loss:0.1540291029959917 | L1 Loss:0.29792059399187565 | R2:0.5836406938347032 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[150] Loss:0.25196677446365356 | L1 Loss:0.38337181210517884 | R2:0.39332511772311496 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[151] Loss:0.15707834297791123 | L1 Loss:0.30246658995747566 | R2:0.5772569502706419 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[151] Loss:0.2642397478222847 | L1 Loss:0.3919157087802887 | R2:0.361197447431339 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[152] Loss:0.15846926486119628 | L1 Loss:0.3041954692453146 | R2:0.569967677956401 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[152] Loss:0.25726315230131147 | L1 Loss:0.3848318636417389 | R2:0.37801698805856443 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[153] Loss:0.16131375962868333 | L1 Loss:0.3098248466849327 | R2:0.5610003927622953 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[153] Loss:0.2640468701720238 | L1 Loss:0.3930907666683197 | R2:0.3589968214604927 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[154] Loss:0.15967497136443853 | L1 Loss:0.3062974847853184 | R2:0.5664899198930531 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[154] Loss:0.2591339111328125 | L1 Loss:0.39007237255573274 | R2:0.37098020208412436 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[155] Loss:0.15560857485979795 | L1 Loss:0.3014427497982979 | R2:0.5800869392633408 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[155] Loss:0.2723407819867134 | L1 Loss:0.3976606696844101 | R2:0.342062403964629 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[156] Loss:0.1592926774173975 | L1 Loss:0.3060940522700548 | R2:0.5697803692107223 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[156] Loss:0.26550369113683703 | L1 Loss:0.39170840084552766 | R2:0.35593988157136053 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[157] Loss:0.15465806517750025 | L1 Loss:0.298647197894752 | R2:0.5815448932805433 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[157] Loss:0.26048441380262377 | L1 Loss:0.3870952308177948 | R2:0.36782069621701413 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[158] Loss:0.15218835743144155 | L1 Loss:0.29871572367846966 | R2:0.5878917265766275 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[158] Loss:0.25726101696491244 | L1 Loss:0.3822482258081436 | R2:0.3787402738453214 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[159] Loss:0.1533658648841083 | L1 Loss:0.30019143782556057 | R2:0.5863769150376638 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[159] Loss:0.24922621697187425 | L1 Loss:0.38074601292610166 | R2:0.3965057677992462 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[160] Loss:0.15862992499023676 | L1 Loss:0.3017231831327081 | R2:0.5734282987544732 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[160] Loss:0.24377981424331666 | L1 Loss:0.37636823058128355 | R2:0.41492814011648826 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[161] Loss:0.16226503672078252 | L1 Loss:0.3061064537614584 | R2:0.5648338136467492 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[161] Loss:0.24379879534244536 | L1 Loss:0.37914261817932127 | R2:0.4115193057623463 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[162] Loss:0.15877037169411778 | L1 Loss:0.30447928607463837 | R2:0.5736190122072798 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[162] Loss:0.24865313917398452 | L1 Loss:0.3793122351169586 | R2:0.4012444726775623 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[163] Loss:0.15439417073503137 | L1 Loss:0.2988957902416587 | R2:0.5852106982625387 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[163] Loss:0.24677450805902482 | L1 Loss:0.379310542345047 | R2:0.40397108357958117 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[164] Loss:0.15268659265711904 | L1 Loss:0.2982925623655319 | R2:0.5877582351383147 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[164] Loss:0.2453237235546112 | L1 Loss:0.3760084122419357 | R2:0.4069788182817219 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[165] Loss:0.15324679901823401 | L1 Loss:0.29938811622560024 | R2:0.5858850892881318 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[165] Loss:0.24991824179887773 | L1 Loss:0.38128574192523956 | R2:0.39346437894497754 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[166] Loss:0.1542200748808682 | L1 Loss:0.3016692539677024 | R2:0.5823342588972332 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[166] Loss:0.2516171246767044 | L1 Loss:0.38208508491516113 | R2:0.39077231077205815 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[167] Loss:0.15991579135879874 | L1 Loss:0.3095938991755247 | R2:0.5663790238267686 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[167] Loss:0.2545931816101074 | L1 Loss:0.3839434176683426 | R2:0.3816970614643814 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[168] Loss:0.15120396669954062 | L1 Loss:0.3031272888183594 | R2:0.587314675944867 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[168] Loss:0.25579688251018523 | L1 Loss:0.3869129836559296 | R2:0.3770169849893904 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[169] Loss:0.15421464713290334 | L1 Loss:0.30096988566219807 | R2:0.5797974288289801 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[169] Loss:0.25525055080652237 | L1 Loss:0.38425965905189513 | R2:0.3779610619677255 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[170] Loss:0.15034656785428524 | L1 Loss:0.2975370232015848 | R2:0.5934282315311538 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[170] Loss:0.26583741009235384 | L1 Loss:0.39251756072044375 | R2:0.35467686617347927 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[171] Loss:0.1533895987085998 | L1 Loss:0.29975141771137714 | R2:0.5854296553053273 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[171] Loss:0.2645626232028008 | L1 Loss:0.3966727316379547 | R2:0.3588784048957915 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[172] Loss:0.1548118693754077 | L1 Loss:0.29937823209911585 | R2:0.5813581816276878 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[172] Loss:0.26278396397829057 | L1 Loss:0.38981525003910067 | R2:0.3634291765902915 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[173] Loss:0.15455510653555393 | L1 Loss:0.297417763620615 | R2:0.5816650068577768 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[173] Loss:0.26367989331483843 | L1 Loss:0.3933567225933075 | R2:0.361477561632042 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[174] Loss:0.15500881662592292 | L1 Loss:0.30412305891513824 | R2:0.5796797240532197 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[174] Loss:0.2595879539847374 | L1 Loss:0.3878002822399139 | R2:0.3710803276676006 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[175] Loss:0.1495676003396511 | L1 Loss:0.2966734208166599 | R2:0.5936017561371445 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[175] Loss:0.2548435553908348 | L1 Loss:0.38273183405399325 | R2:0.3856619717574026 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[176] Loss:0.145185433793813 | L1 Loss:0.29362630657851696 | R2:0.6057219544794552 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[176] Loss:0.2493818834424019 | L1 Loss:0.38499071896076204 | R2:0.3966697152394275 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[177] Loss:0.1430428922176361 | L1 Loss:0.288914960809052 | R2:0.6104368699483858 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[177] Loss:0.2501875624060631 | L1 Loss:0.3821935445070267 | R2:0.3949286113381817 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[178] Loss:0.14447440626099706 | L1 Loss:0.2912495890632272 | R2:0.6049344419481226 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[178] Loss:0.24001309424638748 | L1 Loss:0.37691396176815034 | R2:0.417996285615717 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[179] Loss:0.1472323266789317 | L1 Loss:0.2904995996505022 | R2:0.5961114421039581 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[179] Loss:0.2358563005924225 | L1 Loss:0.3762838840484619 | R2:0.42662537989682764 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[180] Loss:0.14850255753844976 | L1 Loss:0.29253928270190954 | R2:0.5949969811035589 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[180] Loss:0.23508407175540924 | L1 Loss:0.3729940205812454 | R2:0.4298006070738582 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[181] Loss:0.1499779997393489 | L1 Loss:0.29195146821439266 | R2:0.5887240976594387 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[181] Loss:0.2456391856074333 | L1 Loss:0.37819274365901945 | R2:0.40355641114390883 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[182] Loss:0.14959530904889107 | L1 Loss:0.2949331048876047 | R2:0.5908911501823797 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[182] Loss:0.24854016900062562 | L1 Loss:0.38443830609321594 | R2:0.3963467906102122 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[183] Loss:0.1541057974100113 | L1 Loss:0.29987036995589733 | R2:0.5807805792197264 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[183] Loss:0.2434024915099144 | L1 Loss:0.3796133458614349 | R2:0.411053986298203 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[184] Loss:0.1602106154896319 | L1 Loss:0.30691811069846153 | R2:0.56575751813708 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[184] Loss:0.2518701389431953 | L1 Loss:0.383135661482811 | R2:0.3912909286486123 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[185] Loss:0.1584725733846426 | L1 Loss:0.30278734862804413 | R2:0.5720075784600239 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[185] Loss:0.2475543737411499 | L1 Loss:0.3832481473684311 | R2:0.4022387885718639 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[186] Loss:0.15260790288448334 | L1 Loss:0.29691300727427006 | R2:0.5881267992497922 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[186] Loss:0.24533960968255997 | L1 Loss:0.37688385248184203 | R2:0.40800309969727894 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[187] Loss:0.15592993376776576 | L1 Loss:0.3003125675022602 | R2:0.5787219502447997 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[187] Loss:0.23908568918704987 | L1 Loss:0.3694734126329422 | R2:0.42261929407414095 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[188] Loss:0.15298714768141508 | L1 Loss:0.29463241063058376 | R2:0.5859499670012515 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[188] Loss:0.23454943895339966 | L1 Loss:0.36808432042598727 | R2:0.4360861388397172 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[189] Loss:0.15664833039045334 | L1 Loss:0.30267170909792185 | R2:0.5758990682309484 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[189] Loss:0.23000905215740203 | L1 Loss:0.364898481965065 | R2:0.4462282091191341 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[190] Loss:0.1555235655978322 | L1 Loss:0.3019725475460291 | R2:0.5786013413127183 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[190] Loss:0.23306529372930526 | L1 Loss:0.3670989900827408 | R2:0.43883982907550506 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[191] Loss:0.1543499487452209 | L1 Loss:0.3025435786694288 | R2:0.5826851405539113 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[191] Loss:0.2368534415960312 | L1 Loss:0.3723712205886841 | R2:0.4271443134135405 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[192] Loss:0.154344845097512 | L1 Loss:0.30166869051754475 | R2:0.583806340580215 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[192] Loss:0.24458972364664078 | L1 Loss:0.3767212420701981 | R2:0.4063019658648409 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[193] Loss:0.14782638289034367 | L1 Loss:0.29496974404901266 | R2:0.6029255130023433 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[193] Loss:0.25160727351903917 | L1 Loss:0.3786057472229004 | R2:0.3901342633530064 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[194] Loss:0.14855853701010346 | L1 Loss:0.29055435210466385 | R2:0.6007667066962948 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[194] Loss:0.25021065920591357 | L1 Loss:0.3772363156080246 | R2:0.39260294874193635 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[195] Loss:0.14530745008960366 | L1 Loss:0.2864299500361085 | R2:0.6096394899494771 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[195] Loss:0.2473609685897827 | L1 Loss:0.3768798500299454 | R2:0.39972949120163764 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[196] Loss:0.1445401106029749 | L1 Loss:0.28722925297915936 | R2:0.6139163067003769 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[196] Loss:0.24815075397491454 | L1 Loss:0.37149839401245116 | R2:0.3992646683660992 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[197] Loss:0.13974748505279422 | L1 Loss:0.2846082644537091 | R2:0.6259307395046916 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[197] Loss:0.2402503252029419 | L1 Loss:0.3661415308713913 | R2:0.41673332131386565 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[198] Loss:0.1387570984661579 | L1 Loss:0.28635393362492323 | R2:0.6248936164538693 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[198] Loss:0.24413896799087526 | L1 Loss:0.36660284698009493 | R2:0.4067475305563786 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[199] Loss:0.13906239159405231 | L1 Loss:0.2841273173689842 | R2:0.6241559583959725 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[199] Loss:0.23893684148788452 | L1 Loss:0.36886473894119265 | R2:0.41817517824880224 | ACC: 72.0000%(216/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "# train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "# val_acc_record = []\n",
        "# val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "# test_acc_record = []\n",
        "# test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(100)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUE6FqixxJU0",
        "outputId": "ceaa5ca4-db9b-4818-f2b4-851d59d91996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTesting Epoch[0] Loss:0.13950825296342373 | L1 Loss:0.2816904103383422 | R2:0.6210817801373295 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[0] Loss:0.24421013444662093 | L1 Loss:0.37415973246097567 | R2:0.4066150293503135 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[1] Loss:0.14090487640351057 | L1 Loss:0.28125882986932993 | R2:0.6195254275983637 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[1] Loss:0.2406184509396553 | L1 Loss:0.3744694471359253 | R2:0.4134224711135509 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[2] Loss:0.14462526794523 | L1 Loss:0.2903771474957466 | R2:0.6113558475648112 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[2] Loss:0.23318011164665223 | L1 Loss:0.36509264409542086 | R2:0.4322389624642134 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[3] Loss:0.14924994204193354 | L1 Loss:0.29535956867039204 | R2:0.5981830381550708 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[3] Loss:0.2340471178293228 | L1 Loss:0.36813656985759735 | R2:0.43059603379584377 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[4] Loss:0.148725641425699 | L1 Loss:0.2926841611042619 | R2:0.6007315957981156 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[4] Loss:0.23570276498794557 | L1 Loss:0.36996991038322447 | R2:0.427489748924154 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[5] Loss:0.14780480647459626 | L1 Loss:0.2927121752873063 | R2:0.6027273637689852 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[5] Loss:0.23803696781396866 | L1 Loss:0.37480097711086274 | R2:0.4227049044467851 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[6] Loss:0.1536411540582776 | L1 Loss:0.30154446698725224 | R2:0.5853901458204117 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[6] Loss:0.2440945938229561 | L1 Loss:0.37957843244075773 | R2:0.4084510564532716 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[7] Loss:0.1535819973796606 | L1 Loss:0.30290559213608503 | R2:0.5855868150436669 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[7] Loss:0.2404276341199875 | L1 Loss:0.375589519739151 | R2:0.4151040234857396 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[8] Loss:0.15583170857280493 | L1 Loss:0.30620139837265015 | R2:0.5785173539768216 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[8] Loss:0.25619162917137145 | L1 Loss:0.38968667685985564 | R2:0.3793002944494919 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[9] Loss:0.1523226359859109 | L1 Loss:0.30224739853292704 | R2:0.5876657530459057 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[9] Loss:0.2509555324912071 | L1 Loss:0.3846973657608032 | R2:0.3906567145934254 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[10] Loss:0.1518163001164794 | L1 Loss:0.3001101464033127 | R2:0.5873105329937649 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[10] Loss:0.24982289522886275 | L1 Loss:0.3859283357858658 | R2:0.39249247331904974 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[11] Loss:0.14742068853229284 | L1 Loss:0.2944308705627918 | R2:0.6000049870149695 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[11] Loss:0.24636733084917067 | L1 Loss:0.3844750165939331 | R2:0.4029736266298138 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[12] Loss:0.1461046994663775 | L1 Loss:0.294113801792264 | R2:0.6037142302678467 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[12] Loss:0.24828725159168244 | L1 Loss:0.38416237831115724 | R2:0.4002143842879187 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[13] Loss:0.1474000820890069 | L1 Loss:0.29628374334424734 | R2:0.5982361337347253 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[13] Loss:0.23662344217300416 | L1 Loss:0.37641916871070863 | R2:0.42703313968098966 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[14] Loss:0.1435696128755808 | L1 Loss:0.29004627745598555 | R2:0.6089374141783939 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[14] Loss:0.2393515706062317 | L1 Loss:0.3765117675065994 | R2:0.42219179813230256 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[15] Loss:0.1476690680719912 | L1 Loss:0.29439499229192734 | R2:0.6006124530495682 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[15] Loss:0.23197417855262756 | L1 Loss:0.3712093323469162 | R2:0.43933553008935633 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[16] Loss:0.147010229062289 | L1 Loss:0.2934577539563179 | R2:0.6019025308450218 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[16] Loss:0.23004078716039658 | L1 Loss:0.36539811789989474 | R2:0.44437485010699057 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[17] Loss:0.14725995482876897 | L1 Loss:0.2929299958050251 | R2:0.6032328133611456 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[17] Loss:0.22884564101696014 | L1 Loss:0.3603776693344116 | R2:0.4470081919020103 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[18] Loss:0.14689483866095543 | L1 Loss:0.2918021157383919 | R2:0.6037280661561009 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[18] Loss:0.22330205738544465 | L1 Loss:0.35957983136177063 | R2:0.46154792003149697 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[19] Loss:0.1464909198693931 | L1 Loss:0.2913198582828045 | R2:0.604898371623037 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[19] Loss:0.23139632642269134 | L1 Loss:0.36740151345729827 | R2:0.4420657584443711 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[20] Loss:0.1479660295881331 | L1 Loss:0.2923134798184037 | R2:0.601773975374371 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[20] Loss:0.22552702575922012 | L1 Loss:0.36365906298160555 | R2:0.45576316191518174 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[21] Loss:0.1482315524481237 | L1 Loss:0.2928452165797353 | R2:0.6005619282428217 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[21] Loss:0.22362420260906218 | L1 Loss:0.3631106734275818 | R2:0.46265232316500005 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[22] Loss:0.15132794762030244 | L1 Loss:0.2927301339805126 | R2:0.5903816463135365 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[22] Loss:0.2277919188141823 | L1 Loss:0.36791213154792785 | R2:0.4530014598383822 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[23] Loss:0.15054236771538854 | L1 Loss:0.2941969558596611 | R2:0.5927044555425158 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[23] Loss:0.2316028207540512 | L1 Loss:0.370222681760788 | R2:0.44340156996334273 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[24] Loss:0.14959681499749422 | L1 Loss:0.29401059728115797 | R2:0.597061562361257 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[24] Loss:0.23445638716220857 | L1 Loss:0.3701344966888428 | R2:0.4362538576049476 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[25] Loss:0.14826502837240696 | L1 Loss:0.2928001759573817 | R2:0.599567847533801 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[25] Loss:0.2419588476419449 | L1 Loss:0.3775448799133301 | R2:0.4152147209563866 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[26] Loss:0.146773268468678 | L1 Loss:0.29243408516049385 | R2:0.6031400591064909 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[26] Loss:0.24539024829864503 | L1 Loss:0.3779429614543915 | R2:0.40863119496277245 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[27] Loss:0.14450485445559025 | L1 Loss:0.2904407065361738 | R2:0.6094687732710586 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[27] Loss:0.24645402282476425 | L1 Loss:0.37885123789310454 | R2:0.40475088483793736 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[28] Loss:0.14245388517156243 | L1 Loss:0.2895701518282294 | R2:0.6167591707618112 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[28] Loss:0.2460158124566078 | L1 Loss:0.37680413722991946 | R2:0.4055459716955159 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[29] Loss:0.1410827562212944 | L1 Loss:0.28945835493505 | R2:0.6200524514321214 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[29] Loss:0.2509467571973801 | L1 Loss:0.3786210000514984 | R2:0.39360153094853956 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[30] Loss:0.1401454065926373 | L1 Loss:0.28855947963893414 | R2:0.624438532297555 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[30] Loss:0.252746607363224 | L1 Loss:0.37933432757854463 | R2:0.3873582764952511 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[31] Loss:0.1387839838862419 | L1 Loss:0.2881766567006707 | R2:0.6273787645421497 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[31] Loss:0.25017336308956145 | L1 Loss:0.37519653141498566 | R2:0.394257607502933 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[32] Loss:0.1379196224734187 | L1 Loss:0.2852555550634861 | R2:0.6309608146013727 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[32] Loss:0.25249881446361544 | L1 Loss:0.3784844636917114 | R2:0.38841564669241735 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[33] Loss:0.1400221330113709 | L1 Loss:0.2870308570563793 | R2:0.6232392060696137 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[33] Loss:0.2530697286128998 | L1 Loss:0.38081803619861604 | R2:0.3888995924903748 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[34] Loss:0.1441954281181097 | L1 Loss:0.2877285797148943 | R2:0.6115199549791127 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[34] Loss:0.26113093495368955 | L1 Loss:0.3901720255613327 | R2:0.37007674794122974 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[35] Loss:0.1426690318621695 | L1 Loss:0.28845487628132105 | R2:0.6156292666053264 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[35] Loss:0.25255833864212035 | L1 Loss:0.3855237901210785 | R2:0.39074220894025535 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[36] Loss:0.1458247620612383 | L1 Loss:0.2931265728548169 | R2:0.6078226247999677 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[36] Loss:0.2572537466883659 | L1 Loss:0.3883371651172638 | R2:0.38161918540261985 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[37] Loss:0.1501849628984928 | L1 Loss:0.2989798514172435 | R2:0.5950256229366903 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[37] Loss:0.25627640187740325 | L1 Loss:0.38738608062267305 | R2:0.3809782552476528 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[38] Loss:0.15127518819645047 | L1 Loss:0.29985122103244066 | R2:0.5935153199335638 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[38] Loss:0.2512398958206177 | L1 Loss:0.38655657768249513 | R2:0.39252198152426726 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[39] Loss:0.1535563482902944 | L1 Loss:0.30469275265932083 | R2:0.5869975193476155 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[39] Loss:0.25235253274440766 | L1 Loss:0.38620292246341703 | R2:0.385896162355112 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[40] Loss:0.15052639320492744 | L1 Loss:0.30317328218370676 | R2:0.5949514421093239 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[40] Loss:0.254144561290741 | L1 Loss:0.387596395611763 | R2:0.38293464016997414 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[41] Loss:0.14871053444221616 | L1 Loss:0.2981384387239814 | R2:0.5980247453462026 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[41] Loss:0.2485770806670189 | L1 Loss:0.37936545014381406 | R2:0.39645455176100686 | ACC: 70.0000%(210/300)\n",
            "Testing Epoch[42] Loss:0.14620243618264794 | L1 Loss:0.2944236844778061 | R2:0.6033823911282146 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[42] Loss:0.24825805723667144 | L1 Loss:0.3757889121770859 | R2:0.3970645784314457 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[43] Loss:0.1463458752259612 | L1 Loss:0.2956847697496414 | R2:0.6034786946359514 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[43] Loss:0.25027375668287277 | L1 Loss:0.374823671579361 | R2:0.39535192512047623 | ACC: 70.3333%(211/300)\n",
            "Testing Epoch[44] Loss:0.1446620230562985 | L1 Loss:0.2959795705974102 | R2:0.6087453198535248 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[44] Loss:0.24631526321172714 | L1 Loss:0.37691997587680814 | R2:0.40724882767910336 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[45] Loss:0.1438022800721228 | L1 Loss:0.29442939069122076 | R2:0.6117155791327997 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[45] Loss:0.25018410533666613 | L1 Loss:0.376296466588974 | R2:0.39772970142484765 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[46] Loss:0.14278333261609077 | L1 Loss:0.29271387588232756 | R2:0.6141993838041173 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[46] Loss:0.2490450620651245 | L1 Loss:0.3717625081539154 | R2:0.40282772366198377 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[47] Loss:0.145292101893574 | L1 Loss:0.29532519541680813 | R2:0.6091231917323536 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[47] Loss:0.2422830045223236 | L1 Loss:0.36906281411647796 | R2:0.41922425245743566 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[48] Loss:0.1439886810258031 | L1 Loss:0.2914160341024399 | R2:0.6126590512818115 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[48] Loss:0.2411806121468544 | L1 Loss:0.3667808622121811 | R2:0.42184717878211336 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[49] Loss:0.14432939141988754 | L1 Loss:0.2915597716346383 | R2:0.6113492753128767 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[49] Loss:0.23873771876096725 | L1 Loss:0.367606446146965 | R2:0.42734094168193903 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[50] Loss:0.1438473118469119 | L1 Loss:0.29069214686751366 | R2:0.6136719741129776 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[50] Loss:0.23845954984426498 | L1 Loss:0.3689700156450272 | R2:0.42614537080023907 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[51] Loss:0.14517979184165597 | L1 Loss:0.289543847553432 | R2:0.6108838127517677 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[51] Loss:0.24088499695062637 | L1 Loss:0.3712512284517288 | R2:0.4209403279034033 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[52] Loss:0.14423401420935988 | L1 Loss:0.2899288749322295 | R2:0.6128526143892685 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[52] Loss:0.24118712544441223 | L1 Loss:0.36872267425060273 | R2:0.4196647472545024 | ACC: 75.6667%(227/300)\n",
            "Testing Epoch[53] Loss:0.14268908090889454 | L1 Loss:0.28742294292896986 | R2:0.6168187391965014 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[53] Loss:0.24225089699029922 | L1 Loss:0.3716753304004669 | R2:0.41654010197618996 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[54] Loss:0.13986107893288136 | L1 Loss:0.2878150474280119 | R2:0.6241457711389902 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[54] Loss:0.23936473727226257 | L1 Loss:0.36815654039382933 | R2:0.4233592549318862 | ACC: 75.0000%(225/300)\n",
            "Testing Epoch[55] Loss:0.14203281374648213 | L1 Loss:0.2878489037975669 | R2:0.6188022286702624 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[55] Loss:0.24242273569107056 | L1 Loss:0.37305944263935087 | R2:0.41425298742353817 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[56] Loss:0.13853998808190227 | L1 Loss:0.2849097540602088 | R2:0.6280036630137306 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[56] Loss:0.24172430336475373 | L1 Loss:0.37439297139644623 | R2:0.4145986224835174 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[57] Loss:0.1386139988899231 | L1 Loss:0.28540088422596455 | R2:0.6274863615310317 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[57] Loss:0.24126408398151397 | L1 Loss:0.3755414545536041 | R2:0.4177662183638223 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[58] Loss:0.13622780051082373 | L1 Loss:0.2842279188334942 | R2:0.6340585561536998 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[58] Loss:0.24748793691396714 | L1 Loss:0.3808015167713165 | R2:0.40177555194250864 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[59] Loss:0.1348992451094091 | L1 Loss:0.28202719893306494 | R2:0.6374522574596376 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[59] Loss:0.24117568135261536 | L1 Loss:0.3765535533428192 | R2:0.4175483788062772 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[60] Loss:0.1360899293795228 | L1 Loss:0.28233245108276606 | R2:0.634562090382199 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[60] Loss:0.24225325584411622 | L1 Loss:0.3813899248838425 | R2:0.41452098193013676 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[61] Loss:0.1395591739565134 | L1 Loss:0.2873950805515051 | R2:0.6256412738402638 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[61] Loss:0.24560118466615677 | L1 Loss:0.38399138748645784 | R2:0.4063738554283187 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[62] Loss:0.1463310713879764 | L1 Loss:0.29410109855234623 | R2:0.6096296049038907 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[62] Loss:0.24652472138404846 | L1 Loss:0.38416965007781984 | R2:0.40479654099522244 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[63] Loss:0.14529714034870267 | L1 Loss:0.29508503433316946 | R2:0.6127834259117843 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[63] Loss:0.24551639705896378 | L1 Loss:0.38156995475292205 | R2:0.4086295977039036 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[64] Loss:0.1444557523354888 | L1 Loss:0.2947476301342249 | R2:0.6139881141978485 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[64] Loss:0.24655238538980484 | L1 Loss:0.3792977571487427 | R2:0.4052662499585667 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[65] Loss:0.1444529122672975 | L1 Loss:0.29539330676198006 | R2:0.612465531160504 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[65] Loss:0.24204460382461548 | L1 Loss:0.3742510318756104 | R2:0.4151089463819235 | ACC: 70.6667%(212/300)\n",
            "Testing Epoch[66] Loss:0.14075561193749309 | L1 Loss:0.29063322488218546 | R2:0.6212038594471604 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[66] Loss:0.24150211960077286 | L1 Loss:0.3726832062005997 | R2:0.4166952711690269 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[67] Loss:0.13938341150060296 | L1 Loss:0.2863486120477319 | R2:0.6242866514109869 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[67] Loss:0.24208728075027466 | L1 Loss:0.37463420927524566 | R2:0.41754950915988215 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[68] Loss:0.13749890215694904 | L1 Loss:0.2829765994101763 | R2:0.6303227561560221 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[68] Loss:0.24142923951148987 | L1 Loss:0.37727569341659545 | R2:0.4192350465706222 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[69] Loss:0.14093270152807236 | L1 Loss:0.28905860986560583 | R2:0.6219472978696707 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[69] Loss:0.24682988375425338 | L1 Loss:0.3801156610250473 | R2:0.4039205118521713 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[70] Loss:0.1448834426701069 | L1 Loss:0.29618153162300587 | R2:0.6102719862838991 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[70] Loss:0.2429991453886032 | L1 Loss:0.3780573636293411 | R2:0.414578283114027 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[71] Loss:0.14529330050572753 | L1 Loss:0.29591129906475544 | R2:0.6087787670350201 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[71] Loss:0.2450638011097908 | L1 Loss:0.37742952108383176 | R2:0.4091041536050528 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[72] Loss:0.14759677834808826 | L1 Loss:0.2978951819241047 | R2:0.6015809196983974 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[72] Loss:0.23887702077627182 | L1 Loss:0.37303113639354707 | R2:0.4231195778503201 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[73] Loss:0.14514981396496296 | L1 Loss:0.29612474516034126 | R2:0.607612251263428 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[73] Loss:0.24020417183637618 | L1 Loss:0.37374916672706604 | R2:0.4216313302049043 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[74] Loss:0.14580195536836982 | L1 Loss:0.2944859303534031 | R2:0.6078367460003629 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[74] Loss:0.2390369415283203 | L1 Loss:0.37367557287216185 | R2:0.42448272320574415 | ACC: 72.3333%(217/300)\n",
            "Testing Epoch[75] Loss:0.14421797078102827 | L1 Loss:0.2925132317468524 | R2:0.6122685497432081 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[75] Loss:0.2420916184782982 | L1 Loss:0.37824084758758547 | R2:0.41893773029638065 | ACC: 72.6667%(218/300)\n",
            "Testing Epoch[76] Loss:0.1431272723712027 | L1 Loss:0.29173313453793526 | R2:0.6168158310510665 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[76] Loss:0.23361019119620324 | L1 Loss:0.36530907452106476 | R2:0.44018670255130343 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[77] Loss:0.14153237082064152 | L1 Loss:0.2889857068657875 | R2:0.6214309049666785 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[77] Loss:0.2356017641723156 | L1 Loss:0.36572830080986024 | R2:0.43648618320332283 | ACC: 74.3333%(223/300)\n",
            "Testing Epoch[78] Loss:0.14161631977185607 | L1 Loss:0.2865892546251416 | R2:0.6209067447310167 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[78] Loss:0.24050871878862382 | L1 Loss:0.3693999469280243 | R2:0.4249098005084028 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[79] Loss:0.141047902405262 | L1 Loss:0.28347682673484087 | R2:0.6232815696876693 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[79] Loss:0.23653139770030976 | L1 Loss:0.3678014039993286 | R2:0.4319127891139609 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[80] Loss:0.14001315739005804 | L1 Loss:0.2838013283908367 | R2:0.6248886827790151 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[80] Loss:0.23775174766778945 | L1 Loss:0.36949590146541594 | R2:0.42873386034099 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[81] Loss:0.1420394778251648 | L1 Loss:0.2867085635662079 | R2:0.6177405725268217 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[81] Loss:0.2411206379532814 | L1 Loss:0.37325307428836824 | R2:0.4206205881016617 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[82] Loss:0.14386249287053943 | L1 Loss:0.28800223860889673 | R2:0.6132354026122275 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[82] Loss:0.24828224033117294 | L1 Loss:0.3760386109352112 | R2:0.4033298082974133 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[83] Loss:0.14436248503625393 | L1 Loss:0.289704698137939 | R2:0.6116220529163534 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[83] Loss:0.24921836107969284 | L1 Loss:0.37713380455970763 | R2:0.4018098427020579 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[84] Loss:0.144853294827044 | L1 Loss:0.2938396781682968 | R2:0.6109854333387463 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[84] Loss:0.25128249675035474 | L1 Loss:0.3825692057609558 | R2:0.39524850942563894 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[85] Loss:0.145700350869447 | L1 Loss:0.29476130567491055 | R2:0.6075486091192516 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[85] Loss:0.24973667562007903 | L1 Loss:0.3827875107526779 | R2:0.3989420422420381 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[86] Loss:0.1411734907887876 | L1 Loss:0.2926141209900379 | R2:0.6187200277473877 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[86] Loss:0.24909349977970124 | L1 Loss:0.38011608421802523 | R2:0.39703607653950074 | ACC: 72.0000%(216/300)\n",
            "Testing Epoch[87] Loss:0.14214701717719436 | L1 Loss:0.29152979888021946 | R2:0.6170218561410684 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[87] Loss:0.2488554149866104 | L1 Loss:0.377705505490303 | R2:0.3990553261911213 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[88] Loss:0.13656896259635687 | L1 Loss:0.2841288475319743 | R2:0.6302040481915837 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[88] Loss:0.24285658001899718 | L1 Loss:0.37426766455173494 | R2:0.4132346649468392 | ACC: 73.3333%(220/300)\n",
            "Testing Epoch[89] Loss:0.1355063384398818 | L1 Loss:0.28534336388111115 | R2:0.63330992095006 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[89] Loss:0.24111736714839935 | L1 Loss:0.37673444151878355 | R2:0.41924991711671405 | ACC: 71.3333%(214/300)\n",
            "Testing Epoch[90] Loss:0.1399954752996564 | L1 Loss:0.2894852850586176 | R2:0.6222775577493185 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[90] Loss:0.2397662028670311 | L1 Loss:0.37349199056625365 | R2:0.42433033446649276 | ACC: 75.3333%(226/300)\n",
            "Testing Epoch[91] Loss:0.14473220752552152 | L1 Loss:0.29301693104207516 | R2:0.6084465775380724 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[91] Loss:0.23802688270807265 | L1 Loss:0.3731749355792999 | R2:0.4279169342916834 | ACC: 74.0000%(222/300)\n",
            "Testing Epoch[92] Loss:0.14575780555605888 | L1 Loss:0.29244905337691307 | R2:0.6060061588948931 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[92] Loss:0.23719821572303773 | L1 Loss:0.370084348320961 | R2:0.43097208232990053 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[93] Loss:0.1454281099140644 | L1 Loss:0.29374412447214127 | R2:0.6069682562057377 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[93] Loss:0.2351611375808716 | L1 Loss:0.36814398467540743 | R2:0.4361646188007433 | ACC: 73.6667%(221/300)\n",
            "Testing Epoch[94] Loss:0.1458997200243175 | L1 Loss:0.293093035928905 | R2:0.60770321482356 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[94] Loss:0.23553190678358077 | L1 Loss:0.36730128824710845 | R2:0.43462093273997465 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[95] Loss:0.14607434626668692 | L1 Loss:0.29259971529245377 | R2:0.6076168857967575 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[95] Loss:0.23314117938280104 | L1 Loss:0.36540543138980863 | R2:0.44095864757950015 | ACC: 74.6667%(224/300)\n",
            "Testing Epoch[96] Loss:0.14760995004326105 | L1 Loss:0.29395358357578516 | R2:0.604329709470586 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[96] Loss:0.2338876485824585 | L1 Loss:0.3673881679773331 | R2:0.4372106083502995 | ACC: 73.0000%(219/300)\n",
            "Testing Epoch[97] Loss:0.14366694586351514 | L1 Loss:0.2900067809969187 | R2:0.6145990752760312 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[97] Loss:0.2354468509554863 | L1 Loss:0.36826121211051943 | R2:0.4329038186574188 | ACC: 71.6667%(215/300)\n",
            "Testing Epoch[98] Loss:0.14435589173808694 | L1 Loss:0.29219840094447136 | R2:0.6125460765399154 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[98] Loss:0.24048333913087844 | L1 Loss:0.3736726135015488 | R2:0.420748699524062 | ACC: 71.0000%(213/300)\n",
            "Testing Epoch[99] Loss:0.14180654101073742 | L1 Loss:0.2893353756517172 | R2:0.6187023310520697 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[99] Loss:0.2411745458841324 | L1 Loss:0.3744682878255844 | R2:0.41738515763026596 | ACC: 71.6667%(215/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----BERT+ MLP 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHI8Rsp7xNJF",
        "outputId": "52135491-a609-4a19-a3d7-ebb4180b51ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----BERT+ MLP 300 epochs-----\n",
            "max test_r2_record  0.46265232316500005\n",
            "min test_loss_l1_record  0.35957983136177063\n",
            "min test_loss_record  0.22330205738544465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----BERT+ MLP 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847d8f53-dd34-4c25-c14a-05566c9dcb88",
        "id": "49HEpUBDuQ49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----BERT+ MLP 200 epochs-----\n",
            "max test_r2_record  0.4462282091191341\n",
            "min test_loss_l1_record  0.364898481965065\n",
            "min test_loss_record  0.23000905215740203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----BERT+ MLP 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72097cb0-bad3-4a28-ff84-3379c2259e10",
        "id": "IG2UWMaMuQ49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----BERT+ MLP 100 epochs-----\n",
            "max test_r2_record  0.4034281074029506\n",
            "min test_loss_l1_record  0.3837520211935043\n",
            "min test_loss_record  0.24860855489969252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----BERT+ MLP 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "e7e52f0b-e15e-4337-ed9a-d77171bcbd2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CGpE8VquQ49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----BERT+ MLP 50 epochs-----\n",
            "max test_r2_record  0.36623230131394\n",
            "min test_loss_l1_record  0.3920943856239319\n",
            "min test_loss_record  0.26455709636211394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv1d"
      ],
      "metadata": {
        "id": "L8PiKU1Fatti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qKaiEB6rath0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x, is_conv1d=True)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "5lgRZEa9GvwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b3d4d2-c81c-4526-cf10-abc456780454",
        "id": "vuB9n8-0GvwM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.0008, weight_decay=0.0001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "fMnSTUwdGvwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4178fa3-4239-4b27-aa63-da0973c103b3",
        "id": "XKqGc0WYGvwN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (conv1d_module): Conv1d_module(\n",
              "    (conv1d_1): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (batchnorm_1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv1d_2): Conv1d(1, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (batchnorm_2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv1d_3): Conv1d(2, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (batchnorm_3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (linear_1): Linear(in_features=120, out_features=64, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=428, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(test_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "yFYiKGu7GvwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(val_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    label = label.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "vNpHFlurGvwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(200)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76e1f11-92b0-48ee-d1c8-41f2103fa137",
        "id": "299E8Mj5GvwP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTesting Epoch[0] Loss:0.36702448781579733 | L1 Loss:0.45939972065389156 | R2:0.08924380006036996 | ACC: 59.4000%(297/500)\n",
            "Testing Epoch[0] Loss:0.3764041155576706 | L1 Loss:0.45881304144859314 | R2:0.038080211995390564 | ACC: 60.0000%(180/300)\n",
            "Testing Epoch[1] Loss:0.32636731397360563 | L1 Loss:0.4391866121441126 | R2:0.18327849323163625 | ACC: 63.0000%(315/500)\n",
            "Testing Epoch[1] Loss:0.3621054351329803 | L1 Loss:0.459523594379425 | R2:0.07369013037055577 | ACC: 60.0000%(180/300)\n",
            "Testing Epoch[2] Loss:0.2943747378885746 | L1 Loss:0.41690921783447266 | R2:0.26456024718187343 | ACC: 65.2000%(326/500)\n",
            "Testing Epoch[2] Loss:0.3572265043854713 | L1 Loss:0.4603163182735443 | R2:0.08615605220768437 | ACC: 60.6667%(182/300)\n",
            "Testing Epoch[3] Loss:0.27971675898879766 | L1 Loss:0.4071177467703819 | R2:0.30047380383970085 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[3] Loss:0.3610179856419563 | L1 Loss:0.4621302425861359 | R2:0.07143303122870255 | ACC: 61.3333%(184/300)\n",
            "Testing Epoch[4] Loss:0.25872575864195824 | L1 Loss:0.39506863057613373 | R2:0.35003121765696593 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[4] Loss:0.35774923861026764 | L1 Loss:0.46449915766716005 | R2:0.08030072618919751 | ACC: 61.0000%(183/300)\n",
            "Testing Epoch[5] Loss:0.25207543931901455 | L1 Loss:0.39311147294938564 | R2:0.36148877368620613 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[5] Loss:0.3694154232740402 | L1 Loss:0.47510494887828825 | R2:0.05071498155608295 | ACC: 57.6667%(173/300)\n",
            "Testing Epoch[6] Loss:0.24947928730398417 | L1 Loss:0.3895530104637146 | R2:0.36868859449117647 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[6] Loss:0.3674387514591217 | L1 Loss:0.4722551882266998 | R2:0.04643289419891543 | ACC: 60.3333%(181/300)\n",
            "Testing Epoch[7] Loss:0.2434025425463915 | L1 Loss:0.38768661953508854 | R2:0.38309575153025993 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[7] Loss:0.36937197744846345 | L1 Loss:0.47501087486743926 | R2:0.04536053882056808 | ACC: 60.6667%(182/300)\n",
            "Testing Epoch[8] Loss:0.23731184843927622 | L1 Loss:0.3766942508518696 | R2:0.3954944627478445 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[8] Loss:0.3808083713054657 | L1 Loss:0.4787958025932312 | R2:0.010270414834891384 | ACC: 59.3333%(178/300)\n",
            "Testing Epoch[9] Loss:0.23533240146934986 | L1 Loss:0.38052570819854736 | R2:0.39712348977794926 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[9] Loss:0.3811413675546646 | L1 Loss:0.48146226704120637 | R2:0.014284794883756457 | ACC: 60.0000%(180/300)\n",
            "Testing Epoch[10] Loss:0.23273040167987347 | L1 Loss:0.3765110056847334 | R2:0.403953638310521 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[10] Loss:0.3846195966005325 | L1 Loss:0.4831980973482132 | R2:0.012769792643165778 | ACC: 60.0000%(180/300)\n",
            "Testing Epoch[11] Loss:0.23943437729030848 | L1 Loss:0.37926939874887466 | R2:0.39141829787264265 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[11] Loss:0.3834933936595917 | L1 Loss:0.483864089846611 | R2:0.01724702772938743 | ACC: 59.6667%(179/300)\n",
            "Testing Epoch[12] Loss:0.22724803071469069 | L1 Loss:0.3720403090119362 | R2:0.4258195951300806 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[12] Loss:0.38224668651819227 | L1 Loss:0.48564050793647767 | R2:0.018688551160092624 | ACC: 60.0000%(180/300)\n",
            "Testing Epoch[13] Loss:0.23255103453993797 | L1 Loss:0.37777436152100563 | R2:0.4130235158911993 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[13] Loss:0.3714715301990509 | L1 Loss:0.4790577471256256 | R2:0.05413656050465956 | ACC: 60.6667%(182/300)\n",
            "Testing Epoch[14] Loss:0.2231371197849512 | L1 Loss:0.366583239287138 | R2:0.4316001885497526 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[14] Loss:0.38504732996225355 | L1 Loss:0.48108780682086943 | R2:0.01831563848131329 | ACC: 60.3333%(181/300)\n",
            "Testing Epoch[15] Loss:0.20945461094379425 | L1 Loss:0.35484617576003075 | R2:0.46114816881768694 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[15] Loss:0.3779561400413513 | L1 Loss:0.47421502470970156 | R2:0.031432522759511194 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[16] Loss:0.21519315149635077 | L1 Loss:0.35714079067111015 | R2:0.4519369249275636 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[16] Loss:0.3583964318037033 | L1 Loss:0.4662550538778305 | R2:0.08875775839139298 | ACC: 61.3333%(184/300)\n",
            "Testing Epoch[17] Loss:0.2104479130357504 | L1 Loss:0.3573102839291096 | R2:0.4591482953191357 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[17] Loss:0.36663737446069716 | L1 Loss:0.47507734000682833 | R2:0.0711148668447926 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[18] Loss:0.21415096148848534 | L1 Loss:0.3617500513792038 | R2:0.45299299066636056 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[18] Loss:0.37741495966911315 | L1 Loss:0.46788970232009885 | R2:0.03576163592256141 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[19] Loss:0.22439865209162235 | L1 Loss:0.37087150663137436 | R2:0.4246854015883776 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[19] Loss:0.3839349985122681 | L1 Loss:0.48201653063297273 | R2:0.0183834864851428 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[20] Loss:0.22426546085625887 | L1 Loss:0.3697007317095995 | R2:0.427311026387879 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[20] Loss:0.38195476233959197 | L1 Loss:0.48451403379440305 | R2:0.03300011358461376 | ACC: 62.3333%(187/300)\n",
            "Testing Epoch[21] Loss:0.2295009819790721 | L1 Loss:0.3739168234169483 | R2:0.4124091232720753 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[21] Loss:0.3870514094829559 | L1 Loss:0.4801480293273926 | R2:0.010952926014249765 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[22] Loss:0.21946975216269493 | L1 Loss:0.3620160985738039 | R2:0.43696587507546497 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[22] Loss:0.384310656785965 | L1 Loss:0.47897165417671206 | R2:0.00809613459873807 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[23] Loss:0.2190192462876439 | L1 Loss:0.3652245718985796 | R2:0.436120632722296 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[23] Loss:0.36870712637901304 | L1 Loss:0.4721546173095703 | R2:0.05172891869332955 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[24] Loss:0.21729128528386354 | L1 Loss:0.3638795856386423 | R2:0.44907894165424356 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[24] Loss:0.35606185644865035 | L1 Loss:0.46369006037712096 | R2:0.08238829944670581 | ACC: 62.6667%(188/300)\n",
            "Testing Epoch[25] Loss:0.20875906944274902 | L1 Loss:0.3624702524393797 | R2:0.4686678924479462 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[25] Loss:0.36109286099672316 | L1 Loss:0.46096061170101166 | R2:0.06832260818757066 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[26] Loss:0.2139219269156456 | L1 Loss:0.36494605615735054 | R2:0.4549802201634963 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[26] Loss:0.34603196531534197 | L1 Loss:0.4537410080432892 | R2:0.10578949509954798 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[27] Loss:0.20328590646386147 | L1 Loss:0.35621508583426476 | R2:0.4856486993212101 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[27] Loss:0.34994620084762573 | L1 Loss:0.45368793308734895 | R2:0.09735444128906655 | ACC: 62.6667%(188/300)\n",
            "Testing Epoch[28] Loss:0.195474443025887 | L1 Loss:0.3479455169290304 | R2:0.5023602123007427 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[28] Loss:0.3579451560974121 | L1 Loss:0.46044123768806455 | R2:0.06986427413741356 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[29] Loss:0.19156312569975853 | L1 Loss:0.34062198735773563 | R2:0.5150809629984935 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[29] Loss:0.34889930486679077 | L1 Loss:0.4565415054559708 | R2:0.08972981906507631 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[30] Loss:0.19919945020228624 | L1 Loss:0.3496315209195018 | R2:0.4931043702673367 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[30] Loss:0.3458732068538666 | L1 Loss:0.44657894372940066 | R2:0.10299762001673321 | ACC: 62.3333%(187/300)\n",
            "Testing Epoch[31] Loss:0.19665815075859427 | L1 Loss:0.34873244166374207 | R2:0.49826392458386465 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[31] Loss:0.3468250721693039 | L1 Loss:0.44675619900226593 | R2:0.09285934491828265 | ACC: 62.6667%(188/300)\n",
            "Testing Epoch[32] Loss:0.20737205119803548 | L1 Loss:0.3533189073204994 | R2:0.476311408413584 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[32] Loss:0.3524122565984726 | L1 Loss:0.44974674582481383 | R2:0.08477029307786885 | ACC: 60.3333%(181/300)\n",
            "Testing Epoch[33] Loss:0.21530606597661972 | L1 Loss:0.35792920365929604 | R2:0.45900506114144857 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[33] Loss:0.3560990035533905 | L1 Loss:0.46202537715435027 | R2:0.08015870183365804 | ACC: 59.6667%(179/300)\n",
            "Testing Epoch[34] Loss:0.22435941267758608 | L1 Loss:0.3698731083422899 | R2:0.43033874929306126 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[34] Loss:0.3580785796046257 | L1 Loss:0.4604058563709259 | R2:0.07863754584328617 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[35] Loss:0.23055701237171888 | L1 Loss:0.37695615366101265 | R2:0.40990737317104475 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[35] Loss:0.3639549374580383 | L1 Loss:0.46452966928482053 | R2:0.06240758710544068 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[36] Loss:0.2216094732284546 | L1 Loss:0.3732882849872112 | R2:0.4279687851309427 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[36] Loss:0.36541437953710554 | L1 Loss:0.4692527949810028 | R2:0.05492962465832464 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[37] Loss:0.2200458450242877 | L1 Loss:0.3686132188886404 | R2:0.43222739972446866 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[37] Loss:0.37279356122016905 | L1 Loss:0.48058706521987915 | R2:0.03716602669668739 | ACC: 61.3333%(184/300)\n",
            "Testing Epoch[38] Loss:0.222126760520041 | L1 Loss:0.37422581762075424 | R2:0.4309211276444074 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[38] Loss:0.3697814136743546 | L1 Loss:0.47874222695827484 | R2:0.040630272771645916 | ACC: 60.6667%(182/300)\n",
            "Testing Epoch[39] Loss:0.2330488646402955 | L1 Loss:0.382727088406682 | R2:0.40463073057022464 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[39] Loss:0.3797321230173111 | L1 Loss:0.4777515411376953 | R2:0.01255170649071775 | ACC: 59.3333%(178/300)\n",
            "Testing Epoch[40] Loss:0.21731478162109852 | L1 Loss:0.36680995114147663 | R2:0.4462042349245312 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[40] Loss:0.36208483278751374 | L1 Loss:0.4674177706241608 | R2:0.06845762018420207 | ACC: 61.6667%(185/300)\n",
            "Testing Epoch[41] Loss:0.20905996765941381 | L1 Loss:0.3602632787078619 | R2:0.4728836748075793 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[41] Loss:0.3696135312318802 | L1 Loss:0.47213226854801177 | R2:0.0528582739708401 | ACC: 60.6667%(182/300)\n",
            "Testing Epoch[42] Loss:0.20190434344112873 | L1 Loss:0.35564814507961273 | R2:0.4914984497481486 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[42] Loss:0.38112103641033174 | L1 Loss:0.47877312302589414 | R2:0.03413637320934948 | ACC: 58.3333%(175/300)\n",
            "Testing Epoch[43] Loss:0.20221215579658747 | L1 Loss:0.35571652837097645 | R2:0.4865762984391254 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[43] Loss:0.371794331073761 | L1 Loss:0.4691906303167343 | R2:0.060733329735977636 | ACC: 62.0000%(186/300)\n",
            "Testing Epoch[44] Loss:0.2022085301578045 | L1 Loss:0.3532755021005869 | R2:0.4870059517857617 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[44] Loss:0.36297163367271423 | L1 Loss:0.4627462804317474 | R2:0.07429416372189293 | ACC: 62.0000%(186/300)\n",
            "Testing Epoch[45] Loss:0.2000215221196413 | L1 Loss:0.352892329916358 | R2:0.491926366761292 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[45] Loss:0.36125344038009644 | L1 Loss:0.4511350989341736 | R2:0.07920460097480705 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[46] Loss:0.20131127396598458 | L1 Loss:0.3513736370950937 | R2:0.48995951366865526 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[46] Loss:0.34799960255622864 | L1 Loss:0.44634541869163513 | R2:0.10476972705589616 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[47] Loss:0.19621978560462594 | L1 Loss:0.3484723847359419 | R2:0.5057050981102815 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[47] Loss:0.33504715859889983 | L1 Loss:0.4375958025455475 | R2:0.13400744547230298 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[48] Loss:0.18396624084562063 | L1 Loss:0.3395331148058176 | R2:0.5340927824340445 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[48] Loss:0.3389928683638573 | L1 Loss:0.445603808760643 | R2:0.12263481729994494 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[49] Loss:0.17995786620303988 | L1 Loss:0.3357311747968197 | R2:0.5423319284385496 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[49] Loss:0.3351013109087944 | L1 Loss:0.44433042109012605 | R2:0.14085059173894407 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[50] Loss:0.18406622298061848 | L1 Loss:0.3354647271335125 | R2:0.536219497054812 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[50] Loss:0.3545455768704414 | L1 Loss:0.4558595597743988 | R2:0.08995137144500152 | ACC: 62.3333%(187/300)\n",
            "Testing Epoch[51] Loss:0.18747440166771412 | L1 Loss:0.3443934731185436 | R2:0.5277698930234529 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[51] Loss:0.35123142302036287 | L1 Loss:0.44889686107635496 | R2:0.0984965851446474 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[52] Loss:0.19324835808947682 | L1 Loss:0.3431867975741625 | R2:0.5142693433655485 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[52] Loss:0.3655026376247406 | L1 Loss:0.4553580105304718 | R2:0.06258298887871665 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[53] Loss:0.18213674798607826 | L1 Loss:0.3341095317155123 | R2:0.5399107495805738 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[53] Loss:0.36822365820407865 | L1 Loss:0.46408720314502716 | R2:0.05410218805166474 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[54] Loss:0.18355832993984222 | L1 Loss:0.33712589368224144 | R2:0.5358384943548377 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[54] Loss:0.36064079254865644 | L1 Loss:0.4520996630191803 | R2:0.08171184477666818 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[55] Loss:0.182226010132581 | L1 Loss:0.3326500430703163 | R2:0.5397770553607038 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[55] Loss:0.3469348609447479 | L1 Loss:0.4415780812501907 | R2:0.12131032919116333 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[56] Loss:0.1958204936236143 | L1 Loss:0.34923160821199417 | R2:0.5030568377455744 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[56] Loss:0.3659020721912384 | L1 Loss:0.4638156980276108 | R2:0.0694232619939563 | ACC: 61.6667%(185/300)\n",
            "Testing Epoch[57] Loss:0.19855338335037231 | L1 Loss:0.3498528469353914 | R2:0.49881091759336554 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[57] Loss:0.3596700891852379 | L1 Loss:0.4582217961549759 | R2:0.09012989764528612 | ACC: 62.3333%(187/300)\n",
            "Testing Epoch[58] Loss:0.1991217751055956 | L1 Loss:0.3517113458365202 | R2:0.4987397359250136 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[58] Loss:0.36040444374084474 | L1 Loss:0.46162493228912355 | R2:0.08684034848465191 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[59] Loss:0.1918061445467174 | L1 Loss:0.34793019481003284 | R2:0.5175963693336789 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[59] Loss:0.3486571505665779 | L1 Loss:0.45248785614967346 | R2:0.12096690018983125 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[60] Loss:0.18505498580634594 | L1 Loss:0.3380598649382591 | R2:0.5307550393273793 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[60] Loss:0.3426064282655716 | L1 Loss:0.44492709040641787 | R2:0.13247402723239424 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[61] Loss:0.19572955276817083 | L1 Loss:0.3511441107839346 | R2:0.5094489076011944 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[61] Loss:0.3425954654812813 | L1 Loss:0.4473598599433899 | R2:0.12819176659866488 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[62] Loss:0.1834383704699576 | L1 Loss:0.3352969400584698 | R2:0.5362124960105618 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[62] Loss:0.3478283748030663 | L1 Loss:0.45161646902561187 | R2:0.12080014769329701 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[63] Loss:0.1788144651800394 | L1 Loss:0.3322277031838894 | R2:0.5449807674416987 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[63] Loss:0.3431585907936096 | L1 Loss:0.4401526004076004 | R2:0.13070515184932147 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[64] Loss:0.18806641455739737 | L1 Loss:0.33507745899260044 | R2:0.5220776726938113 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[64] Loss:0.3494786024093628 | L1 Loss:0.44448872208595275 | R2:0.1082061785016449 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[65] Loss:0.1864517806097865 | L1 Loss:0.3388534393161535 | R2:0.5196609125499984 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[65] Loss:0.3524056702852249 | L1 Loss:0.44749501943588255 | R2:0.10452807975647634 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[66] Loss:0.1845235307700932 | L1 Loss:0.33639009669423103 | R2:0.5228346058981608 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[66] Loss:0.340882770717144 | L1 Loss:0.43858617842197417 | R2:0.13608162633023452 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[67] Loss:0.17664914671331644 | L1 Loss:0.32904032338410616 | R2:0.5467862842889697 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[67] Loss:0.34700667560100557 | L1 Loss:0.452879935503006 | R2:0.12768847611311251 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[68] Loss:0.1691889395006001 | L1 Loss:0.31870709359645844 | R2:0.5708416259576643 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[68] Loss:0.33770749121904375 | L1 Loss:0.4467030495405197 | R2:0.15345353694175934 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[69] Loss:0.16563797602429986 | L1 Loss:0.3192274384200573 | R2:0.580037465454624 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[69] Loss:0.34503712207078935 | L1 Loss:0.4587047189474106 | R2:0.12650465602291197 | ACC: 62.0000%(186/300)\n",
            "Testing Epoch[70] Loss:0.17045059660449624 | L1 Loss:0.32560266461223364 | R2:0.5606859501227651 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[70] Loss:0.3441726267337799 | L1 Loss:0.4531497210264206 | R2:0.1296797828675424 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[71] Loss:0.1760720917955041 | L1 Loss:0.3346075229346752 | R2:0.5472632810093565 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[71] Loss:0.33961206674575806 | L1 Loss:0.44800250232219696 | R2:0.13770775315469913 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[72] Loss:0.17807810846716166 | L1 Loss:0.33253007754683495 | R2:0.5436752924902192 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[72] Loss:0.33680032640695573 | L1 Loss:0.4498666912317276 | R2:0.13786515771661706 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[73] Loss:0.17306885495781898 | L1 Loss:0.33096873573958874 | R2:0.5600193038735403 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[73] Loss:0.3396432712674141 | L1 Loss:0.4536020964384079 | R2:0.13268509266693213 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[74] Loss:0.17320166574791074 | L1 Loss:0.3255347516387701 | R2:0.5587289818275845 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[74] Loss:0.35355756282806394 | L1 Loss:0.46437216401100156 | R2:0.10406164256156214 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[75] Loss:0.16993102338165045 | L1 Loss:0.3184508755803108 | R2:0.5673005358202917 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[75] Loss:0.35130312740802766 | L1 Loss:0.46155705749988557 | R2:0.10953151596485133 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[76] Loss:0.17331303376704454 | L1 Loss:0.3234701659530401 | R2:0.5575589564817095 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[76] Loss:0.3462446451187134 | L1 Loss:0.4551300883293152 | R2:0.11865400630811877 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[77] Loss:0.18057881947606802 | L1 Loss:0.33380539156496525 | R2:0.5436852667707831 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[77] Loss:0.34472025781869886 | L1 Loss:0.45539681017398836 | R2:0.1296988768517461 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[78] Loss:0.1797312735579908 | L1 Loss:0.3310624174773693 | R2:0.5399654353527712 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[78] Loss:0.3463401645421982 | L1 Loss:0.4527296036481857 | R2:0.12424263353764246 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[79] Loss:0.1831678538583219 | L1 Loss:0.3370092110708356 | R2:0.534762934641717 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[79] Loss:0.34170131385326385 | L1 Loss:0.44820471704006193 | R2:0.14052409614667324 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[80] Loss:0.1794310244731605 | L1 Loss:0.33344764076173306 | R2:0.5430239501721469 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[80] Loss:0.3464011400938034 | L1 Loss:0.45141802728176117 | R2:0.13055430076478486 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[81] Loss:0.18219853658229113 | L1 Loss:0.338072769343853 | R2:0.534358450521243 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[81] Loss:0.3460602045059204 | L1 Loss:0.4552837640047073 | R2:0.124870355742889 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[82] Loss:0.17590187210589647 | L1 Loss:0.3317894674837589 | R2:0.5507053260465355 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[82] Loss:0.3405751198530197 | L1 Loss:0.44866575598716735 | R2:0.14037872434847443 | ACC: 62.6667%(188/300)\n",
            "Testing Epoch[83] Loss:0.17285562865436077 | L1 Loss:0.3234043288975954 | R2:0.5582740229130801 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[83] Loss:0.3246738836169243 | L1 Loss:0.43326640129089355 | R2:0.18467711826544825 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[84] Loss:0.17786297481507063 | L1 Loss:0.33336096815764904 | R2:0.5452500090771646 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[84] Loss:0.334553425014019 | L1 Loss:0.4394395679235458 | R2:0.15355054159898182 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[85] Loss:0.17790440376847982 | L1 Loss:0.33434013091027737 | R2:0.5428102977728214 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[85] Loss:0.3449534699320793 | L1 Loss:0.44224515855312346 | R2:0.12486972005500707 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[86] Loss:0.18203405989333987 | L1 Loss:0.34012564830482006 | R2:0.5325407125729532 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[86] Loss:0.35406920313835144 | L1 Loss:0.4450121283531189 | R2:0.10540126391174409 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[87] Loss:0.17561931861564517 | L1 Loss:0.32628269121050835 | R2:0.5508018332654994 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[87] Loss:0.3400792255997658 | L1 Loss:0.4423320859670639 | R2:0.13277511583150803 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[88] Loss:0.1622968497686088 | L1 Loss:0.31361703760921955 | R2:0.5839881049294551 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[88] Loss:0.32866166681051256 | L1 Loss:0.44004500210285186 | R2:0.15656272431709045 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[89] Loss:0.1555348583497107 | L1 Loss:0.310973328538239 | R2:0.6020086084677753 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[89] Loss:0.3294066086411476 | L1 Loss:0.44390600323677065 | R2:0.15670782825904941 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[90] Loss:0.14923768443986773 | L1 Loss:0.3032244397327304 | R2:0.6172712276046046 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[90] Loss:0.3199086979031563 | L1 Loss:0.4402346223592758 | R2:0.1885475486742953 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[91] Loss:0.1504381913691759 | L1 Loss:0.3068706654012203 | R2:0.6142155421514441 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[91] Loss:0.3209842056035995 | L1 Loss:0.4374562233686447 | R2:0.18638971005376276 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[92] Loss:0.15096266707405448 | L1 Loss:0.30910321697592735 | R2:0.6121280369814429 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[92] Loss:0.32810716778039933 | L1 Loss:0.4414263039827347 | R2:0.1691229545959949 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[93] Loss:0.1512292823754251 | L1 Loss:0.31212277431041 | R2:0.6120557765505885 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[93] Loss:0.3173360928893089 | L1 Loss:0.4323703944683075 | R2:0.19540621457982135 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[94] Loss:0.15274680452421308 | L1 Loss:0.31207582354545593 | R2:0.6116296485328568 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[94] Loss:0.31812207251787183 | L1 Loss:0.42881438732147215 | R2:0.1974740361861207 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[95] Loss:0.14713323768228292 | L1 Loss:0.3060020776465535 | R2:0.6276806531612492 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[95] Loss:0.3102374985814095 | L1 Loss:0.42587103247642516 | R2:0.22000390897115057 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[96] Loss:0.15735554648563266 | L1 Loss:0.31570890173316 | R2:0.6013439163232361 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[96] Loss:0.31757353991270065 | L1 Loss:0.42940452694892883 | R2:0.19818299946866322 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[97] Loss:0.15879255486652255 | L1 Loss:0.3155133370310068 | R2:0.593668570876482 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[97] Loss:0.32039174884557725 | L1 Loss:0.4310223668813705 | R2:0.19319859091232489 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[98] Loss:0.16481413366273046 | L1 Loss:0.3189902389422059 | R2:0.5737747944233812 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[98] Loss:0.32144185304641726 | L1 Loss:0.4299706816673279 | R2:0.18777420447548335 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[99] Loss:0.1666158284060657 | L1 Loss:0.31736999843269587 | R2:0.5694216846385973 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[99] Loss:0.32147806584835054 | L1 Loss:0.43060967028141023 | R2:0.1849176490835745 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[100] Loss:0.16534972377121449 | L1 Loss:0.3195883771404624 | R2:0.5742844694174196 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[100] Loss:0.32424200475215914 | L1 Loss:0.4354784607887268 | R2:0.1735797137104263 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[101] Loss:0.1518714646808803 | L1 Loss:0.3063838295638561 | R2:0.6107019783151652 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[101] Loss:0.32146157026290895 | L1 Loss:0.4391060799360275 | R2:0.18266554650621614 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[102] Loss:0.14090062584728003 | L1 Loss:0.2945591565221548 | R2:0.6396063918382991 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[102] Loss:0.3317061513662338 | L1 Loss:0.4458366960287094 | R2:0.15274516750676181 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[103] Loss:0.14788012858480215 | L1 Loss:0.3055674349889159 | R2:0.6243311541141997 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[103] Loss:0.3290471524000168 | L1 Loss:0.44140991270542146 | R2:0.16154797266901683 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[104] Loss:0.14899585861712694 | L1 Loss:0.3073846623301506 | R2:0.6210029554450868 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[104] Loss:0.3127707228064537 | L1 Loss:0.4312687635421753 | R2:0.20874108273980582 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[105] Loss:0.15531834214925766 | L1 Loss:0.3133838940411806 | R2:0.6050676902070902 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[105] Loss:0.32942858189344404 | L1 Loss:0.4472228497266769 | R2:0.1627002951233097 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[106] Loss:0.16310408152639866 | L1 Loss:0.32289063557982445 | R2:0.5872225134014967 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[106] Loss:0.33156688809394835 | L1 Loss:0.4460607051849365 | R2:0.1548779658600008 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[107] Loss:0.16054270369932055 | L1 Loss:0.3166503272950649 | R2:0.5915416938643012 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[107] Loss:0.3300541818141937 | L1 Loss:0.4478108733892441 | R2:0.1624101558770943 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[108] Loss:0.1560062561184168 | L1 Loss:0.31473456509411335 | R2:0.6042190723540426 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[108] Loss:0.3261657014489174 | L1 Loss:0.4452044278383255 | R2:0.1739946989979128 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[109] Loss:0.15437461901456118 | L1 Loss:0.311115775257349 | R2:0.6073759188535451 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[109] Loss:0.33682009428739546 | L1 Loss:0.4479102581739426 | R2:0.15072396750228562 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[110] Loss:0.16226855805143714 | L1 Loss:0.31657367292791605 | R2:0.5850142492947383 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[110] Loss:0.33334777802228927 | L1 Loss:0.4470881313085556 | R2:0.15227810735065322 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[111] Loss:0.15778183424845338 | L1 Loss:0.30871361307799816 | R2:0.5970922542086933 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[111] Loss:0.33353955000638963 | L1 Loss:0.44983412623405455 | R2:0.1489609899438626 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[112] Loss:0.15749951265752316 | L1 Loss:0.3123395759612322 | R2:0.5982836882988297 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[112] Loss:0.33629789054393766 | L1 Loss:0.44859706461429594 | R2:0.14206638240816488 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[113] Loss:0.1472044731490314 | L1 Loss:0.30343394074589014 | R2:0.6232636432506832 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[113] Loss:0.32265160232782364 | L1 Loss:0.4440895140171051 | R2:0.17680806787652387 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[114] Loss:0.1477529974654317 | L1 Loss:0.3049258328974247 | R2:0.6232898236945269 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[114] Loss:0.3296734794974327 | L1 Loss:0.4520643502473831 | R2:0.15982450366474565 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[115] Loss:0.1522987987846136 | L1 Loss:0.30952305532991886 | R2:0.6125627779289244 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[115] Loss:0.3258272185921669 | L1 Loss:0.45068650543689726 | R2:0.16994686665543882 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[116] Loss:0.15307043166831136 | L1 Loss:0.30885772593319416 | R2:0.6104682514799059 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[116] Loss:0.334423790872097 | L1 Loss:0.45120706260204313 | R2:0.1491936545368961 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[117] Loss:0.15524312062188983 | L1 Loss:0.31378532387316227 | R2:0.6079840518688262 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[117] Loss:0.33795070499181745 | L1 Loss:0.45119627118110656 | R2:0.14859979730963038 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[118] Loss:0.15937530063092709 | L1 Loss:0.31276824325323105 | R2:0.5980376593803101 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[118] Loss:0.33475787937641144 | L1 Loss:0.4437086820602417 | R2:0.15855950278377376 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[119] Loss:0.15953028108924627 | L1 Loss:0.3110252935439348 | R2:0.5953374689184314 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[119] Loss:0.3430633947253227 | L1 Loss:0.4480990141630173 | R2:0.13785144964979495 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[120] Loss:0.15751021495088935 | L1 Loss:0.3057323507964611 | R2:0.5977343803003081 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[120] Loss:0.3214274436235428 | L1 Loss:0.4340099304914474 | R2:0.19008847302660806 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[121] Loss:0.16067193308845162 | L1 Loss:0.30925992503762245 | R2:0.5882670940774772 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[121] Loss:0.3250152036547661 | L1 Loss:0.43755927979946135 | R2:0.17541780452588124 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[122] Loss:0.15867809252813458 | L1 Loss:0.31110492162406445 | R2:0.5927514821673222 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[122] Loss:0.322894299030304 | L1 Loss:0.4383174777030945 | R2:0.1761100622294804 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[123] Loss:0.15687496028840542 | L1 Loss:0.31071802973747253 | R2:0.5996732077187394 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[123] Loss:0.325124055147171 | L1 Loss:0.44189954102039336 | R2:0.16702343249479232 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[124] Loss:0.1492449757643044 | L1 Loss:0.302970414981246 | R2:0.6190373975631526 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[124] Loss:0.3241787388920784 | L1 Loss:0.4420217633247375 | R2:0.16961888369306188 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[125] Loss:0.14866581512615085 | L1 Loss:0.30339307989925146 | R2:0.6207697944929775 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[125] Loss:0.3348431721329689 | L1 Loss:0.453688645362854 | R2:0.14210631050926142 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[126] Loss:0.1493966863490641 | L1 Loss:0.302580707706511 | R2:0.6187631169064793 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[126] Loss:0.33269200623035433 | L1 Loss:0.44970426261425017 | R2:0.1599473523951313 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[127] Loss:0.151127012912184 | L1 Loss:0.303329074755311 | R2:0.6165545642397579 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[127] Loss:0.32941455245018003 | L1 Loss:0.4411792278289795 | R2:0.16262356110532625 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[128] Loss:0.15571636520326138 | L1 Loss:0.3086094353348017 | R2:0.6036980639032512 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[128] Loss:0.3247091382741928 | L1 Loss:0.4307657331228256 | R2:0.179787473416293 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[129] Loss:0.15756485098972917 | L1 Loss:0.3095493000000715 | R2:0.5939359618597491 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[129] Loss:0.326322902739048 | L1 Loss:0.4343888252973557 | R2:0.17850677581864866 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[130] Loss:0.16110186045989394 | L1 Loss:0.31153171695768833 | R2:0.5854734461346358 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[130] Loss:0.3283305957913399 | L1 Loss:0.43549796044826505 | R2:0.17731810289158584 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[131] Loss:0.166053781285882 | L1 Loss:0.31571322306990623 | R2:0.5696354870361306 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[131] Loss:0.32296801209449766 | L1 Loss:0.43895098865032195 | R2:0.18742751649635606 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[132] Loss:0.1654256759211421 | L1 Loss:0.3185298312455416 | R2:0.5721102551269255 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[132] Loss:0.3199062943458557 | L1 Loss:0.4409241914749146 | R2:0.190578787416731 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[133] Loss:0.16759522818028927 | L1 Loss:0.32470564916729927 | R2:0.5647696541961214 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[133] Loss:0.31678341925144193 | L1 Loss:0.44062191247940063 | R2:0.19047882195996568 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[134] Loss:0.15480282343924046 | L1 Loss:0.3097673989832401 | R2:0.5984944498207964 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[134] Loss:0.3241660386323929 | L1 Loss:0.44836297929286956 | R2:0.17186350992671695 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[135] Loss:0.14710065629333258 | L1 Loss:0.3025809722021222 | R2:0.617856335086119 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[135] Loss:0.3325978934764862 | L1 Loss:0.45378938913345335 | R2:0.14918657216915757 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[136] Loss:0.14498168556019664 | L1 Loss:0.29632396064698696 | R2:0.6259215029136548 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[136] Loss:0.33148655891418455 | L1 Loss:0.4466564357280731 | R2:0.15201436412854075 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[137] Loss:0.14429555088281631 | L1 Loss:0.29996067844331264 | R2:0.6299500878483155 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[137] Loss:0.33156131505966185 | L1 Loss:0.4449432760477066 | R2:0.1546273602612131 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[138] Loss:0.14170422917231917 | L1 Loss:0.29939625039696693 | R2:0.6371496455641769 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[138] Loss:0.33952158540487287 | L1 Loss:0.45189303159713745 | R2:0.14069962135341513 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[139] Loss:0.14353409223258495 | L1 Loss:0.29840181954205036 | R2:0.6351873997964075 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[139] Loss:0.33959123194217683 | L1 Loss:0.45155892670154574 | R2:0.13893768034729984 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[140] Loss:0.14694732567295432 | L1 Loss:0.30288567394018173 | R2:0.6284758338114277 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[140] Loss:0.3326841428875923 | L1 Loss:0.4496239960193634 | R2:0.15801764940331187 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[141] Loss:0.14806083962321281 | L1 Loss:0.3016418982297182 | R2:0.6246514338997213 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[141] Loss:0.3163937196135521 | L1 Loss:0.4331501603126526 | R2:0.19966679043760474 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[142] Loss:0.15255140233784914 | L1 Loss:0.3090943181887269 | R2:0.6101854189843955 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[142] Loss:0.3124962866306305 | L1 Loss:0.43169474601745605 | R2:0.20628071472457976 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[143] Loss:0.15692955907434225 | L1 Loss:0.3114707497879863 | R2:0.5949443224375931 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[143] Loss:0.31079392731189726 | L1 Loss:0.4241448640823364 | R2:0.2151165049468135 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[144] Loss:0.1510584349744022 | L1 Loss:0.3076165048405528 | R2:0.6156038115478453 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[144] Loss:0.3098632127046585 | L1 Loss:0.42639235854148866 | R2:0.21064473088517838 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[145] Loss:0.14578455686569214 | L1 Loss:0.2999600553885102 | R2:0.6279832195698 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[145] Loss:0.3117845684289932 | L1 Loss:0.43431879580020905 | R2:0.2092327501025221 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[146] Loss:0.14528196211904287 | L1 Loss:0.30030545964837074 | R2:0.6279250753773512 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[146] Loss:0.3186966121196747 | L1 Loss:0.43745535910129546 | R2:0.18915849223977083 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[147] Loss:0.14393818052485585 | L1 Loss:0.29877519328147173 | R2:0.6347084984998295 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[147] Loss:0.3206374779343605 | L1 Loss:0.437276229262352 | R2:0.18618583387243695 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[148] Loss:0.14252491295337677 | L1 Loss:0.2959917364642024 | R2:0.6391384764436986 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[148] Loss:0.3149087160825729 | L1 Loss:0.43233610689640045 | R2:0.2025409336232648 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[149] Loss:0.14407464675605297 | L1 Loss:0.29890308529138565 | R2:0.6361211454601117 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[149] Loss:0.313503634929657 | L1 Loss:0.4294426113367081 | R2:0.20662913547115797 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[150] Loss:0.14382306206971407 | L1 Loss:0.29837656021118164 | R2:0.6354819819995055 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[150] Loss:0.31281107366085054 | L1 Loss:0.4288527280092239 | R2:0.20427473786036635 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[151] Loss:0.14673024928197265 | L1 Loss:0.30137473717331886 | R2:0.6253157488255108 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[151] Loss:0.3272430822253227 | L1 Loss:0.43933734893798826 | R2:0.16627476221313625 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[152] Loss:0.1489808945916593 | L1 Loss:0.3015259923413396 | R2:0.6202943068137041 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[152] Loss:0.324826517701149 | L1 Loss:0.438397490978241 | R2:0.1742536799180779 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[153] Loss:0.15055328514426947 | L1 Loss:0.3053375147283077 | R2:0.6174131677131599 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[153] Loss:0.32303441017866136 | L1 Loss:0.443074107170105 | R2:0.17067589869557848 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[154] Loss:0.14828435773961246 | L1 Loss:0.29831742960959673 | R2:0.6189579226403285 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[154] Loss:0.3296381920576096 | L1 Loss:0.4510386556386948 | R2:0.15894926772041257 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[155] Loss:0.14293250488117337 | L1 Loss:0.2927252259105444 | R2:0.6334941775492836 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[155] Loss:0.30750891417264936 | L1 Loss:0.43324301242828367 | R2:0.21485221863257248 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[156] Loss:0.14067945908755064 | L1 Loss:0.2934713941067457 | R2:0.6397421626560301 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[156] Loss:0.3034987524151802 | L1 Loss:0.4268187522888184 | R2:0.22668202205961369 | ACC: 63.3333%(190/300)\n",
            "Testing Epoch[157] Loss:0.137523477897048 | L1 Loss:0.29421649500727654 | R2:0.6511096304426276 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[157] Loss:0.32167263329029083 | L1 Loss:0.4416992098093033 | R2:0.17862449071514544 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[158] Loss:0.13548484211787581 | L1 Loss:0.29019647650420666 | R2:0.6535108421021315 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[158] Loss:0.3249548465013504 | L1 Loss:0.4418594717979431 | R2:0.17616604696067598 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[159] Loss:0.13980810530483723 | L1 Loss:0.2974698022007942 | R2:0.6424993566085218 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[159] Loss:0.3341511026024818 | L1 Loss:0.44722277522087095 | R2:0.15018704570472466 | ACC: 64.3333%(193/300)\n",
            "Testing Epoch[160] Loss:0.13747665146365762 | L1 Loss:0.2941074464470148 | R2:0.6489182231727085 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[160] Loss:0.32050197571516037 | L1 Loss:0.4396988034248352 | R2:0.18737960138498413 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[161] Loss:0.1430349089205265 | L1 Loss:0.29879594780504704 | R2:0.6357375907828096 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[161] Loss:0.32146174758672713 | L1 Loss:0.43903466165065763 | R2:0.1856078058842522 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[162] Loss:0.14331053849309683 | L1 Loss:0.298126588575542 | R2:0.6381935388045626 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[162] Loss:0.3184207111597061 | L1 Loss:0.4364208161830902 | R2:0.18842761586245066 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[163] Loss:0.14386046724393964 | L1 Loss:0.29936395306140184 | R2:0.6346518375724101 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[163] Loss:0.32267100512981417 | L1 Loss:0.4387756079435349 | R2:0.1761535019211331 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[164] Loss:0.14816920878365636 | L1 Loss:0.30189845245331526 | R2:0.6203978359486054 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[164] Loss:0.3214531570672989 | L1 Loss:0.43989711403846743 | R2:0.18080475835563356 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[165] Loss:0.1442237882874906 | L1 Loss:0.29905514512211084 | R2:0.6333627968679724 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[165] Loss:0.32341711819171903 | L1 Loss:0.4386992812156677 | R2:0.1754584248475583 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[166] Loss:0.14535277662798762 | L1 Loss:0.2968275900930166 | R2:0.6309532919455114 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[166] Loss:0.32451101690530776 | L1 Loss:0.4390157014131546 | R2:0.17295950571102445 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[167] Loss:0.14444337459281087 | L1 Loss:0.2942963242530823 | R2:0.6329726578757231 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[167] Loss:0.3152641490101814 | L1 Loss:0.42810951769351957 | R2:0.20180939314015575 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[168] Loss:0.14217462437227368 | L1 Loss:0.29345084726810455 | R2:0.639566786690908 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[168] Loss:0.3182034566998482 | L1 Loss:0.4275180697441101 | R2:0.19103826391916257 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[169] Loss:0.14308394538238645 | L1 Loss:0.29662700090557337 | R2:0.6389040380318647 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[169] Loss:0.3136961802840233 | L1 Loss:0.4279384821653366 | R2:0.20583796316248654 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[170] Loss:0.14267707942053676 | L1 Loss:0.2949298471212387 | R2:0.6386767051922532 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[170] Loss:0.3150993183255196 | L1 Loss:0.4300301641225815 | R2:0.20303134647405446 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[171] Loss:0.1435620547272265 | L1 Loss:0.2971467487514019 | R2:0.6338485068228668 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[171] Loss:0.3185987487435341 | L1 Loss:0.4316121220588684 | R2:0.19273166632902047 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[172] Loss:0.14762133825570345 | L1 Loss:0.3021198492497206 | R2:0.6247979567881092 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[172] Loss:0.3258502945303917 | L1 Loss:0.4343363791704178 | R2:0.17313502516629786 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[173] Loss:0.14792456338182092 | L1 Loss:0.2998803621158004 | R2:0.6213907848529647 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[173] Loss:0.3279778063297272 | L1 Loss:0.4372177332639694 | R2:0.17374092599823357 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[174] Loss:0.15169852832332253 | L1 Loss:0.3050389038398862 | R2:0.6111107311102358 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[174] Loss:0.3325722560286522 | L1 Loss:0.44086462557315825 | R2:0.1628368275953484 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[175] Loss:0.1577986078336835 | L1 Loss:0.3129790732637048 | R2:0.593657241503371 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[175] Loss:0.32716392129659655 | L1 Loss:0.4369034647941589 | R2:0.17957910415836595 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[176] Loss:0.1575606749393046 | L1 Loss:0.31245123874396086 | R2:0.5960600927355233 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[176] Loss:0.31743766367435455 | L1 Loss:0.4264416217803955 | R2:0.198441076207836 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[177] Loss:0.15723937237635255 | L1 Loss:0.3142093028873205 | R2:0.5940464882526261 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[177] Loss:0.31790269166231155 | L1 Loss:0.4285684019327164 | R2:0.19284939754095182 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[178] Loss:0.15185063192620873 | L1 Loss:0.31079360377043486 | R2:0.6097780548098309 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[178] Loss:0.3211514577269554 | L1 Loss:0.4313701868057251 | R2:0.18531750009309128 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[179] Loss:0.14819117402657866 | L1 Loss:0.30748544819653034 | R2:0.618858778330292 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[179] Loss:0.33000569939613345 | L1 Loss:0.43773668110370634 | R2:0.16300819190275004 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[180] Loss:0.1428132406435907 | L1 Loss:0.29682871885597706 | R2:0.6364915018490549 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[180] Loss:0.3382844105362892 | L1 Loss:0.4505338728427887 | R2:0.14222929973920223 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[181] Loss:0.1439675558358431 | L1 Loss:0.29584137722849846 | R2:0.6344133658693554 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[181] Loss:0.3297191932797432 | L1 Loss:0.4475987106561661 | R2:0.1661953244285692 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[182] Loss:0.15013678511604667 | L1 Loss:0.3002296006307006 | R2:0.6201893462221985 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[182] Loss:0.3323930338025093 | L1 Loss:0.45188980996608735 | R2:0.16232398609593773 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[183] Loss:0.1513516530394554 | L1 Loss:0.30178089905530214 | R2:0.617960912131996 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[183] Loss:0.32924890518188477 | L1 Loss:0.4504934251308441 | R2:0.16764795327167578 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[184] Loss:0.14709184924140573 | L1 Loss:0.2996063008904457 | R2:0.6248679299457304 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[184] Loss:0.32510016560554506 | L1 Loss:0.44097343385219573 | R2:0.18234879182791802 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[185] Loss:0.14374432526528835 | L1 Loss:0.2931213201954961 | R2:0.6333163850420006 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[185] Loss:0.31870325803756716 | L1 Loss:0.43512156009674074 | R2:0.1978485630499399 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[186] Loss:0.14247536938637495 | L1 Loss:0.2911316156387329 | R2:0.6349087572463445 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[186] Loss:0.3257192388176918 | L1 Loss:0.4403313934803009 | R2:0.18169259052373268 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[187] Loss:0.1470644185319543 | L1 Loss:0.29688939824700356 | R2:0.6222634802027995 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[187] Loss:0.3358559846878052 | L1 Loss:0.44508672058582305 | R2:0.15881933438116885 | ACC: 62.6667%(188/300)\n",
            "Testing Epoch[188] Loss:0.14476234931498766 | L1 Loss:0.2919306894764304 | R2:0.6295161438016815 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[188] Loss:0.32904806882143023 | L1 Loss:0.44099311232566835 | R2:0.17343958061708836 | ACC: 63.6667%(191/300)\n",
            "Testing Epoch[189] Loss:0.14157212059944868 | L1 Loss:0.29163182992488146 | R2:0.6404456574265154 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[189] Loss:0.3212972700595856 | L1 Loss:0.4339576452970505 | R2:0.195269561068984 | ACC: 63.0000%(189/300)\n",
            "Testing Epoch[190] Loss:0.14510409766808152 | L1 Loss:0.2956136241555214 | R2:0.6345310716306142 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[190] Loss:0.32004587799310685 | L1 Loss:0.42953685820102694 | R2:0.19738257141169885 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[191] Loss:0.1467950940132141 | L1 Loss:0.29759575985372066 | R2:0.6310543160636424 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[191] Loss:0.31671417951583863 | L1 Loss:0.42492838203907013 | R2:0.20234283269911774 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[192] Loss:0.145957475528121 | L1 Loss:0.2966823847964406 | R2:0.6323488201773623 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[192] Loss:0.3110587254166603 | L1 Loss:0.42359311878681183 | R2:0.21384173196783834 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[193] Loss:0.14528029318898916 | L1 Loss:0.2965695131570101 | R2:0.6348910040837563 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[193] Loss:0.30601926892995834 | L1 Loss:0.42276514172554014 | R2:0.2268296748256234 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[194] Loss:0.14833510713651776 | L1 Loss:0.3006836287677288 | R2:0.6255864573010397 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[194] Loss:0.30883303880691526 | L1 Loss:0.4263675421476364 | R2:0.2203956500639875 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[195] Loss:0.1476596063002944 | L1 Loss:0.2997785918414593 | R2:0.6222018465863288 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[195] Loss:0.30857455879449847 | L1 Loss:0.42892573177814486 | R2:0.22082479505403868 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[196] Loss:0.15192219149321318 | L1 Loss:0.30579846538603306 | R2:0.6095305408577613 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[196] Loss:0.30892073214054105 | L1 Loss:0.4298094719648361 | R2:0.2199329191283951 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[197] Loss:0.1524448785930872 | L1 Loss:0.30993987433612347 | R2:0.6089642726905604 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[197] Loss:0.311137892305851 | L1 Loss:0.43216860890388487 | R2:0.21695214360260212 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[198] Loss:0.14804553519934416 | L1 Loss:0.3048750013113022 | R2:0.6200089778053546 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[198] Loss:0.3167347311973572 | L1 Loss:0.435382941365242 | R2:0.20044824765440605 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[199] Loss:0.1502969851717353 | L1 Loss:0.3057176945731044 | R2:0.613012943721174 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[199] Loss:0.3129828378558159 | L1 Loss:0.43064881265163424 | R2:0.21388266130970512 | ACC: 67.6667%(203/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "# train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "# val_acc_record = []\n",
        "# val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "# test_acc_record = []\n",
        "# test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(100)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, mirna, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5rZXvDpIQTD",
        "outputId": "188227ff-6290-4585-9b4d-c441b8133332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTesting Epoch[0] Loss:0.14997074706479907 | L1 Loss:0.3028674805536866 | R2:0.6145739861934961 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[0] Loss:0.3130040973424911 | L1 Loss:0.4351896971464157 | R2:0.21279177765598845 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[1] Loss:0.147736472543329 | L1 Loss:0.2990090372040868 | R2:0.6216583333089616 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[1] Loss:0.3129752531647682 | L1 Loss:0.4345860332250595 | R2:0.2088538351472132 | ACC: 66.3333%(199/300)\n",
            "Testing Epoch[2] Loss:0.14597220160067081 | L1 Loss:0.3000742308795452 | R2:0.6265223390822223 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[2] Loss:0.31129287332296374 | L1 Loss:0.4314689815044403 | R2:0.2127506768991334 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[3] Loss:0.14482517819851637 | L1 Loss:0.2976503847166896 | R2:0.6329657753538975 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[3] Loss:0.3162830501794815 | L1 Loss:0.43332878351211546 | R2:0.19794078188592273 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[4] Loss:0.14123255759477615 | L1 Loss:0.29428772535175085 | R2:0.64235015633036 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[4] Loss:0.31480015218257906 | L1 Loss:0.4352161377668381 | R2:0.20086974259588533 | ACC: 64.0000%(192/300)\n",
            "Testing Epoch[5] Loss:0.14136986806988716 | L1 Loss:0.2932178610935807 | R2:0.6415167721663866 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[5] Loss:0.3173018380999565 | L1 Loss:0.4318429410457611 | R2:0.19703520468495592 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[6] Loss:0.1387103321030736 | L1 Loss:0.287684322334826 | R2:0.6467068363019342 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[6] Loss:0.32013606280088425 | L1 Loss:0.4338700294494629 | R2:0.19189643987614333 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[7] Loss:0.1417962615378201 | L1 Loss:0.2918669832870364 | R2:0.6365766206734085 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[7] Loss:0.31650461852550504 | L1 Loss:0.430074343085289 | R2:0.20465900197562478 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[8] Loss:0.1432727249339223 | L1 Loss:0.29276504553854465 | R2:0.6331359144057563 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[8] Loss:0.30859278589487077 | L1 Loss:0.425481390953064 | R2:0.2239945951435204 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[9] Loss:0.14375735772773623 | L1 Loss:0.29529177490621805 | R2:0.6334718567444217 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[9] Loss:0.3040733724832535 | L1 Loss:0.4233984053134918 | R2:0.23468695239974657 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[10] Loss:0.14123464934527874 | L1 Loss:0.2966247834265232 | R2:0.6431591100000175 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[10] Loss:0.3090514957904816 | L1 Loss:0.4312315821647644 | R2:0.21638956680176089 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[11] Loss:0.1413567252457142 | L1 Loss:0.29653059132397175 | R2:0.6433977971496126 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[11] Loss:0.31661778390407563 | L1 Loss:0.43498910665512086 | R2:0.19933654253237162 | ACC: 69.6667%(209/300)\n",
            "Testing Epoch[12] Loss:0.14157708315178752 | L1 Loss:0.2957326713949442 | R2:0.6432927810061909 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[12] Loss:0.31882363855838775 | L1 Loss:0.43620376884937284 | R2:0.1943667564072825 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[13] Loss:0.14087764360010624 | L1 Loss:0.29337191954255104 | R2:0.645037263479752 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[13] Loss:0.31953044086694715 | L1 Loss:0.43648322522640226 | R2:0.18995134217987464 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[14] Loss:0.13982214918360114 | L1 Loss:0.29293480422347784 | R2:0.6461928735228479 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[14] Loss:0.31167364716529844 | L1 Loss:0.4264644384384155 | R2:0.21145971701306926 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[15] Loss:0.14232305227778852 | L1 Loss:0.2935828734189272 | R2:0.6400416844018703 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[15] Loss:0.3109297171235085 | L1 Loss:0.42152110636234286 | R2:0.2172703180090691 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[16] Loss:0.14529624488204718 | L1 Loss:0.2953531127423048 | R2:0.6317135143659429 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[16] Loss:0.31561736315488814 | L1 Loss:0.4200165718793869 | R2:0.2086739045156864 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[17] Loss:0.15185742545872927 | L1 Loss:0.30198668874800205 | R2:0.613190889268845 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[17] Loss:0.31115168482065203 | L1 Loss:0.41926794648170473 | R2:0.21982878070092382 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[18] Loss:0.15377015341073275 | L1 Loss:0.3027178570628166 | R2:0.6083851614127116 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[18] Loss:0.3188041910529137 | L1 Loss:0.42892506122589114 | R2:0.1993403925849122 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[19] Loss:0.14891358138993382 | L1 Loss:0.3003395590931177 | R2:0.6192600212367002 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[19] Loss:0.31414712518453597 | L1 Loss:0.4268266141414642 | R2:0.20900606324672566 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[20] Loss:0.1447415119037032 | L1 Loss:0.29704164154827595 | R2:0.630341927678883 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[20] Loss:0.3108700156211853 | L1 Loss:0.42723641395568845 | R2:0.2170523017891146 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[21] Loss:0.14360108179971576 | L1 Loss:0.29719293117523193 | R2:0.6356315332379017 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[21] Loss:0.30970538407564163 | L1 Loss:0.4254663914442062 | R2:0.21908686425642782 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[22] Loss:0.14490507449954748 | L1 Loss:0.30016976594924927 | R2:0.6327198367749368 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[22] Loss:0.3066253483295441 | L1 Loss:0.42085641622543335 | R2:0.22568907722242307 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[23] Loss:0.14288190007209778 | L1 Loss:0.29963339027017355 | R2:0.6375409338167596 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[23] Loss:0.30416620075702666 | L1 Loss:0.4201852440834045 | R2:0.231147264700143 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[24] Loss:0.13982238760218024 | L1 Loss:0.2992495335638523 | R2:0.6443530684331613 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[24] Loss:0.3069354698061943 | L1 Loss:0.42004876434803007 | R2:0.224141235843705 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[25] Loss:0.14494120050221682 | L1 Loss:0.3028466133400798 | R2:0.6288147840710097 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[25] Loss:0.3054696723818779 | L1 Loss:0.4199586272239685 | R2:0.22710170706545024 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[26] Loss:0.1462787790223956 | L1 Loss:0.30422302056103945 | R2:0.6250339013340882 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[26] Loss:0.30563394278287886 | L1 Loss:0.42228407859802247 | R2:0.22519425437035173 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[27] Loss:0.1478973152115941 | L1 Loss:0.30201625265181065 | R2:0.6221920810870666 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[27] Loss:0.3054904341697693 | L1 Loss:0.42813592553138735 | R2:0.2250491316648784 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[28] Loss:0.14820198575034738 | L1 Loss:0.29915107041597366 | R2:0.620412033022108 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[28] Loss:0.30983908474445343 | L1 Loss:0.43025378286838534 | R2:0.2157117720619361 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[29] Loss:0.15101504093036056 | L1 Loss:0.301109267398715 | R2:0.6159156983757231 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[29] Loss:0.3144864350557327 | L1 Loss:0.43572005331516267 | R2:0.20324669146888824 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[30] Loss:0.14506931835785508 | L1 Loss:0.2981886649504304 | R2:0.6311672096593082 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[30] Loss:0.3235737934708595 | L1 Loss:0.43988666534423826 | R2:0.18096007116953844 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[31] Loss:0.14017594512552023 | L1 Loss:0.2926756199449301 | R2:0.6441056040986854 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[31] Loss:0.3280563697218895 | L1 Loss:0.441149166226387 | R2:0.17216563164364768 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[32] Loss:0.13772619096562266 | L1 Loss:0.2886452553793788 | R2:0.6498210818021211 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[32] Loss:0.32557815611362456 | L1 Loss:0.43553459346294404 | R2:0.1801228506255303 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[33] Loss:0.1417487938888371 | L1 Loss:0.2895770100876689 | R2:0.6385986194532053 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[33] Loss:0.31789956986904144 | L1 Loss:0.4326483219861984 | R2:0.2007240068590927 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[34] Loss:0.14260822581127286 | L1 Loss:0.29228488355875015 | R2:0.638584977266217 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[34] Loss:0.31302600651979445 | L1 Loss:0.4264496356248856 | R2:0.2103567327990261 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[35] Loss:0.14452939806506038 | L1 Loss:0.29499666299670935 | R2:0.6354197343851294 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[35] Loss:0.3113009050488472 | L1 Loss:0.4279582053422928 | R2:0.21325467503506687 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[36] Loss:0.145382858812809 | L1 Loss:0.2964860685169697 | R2:0.6335229303085234 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[36] Loss:0.3080213710665703 | L1 Loss:0.42500743865966795 | R2:0.22351726912874445 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[37] Loss:0.14365837816148996 | L1 Loss:0.2966256942600012 | R2:0.6340983664095041 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[37] Loss:0.3043284028768539 | L1 Loss:0.4263032525777817 | R2:0.23205939835004646 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[38] Loss:0.14274753537029028 | L1 Loss:0.3006338141858578 | R2:0.6347766715654475 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[38] Loss:0.30604240447282793 | L1 Loss:0.42707461714744566 | R2:0.23141792826369736 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[39] Loss:0.14655362255871296 | L1 Loss:0.305217900313437 | R2:0.6253925745555415 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[39] Loss:0.30627143383026123 | L1 Loss:0.42327877283096316 | R2:0.23247502421539323 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[40] Loss:0.1433953084051609 | L1 Loss:0.3007009821012616 | R2:0.633494527718793 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[40] Loss:0.3056786864995956 | L1 Loss:0.42215907871723174 | R2:0.23365101295463492 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[41] Loss:0.14297062437981367 | L1 Loss:0.2961456635966897 | R2:0.6351651639051497 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[41] Loss:0.304219251871109 | L1 Loss:0.4206276923418045 | R2:0.23572590251143755 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[42] Loss:0.1481488705612719 | L1 Loss:0.30067059863358736 | R2:0.6223605426003589 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[42] Loss:0.3082811489701271 | L1 Loss:0.4271771818399429 | R2:0.22347108229139584 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[43] Loss:0.14940962055698037 | L1 Loss:0.3011688496917486 | R2:0.6169246959255854 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[43] Loss:0.3196010127663612 | L1 Loss:0.4381762892007828 | R2:0.19520158985629002 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[44] Loss:0.14879736863076687 | L1 Loss:0.2987297410145402 | R2:0.6197294151016629 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[44] Loss:0.3261466324329376 | L1 Loss:0.4410672575235367 | R2:0.18072447540446493 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[45] Loss:0.14423749828711152 | L1 Loss:0.2973108058795333 | R2:0.6321581491229837 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[45] Loss:0.3251208171248436 | L1 Loss:0.44165302217006686 | R2:0.18306727924456134 | ACC: 64.6667%(194/300)\n",
            "Testing Epoch[46] Loss:0.13804029440507293 | L1 Loss:0.29222607612609863 | R2:0.6475689535572761 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[46] Loss:0.32004732489585874 | L1 Loss:0.4395760029554367 | R2:0.19489885164846243 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[47] Loss:0.13680259929969907 | L1 Loss:0.29084097035229206 | R2:0.651017691450267 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[47] Loss:0.3106847867369652 | L1 Loss:0.4333751738071442 | R2:0.22118570558059628 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[48] Loss:0.13267414458096027 | L1 Loss:0.2849763594567776 | R2:0.6629519824740571 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[48] Loss:0.3042981386184692 | L1 Loss:0.4291222244501114 | R2:0.23681412340101912 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[49] Loss:0.13544571585953236 | L1 Loss:0.28734451439231634 | R2:0.6567743349501477 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[49] Loss:0.2997960403561592 | L1 Loss:0.42715218365192414 | R2:0.2472973259068758 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[50] Loss:0.13827551947906613 | L1 Loss:0.2900257185101509 | R2:0.6505636340775449 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[50] Loss:0.2903534650802612 | L1 Loss:0.4153622895479202 | R2:0.2696013764214906 | ACC: 65.0000%(195/300)\n",
            "Testing Epoch[51] Loss:0.13967209309339523 | L1 Loss:0.2922361455857754 | R2:0.6489321678496976 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[51] Loss:0.2923377901315689 | L1 Loss:0.4132752656936646 | R2:0.26507117540668135 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[52] Loss:0.1439523408189416 | L1 Loss:0.2952759191393852 | R2:0.6373351593158991 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[52] Loss:0.2975520446896553 | L1 Loss:0.41437841951847076 | R2:0.2520869710623158 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[53] Loss:0.1426448430866003 | L1 Loss:0.2931890096515417 | R2:0.6392921834007405 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[53] Loss:0.3006513923406601 | L1 Loss:0.4150463819503784 | R2:0.24372103819744884 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[54] Loss:0.14384975377470255 | L1 Loss:0.2928916048258543 | R2:0.6333725121909112 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[54] Loss:0.2992585927248001 | L1 Loss:0.41045455634593964 | R2:0.24937537509171287 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[55] Loss:0.14522640500217676 | L1 Loss:0.2958244550973177 | R2:0.6282071622753951 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[55] Loss:0.29895860999822615 | L1 Loss:0.41281144320964813 | R2:0.249222919654557 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[56] Loss:0.14624645467847586 | L1 Loss:0.29966154508292675 | R2:0.622910977569065 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[56] Loss:0.30963258296251295 | L1 Loss:0.42156401872634885 | R2:0.21810457580107784 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[57] Loss:0.15278661088086665 | L1 Loss:0.3081136317923665 | R2:0.6060852785636599 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[57] Loss:0.30681307315826417 | L1 Loss:0.42098675668239594 | R2:0.22481715296060978 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[58] Loss:0.15023925993591547 | L1 Loss:0.30470424704253674 | R2:0.6135503030408785 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[58] Loss:0.30129740536212923 | L1 Loss:0.4200377970933914 | R2:0.23919202905829642 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[59] Loss:0.14363082312047482 | L1 Loss:0.29911430925130844 | R2:0.6316983233486158 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[59] Loss:0.2973761558532715 | L1 Loss:0.4184459388256073 | R2:0.24824638494682433 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[60] Loss:0.1404523546807468 | L1 Loss:0.29391190968453884 | R2:0.6415057273673812 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[60] Loss:0.29944802820682526 | L1 Loss:0.41742328107357024 | R2:0.24496459459964895 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[61] Loss:0.14062079461291432 | L1 Loss:0.293609332293272 | R2:0.6425814890896818 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[61] Loss:0.3018063172698021 | L1 Loss:0.41718279421329496 | R2:0.24237856821222312 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[62] Loss:0.14247106993570924 | L1 Loss:0.293559561483562 | R2:0.6366791233504239 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[62] Loss:0.3006281241774559 | L1 Loss:0.4184602975845337 | R2:0.24693187588059723 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[63] Loss:0.14301414554938674 | L1 Loss:0.29356652963906527 | R2:0.6357566033093275 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[63] Loss:0.3018025949597359 | L1 Loss:0.41841798424720766 | R2:0.2452145422161433 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[64] Loss:0.14156154496595263 | L1 Loss:0.2911336626857519 | R2:0.6397083548357743 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[64] Loss:0.2985756829380989 | L1 Loss:0.4211487591266632 | R2:0.2534604790127513 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[65] Loss:0.14225663943216205 | L1 Loss:0.2933228248730302 | R2:0.6365138305261541 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[65] Loss:0.29919810891151427 | L1 Loss:0.42008980810642244 | R2:0.24845825366810867 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[66] Loss:0.14168248185887933 | L1 Loss:0.2929235743358731 | R2:0.6395755837163344 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[66] Loss:0.29600943624973297 | L1 Loss:0.4189258426427841 | R2:0.25736346935579163 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[67] Loss:0.14219227200374007 | L1 Loss:0.29456907883286476 | R2:0.639827934269858 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[67] Loss:0.29570257663726807 | L1 Loss:0.4179858386516571 | R2:0.25692474391479647 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[68] Loss:0.14210605807602406 | L1 Loss:0.29340051859617233 | R2:0.641897015487534 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[68] Loss:0.30385234355926516 | L1 Loss:0.421032178401947 | R2:0.23465997072907357 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[69] Loss:0.1408887947909534 | L1 Loss:0.29292657133191824 | R2:0.6436508808539039 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[69] Loss:0.30561719834804535 | L1 Loss:0.41916599571704866 | R2:0.22803876643273843 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[70] Loss:0.14263997320085764 | L1 Loss:0.29257199354469776 | R2:0.640312251281745 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[70] Loss:0.30383560955524447 | L1 Loss:0.4172707796096802 | R2:0.23528362999284136 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[71] Loss:0.1405185549519956 | L1 Loss:0.28965239971876144 | R2:0.6446812611526456 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[71] Loss:0.3013624608516693 | L1 Loss:0.41697993874549866 | R2:0.2415679093700816 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[72] Loss:0.1421463959850371 | L1 Loss:0.29188396595418453 | R2:0.6395380422909926 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[72] Loss:0.30428036004304887 | L1 Loss:0.42155525982379916 | R2:0.23529723987047912 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[73] Loss:0.1393432724289596 | L1 Loss:0.2918860288336873 | R2:0.6439569947915813 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[73] Loss:0.3027467429637909 | L1 Loss:0.420945143699646 | R2:0.23998659131298075 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[74] Loss:0.1390479886904359 | L1 Loss:0.29227646347135305 | R2:0.6426434422187118 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[74] Loss:0.29697457551956175 | L1 Loss:0.42063080370426176 | R2:0.25531150329896535 | ACC: 66.6667%(200/300)\n",
            "Testing Epoch[75] Loss:0.14200724055990577 | L1 Loss:0.29446748085319996 | R2:0.635443584416721 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[75] Loss:0.29828418493270875 | L1 Loss:0.4156196653842926 | R2:0.2509195000963727 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[76] Loss:0.14078610157594085 | L1 Loss:0.29350895807147026 | R2:0.6393373969742486 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[76] Loss:0.30431804358959197 | L1 Loss:0.4189140617847443 | R2:0.23737076822590372 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[77] Loss:0.14339071745052934 | L1 Loss:0.29514619149267673 | R2:0.6345489389125425 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[77] Loss:0.3016243100166321 | L1 Loss:0.41794873774051666 | R2:0.2413049359652027 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[78] Loss:0.14288688218221068 | L1 Loss:0.2935690525919199 | R2:0.6331798888370624 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[78] Loss:0.3062636315822601 | L1 Loss:0.4175653398036957 | R2:0.23528093637174 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[79] Loss:0.14269938133656979 | L1 Loss:0.29410194233059883 | R2:0.6352540323479221 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[79] Loss:0.30701514184474943 | L1 Loss:0.41682452261447905 | R2:0.23348317581867067 | ACC: 67.0000%(201/300)\n",
            "Testing Epoch[80] Loss:0.14390396513044834 | L1 Loss:0.2943450231105089 | R2:0.6318292012217301 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[80] Loss:0.29821846187114714 | L1 Loss:0.4086230456829071 | R2:0.2532971999910062 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[81] Loss:0.14018711261451244 | L1 Loss:0.2918120827525854 | R2:0.6411578573327524 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[81] Loss:0.2975318059325218 | L1 Loss:0.4096262037754059 | R2:0.25202864733467545 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[82] Loss:0.13816215749830008 | L1 Loss:0.2898806035518646 | R2:0.6476521333054321 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[82] Loss:0.29451632499694824 | L1 Loss:0.41070937514305117 | R2:0.2570745346833275 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[83] Loss:0.13757319562137127 | L1 Loss:0.2916601933538914 | R2:0.6484266718824571 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[83] Loss:0.2910515859723091 | L1 Loss:0.4075911283493042 | R2:0.2652472180300899 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[84] Loss:0.13662032410502434 | L1 Loss:0.2891918011009693 | R2:0.6510125099257851 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[84] Loss:0.29560036212205887 | L1 Loss:0.413421431183815 | R2:0.25459528821589045 | ACC: 66.0000%(198/300)\n",
            "Testing Epoch[85] Loss:0.13756782421842217 | L1 Loss:0.28964355029165745 | R2:0.648304375112069 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[85] Loss:0.3020597070455551 | L1 Loss:0.42028621435165403 | R2:0.238232085489894 | ACC: 67.6667%(203/300)\n",
            "Testing Epoch[86] Loss:0.13588470919057727 | L1 Loss:0.28501637000590563 | R2:0.6524232855930066 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[86] Loss:0.3017988383769989 | L1 Loss:0.41781742572784425 | R2:0.24362840118347365 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[87] Loss:0.13289132481440902 | L1 Loss:0.28048547077924013 | R2:0.6579034935628034 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[87] Loss:0.3043779477477074 | L1 Loss:0.4204465925693512 | R2:0.23387367721926147 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[88] Loss:0.1344241516198963 | L1 Loss:0.2816401179879904 | R2:0.6540169535567653 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[88] Loss:0.3089796811342239 | L1 Loss:0.4215006649494171 | R2:0.22484020402409696 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[89] Loss:0.13788807927630842 | L1 Loss:0.2851479025557637 | R2:0.6479975445103223 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[89] Loss:0.3116548597812653 | L1 Loss:0.4233266323804855 | R2:0.2169142728753497 | ACC: 65.6667%(197/300)\n",
            "Testing Epoch[90] Loss:0.13840977242216468 | L1 Loss:0.2875986760482192 | R2:0.6478903472049002 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[90] Loss:0.31279514729976654 | L1 Loss:0.42196882963180543 | R2:0.2106355521553564 | ACC: 65.3333%(196/300)\n",
            "Testing Epoch[91] Loss:0.13993657287210226 | L1 Loss:0.29051712062209845 | R2:0.6452459172566122 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[91] Loss:0.30777785181999207 | L1 Loss:0.4167808532714844 | R2:0.22163419327340125 | ACC: 68.0000%(204/300)\n",
            "Testing Epoch[92] Loss:0.14066855423152447 | L1 Loss:0.2922280766069889 | R2:0.644618983103637 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[92] Loss:0.3053782060742378 | L1 Loss:0.4139655530452728 | R2:0.23203432402823587 | ACC: 69.0000%(207/300)\n",
            "Testing Epoch[93] Loss:0.1390862474218011 | L1 Loss:0.2906945161521435 | R2:0.648913960512035 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[93] Loss:0.3000699445605278 | L1 Loss:0.41192019581794737 | R2:0.2471505882715817 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[94] Loss:0.13801393983885646 | L1 Loss:0.289250205270946 | R2:0.6508221339751065 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[94] Loss:0.30109839886426926 | L1 Loss:0.4114802449941635 | R2:0.24875676206541683 | ACC: 68.3333%(205/300)\n",
            "Testing Epoch[95] Loss:0.14097881922498345 | L1 Loss:0.29347767774015665 | R2:0.6410229766112021 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[95] Loss:0.3016555666923523 | L1 Loss:0.41023901402950286 | R2:0.24970367803270294 | ACC: 69.3333%(208/300)\n",
            "Testing Epoch[96] Loss:0.1458515003323555 | L1 Loss:0.2995581887662411 | R2:0.6283432089374349 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[96] Loss:0.30118470937013625 | L1 Loss:0.4127738445997238 | R2:0.24772338691773968 | ACC: 68.6667%(206/300)\n",
            "Testing Epoch[97] Loss:0.1449484582990408 | L1 Loss:0.2994187083095312 | R2:0.6319662603823719 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[97] Loss:0.30821457505226135 | L1 Loss:0.41740745306015015 | R2:0.22982683959498001 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[98] Loss:0.14407552685588598 | L1 Loss:0.2956801215186715 | R2:0.6339076279937271 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[98] Loss:0.3060878798365593 | L1 Loss:0.41576442420482634 | R2:0.2334616762487308 | ACC: 67.3333%(202/300)\n",
            "Testing Epoch[99] Loss:0.14009707560762763 | L1 Loss:0.2915047714486718 | R2:0.6437192073346373 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[99] Loss:0.30263008624315263 | L1 Loss:0.415444153547287 | R2:0.24112080792383753 | ACC: 66.6667%(200/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----Conv1d+ MLP 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dggn_GbrITxd",
        "outputId": "f4612fe5-8869-4ca6-cadf-ef12916a7167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Conv1d+ MLP 300 epochs-----\n",
            "max test_r2_record  0.2696013764214906\n",
            "min test_loss_l1_record  0.4075911283493042\n",
            "min test_loss_record  0.2903534650802612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----Conv1d+ MLP 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be576b86-1cd5-4b82-9255-0e06d56b1b88",
        "id": "Y5c0bUZoGvwP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Conv1d+ MLP 200 epochs-----\n",
            "max test_r2_record  0.2268296748256234\n",
            "min test_loss_l1_record  0.42276514172554014\n",
            "min test_loss_record  0.3034987524151802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----Conv1d+ MLP 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23273593-36d5-4e59-a026-e4f795040a9e",
        "id": "o4Sn65x1GvwP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Conv1d+ MLP 100 epochs-----\n",
            "max test_r2_record  0.22000390897115057\n",
            "min test_loss_l1_record  0.42587103247642516\n",
            "min test_loss_record  0.3102374985814095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----Conv1d+ MLP 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "cecd7c7d-9621-4224-c21f-edd9d30ea10b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFKHnwIzGvwP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Conv1d+ MLP 50 epochs-----\n",
            "max test_r2_record  0.14085059173894407\n",
            "min test_loss_l1_record  0.4375958025455475\n",
            "min test_loss_record  0.33504715859889983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "akPx4cZTawRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nOzw87THawG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = RegularDataset(x_disease, x_rna, y_soft, hair_x, is_mlp=True)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "k2ixS1MNJ3Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026823ef-4a30-4750-804a-5bcc67306a23",
        "id": "jIzCML_lJ3Ns"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.0008, weight_decay=0.0001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "nGYOzRS2J3Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326564b9-90be-44fd-d57f-ea64ab8c1c92",
        "id": "ftdLFtItJ3Ns"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (linear_1): Linear(in_features=71, out_features=128, bias=True)\n",
              "  (bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear_2): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (bn_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear_3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear_4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (bn_4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (RELU): ReLU()\n",
              "  (FC): Sequential(\n",
              "    (0): Linear(in_features=364, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(test_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    # mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    digits = model_class(disease, pre_hair_x)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "YL6Z6fugJ3Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in enumerate(val_dataloader):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    label = label.cuda()\n",
        "    # mirna = mirna.cuda()\n",
        "    digits = digits = model_class(disease, pre_hair_x)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "5MprFzZ7J3Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, (disease, mirna, label, pre_hair_x, _, _) in (enumerate(dataloader)):\n",
        "    disease = disease.to(torch.float32).cuda()\n",
        "    pre_hair_x = pre_hair_x.cuda()\n",
        "    # mirna = mirna.cuda()\n",
        "    label = label.cuda()\n",
        "    \n",
        "\n",
        "    digits = model_class(disease, pre_hair_x)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8908231f-467f-4180-ce9f-e59ff153be03",
        "id": "SaAHDplpJ3Nt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTraining Epoch[0] Loss:0.38705723732709885 | L1 Loss:0.4750458225607872 | R2:-0.008054308456443615 | ACC: 59.4000%(297/500)\n",
            "Testing Epoch[0] Loss:0.4496536239981651 | L1 Loss:0.5306484311819076 | R2:-0.09017388554207299 | ACC: 53.0000%(159/300)\n",
            "Training Epoch[1] Loss:0.3587271347641945 | L1 Loss:0.46102781035006046 | R2:0.06032875772210941 | ACC: 62.4000%(312/500)\n",
            "Testing Epoch[1] Loss:0.4260165929794312 | L1 Loss:0.5114799350500107 | R2:-0.05148092920864815 | ACC: 53.6667%(161/300)\n",
            "Training Epoch[2] Loss:0.33272701036185026 | L1 Loss:0.447082344442606 | R2:0.1243728944517026 | ACC: 61.8000%(309/500)\n",
            "Testing Epoch[2] Loss:0.39884822964668276 | L1 Loss:0.49740455150604246 | R2:0.01124463864517138 | ACC: 55.0000%(165/300)\n",
            "Training Epoch[3] Loss:0.29524368047714233 | L1 Loss:0.41658322885632515 | R2:0.2225922586131233 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[3] Loss:0.38084503412246706 | L1 Loss:0.47566726207733157 | R2:0.03661602313175634 | ACC: 59.6667%(179/300)\n",
            "Training Epoch[4] Loss:0.29435573890805244 | L1 Loss:0.40914820320904255 | R2:0.21624123139975782 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[4] Loss:0.40134030282497407 | L1 Loss:0.4834838151931763 | R2:-0.013031201970331651 | ACC: 58.6667%(176/300)\n",
            "Training Epoch[5] Loss:0.2726987563073635 | L1 Loss:0.39772952906787395 | R2:0.2723985885536687 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[5] Loss:0.40500673949718474 | L1 Loss:0.48968186378479006 | R2:-0.032146093937964894 | ACC: 57.3333%(172/300)\n",
            "Training Epoch[6] Loss:0.2609970383346081 | L1 Loss:0.38919822312891483 | R2:0.2962881408882984 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[6] Loss:0.39642539620399475 | L1 Loss:0.4826745539903641 | R2:0.0006183609702980041 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[7] Loss:0.2383065577596426 | L1 Loss:0.3655254989862442 | R2:0.36329730667286 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[7] Loss:0.3826122432947159 | L1 Loss:0.47664287090301516 | R2:0.04198006902825692 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[8] Loss:0.2392849111929536 | L1 Loss:0.36807404085993767 | R2:0.35698932024178587 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[8] Loss:0.3758421540260315 | L1 Loss:0.47183825075626373 | R2:0.05448475759773554 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[9] Loss:0.22087793238461018 | L1 Loss:0.35345374792814255 | R2:0.4072790737574563 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[9] Loss:0.3810944676399231 | L1 Loss:0.4865945279598236 | R2:0.043963253689904516 | ACC: 58.6667%(176/300)\n",
            "Training Epoch[10] Loss:0.22964482568204403 | L1 Loss:0.3621676042675972 | R2:0.38488767601566476 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[10] Loss:0.36360794901847837 | L1 Loss:0.4696546018123627 | R2:0.08469967127318707 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[11] Loss:0.20890455227345228 | L1 Loss:0.35856231674551964 | R2:0.44295117363509073 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[11] Loss:0.3545487463474274 | L1 Loss:0.46378621757030486 | R2:0.09345688356592588 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[12] Loss:0.21401810087263584 | L1 Loss:0.35120086930692196 | R2:0.42503545577447627 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[12] Loss:0.37162530720233916 | L1 Loss:0.47288664281368253 | R2:0.06473189060416122 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[13] Loss:0.20701183937489986 | L1 Loss:0.34803796745836735 | R2:0.44765886632328244 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[13] Loss:0.39366709589958193 | L1 Loss:0.4872134685516357 | R2:0.0010772054787457574 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[14] Loss:0.21632602019235492 | L1 Loss:0.34591467306017876 | R2:0.4259006508744986 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[14] Loss:0.34992824494838715 | L1 Loss:0.46509465873241423 | R2:0.1129743326074962 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[15] Loss:0.20836247317492962 | L1 Loss:0.3493374017998576 | R2:0.44874145360762063 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[15] Loss:0.37737353444099425 | L1 Loss:0.4852334588766098 | R2:0.04416582521573979 | ACC: 59.3333%(178/300)\n",
            "Training Epoch[16] Loss:0.20330256782472134 | L1 Loss:0.340922087430954 | R2:0.442051737074047 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[16] Loss:0.36880282163619993 | L1 Loss:0.470247408747673 | R2:0.0655126510189932 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[17] Loss:0.1987505005672574 | L1 Loss:0.33972078189253807 | R2:0.46331427017203614 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[17] Loss:0.3631256252527237 | L1 Loss:0.4637425035238266 | R2:0.06007667388166971 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[18] Loss:0.20541633991524577 | L1 Loss:0.3385218307375908 | R2:0.42988542674748026 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[18] Loss:0.3571200639009476 | L1 Loss:0.4639986753463745 | R2:0.0991374060564482 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[19] Loss:0.19962737243622541 | L1 Loss:0.33847207203507423 | R2:0.44943229969306797 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[19] Loss:0.3648818612098694 | L1 Loss:0.4693132728338242 | R2:0.07794293092183406 | ACC: 61.3333%(184/300)\n",
            "Training Epoch[20] Loss:0.17428460391238332 | L1 Loss:0.3164932653307915 | R2:0.5331522365615549 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[20] Loss:0.32581634223461153 | L1 Loss:0.44290720522403715 | R2:0.17758511316858194 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[21] Loss:0.19023194024339318 | L1 Loss:0.3265430834144354 | R2:0.47600833050847435 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[21] Loss:0.32500317841768267 | L1 Loss:0.4416641443967819 | R2:0.16972042205278126 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[22] Loss:0.17991730757057667 | L1 Loss:0.3240786846727133 | R2:0.511517161997565 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[22] Loss:0.3498111441731453 | L1 Loss:0.4623342752456665 | R2:0.08753615391673726 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[23] Loss:0.186900791246444 | L1 Loss:0.3258439898490906 | R2:0.48595185321069884 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[23] Loss:0.3595244109630585 | L1 Loss:0.46051732301712034 | R2:0.07287279279606254 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[24] Loss:0.1937261507846415 | L1 Loss:0.33652456663548946 | R2:0.4822418638734161 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[24] Loss:0.3513733148574829 | L1 Loss:0.45436050891876223 | R2:0.08333459153512326 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[25] Loss:0.1796898073516786 | L1 Loss:0.3178512491285801 | R2:0.5134556725781245 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[25] Loss:0.3565364763140678 | L1 Loss:0.4609145879745483 | R2:0.06625340135571436 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[26] Loss:0.18141757883131504 | L1 Loss:0.3210058305412531 | R2:0.5057121365386568 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[26] Loss:0.3385592117905617 | L1 Loss:0.4360879689455032 | R2:0.12226938273289986 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[27] Loss:0.1884934725239873 | L1 Loss:0.32868226058781147 | R2:0.49078806482879533 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[27] Loss:0.34339959621429444 | L1 Loss:0.45024566650390624 | R2:0.11604756323031466 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[28] Loss:0.17910508485510945 | L1 Loss:0.32142363023012877 | R2:0.5189806490211563 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[28] Loss:0.33943032920360566 | L1 Loss:0.4431440234184265 | R2:0.12054619870917241 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[29] Loss:0.17340061161667109 | L1 Loss:0.3120240829885006 | R2:0.5320861981166939 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[29] Loss:0.3324293330311775 | L1 Loss:0.44075638949871065 | R2:0.13624192897399245 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[30] Loss:0.18814630061388016 | L1 Loss:0.32440318912267685 | R2:0.5005423199612992 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[30] Loss:0.339155387878418 | L1 Loss:0.44018749594688417 | R2:0.1309002649411389 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[31] Loss:0.1752813309431076 | L1 Loss:0.31584984343498945 | R2:0.5324025185261366 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[31] Loss:0.3341436982154846 | L1 Loss:0.4442072927951813 | R2:0.10982758927264266 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[32] Loss:0.18596459925174713 | L1 Loss:0.31916153710335493 | R2:0.502607031588221 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[32] Loss:0.3235044449567795 | L1 Loss:0.44136073589324953 | R2:0.175120729688155 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[33] Loss:0.16797824949026108 | L1 Loss:0.31531595438718796 | R2:0.5566202226051143 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[33] Loss:0.3212119624018669 | L1 Loss:0.43993400037288666 | R2:0.16928950241767393 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[34] Loss:0.1719508646056056 | L1 Loss:0.3093476928770542 | R2:0.5394851316208443 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[34] Loss:0.3319992825388908 | L1 Loss:0.44677778482437136 | R2:0.12174875310077123 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[35] Loss:0.17197494208812714 | L1 Loss:0.3089641174301505 | R2:0.5410081411844736 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[35] Loss:0.35148601084947584 | L1 Loss:0.45388678312301634 | R2:0.07118354550998776 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[36] Loss:0.17249053623527288 | L1 Loss:0.3136307653039694 | R2:0.5437200290354446 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[36] Loss:0.3238321915268898 | L1 Loss:0.43327248394489287 | R2:0.1556971138307044 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[37] Loss:0.1678967233747244 | L1 Loss:0.3097139038145542 | R2:0.5533070156278523 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[37] Loss:0.3123509168624878 | L1 Loss:0.4295943289995193 | R2:0.17559800034741907 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[38] Loss:0.16990438802167773 | L1 Loss:0.30709218699485064 | R2:0.5461871108716642 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[38] Loss:0.30860331654548645 | L1 Loss:0.43514750599861146 | R2:0.20461205085150333 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[39] Loss:0.17097233654931188 | L1 Loss:0.3062986973673105 | R2:0.5423334724579205 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[39] Loss:0.3263292908668518 | L1 Loss:0.4461347222328186 | R2:0.1515657222229398 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[40] Loss:0.16058082692325115 | L1 Loss:0.30062085669487715 | R2:0.5684322633854912 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[40] Loss:0.32974119782447814 | L1 Loss:0.4334296464920044 | R2:0.1560922718261529 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[41] Loss:0.1622606902383268 | L1 Loss:0.30285887978971004 | R2:0.572916809495132 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[41] Loss:0.3411868095397949 | L1 Loss:0.4446652114391327 | R2:0.1228092655427973 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[42] Loss:0.16059959726408124 | L1 Loss:0.3024632232263684 | R2:0.5792359323202851 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[42] Loss:0.31447595059871675 | L1 Loss:0.42604627311229704 | R2:0.17548641370686524 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[43] Loss:0.15848345775157213 | L1 Loss:0.30130060482770205 | R2:0.5802255984456363 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[43] Loss:0.3150095298886299 | L1 Loss:0.43121205270290375 | R2:0.18779658165447627 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[44] Loss:0.16049205977469683 | L1 Loss:0.3040169272571802 | R2:0.5715346069261513 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[44] Loss:0.3280932202935219 | L1 Loss:0.42986890971660613 | R2:0.15121439058221972 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[45] Loss:0.1528804306872189 | L1 Loss:0.29521993082016706 | R2:0.5936148531712849 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[45] Loss:0.30839959979057313 | L1 Loss:0.42816569805145266 | R2:0.20738700487910214 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[46] Loss:0.15848284447565675 | L1 Loss:0.29445006512105465 | R2:0.5880007608907188 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[46] Loss:0.31379729211330415 | L1 Loss:0.42636030316352846 | R2:0.17625953044142278 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[47] Loss:0.1630702600814402 | L1 Loss:0.3026571199297905 | R2:0.5600138416969662 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[47] Loss:0.3228545427322388 | L1 Loss:0.4308894485235214 | R2:0.1580206752657546 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[48] Loss:0.16074588941410184 | L1 Loss:0.302688404917717 | R2:0.5703086761340519 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[48] Loss:0.33509535789489747 | L1 Loss:0.4470410108566284 | R2:0.11383907378582733 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[49] Loss:0.16294128261506557 | L1 Loss:0.30806543584913015 | R2:0.5644898289415425 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[49] Loss:0.3281427755951881 | L1 Loss:0.43318826854228976 | R2:0.12750700175870056 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[50] Loss:0.15853543346747756 | L1 Loss:0.29946244414895773 | R2:0.5827656455167098 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[50] Loss:0.31808592528104784 | L1 Loss:0.4250985562801361 | R2:0.1519151220290889 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[51] Loss:0.16614194121211767 | L1 Loss:0.30717787984758615 | R2:0.5660868055870717 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[51] Loss:0.32107728719711304 | L1 Loss:0.42995725870132445 | R2:0.1553644557647637 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[52] Loss:0.15701302839443088 | L1 Loss:0.30176572874188423 | R2:0.5812357643577817 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[52] Loss:0.3069365322589874 | L1 Loss:0.4192983001470566 | R2:0.18893086695219635 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[53] Loss:0.1588017586618662 | L1 Loss:0.29659478832036257 | R2:0.5803665994173222 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[53] Loss:0.28802160620689393 | L1 Loss:0.40831492841243744 | R2:0.25292746124955123 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[54] Loss:0.16975878505036235 | L1 Loss:0.30975488293915987 | R2:0.5468024754552221 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[54] Loss:0.3020512491464615 | L1 Loss:0.42326712906360625 | R2:0.2157631294660038 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[55] Loss:0.15154200373217463 | L1 Loss:0.2897989898920059 | R2:0.5965165977321506 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[55] Loss:0.3002872675657272 | L1 Loss:0.4180246263742447 | R2:0.22674446417072813 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[56] Loss:0.15210840618237853 | L1 Loss:0.2887292839586735 | R2:0.5974466831756311 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[56] Loss:0.3134799852967262 | L1 Loss:0.42679373025894163 | R2:0.18004752355348333 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[57] Loss:0.15827396791428328 | L1 Loss:0.2944910107180476 | R2:0.5799984942930564 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[57] Loss:0.2973775528371334 | L1 Loss:0.4124380528926849 | R2:0.22122563342821686 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[58] Loss:0.1444488512352109 | L1 Loss:0.28361971490085125 | R2:0.6205330712303583 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[58] Loss:0.3183489337563515 | L1 Loss:0.4324287623167038 | R2:0.17590753986156696 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[59] Loss:0.1462282775901258 | L1 Loss:0.28852851316332817 | R2:0.6126413349689614 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[59] Loss:0.3060371443629265 | L1 Loss:0.4188659727573395 | R2:0.20255495385107958 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[60] Loss:0.14714943803846836 | L1 Loss:0.28297886438667774 | R2:0.607705775760623 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[60] Loss:0.3205861374735832 | L1 Loss:0.42871726453304293 | R2:0.14788345817405482 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[61] Loss:0.14894729433581233 | L1 Loss:0.28913408052176237 | R2:0.6040998869887233 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[61] Loss:0.3215624913573265 | L1 Loss:0.4395385205745697 | R2:0.1514079343803565 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[62] Loss:0.14960865955799818 | L1 Loss:0.29252718575298786 | R2:0.6013703966185076 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[62] Loss:0.3177874594926834 | L1 Loss:0.4296268343925476 | R2:0.1862733148276899 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[63] Loss:0.15272110980004072 | L1 Loss:0.2923368662595749 | R2:0.5927772848624582 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[63] Loss:0.30378125309944154 | L1 Loss:0.41932958364486694 | R2:0.21469966098198393 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[64] Loss:0.1514461007900536 | L1 Loss:0.2876526406034827 | R2:0.6002515688802209 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[64] Loss:0.30424271821975707 | L1 Loss:0.415754160284996 | R2:0.22387790634663524 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[65] Loss:0.1508442321792245 | L1 Loss:0.2981848930940032 | R2:0.5981601970029075 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[65] Loss:0.3043450862169266 | L1 Loss:0.42094112634658815 | R2:0.21019251646100937 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[66] Loss:0.1504936390556395 | L1 Loss:0.28898360952734947 | R2:0.6058905046971305 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[66] Loss:0.29432820677757265 | L1 Loss:0.41398494243621825 | R2:0.2256489967190022 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[67] Loss:0.15209349943324924 | L1 Loss:0.28823055792599916 | R2:0.5943736575488893 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[67] Loss:0.3083602026104927 | L1 Loss:0.425111198425293 | R2:0.2127544989405493 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[68] Loss:0.14682899042963982 | L1 Loss:0.28531229589134455 | R2:0.6148598994486592 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[68] Loss:0.31177677065134046 | L1 Loss:0.4179426819086075 | R2:0.19407186844287558 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[69] Loss:0.145945037715137 | L1 Loss:0.2837499501183629 | R2:0.6153217622059788 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[69] Loss:0.3136748492717743 | L1 Loss:0.42901602387428284 | R2:0.19430525960199718 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[70] Loss:0.14707321161404252 | L1 Loss:0.2843849044293165 | R2:0.6130659466249826 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[70] Loss:0.2973365768790245 | L1 Loss:0.41857380270957945 | R2:0.2320408711928879 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[71] Loss:0.14906920865178108 | L1 Loss:0.2782246097922325 | R2:0.6039072298678262 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[71] Loss:0.30781897008419035 | L1 Loss:0.4281488746404648 | R2:0.2102411017096797 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[72] Loss:0.15019475622102618 | L1 Loss:0.2813995126634836 | R2:0.6003802467496966 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[72] Loss:0.3107521802186966 | L1 Loss:0.4264454632997513 | R2:0.20290659769755975 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[73] Loss:0.1602268936112523 | L1 Loss:0.2905380455777049 | R2:0.5821815343344093 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[73] Loss:0.30015448927879335 | L1 Loss:0.41147024631500245 | R2:0.21695326401193724 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[74] Loss:0.14927900629118085 | L1 Loss:0.2834299532696605 | R2:0.6065000792242486 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[74] Loss:0.28918216526508334 | L1 Loss:0.4110920488834381 | R2:0.2481033558413505 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[75] Loss:0.14830020628869534 | L1 Loss:0.28265413269400597 | R2:0.6086981288878068 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[75] Loss:0.3051199480891228 | L1 Loss:0.41092418134212494 | R2:0.21545692644607478 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[76] Loss:0.1449950998649001 | L1 Loss:0.2810949394479394 | R2:0.6158746411976951 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[76] Loss:0.28808832764625547 | L1 Loss:0.4077728182077408 | R2:0.2527985314264608 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[77] Loss:0.14597680047154427 | L1 Loss:0.2807812448590994 | R2:0.6113145941819527 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[77] Loss:0.2985852435231209 | L1 Loss:0.4136541336774826 | R2:0.22910255412230987 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[78] Loss:0.14635314140468836 | L1 Loss:0.281509667634964 | R2:0.6156739927368562 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[78] Loss:0.30360658168792726 | L1 Loss:0.42077016532421113 | R2:0.22844609762263576 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[79] Loss:0.1471402170136571 | L1 Loss:0.2805274585261941 | R2:0.61023652912413 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[79] Loss:0.29296495765447617 | L1 Loss:0.4076263517141342 | R2:0.24165258927919106 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[80] Loss:0.1495458809658885 | L1 Loss:0.2832852331921458 | R2:0.603076635889204 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[80] Loss:0.31252248734235766 | L1 Loss:0.420584112405777 | R2:0.18912875090233996 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[81] Loss:0.14167462894693017 | L1 Loss:0.26983866933733225 | R2:0.6209235379900365 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[81] Loss:0.2716941736638546 | L1 Loss:0.3993300199508667 | R2:0.2979783161699098 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[82] Loss:0.14544296776875854 | L1 Loss:0.27868014480918646 | R2:0.6194852132951219 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[82] Loss:0.3065506249666214 | L1 Loss:0.4250384509563446 | R2:0.19544639523075027 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[83] Loss:0.14778500236570835 | L1 Loss:0.28173141926527023 | R2:0.6133140152433948 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[83] Loss:0.29522038251161575 | L1 Loss:0.4166884899139404 | R2:0.21478560254865048 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[84] Loss:0.14009413355961442 | L1 Loss:0.27723727375268936 | R2:0.632627393388383 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[84] Loss:0.288918437063694 | L1 Loss:0.41346879601478576 | R2:0.2535442847752725 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[85] Loss:0.1429035805631429 | L1 Loss:0.2784779677167535 | R2:0.6221719707268666 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[85] Loss:0.2932549074292183 | L1 Loss:0.4144333153963089 | R2:0.23489482070759307 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[86] Loss:0.1421268475241959 | L1 Loss:0.2822401477023959 | R2:0.6230429920574767 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[86] Loss:0.2947935685515404 | L1 Loss:0.41488215029239656 | R2:0.2306643972494804 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[87] Loss:0.1442874779459089 | L1 Loss:0.28070581518113613 | R2:0.6244415151139925 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[87] Loss:0.28515462428331373 | L1 Loss:0.4076963156461716 | R2:0.2753144366684261 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[88] Loss:0.1415224289521575 | L1 Loss:0.2788989022374153 | R2:0.6262822664964306 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[88] Loss:0.2910817012190819 | L1 Loss:0.4125706672668457 | R2:0.2468565140435155 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[89] Loss:0.14359424822032452 | L1 Loss:0.2768601328134537 | R2:0.6224668613115143 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[89] Loss:0.31122183948755267 | L1 Loss:0.42084053456783294 | R2:0.19333489660174957 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[90] Loss:0.1357961376197636 | L1 Loss:0.27010622806847095 | R2:0.644871818021917 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[90] Loss:0.30108101963996886 | L1 Loss:0.42063674330711365 | R2:0.21546135026705807 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[91] Loss:0.13837509648874402 | L1 Loss:0.28336944337934256 | R2:0.6326181124881288 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[91] Loss:0.3010281637310982 | L1 Loss:0.4194434702396393 | R2:0.21003404942810247 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[92] Loss:0.1354431640356779 | L1 Loss:0.27405400667339563 | R2:0.6481550550076647 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[92] Loss:0.308255934715271 | L1 Loss:0.42825004160404206 | R2:0.19225430828466897 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[93] Loss:0.1343697803094983 | L1 Loss:0.27253809943795204 | R2:0.6485640038218984 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[93] Loss:0.2964161098003387 | L1 Loss:0.41629986464977264 | R2:0.2435090712648446 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[94] Loss:0.1371962579432875 | L1 Loss:0.27296082861721516 | R2:0.6418337545767019 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[94] Loss:0.29406550973653794 | L1 Loss:0.41198004186153414 | R2:0.24319102175948984 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[95] Loss:0.13344188034534454 | L1 Loss:0.27412659768015146 | R2:0.6500044552955466 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[95] Loss:0.2860944658517838 | L1 Loss:0.40963962972164153 | R2:0.26865453440369447 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[96] Loss:0.143089447170496 | L1 Loss:0.2814700808376074 | R2:0.6237444439463147 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[96] Loss:0.29794501066207885 | L1 Loss:0.4155715018510818 | R2:0.22448035820995965 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[97] Loss:0.13777893921360373 | L1 Loss:0.2790994308888912 | R2:0.6341453715921852 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[97] Loss:0.2827188611030579 | L1 Loss:0.4027405947446823 | R2:0.26202598597587257 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[98] Loss:0.14267627941444516 | L1 Loss:0.27784881927073 | R2:0.6208196066485064 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[98] Loss:0.2862940788269043 | L1 Loss:0.4061827838420868 | R2:0.2566878839818415 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[99] Loss:0.13624181505292654 | L1 Loss:0.27483487501740456 | R2:0.6366284060315456 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[99] Loss:0.29930194318294523 | L1 Loss:0.40936665534973143 | R2:0.2328409265484578 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[100] Loss:0.1389573854394257 | L1 Loss:0.2752322144806385 | R2:0.6345004966230463 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[100] Loss:0.29192571341991425 | L1 Loss:0.40903661251068113 | R2:0.24869550283716899 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[101] Loss:0.14507708046585321 | L1 Loss:0.27842795476317406 | R2:0.6096353381072732 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[101] Loss:0.29067132622003555 | L1 Loss:0.4147376984357834 | R2:0.2430456502446487 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[102] Loss:0.13241537613794208 | L1 Loss:0.26834256295114756 | R2:0.6475943420178167 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[102] Loss:0.28216034173965454 | L1 Loss:0.40326679646968844 | R2:0.2705106121685471 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[103] Loss:0.13546549435704947 | L1 Loss:0.26911209616810083 | R2:0.6436880880845691 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[103] Loss:0.2889536589384079 | L1 Loss:0.4091224104166031 | R2:0.2455275271998434 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[104] Loss:0.13247813982889056 | L1 Loss:0.2673770831897855 | R2:0.6495909952045358 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[104] Loss:0.28322807550430296 | L1 Loss:0.40056163966655733 | R2:0.2674303196455019 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[105] Loss:0.1394613767042756 | L1 Loss:0.28000834211707115 | R2:0.6297646403042632 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[105] Loss:0.28777393996715545 | L1 Loss:0.4098666846752167 | R2:0.24458262741490336 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[106] Loss:0.13186890399083495 | L1 Loss:0.2718364167958498 | R2:0.6537110674111882 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[106] Loss:0.2845322221517563 | L1 Loss:0.40896913409233093 | R2:0.2557095105348469 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[107] Loss:0.13438681652769446 | L1 Loss:0.2721202224493027 | R2:0.6439148678473239 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[107] Loss:0.29861392378807067 | L1 Loss:0.4098586142063141 | R2:0.22075850397909758 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[108] Loss:0.1346630654297769 | L1 Loss:0.26378432009369135 | R2:0.6408141022570949 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[108] Loss:0.2933981016278267 | L1 Loss:0.4094166666269302 | R2:0.2244591306538975 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[109] Loss:0.132963455747813 | L1 Loss:0.26985359471291304 | R2:0.6516270781970788 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[109] Loss:0.2992717757821083 | L1 Loss:0.4159259796142578 | R2:0.21457306248363534 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[110] Loss:0.1373142278753221 | L1 Loss:0.2787660975009203 | R2:0.636742973310683 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[110] Loss:0.3089331477880478 | L1 Loss:0.4201689213514328 | R2:0.19946070218064715 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[111] Loss:0.12539219297468662 | L1 Loss:0.26492087356746197 | R2:0.6720043766518333 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[111] Loss:0.2900322899222374 | L1 Loss:0.41740932166576383 | R2:0.23973568023544675 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[112] Loss:0.13544528419151902 | L1 Loss:0.27179907727986574 | R2:0.6447864751048108 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[112] Loss:0.28744782954454423 | L1 Loss:0.4073724389076233 | R2:0.259005662410401 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[113] Loss:0.1372160972096026 | L1 Loss:0.2768345959484577 | R2:0.6342430664221577 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[113] Loss:0.2943252041935921 | L1 Loss:0.4155991315841675 | R2:0.23254357129462533 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[114] Loss:0.13153366604819894 | L1 Loss:0.266045986674726 | R2:0.654408827878029 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[114] Loss:0.2899250820279121 | L1 Loss:0.40970045924186704 | R2:0.2510053573169636 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[115] Loss:0.1352456989698112 | L1 Loss:0.2729374850168824 | R2:0.6434909417358036 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[115] Loss:0.27226819694042204 | L1 Loss:0.4014575362205505 | R2:0.2963202428198545 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[116] Loss:0.14073791494593024 | L1 Loss:0.28269906155765057 | R2:0.6248844526461288 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[116] Loss:0.282516211271286 | L1 Loss:0.406423419713974 | R2:0.2609406760470094 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[117] Loss:0.13203764334321022 | L1 Loss:0.26616682205349207 | R2:0.6563003700630494 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[117] Loss:0.2844398468732834 | L1 Loss:0.4017214596271515 | R2:0.25972124805204544 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[118] Loss:0.13179499632678926 | L1 Loss:0.2663960102945566 | R2:0.6489163992001917 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[118] Loss:0.2816836953163147 | L1 Loss:0.4106213241815567 | R2:0.27241823189248904 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[119] Loss:0.12579204724170268 | L1 Loss:0.26397963147610426 | R2:0.6688257757442417 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[119] Loss:0.27400305271148684 | L1 Loss:0.40600845217704773 | R2:0.289789089569488 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[120] Loss:0.1272255713120103 | L1 Loss:0.26313432212918997 | R2:0.6651284238332485 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[120] Loss:0.28594699054956435 | L1 Loss:0.4102486133575439 | R2:0.25703360902848954 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[121] Loss:0.13411411130800843 | L1 Loss:0.27252825628966093 | R2:0.6440722938295023 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[121] Loss:0.29598116874694824 | L1 Loss:0.4141730576753616 | R2:0.2462052890766609 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[122] Loss:0.12784547870978713 | L1 Loss:0.2602180875837803 | R2:0.6618909908207383 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[122] Loss:0.2874936729669571 | L1 Loss:0.4092787355184555 | R2:0.2710739624807559 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[123] Loss:0.12841278687119484 | L1 Loss:0.26424337923526764 | R2:0.6582342804565178 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[123] Loss:0.28378638029098513 | L1 Loss:0.4064117044210434 | R2:0.2714513743847549 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[124] Loss:0.12454952346161008 | L1 Loss:0.2596233254298568 | R2:0.6669175088817707 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[124] Loss:0.28248789012432096 | L1 Loss:0.40510999262332914 | R2:0.25559973067269076 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[125] Loss:0.12494224193505943 | L1 Loss:0.2604015748947859 | R2:0.6736459277406442 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[125] Loss:0.29211815297603605 | L1 Loss:0.4085134923458099 | R2:0.2508305834162153 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[126] Loss:0.12844026321545243 | L1 Loss:0.2657959247007966 | R2:0.6594499998119513 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[126] Loss:0.28732976615428923 | L1 Loss:0.41256387531757355 | R2:0.25797918243098816 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[127] Loss:0.13243350037373602 | L1 Loss:0.26931572426110506 | R2:0.6529124504590497 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[127] Loss:0.2954266667366028 | L1 Loss:0.40946744978427885 | R2:0.23201478287677157 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[128] Loss:0.12238624598830938 | L1 Loss:0.2621622998267412 | R2:0.6789577007899182 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[128] Loss:0.300417660176754 | L1 Loss:0.414074581861496 | R2:0.22946357097595946 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[129] Loss:0.13088645413517952 | L1 Loss:0.2706085713580251 | R2:0.6508156958800358 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[129] Loss:0.27481465190649035 | L1 Loss:0.39895786345005035 | R2:0.2893482957756963 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[130] Loss:0.13486104970797896 | L1 Loss:0.2721116188913584 | R2:0.6422200849512838 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[130] Loss:0.2722874104976654 | L1 Loss:0.3961600184440613 | R2:0.293520151054459 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[131] Loss:0.12934534065425396 | L1 Loss:0.26947784796357155 | R2:0.6576088548693452 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[131] Loss:0.2875651389360428 | L1 Loss:0.4044726073741913 | R2:0.2496112669320513 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[132] Loss:0.13424724759534 | L1 Loss:0.26879037730395794 | R2:0.6422939946387406 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[132] Loss:0.2791322335600853 | L1 Loss:0.39612986147403717 | R2:0.2631871033517263 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[133] Loss:0.1332325900439173 | L1 Loss:0.26797354593873024 | R2:0.653153478393555 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[133] Loss:0.28369101136922836 | L1 Loss:0.3998943090438843 | R2:0.2735282480854284 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[134] Loss:0.12732531456276774 | L1 Loss:0.2651178892701864 | R2:0.663997476203426 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[134] Loss:0.29394135922193526 | L1 Loss:0.4108117759227753 | R2:0.2464103861653008 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[135] Loss:0.13541551865637302 | L1 Loss:0.2753340443596244 | R2:0.6439420880408038 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[135] Loss:0.29122363179922106 | L1 Loss:0.4066528588533401 | R2:0.24953543703116182 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[136] Loss:0.12996133835986257 | L1 Loss:0.27054995112121105 | R2:0.6565294057400313 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[136] Loss:0.28472230583429337 | L1 Loss:0.39623071551322936 | R2:0.2682234002162677 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[137] Loss:0.13056871807202697 | L1 Loss:0.2622118638828397 | R2:0.6502580664970654 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[137] Loss:0.2925094664096832 | L1 Loss:0.4139866977930069 | R2:0.24442939911358308 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[138] Loss:0.1353399665094912 | L1 Loss:0.2716181855648756 | R2:0.6402958142295245 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[138] Loss:0.2900762528181076 | L1 Loss:0.4079309642314911 | R2:0.2559143896181721 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[139] Loss:0.13288985285907984 | L1 Loss:0.2737189941108227 | R2:0.6471488655759203 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[139] Loss:0.29118668138980863 | L1 Loss:0.4081400990486145 | R2:0.2417836249470771 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[140] Loss:0.13485998380929232 | L1 Loss:0.27246631775051355 | R2:0.6417782670827538 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[140] Loss:0.28251959532499316 | L1 Loss:0.4032143533229828 | R2:0.2595380291297526 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[141] Loss:0.12958076247014105 | L1 Loss:0.2688351096585393 | R2:0.6582082810180928 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[141] Loss:0.29537215530872346 | L1 Loss:0.4122077405452728 | R2:0.23683160580645093 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[142] Loss:0.12373258033767343 | L1 Loss:0.26030822563916445 | R2:0.6748619694591667 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[142] Loss:0.27034775763750074 | L1 Loss:0.39419731199741365 | R2:0.3066761830336765 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[143] Loss:0.12288731709122658 | L1 Loss:0.25810738652944565 | R2:0.6727442634130576 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[143] Loss:0.28303673416376113 | L1 Loss:0.4051222264766693 | R2:0.26420764129130303 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[144] Loss:0.12472472572699189 | L1 Loss:0.263001412153244 | R2:0.669030794469099 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[144] Loss:0.2738695666193962 | L1 Loss:0.4016326516866684 | R2:0.2915843827046337 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[145] Loss:0.12633561622351408 | L1 Loss:0.26275209430605173 | R2:0.6660576164423704 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[145] Loss:0.2782853588461876 | L1 Loss:0.3997685670852661 | R2:0.2690254346438098 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[146] Loss:0.13185861613601446 | L1 Loss:0.2663954282179475 | R2:0.6506532652927602 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[146] Loss:0.28412843197584153 | L1 Loss:0.40543381571769715 | R2:0.2591369750098989 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[147] Loss:0.13185938447713852 | L1 Loss:0.26802052091807127 | R2:0.6528164546089971 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[147] Loss:0.2851815238595009 | L1 Loss:0.40011152923107146 | R2:0.2617612599225941 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[148] Loss:0.12919024983420968 | L1 Loss:0.2668917756527662 | R2:0.6559253288202015 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[148] Loss:0.285016930103302 | L1 Loss:0.408020144701004 | R2:0.2650245833187762 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[149] Loss:0.12214324902743101 | L1 Loss:0.2598516074940562 | R2:0.6766567090712529 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[149] Loss:0.2793413385748863 | L1 Loss:0.3999205678701401 | R2:0.27807624809386666 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[150] Loss:0.12634697975590825 | L1 Loss:0.2623153440654278 | R2:0.669583961236397 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[150] Loss:0.29733286798000336 | L1 Loss:0.41767231822013856 | R2:0.2175927833537729 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[151] Loss:0.12761883228085935 | L1 Loss:0.2635068818926811 | R2:0.6668089152748131 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[151] Loss:0.29173344522714617 | L1 Loss:0.4059823602437973 | R2:0.2461585652589684 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[152] Loss:0.12401111423969269 | L1 Loss:0.2646512696519494 | R2:0.6756926674177134 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[152] Loss:0.28885486871004107 | L1 Loss:0.40988663733005526 | R2:0.2535217462204959 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[153] Loss:0.12676225742325187 | L1 Loss:0.26192884147167206 | R2:0.66772370854397 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[153] Loss:0.29147456139326094 | L1 Loss:0.41144772469997404 | R2:0.23601870610989573 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[154] Loss:0.12675123615190387 | L1 Loss:0.2639099443331361 | R2:0.6668430417534469 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[154] Loss:0.2937976971268654 | L1 Loss:0.41255220770835876 | R2:0.23898017719375325 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[155] Loss:0.12613752065226436 | L1 Loss:0.258078983053565 | R2:0.671990112471925 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[155] Loss:0.2880888134241104 | L1 Loss:0.405914181470871 | R2:0.253427229676887 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[156] Loss:0.12857656506821513 | L1 Loss:0.2660963563248515 | R2:0.6632877963632478 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[156] Loss:0.2945081517100334 | L1 Loss:0.4110181838274002 | R2:0.24429333372190917 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[157] Loss:0.12073711794801056 | L1 Loss:0.2599628437310457 | R2:0.6790546276570574 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[157] Loss:0.295006999373436 | L1 Loss:0.40702125430107117 | R2:0.2234463455871293 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[158] Loss:0.12684330949559808 | L1 Loss:0.25926785729825497 | R2:0.6666998214946548 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[158] Loss:0.2754830077290535 | L1 Loss:0.40455850660800935 | R2:0.2771006113671259 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[159] Loss:0.12782417656853795 | L1 Loss:0.25838627200573683 | R2:0.6625898803253822 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[159] Loss:0.2664166435599327 | L1 Loss:0.3910349577665329 | R2:0.3068995359359115 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[160] Loss:0.12339402921497822 | L1 Loss:0.25786217767745256 | R2:0.674964951879935 | ACC: 88.2000%(441/500)\n",
            "Testing Epoch[160] Loss:0.2755317032337189 | L1 Loss:0.39764273464679717 | R2:0.28412151504233096 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[161] Loss:0.12331554852426052 | L1 Loss:0.2561844689771533 | R2:0.6714790737675954 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[161] Loss:0.2910499840974808 | L1 Loss:0.41371810734272 | R2:0.25152664110919376 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[162] Loss:0.12388795847073197 | L1 Loss:0.26207398157566786 | R2:0.6685685827044479 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[162] Loss:0.29614447355270385 | L1 Loss:0.40721789598464964 | R2:0.22787662652303098 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[163] Loss:0.12367886677384377 | L1 Loss:0.26165868155658245 | R2:0.6693524764419865 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[163] Loss:0.28161855340003966 | L1 Loss:0.39950079619884493 | R2:0.26470385276400343 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[164] Loss:0.12286218535155058 | L1 Loss:0.2562159849330783 | R2:0.6746936511659225 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[164] Loss:0.2929309204220772 | L1 Loss:0.4064630031585693 | R2:0.25271451639415454 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[165] Loss:0.12569095892831683 | L1 Loss:0.25762416049838066 | R2:0.6707713097524772 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[165] Loss:0.2763446420431137 | L1 Loss:0.4068164736032486 | R2:0.2835963138310419 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[166] Loss:0.12238470674492419 | L1 Loss:0.259879894554615 | R2:0.681930181044137 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[166] Loss:0.2902205727994442 | L1 Loss:0.4097563147544861 | R2:0.2509729157533047 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[167] Loss:0.12567232036963105 | L1 Loss:0.25431554205715656 | R2:0.6695040187791627 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[167] Loss:0.28014968782663346 | L1 Loss:0.404018035531044 | R2:0.26981413571477847 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[168] Loss:0.12710888218134642 | L1 Loss:0.25724415574222803 | R2:0.6682958436995043 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[168] Loss:0.2943464800715446 | L1 Loss:0.41672107875347136 | R2:0.22646271586336425 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[169] Loss:0.12668653624132276 | L1 Loss:0.25609328504651785 | R2:0.6617503664662898 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[169] Loss:0.29332410991191865 | L1 Loss:0.41664442121982576 | R2:0.2286130547560734 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[170] Loss:0.11705222772434354 | L1 Loss:0.25163780711591244 | R2:0.6929339270639574 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[170] Loss:0.29169089198112486 | L1 Loss:0.40861524641513824 | R2:0.24133077607767794 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[171] Loss:0.12538512470200658 | L1 Loss:0.2561260359361768 | R2:0.6703724996450857 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[171] Loss:0.2839681044220924 | L1 Loss:0.40549431145191195 | R2:0.2624969149332578 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[172] Loss:0.12910750741139054 | L1 Loss:0.2611275753006339 | R2:0.663022183493916 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[172] Loss:0.3005437284708023 | L1 Loss:0.415662482380867 | R2:0.21611835522230027 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[173] Loss:0.1321430983953178 | L1 Loss:0.2645783852785826 | R2:0.6497834148729529 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[173] Loss:0.27283346503973005 | L1 Loss:0.3974047422409058 | R2:0.30470134933662163 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[174] Loss:0.1318509909324348 | L1 Loss:0.262225684709847 | R2:0.6493922455400685 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[174] Loss:0.2797975651919842 | L1 Loss:0.39919643104076385 | R2:0.26782843259135874 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[175] Loss:0.12356087565422058 | L1 Loss:0.25592644698917866 | R2:0.6785984288853713 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[175] Loss:0.2760727763175964 | L1 Loss:0.4018155366182327 | R2:0.28574572599065207 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[176] Loss:0.12627982767298818 | L1 Loss:0.2600004207342863 | R2:0.6654854432705912 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[176] Loss:0.2787406101822853 | L1 Loss:0.39464919567108153 | R2:0.27816254890271147 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[177] Loss:0.12419605138711631 | L1 Loss:0.25959451124072075 | R2:0.6743531780856447 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[177] Loss:0.2880453810095787 | L1 Loss:0.40206163227558134 | R2:0.2579820823552129 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[178] Loss:0.12463321397081017 | L1 Loss:0.2611059173941612 | R2:0.6789250882028072 | ACC: 87.8000%(439/500)\n",
            "Testing Epoch[178] Loss:0.2890025094151497 | L1 Loss:0.39993681013584137 | R2:0.2551047015787955 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[179] Loss:0.12192655447870493 | L1 Loss:0.25921357702463865 | R2:0.6799518895979993 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[179] Loss:0.27999967634677886 | L1 Loss:0.4049712747335434 | R2:0.2802404951897973 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[180] Loss:0.1246277685277164 | L1 Loss:0.25899491645395756 | R2:0.6733043228454283 | ACC: 87.8000%(439/500)\n",
            "Testing Epoch[180] Loss:0.2778266191482544 | L1 Loss:0.39677700996398924 | R2:0.28613167923182614 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[181] Loss:0.1227371608838439 | L1 Loss:0.25728118885308504 | R2:0.6807653142772025 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[181] Loss:0.2879490464925766 | L1 Loss:0.40468223094940187 | R2:0.2602493796483783 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[182] Loss:0.12058346136473119 | L1 Loss:0.2562858276069164 | R2:0.6858035046088091 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[182] Loss:0.2837818622589111 | L1 Loss:0.39923659563064573 | R2:0.26084511255332066 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[183] Loss:0.12651091674342752 | L1 Loss:0.2619855795055628 | R2:0.667915147468775 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[183] Loss:0.28293499946594236 | L1 Loss:0.39929129779338834 | R2:0.2541813736355845 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[184] Loss:0.12434505904093385 | L1 Loss:0.2617419455200434 | R2:0.674727886575891 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[184] Loss:0.2733095303177834 | L1 Loss:0.3908147007226944 | R2:0.29707670460672175 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[185] Loss:0.12467125244438648 | L1 Loss:0.25994752813130617 | R2:0.6744544049364501 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[185] Loss:0.28863845840096475 | L1 Loss:0.3985329866409302 | R2:0.23745077237100398 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[186] Loss:0.1251785783097148 | L1 Loss:0.2619258612394333 | R2:0.6690351046100094 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[186] Loss:0.28889716416597366 | L1 Loss:0.4081595838069916 | R2:0.2561312567242781 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[187] Loss:0.12303575128316879 | L1 Loss:0.2596161738038063 | R2:0.6748163264985699 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[187] Loss:0.29412305653095244 | L1 Loss:0.4099489450454712 | R2:0.23489891386029801 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[188] Loss:0.12403925368562341 | L1 Loss:0.26420910749584436 | R2:0.6735826877790423 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[188] Loss:0.28656533658504485 | L1 Loss:0.4040185630321503 | R2:0.26562572740802437 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[189] Loss:0.12241287063807249 | L1 Loss:0.2584944926202297 | R2:0.6786659756461142 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[189] Loss:0.29167951494455335 | L1 Loss:0.4027537554502487 | R2:0.24970273377861626 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[190] Loss:0.1299303057603538 | L1 Loss:0.2687433622777462 | R2:0.6577739505417151 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[190] Loss:0.2882821328938007 | L1 Loss:0.4060960292816162 | R2:0.2549488514213961 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[191] Loss:0.12741060834378004 | L1 Loss:0.25978356413543224 | R2:0.6664485981848791 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[191] Loss:0.2953509986400604 | L1 Loss:0.4121846169233322 | R2:0.23023203567838882 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[192] Loss:0.12660331698134542 | L1 Loss:0.26326792128384113 | R2:0.6674905185173856 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[192] Loss:0.30464283376932144 | L1 Loss:0.4172729402780533 | R2:0.2034169779548877 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[193] Loss:0.12313849432393909 | L1 Loss:0.25812642462551594 | R2:0.6757436923815088 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[193] Loss:0.2891687795519829 | L1 Loss:0.40846344232559206 | R2:0.23545320400771805 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[194] Loss:0.12460755277425051 | L1 Loss:0.25997036322951317 | R2:0.6726258163677498 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[194] Loss:0.29622368812561034 | L1 Loss:0.41586423218250274 | R2:0.22073242104408441 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[195] Loss:0.12981372978538275 | L1 Loss:0.26181643083691597 | R2:0.659828687425452 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[195] Loss:0.28440324664115907 | L1 Loss:0.4045916199684143 | R2:0.25399949544409417 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[196] Loss:0.1329818165395409 | L1 Loss:0.26487393490970135 | R2:0.6510069819656217 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[196] Loss:0.28532754778862 | L1 Loss:0.4071633368730545 | R2:0.25356970803674334 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[197] Loss:0.12926154490560293 | L1 Loss:0.26143010053783655 | R2:0.6570517792099652 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[197] Loss:0.2777171194553375 | L1 Loss:0.3971777558326721 | R2:0.279741312847818 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[198] Loss:0.11839586729183793 | L1 Loss:0.25518556870520115 | R2:0.6880487563668087 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[198] Loss:0.2821260765194893 | L1 Loss:0.40232265591621397 | R2:0.26008942614080605 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[199] Loss:0.12689004838466644 | L1 Loss:0.26040229573845863 | R2:0.6643714427347165 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[199] Loss:0.27837745249271395 | L1 Loss:0.40183399319648744 | R2:0.28305983459047995 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[200] Loss:0.12252664100378752 | L1 Loss:0.2576678581535816 | R2:0.6788624926295955 | ACC: 88.2000%(441/500)\n",
            "Testing Epoch[200] Loss:0.27568183541297914 | L1 Loss:0.40339904725551606 | R2:0.2713403907529914 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[201] Loss:0.1253450894728303 | L1 Loss:0.2553775552660227 | R2:0.6722550040813281 | ACC: 87.8000%(439/500)\n",
            "Testing Epoch[201] Loss:0.2784246429800987 | L1 Loss:0.40093397796154023 | R2:0.27807142079773806 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[202] Loss:0.12009974708780646 | L1 Loss:0.25262192636728287 | R2:0.6867166985982295 | ACC: 87.8000%(439/500)\n",
            "Testing Epoch[202] Loss:0.28350666761398313 | L1 Loss:0.4010806530714035 | R2:0.2650132372257405 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[203] Loss:0.11806532926857471 | L1 Loss:0.25047035329043865 | R2:0.685463695636403 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[203] Loss:0.2762764424085617 | L1 Loss:0.398232239484787 | R2:0.2856371283337503 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[204] Loss:0.12487501138821244 | L1 Loss:0.2592598283663392 | R2:0.670892247568882 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[204] Loss:0.27899425476789474 | L1 Loss:0.4061682283878326 | R2:0.2724175464994685 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[205] Loss:0.11855442472733557 | L1 Loss:0.24893418047577143 | R2:0.6908716364240232 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[205] Loss:0.281411799788475 | L1 Loss:0.4018213957548141 | R2:0.26100662325626767 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[206] Loss:0.12060050060972571 | L1 Loss:0.2548501370474696 | R2:0.6812573448877511 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[206] Loss:0.2655139580368996 | L1 Loss:0.38778583109378817 | R2:0.29745497172512037 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[207] Loss:0.12385777616873384 | L1 Loss:0.2586839282885194 | R2:0.6739404635703256 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[207] Loss:0.28147415816783905 | L1 Loss:0.3939137727022171 | R2:0.2763398088977445 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[208] Loss:0.11796602234244347 | L1 Loss:0.25090641248971224 | R2:0.6914006633247939 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[208] Loss:0.27797533869743346 | L1 Loss:0.39230978190898896 | R2:0.28288572167829706 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[209] Loss:0.12127961916849017 | L1 Loss:0.25542551930993795 | R2:0.6825972773790501 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[209] Loss:0.2816831260919571 | L1 Loss:0.39900741875171664 | R2:0.2597600542290234 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[210] Loss:0.11907076369971037 | L1 Loss:0.2566424449905753 | R2:0.6841536632016354 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[210] Loss:0.2772952690720558 | L1 Loss:0.39703518748283384 | R2:0.28303944077211696 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[211] Loss:0.12422671285457909 | L1 Loss:0.26138253323733807 | R2:0.6697777219103471 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[211] Loss:0.2868694320321083 | L1 Loss:0.4023216724395752 | R2:0.2586581936278246 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[212] Loss:0.11655687727034092 | L1 Loss:0.2514117117971182 | R2:0.6945191740397113 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[212] Loss:0.2821608141064644 | L1 Loss:0.4007129430770874 | R2:0.26862157164750067 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[213] Loss:0.12659694533795118 | L1 Loss:0.260671759955585 | R2:0.667148297909524 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[213] Loss:0.28760402500629423 | L1 Loss:0.39920553267002107 | R2:0.24677935809423576 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[214] Loss:0.12495940062217414 | L1 Loss:0.2579792505130172 | R2:0.6692341316302154 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[214] Loss:0.2762190937995911 | L1 Loss:0.39389674067497255 | R2:0.29076887194515677 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[215] Loss:0.1266276929527521 | L1 Loss:0.25955972261726856 | R2:0.6655578263097967 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[215] Loss:0.28070575892925265 | L1 Loss:0.4007227003574371 | R2:0.27288743615513994 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[216] Loss:0.12602729466743767 | L1 Loss:0.25257604103535414 | R2:0.668517386794255 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[216] Loss:0.2815252333879471 | L1 Loss:0.401330304145813 | R2:0.26573887149191255 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[217] Loss:0.1299176304601133 | L1 Loss:0.2636838247999549 | R2:0.6577492314715809 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[217] Loss:0.27214415222406385 | L1 Loss:0.39539774954319 | R2:0.2907035722347347 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[218] Loss:0.13020312460139394 | L1 Loss:0.26232511550188065 | R2:0.6555591122384994 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[218] Loss:0.27447353303432465 | L1 Loss:0.39786133766174314 | R2:0.2946196658733054 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[219] Loss:0.13092261087149382 | L1 Loss:0.26295823790133 | R2:0.6543262131450028 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[219] Loss:0.271315735578537 | L1 Loss:0.38937832713127135 | R2:0.3035119547330249 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[220] Loss:0.12207797961309552 | L1 Loss:0.25282707065343857 | R2:0.6795713251331593 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[220] Loss:0.27286755442619326 | L1 Loss:0.39712522029876707 | R2:0.2942979896550638 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[221] Loss:0.12997477618046105 | L1 Loss:0.26164570450782776 | R2:0.6567247239350918 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[221] Loss:0.2768643915653229 | L1 Loss:0.3967742145061493 | R2:0.27533708629540043 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[222] Loss:0.12208713311702013 | L1 Loss:0.2531101303175092 | R2:0.6766639593978574 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[222] Loss:0.28728031665086745 | L1 Loss:0.3987131267786026 | R2:0.24802727767626606 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[223] Loss:0.11774101573973894 | L1 Loss:0.24842501524835825 | R2:0.6911163140444883 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[223] Loss:0.29625214338302613 | L1 Loss:0.4164934903383255 | R2:0.22086752289182016 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[224] Loss:0.1203487915918231 | L1 Loss:0.25666125398129225 | R2:0.6822317343544568 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[224] Loss:0.2802975088357925 | L1 Loss:0.3993881821632385 | R2:0.26218061104983703 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[225] Loss:0.12051990022882819 | L1 Loss:0.2542970906943083 | R2:0.6822443328649842 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[225] Loss:0.2890066623687744 | L1 Loss:0.402271032333374 | R2:0.24307254067380107 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[226] Loss:0.11503063980489969 | L1 Loss:0.2519526807591319 | R2:0.6968522339377579 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[226] Loss:0.27719236463308333 | L1 Loss:0.3973347246646881 | R2:0.2722566713144734 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[227] Loss:0.11798849911428988 | L1 Loss:0.25656585954129696 | R2:0.6905149870178069 | ACC: 88.8000%(444/500)\n",
            "Testing Epoch[227] Loss:0.2823348730802536 | L1 Loss:0.40195000171661377 | R2:0.2604536752931492 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[228] Loss:0.11670106370002031 | L1 Loss:0.25316447485238314 | R2:0.6899603378855503 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[228] Loss:0.28720991909503935 | L1 Loss:0.40601539313793183 | R2:0.24015798213399453 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[229] Loss:0.11417569359764457 | L1 Loss:0.2506148833781481 | R2:0.6999428842480591 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[229] Loss:0.2799132168292999 | L1 Loss:0.40269885361194613 | R2:0.26590098537512064 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[230] Loss:0.12156334472820163 | L1 Loss:0.2563366014510393 | R2:0.6805893977104677 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[230] Loss:0.2763153612613678 | L1 Loss:0.4017440348863602 | R2:0.27238558016208864 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[231] Loss:0.12253683945164084 | L1 Loss:0.25788034591823816 | R2:0.6763783593717514 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[231] Loss:0.29493241757154465 | L1 Loss:0.4073623716831207 | R2:0.22932781596244603 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[232] Loss:0.11880026152357459 | L1 Loss:0.25070579070597887 | R2:0.6870432055992101 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[232] Loss:0.29162787795066836 | L1 Loss:0.4053271323442459 | R2:0.2333675528158789 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[233] Loss:0.1220292616635561 | L1 Loss:0.2553238580003381 | R2:0.6784195027356098 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[233] Loss:0.27621075212955476 | L1 Loss:0.40047411620616913 | R2:0.2753086892332153 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[234] Loss:0.12563237501308322 | L1 Loss:0.2570428382605314 | R2:0.6702651342689127 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[234] Loss:0.2900990039110184 | L1 Loss:0.4083579957485199 | R2:0.23861727625384196 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[235] Loss:0.1242605522274971 | L1 Loss:0.25550158228725195 | R2:0.6758348634151679 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[235] Loss:0.28433809876441957 | L1 Loss:0.4006442576646805 | R2:0.2569009163795977 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[236] Loss:0.12297867424786091 | L1 Loss:0.2544076107442379 | R2:0.6767470253095701 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[236] Loss:0.27903897166252134 | L1 Loss:0.40001198947429656 | R2:0.27627692936619314 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[237] Loss:0.12288010213524103 | L1 Loss:0.25535836536437273 | R2:0.6810401549169434 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[237] Loss:0.2795549064874649 | L1 Loss:0.39561052024364474 | R2:0.269466711806895 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[238] Loss:0.12728675408288836 | L1 Loss:0.26107282564044 | R2:0.6649940977569985 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[238] Loss:0.2936313420534134 | L1 Loss:0.4074656069278717 | R2:0.23408891854346225 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[239] Loss:0.12401926750317216 | L1 Loss:0.2573970267549157 | R2:0.673088525594028 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[239] Loss:0.2868458852171898 | L1 Loss:0.39802065789699553 | R2:0.25198111184370553 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[240] Loss:0.12418718636035919 | L1 Loss:0.25750683061778545 | R2:0.674016388342069 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[240] Loss:0.2784982532262802 | L1 Loss:0.39697896838188174 | R2:0.2710334852220893 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[241] Loss:0.12012316333130002 | L1 Loss:0.2522395383566618 | R2:0.6875234994631523 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[241] Loss:0.29108374416828153 | L1 Loss:0.40260215699672697 | R2:0.24719181302137022 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[242] Loss:0.12152062309905887 | L1 Loss:0.25436654314398766 | R2:0.6808028903484935 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[242] Loss:0.2885382115840912 | L1 Loss:0.40368062257766724 | R2:0.23906827613633683 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[243] Loss:0.12269566766917706 | L1 Loss:0.2520874757319689 | R2:0.6774323518424412 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[243] Loss:0.2782846838235855 | L1 Loss:0.39683771729469297 | R2:0.27023291948747025 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[244] Loss:0.11459249909967184 | L1 Loss:0.2460508905351162 | R2:0.6995114147557786 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[244] Loss:0.2819369673728943 | L1 Loss:0.3994323670864105 | R2:0.2562790304842175 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[245] Loss:0.11673671985045075 | L1 Loss:0.25091880187392235 | R2:0.6927051612967506 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[245] Loss:0.29894659370183946 | L1 Loss:0.40945633947849275 | R2:0.21860747058666713 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[246] Loss:0.11691519827581942 | L1 Loss:0.2487429454922676 | R2:0.6953324504596672 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[246] Loss:0.2922007039189339 | L1 Loss:0.4015063434839249 | R2:0.23902992355724453 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[247] Loss:0.12047583446837962 | L1 Loss:0.2579854968935251 | R2:0.6831480062995918 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[247] Loss:0.28774840384721756 | L1 Loss:0.4026561826467514 | R2:0.2519596591518864 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[248] Loss:0.12005947856232524 | L1 Loss:0.2532133152708411 | R2:0.6829469521357016 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[248] Loss:0.28303670734167097 | L1 Loss:0.3969134360551834 | R2:0.2580773056621517 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[249] Loss:0.11301809642463923 | L1 Loss:0.24803580529987812 | R2:0.705770630025519 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[249] Loss:0.2856146067380905 | L1 Loss:0.4063966631889343 | R2:0.257140974843683 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[250] Loss:0.12132710660807788 | L1 Loss:0.2534435372799635 | R2:0.6831361488019387 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[250] Loss:0.28063648343086245 | L1 Loss:0.40276455581188203 | R2:0.26143218247687094 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[251] Loss:0.1128451966214925 | L1 Loss:0.24574304092675447 | R2:0.7065556330207092 | ACC: 88.2000%(441/500)\n",
            "Testing Epoch[251] Loss:0.295138792693615 | L1 Loss:0.4068281650543213 | R2:0.21822970609434456 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[252] Loss:0.11998662375845015 | L1 Loss:0.2500851806253195 | R2:0.6867221737386668 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[252] Loss:0.285306903719902 | L1 Loss:0.39536979496479036 | R2:0.25657791449310946 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[253] Loss:0.12120423745363951 | L1 Loss:0.25196890998631716 | R2:0.6817363178876886 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[253] Loss:0.2773625075817108 | L1 Loss:0.3947719246149063 | R2:0.28032145493253824 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[254] Loss:0.12107094377279282 | L1 Loss:0.25366090796887875 | R2:0.6811650949040582 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[254] Loss:0.281276199221611 | L1 Loss:0.3957778632640839 | R2:0.26253165853071153 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[255] Loss:0.12227802793495357 | L1 Loss:0.25666201766580343 | R2:0.681337181484417 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[255] Loss:0.27635152339935304 | L1 Loss:0.39544896185398104 | R2:0.2864474772572119 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[256] Loss:0.12076791981235147 | L1 Loss:0.25342816952615976 | R2:0.6801776521632864 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[256] Loss:0.28807953000068665 | L1 Loss:0.4039157509803772 | R2:0.24358114013158644 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[257] Loss:0.12241050135344267 | L1 Loss:0.25216811522841454 | R2:0.6801129503794453 | ACC: 88.2000%(441/500)\n",
            "Testing Epoch[257] Loss:0.2909864753484726 | L1 Loss:0.40444926619529725 | R2:0.253761613871485 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[258] Loss:0.12185381446033716 | L1 Loss:0.2526998892426491 | R2:0.682344045380229 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[258] Loss:0.28116068840026853 | L1 Loss:0.40102755427360537 | R2:0.27329273244503816 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[259] Loss:0.12083877250552177 | L1 Loss:0.2539540119469166 | R2:0.6834936608528875 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[259] Loss:0.2945040866732597 | L1 Loss:0.40528824329376223 | R2:0.23567597024964354 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[260] Loss:0.11905596917495131 | L1 Loss:0.2497766837477684 | R2:0.6861373016873177 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[260] Loss:0.2787343472242355 | L1 Loss:0.39976113736629487 | R2:0.27742757652389016 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[261] Loss:0.11536554805934429 | L1 Loss:0.24807886965572834 | R2:0.6966717560890516 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[261] Loss:0.2895241901278496 | L1 Loss:0.4040853321552277 | R2:0.24716551888644767 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[262] Loss:0.11467308946885169 | L1 Loss:0.24473769590258598 | R2:0.7012996768407952 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[262] Loss:0.294053053855896 | L1 Loss:0.40401423573493955 | R2:0.2402381964808798 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[263] Loss:0.11629809066653252 | L1 Loss:0.24942446686327457 | R2:0.6953245901043633 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[263] Loss:0.2867476925253868 | L1 Loss:0.4035380661487579 | R2:0.25721829542891733 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[264] Loss:0.11879947572015226 | L1 Loss:0.24884580355137587 | R2:0.6910603603265226 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[264] Loss:0.28119071274995805 | L1 Loss:0.3984238564968109 | R2:0.26134994585640364 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[265] Loss:0.11829902674071491 | L1 Loss:0.2498653419315815 | R2:0.6923583418319548 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[265] Loss:0.2881394147872925 | L1 Loss:0.3991010248661041 | R2:0.23733551909093925 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[266] Loss:0.11941005475819111 | L1 Loss:0.25342905707657337 | R2:0.68922752393972 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[266] Loss:0.2831682026386261 | L1 Loss:0.3995408594608307 | R2:0.25915269418190434 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[267] Loss:0.12213149666786194 | L1 Loss:0.25035513658076525 | R2:0.6815476635869268 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[267] Loss:0.2795685350894928 | L1 Loss:0.3994001388549805 | R2:0.27346572467292785 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[268] Loss:0.11944703198969364 | L1 Loss:0.2533089341595769 | R2:0.6894085760717199 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[268] Loss:0.2843856140971184 | L1 Loss:0.3999051332473755 | R2:0.25786715020132345 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[269] Loss:0.12094166898168623 | L1 Loss:0.25619508512318134 | R2:0.6847051748449854 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[269] Loss:0.2950763046741486 | L1 Loss:0.4042111933231354 | R2:0.2391740228156924 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[270] Loss:0.11893045995384455 | L1 Loss:0.2549027567729354 | R2:0.6890498885341678 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[270] Loss:0.2862074926495552 | L1 Loss:0.4036145478487015 | R2:0.25468842963512384 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[271] Loss:0.1209770580753684 | L1 Loss:0.2503211572766304 | R2:0.681046383349087 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[271] Loss:0.2982566237449646 | L1 Loss:0.4113315373659134 | R2:0.215264654766739 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[272] Loss:0.11829559179022908 | L1 Loss:0.24844426289200783 | R2:0.6892055229709065 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[272] Loss:0.2941926300525665 | L1 Loss:0.4081751585006714 | R2:0.2266299655606469 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[273] Loss:0.12096049450337887 | L1 Loss:0.24971609003841877 | R2:0.6862437255971414 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[273] Loss:0.28197952806949617 | L1 Loss:0.3975950539112091 | R2:0.27226084105525833 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[274] Loss:0.12066198443062603 | L1 Loss:0.2517068348824978 | R2:0.6854898802288599 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[274] Loss:0.2805767834186554 | L1 Loss:0.3969116538763046 | R2:0.26392120863496105 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[275] Loss:0.12224963074550033 | L1 Loss:0.24799685925245285 | R2:0.679692322978711 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[275] Loss:0.2898703694343567 | L1 Loss:0.4040029257535934 | R2:0.23402372417809003 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[276] Loss:0.1231368537992239 | L1 Loss:0.25529707595705986 | R2:0.6792000464300569 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[276] Loss:0.2858436807990074 | L1 Loss:0.40482166707515715 | R2:0.2594144337296741 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[277] Loss:0.12110848631709814 | L1 Loss:0.2517412193119526 | R2:0.6836761499588045 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[277] Loss:0.29486462473869324 | L1 Loss:0.411591574549675 | R2:0.2295154032920322 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[278] Loss:0.11734395520761609 | L1 Loss:0.2460533194243908 | R2:0.6923352078057066 | ACC: 88.8000%(444/500)\n",
            "Testing Epoch[278] Loss:0.28210575580596925 | L1 Loss:0.3954173862934113 | R2:0.274046049835491 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[279] Loss:0.1170666057150811 | L1 Loss:0.24694709572941065 | R2:0.6956453547182786 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[279] Loss:0.2826366305351257 | L1 Loss:0.39959797263145447 | R2:0.260007801341121 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[280] Loss:0.12309869565069675 | L1 Loss:0.2548160972073674 | R2:0.6753397051128406 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[280] Loss:0.2843915045261383 | L1 Loss:0.3995830476284027 | R2:0.2524499334344237 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[281] Loss:0.11773407435975969 | L1 Loss:0.25081808399409056 | R2:0.6938072255089693 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[281] Loss:0.27452942430973054 | L1 Loss:0.3926091343164444 | R2:0.2730619314396677 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[282] Loss:0.12325359042733908 | L1 Loss:0.2554969871416688 | R2:0.6753976939515054 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[282] Loss:0.29079500734806063 | L1 Loss:0.4050270736217499 | R2:0.23043868597760286 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[283] Loss:0.1193071804009378 | L1 Loss:0.2535343812778592 | R2:0.6854670504934751 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[283] Loss:0.2841264545917511 | L1 Loss:0.4034411162137985 | R2:0.25442010020226075 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[284] Loss:0.11783424252644181 | L1 Loss:0.2487329374998808 | R2:0.6921459327740137 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[284] Loss:0.2716497153043747 | L1 Loss:0.3930356651544571 | R2:0.28490482949477947 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[285] Loss:0.11790460720658302 | L1 Loss:0.2518620053306222 | R2:0.6905350336546531 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[285] Loss:0.28459492772817613 | L1 Loss:0.4049726128578186 | R2:0.25262301704137446 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[286] Loss:0.12071156920865178 | L1 Loss:0.2518744757398963 | R2:0.6814647065883355 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[286] Loss:0.2720838233828545 | L1 Loss:0.394742614030838 | R2:0.29266083251104474 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[287] Loss:0.1178976553492248 | L1 Loss:0.24963559862226248 | R2:0.6905692012739773 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[287] Loss:0.2777122139930725 | L1 Loss:0.39525540471076964 | R2:0.28034169947270793 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[288] Loss:0.12223548418842256 | L1 Loss:0.25653634034097195 | R2:0.6795875523377514 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[288] Loss:0.2846219137310982 | L1 Loss:0.39832319021224977 | R2:0.26431595320286105 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[289] Loss:0.11770416842773557 | L1 Loss:0.25020106323063374 | R2:0.690567117228306 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[289] Loss:0.27981865257024763 | L1 Loss:0.3916505739092827 | R2:0.2689557648497489 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[290] Loss:0.11991884000599384 | L1 Loss:0.25494639854878187 | R2:0.6850267355452033 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[290] Loss:0.2850756570696831 | L1 Loss:0.4004665851593018 | R2:0.25223799359237264 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[291] Loss:0.12074533477425575 | L1 Loss:0.2537048766389489 | R2:0.6836943898041288 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[291] Loss:0.2806022509932518 | L1 Loss:0.39801052808761594 | R2:0.2683401746410801 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[292] Loss:0.11987063009291887 | L1 Loss:0.2525910045951605 | R2:0.6861457482388845 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[292] Loss:0.2856812566518784 | L1 Loss:0.39740794003009794 | R2:0.24783365656914005 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[293] Loss:0.11987855564802885 | L1 Loss:0.25352895352989435 | R2:0.6863455515261448 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[293] Loss:0.2839458331465721 | L1 Loss:0.39681691527366636 | R2:0.2541362277386611 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[294] Loss:0.11641374370083213 | L1 Loss:0.24999371264129877 | R2:0.6980906029603995 | ACC: 87.6000%(438/500)\n",
            "Testing Epoch[294] Loss:0.286671082675457 | L1 Loss:0.40232702493667605 | R2:0.25184498276148237 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[295] Loss:0.11894981167279184 | L1 Loss:0.24883135594427586 | R2:0.6897276911517637 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[295] Loss:0.28045277297496796 | L1 Loss:0.39777198135852815 | R2:0.27125743391569723 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[296] Loss:0.1217921725474298 | L1 Loss:0.25065991654992104 | R2:0.6800540976916554 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[296] Loss:0.2638873927295208 | L1 Loss:0.38764765560626985 | R2:0.31595840723083235 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[297] Loss:0.12209638487547636 | L1 Loss:0.2517237802967429 | R2:0.6787014904943744 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[297] Loss:0.27628263980150225 | L1 Loss:0.40195921063423157 | R2:0.2828364729214079 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[298] Loss:0.12009420339018106 | L1 Loss:0.25006784684956074 | R2:0.6843873345584712 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[298] Loss:0.28457657992839813 | L1 Loss:0.403178808093071 | R2:0.24894540528783743 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[299] Loss:0.11848725704476237 | L1 Loss:0.25168358720839024 | R2:0.6894845870063093 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[299] Loss:0.2822530508041382 | L1 Loss:0.4018440037965775 | R2:0.259491654549988 | ACC: 72.0000%(216/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----MLP+ MLP 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7b0f19-e4c9-4977-bedc-88f90fba80be",
        "id": "_BVSShlHJ3Nu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----MLP+ MLP 300 epochs-----\n",
            "max test_r2_record  0.31595840723083235\n",
            "min test_loss_l1_record  0.38764765560626985\n",
            "min test_loss_record  0.2638873927295208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----MLP+ MLP 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e888aab-c34a-4ce9-dbdc-3171c5fbf06d",
        "id": "1flC3CvbJ3Nu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----MLP+ MLP 200 epochs-----\n",
            "max test_r2_record  0.3068995359359115\n",
            "min test_loss_l1_record  0.3908147007226944\n",
            "min test_loss_record  0.2664166435599327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----MLP+ MLP 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef000b1-0a4b-4beb-b5c8-2e0c77a512ae",
        "id": "8guH1rUqJ3Nv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----MLP+ MLP 100 epochs-----\n",
            "max test_r2_record  0.2979783161699098\n",
            "min test_loss_l1_record  0.3993300199508667\n",
            "min test_loss_record  0.2716941736638546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----MLP+ MLP 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "f46f721d-acb5-490d-9573-964c837b83a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWXGGSnZJ3Nw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----MLP+ MLP 50 epochs-----\n",
            "max test_r2_record  0.20738700487910214\n",
            "min test_loss_l1_record  0.42604627311229704\n",
            "min test_loss_record  0.30839959979057313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w44BZYNcNB1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}