{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "8ukuAdqcL6MQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ_Ci_QULpvN",
        "outputId": "635f28cc-4bfa-43d7-fbf9-dd2c8c0ec8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 33.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 40.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "from collections.abc import Iterable\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_geometric\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from torch.nn import Linear\n",
        "\n",
        "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool, BatchNorm\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.data import DataLoader as PYG_DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from itertools import cycle\n",
        "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "from scipy import interp\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X4ZE_WPL8Cf",
        "outputId": "fc6fc4e9-7e6b-46c1-afdd-8aa0f6e70a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Data"
      ],
      "metadata": {
        "id": "FhsZYu3bL-UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TUNzqGG2R3LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_path = \"/content/drive/MyDrive/miRNA/graph/edge.pkl\"\n",
        "edge_attr_path = \"/content/drive/MyDrive/miRNA/graph/edge_attr.pkl\"\n",
        "node_path = \"/content/drive/MyDrive/miRNA/graph/node.pkl\"\n",
        "mesh_emb_path = '/content/drive/MyDrive/miRNA/embedding/mesh_embs.npy'\n",
        "mesh_vocab_path = '/content/drive/MyDrive/miRNA/embedding/mesh_vocab.npy'"
      ],
      "metadata": {
        "id": "cBclIQPzM1sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# circulation\n",
        "all_x_disease_path = '/content/drive/MyDrive/miRNA/last_data/cir_X_Disease_v3.npy'\n",
        "all_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/cir_X_RNA_v3.npy'\n",
        "all_pre_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/cir_pre_RNA_x_v3.npy'\n",
        "all_y_soft_path = '/content/drive/MyDrive/miRNA/last_data/cir_soft_y_v3.npy'\n",
        "all_y_hard_path = '/content/drive/MyDrive/miRNA/last_data/cir_hard_y_v3.npy'\n",
        "cir_hair_x_path = '/content/drive/MyDrive/miRNA/last_data/cir_pre_hair_X_v3.npy'\n",
        "snd_x_path = \"/content/drive/MyDrive/miRNA/last_data/cir_snd_X_v3.npy\""
      ],
      "metadata": {
        "id": "KBa_z9aE9drK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all\n",
        "all_x_disease_path = '/content/drive/MyDrive/miRNA/last_data/all_X_Disease_v3.npy'\n",
        "all_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/all_X_RNA_v3.npy'\n",
        "all_pre_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/all_pre_RNA_x_v3.npy'\n",
        "all_y_soft_path = '/content/drive/MyDrive/miRNA/last_data/all_soft_y_v3.npy'\n",
        "all_y_hard_path = '/content/drive/MyDrive/miRNA/last_data/all_hard_y_v3.npy'\n",
        "all_hair_x_path = '/content/drive/MyDrive/miRNA/last_data/all_pre_hair_X_v3.npy'\n",
        "snd_x_path = \"/content/drive/MyDrive/miRNA/last_data/all_snd_X_v3.npy\""
      ],
      "metadata": {
        "id": "5t9XFkWP9Sya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tissue\n",
        "all_x_disease_path = '/content/drive/MyDrive/miRNA/last_data/tis_X_Disease_v3.npy'\n",
        "all_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/tis_X_RNA_v3.npy'\n",
        "all_pre_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/tis_pre_RNA_x_v3.npy'\n",
        "all_y_soft_path = '/content/drive/MyDrive/miRNA/last_data/tis_soft_y_v3.npy'\n",
        "all_y_hard_path = '/content/drive/MyDrive/miRNA/last_data/tis_hard_y_v3.npy'\n",
        "tis_hair_x_path = '/content/drive/MyDrive/miRNA/last_data/tis_pre_hair_X_v3.npy'\n",
        "snd_x_path = \"/content/drive/MyDrive/miRNA/last_data/tis_snd_X_v3.npy\""
      ],
      "metadata": {
        "id": "u9vEkheWqMk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gen\n",
        "all_x_disease_path = '/content/drive/MyDrive/miRNA/last_data/gen_X_Disease_v3.npy'\n",
        "all_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/gen_X_RNA_v3.npy'\n",
        "all_pre_x_rna_path = '/content/drive/MyDrive/miRNA/last_data/gen_pre_RNA_x_v3.npy'\n",
        "all_y_soft_path = '/content/drive/MyDrive/miRNA/last_data/gen_soft_y_v3.npy'\n",
        "all_y_hard_path = '/content/drive/MyDrive/miRNA/last_data/gen_hard_y_v3.npy'\n",
        "gen_hair_x_path = '/content/drive/MyDrive/miRNA/last_data/gen_pre_hair_X_v3.npy'\n",
        "snd_x_path = \"/content/drive/MyDrive/miRNA/last_data/gen_snd_X_v3.npy\""
      ],
      "metadata": {
        "id": "XRcahA9e5-d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cir\n",
        "all_orig_x_disease = np.load(all_x_disease_path, allow_pickle=True)\n",
        "all_orig_x_rna = np.load(all_x_rna_path, allow_pickle=True)\n",
        "all_orig_pre_x_rna = np.load(all_pre_x_rna_path, allow_pickle=True)\n",
        "\n",
        "all_hair_x = np.load(cir_hair_x_path, allow_pickle=True)\n",
        "snd_x = np.load(snd_x_path, allow_pickle=True)\n",
        "\n",
        "all_orig_Y_soft = np.load(all_y_soft_path, allow_pickle=True)\n",
        "all_orig_Y_hard = np.load(all_y_hard_path, allow_pickle=True)"
      ],
      "metadata": {
        "id": "5kU9U4TV9-R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all\n",
        "all_orig_x_disease = np.load(all_x_disease_path, allow_pickle=True)\n",
        "all_orig_x_rna = np.load(all_x_rna_path, allow_pickle=True)\n",
        "all_orig_pre_x_rna = np.load(all_pre_x_rna_path, allow_pickle=True)\n",
        "\n",
        "all_hair_x = np.load(all_hair_x_path, allow_pickle=True)\n",
        "snd_x = np.load(snd_x_path, allow_pickle=True)\n",
        "\n",
        "all_orig_Y_soft = np.load(all_y_soft_path, allow_pickle=True)\n",
        "all_orig_Y_hard = np.load(all_y_hard_path, allow_pickle=True)"
      ],
      "metadata": {
        "id": "AubikgH-9bfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tis\n",
        "all_orig_x_disease = np.load(all_x_disease_path, allow_pickle=True)\n",
        "all_orig_x_rna = np.load(all_x_rna_path, allow_pickle=True)\n",
        "all_orig_pre_x_rna = np.load(all_pre_x_rna_path, allow_pickle=True)\n",
        "\n",
        "all_hair_x = np.load(tis_hair_x_path, allow_pickle=True)\n",
        "snd_x = np.load(snd_x_path, allow_pickle=True)\n",
        "\n",
        "all_orig_Y_soft = np.load(all_y_soft_path, allow_pickle=True)\n",
        "all_orig_Y_hard = np.load(all_y_hard_path, allow_pickle=True)"
      ],
      "metadata": {
        "id": "7jJQ8TYfqU8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gen\n",
        "all_orig_x_disease = np.load(all_x_disease_path, allow_pickle=True)\n",
        "all_orig_x_rna = np.load(all_x_rna_path, allow_pickle=True)\n",
        "all_orig_pre_x_rna = np.load(all_pre_x_rna_path, allow_pickle=True)\n",
        "\n",
        "all_hair_x = np.load(gen_hair_x_path, allow_pickle=True)\n",
        "snd_x = np.load(snd_x_path, allow_pickle=True)\n",
        "\n",
        "all_orig_Y_soft = np.load(all_y_soft_path, allow_pickle=True)\n",
        "all_orig_Y_hard = np.load(all_y_hard_path, allow_pickle=True)"
      ],
      "metadata": {
        "id": "x7krwL4J6MW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(edge_path, 'rb') as f:\n",
        "    edge_d = pickle.load(f)\n",
        "\n",
        "with open(edge_attr_path, 'rb') as f:\n",
        "    edge_attr_d = pickle.load(f)\n",
        "\n",
        "with open(node_path, 'rb') as f:\n",
        "    node_d = pickle.load(f)"
      ],
      "metadata": {
        "id": "fQ7HEDk3Jl06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for regression\n",
        "x_disease=[]\n",
        "x_rna=[]\n",
        "hair_x=[]\n",
        "y_soft=[]\n",
        "# y_hard=[]\n",
        "max_len = 0\n",
        "for i in range(len(all_orig_x_disease)):\n",
        "  value = edge_d.get(all_orig_x_disease[i])\n",
        "  if value != None:\n",
        "    x_disease.append(all_orig_x_disease[i])\n",
        "    # x_rna.append(all_orig_pre_x_rna[i]+all_orig_x_rna[i])\n",
        "    x_rna.append(all_orig_x_rna[i])\n",
        "    if max_len < len(all_orig_x_rna[i]):max_len = len(all_orig_x_rna[i])\n",
        "    hair_x.append([*all_hair_x[i], *snd_x[i]])\n",
        "    y_soft.append(all_orig_Y_soft[i])\n",
        "    # # y_hard.append(all_orig_Y_hard[i])\n",
        "    # 'down', 'ns', 'up'\n",
        "    # print(i)\n",
        "\n",
        "x_disease=np.array(x_disease)\n",
        "x_rna=np.array(x_rna)\n",
        "hair_x=np.array(hair_x)\n",
        "y_soft=np.array(y_soft)\n",
        "# # y_hard=np.array(y_hard)"
      ],
      "metadata": {
        "id": "oXLQb85iZckY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy7CO87CsKtR",
        "outputId": "a1f79436-04fe-4dad-b1c1-68325180c5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(all_orig_x_disease))\n",
        "print(len(all_orig_x_rna))\n",
        "print(len(all_orig_Y_soft))\n",
        "print(len(all_orig_Y_hard))\n",
        "print(len(all_orig_pre_x_rna))\n",
        "print(len(all_hair_x))\n",
        "print(len(snd_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syhFxTkEDrdX",
        "outputId": "a03259c7-6b37-45b6-95f5-0de76776eedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2444\n",
            "2444\n",
            "2444\n",
            "2444\n",
            "2444\n",
            "2444\n",
            "2444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_disease))\n",
        "print(len(x_rna))\n",
        "print(len(y_soft))\n",
        "# print(len(y_hard))\n",
        "print(len(hair_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr248eyIN1s7",
        "outputId": "82720965-3c33-4a99-9b91-7ec53ca76897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6587\n",
            "6587\n",
            "6587\n",
            "6587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for classification\n",
        "x_disease=[]\n",
        "x_rna=[]\n",
        "y_soft=[]\n",
        "y_hard=[]\n",
        "hair_x=[]\n",
        "y_label={'down':0, 'ns':1, 'up':2}\n",
        "max_len = 0\n",
        "for i in range(len(all_orig_x_disease)):\n",
        "  value = edge_d.get(all_orig_x_disease[i])\n",
        "  if value != None and all_orig_Y_hard[i]!='delete':\n",
        "    x_disease.append(all_orig_x_disease[i])\n",
        "    x_rna.append(all_orig_x_rna[i])\n",
        "    # x_rna.append(all_orig_x_rna[i])\n",
        "    if max_len < len(all_orig_pre_x_rna[i]+all_orig_x_rna[i]):max_len = len(all_orig_pre_x_rna[i]+all_orig_x_rna[i])\n",
        "    y_soft.append(all_orig_Y_soft[i])\n",
        "    hair_x.append([*all_hair_x[i], *snd_x[i]])\n",
        "    # 'down', 'ns', 'up'\n",
        "    # print(i)\n",
        "    y_hard.append(y_label[all_orig_Y_hard[i]])\n",
        "\n",
        "x_disease=np.array(x_disease)\n",
        "x_rna=np.array(x_rna)\n",
        "y_soft=np.array(y_soft)\n",
        "y_hard=np.array(y_hard)\n",
        "hair_x=np.array(hair_x)"
      ],
      "metadata": {
        "id": "i6RdUzBnS8RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for classification gen\n",
        "x_disease=[]\n",
        "x_rna=[]\n",
        "y_soft=[]\n",
        "y_hard=[]\n",
        "hair_x=[]\n",
        "y_label={'down':0, 'up':1}\n",
        "max_len = 0\n",
        "for i in range(len(all_orig_x_disease)):\n",
        "  value = edge_d.get(all_orig_x_disease[i])\n",
        "  if value != None and all_orig_Y_hard[i]!='delete':\n",
        "    x_disease.append(all_orig_x_disease[i])\n",
        "    x_rna.append(all_orig_x_rna[i])\n",
        "    # x_rna.append(all_orig_x_rna[i])\n",
        "    if max_len < len(all_orig_pre_x_rna[i]+all_orig_x_rna[i]):max_len = len(all_orig_pre_x_rna[i]+all_orig_x_rna[i])\n",
        "    y_soft.append(all_orig_Y_soft[i])\n",
        "    hair_x.append([*all_hair_x[i], *snd_x[i]])\n",
        "    # 'down', 'ns', 'up'\n",
        "    # print(i)\n",
        "    y_hard.append(y_label[all_orig_Y_hard[i]])\n",
        "\n",
        "x_disease=np.array(x_disease)\n",
        "x_rna=np.array(x_rna)\n",
        "y_soft=np.array(y_soft)\n",
        "y_hard=np.array(y_hard)\n",
        "hair_x=np.array(hair_x)"
      ],
      "metadata": {
        "id": "dDT4JJrlk7vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_disease))\n",
        "print(len(x_rna))\n",
        "print(len(y_soft))\n",
        "print(len(y_hard))\n",
        "# print(len(hair_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHCGX2T5TkE2",
        "outputId": "3b093014-b0e3-4d28-b60b-e132490a88ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2031\n",
            "2031\n",
            "2031\n",
            "2031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = np.load(r'/content/drive/MyDrive/miRNA/embedding/vocab_npa.npy', allow_pickle=True)\n",
        "values = np.load(r'/content/drive/MyDrive/miRNA/embedding/embs_npa.npy', allow_pickle=True)\n",
        "word2vec = dict(zip(keys, values))"
      ],
      "metadata": {
        "id": "cMIk0-zrUoiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = np.load(mesh_vocab_path, allow_pickle=True)\n",
        "values = np.load(mesh_emb_path, allow_pickle=True)\n",
        "\n",
        "word2vec_mesh = dict(zip(keys, values))"
      ],
      "metadata": {
        "id": "Lx44iSfwSDUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "BQJuYEDvMDhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DY_HCr6fV_WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(mer = {}, n = 1):\n",
        "  pointer = 4\n",
        "  if mer == {}:\n",
        "    for i in range(4):\n",
        "      i+=1\n",
        "      mer[str(i)] = str(pointer)\n",
        "      pointer += 1\n",
        "  else:\n",
        "    old_dict = mer\n",
        "    new_dict = {}\n",
        "    for key in old_dict:\n",
        "      for i in range(4):\n",
        "        i+=1\n",
        "        new_key = key + str(i)\n",
        "        if key[0] != '0' and key[-1] != '0':\n",
        "          new_dict[new_key] = str(pointer)\n",
        "          pointer += 1\n",
        "        # if key[-1] == '0' and i == 0:\n",
        "        #   new_dict[new_key] = str(pointer)\n",
        "        #   pointer += 1\n",
        "    mer = new_dict\n",
        "  # print(mer)\n",
        "  if n != 0:\n",
        "    return make_dict(mer, n-1)\n",
        "  else:\n",
        "    return mer\n",
        "\n"
      ],
      "metadata": {
        "id": "hQLkZQz6ymvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=0)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "twoMer_dict = {**a_dict, **new_mer_dict}\n",
        "twoMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIYkxjG4_13V",
        "outputId": "aa3dbfdb-a211-4355-c3ac-c30161a313b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'11': 4,\n",
              " '12': 5,\n",
              " '13': 6,\n",
              " '14': 7,\n",
              " '21': 8,\n",
              " '22': 9,\n",
              " '23': 10,\n",
              " '24': 11,\n",
              " '31': 12,\n",
              " '32': 13,\n",
              " '33': 14,\n",
              " '34': 15,\n",
              " '41': 16,\n",
              " '42': 17,\n",
              " '43': 18,\n",
              " '44': 19,\n",
              " '[CLS]': 1,\n",
              " '[MASK]': 3,\n",
              " '[PAD]': 0,\n",
              " '[SEP]': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=1)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "threeMer_dict = {**a_dict, **new_mer_dict}\n",
        "threeMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vS_9KIJ0xOx",
        "outputId": "fab0c153-f93e-4ad7-cc26-23d85e014271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'111': 4,\n",
              " '112': 5,\n",
              " '113': 6,\n",
              " '114': 7,\n",
              " '121': 8,\n",
              " '122': 9,\n",
              " '123': 10,\n",
              " '124': 11,\n",
              " '131': 12,\n",
              " '132': 13,\n",
              " '133': 14,\n",
              " '134': 15,\n",
              " '141': 16,\n",
              " '142': 17,\n",
              " '143': 18,\n",
              " '144': 19,\n",
              " '211': 20,\n",
              " '212': 21,\n",
              " '213': 22,\n",
              " '214': 23,\n",
              " '221': 24,\n",
              " '222': 25,\n",
              " '223': 26,\n",
              " '224': 27,\n",
              " '231': 28,\n",
              " '232': 29,\n",
              " '233': 30,\n",
              " '234': 31,\n",
              " '241': 32,\n",
              " '242': 33,\n",
              " '243': 34,\n",
              " '244': 35,\n",
              " '311': 36,\n",
              " '312': 37,\n",
              " '313': 38,\n",
              " '314': 39,\n",
              " '321': 40,\n",
              " '322': 41,\n",
              " '323': 42,\n",
              " '324': 43,\n",
              " '331': 44,\n",
              " '332': 45,\n",
              " '333': 46,\n",
              " '334': 47,\n",
              " '341': 48,\n",
              " '342': 49,\n",
              " '343': 50,\n",
              " '344': 51,\n",
              " '411': 52,\n",
              " '412': 53,\n",
              " '413': 54,\n",
              " '414': 55,\n",
              " '421': 56,\n",
              " '422': 57,\n",
              " '423': 58,\n",
              " '424': 59,\n",
              " '431': 60,\n",
              " '432': 61,\n",
              " '433': 62,\n",
              " '434': 63,\n",
              " '441': 64,\n",
              " '442': 65,\n",
              " '443': 66,\n",
              " '444': 67,\n",
              " '[CLS]': 1,\n",
              " '[MASK]': 3,\n",
              " '[PAD]': 0,\n",
              " '[SEP]': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=2)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "fourMer_dict = {**a_dict, **new_mer_dict}\n",
        "fourMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaw0q3xOeobc",
        "outputId": "0b2ec7f3-8551-46ad-be5c-f3c51cf8e034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1111': 4,\n",
              " '1112': 5,\n",
              " '1113': 6,\n",
              " '1114': 7,\n",
              " '1121': 8,\n",
              " '1122': 9,\n",
              " '1123': 10,\n",
              " '1124': 11,\n",
              " '1131': 12,\n",
              " '1132': 13,\n",
              " '1133': 14,\n",
              " '1134': 15,\n",
              " '1141': 16,\n",
              " '1142': 17,\n",
              " '1143': 18,\n",
              " '1144': 19,\n",
              " '1211': 20,\n",
              " '1212': 21,\n",
              " '1213': 22,\n",
              " '1214': 23,\n",
              " '1221': 24,\n",
              " '1222': 25,\n",
              " '1223': 26,\n",
              " '1224': 27,\n",
              " '1231': 28,\n",
              " '1232': 29,\n",
              " '1233': 30,\n",
              " '1234': 31,\n",
              " '1241': 32,\n",
              " '1242': 33,\n",
              " '1243': 34,\n",
              " '1244': 35,\n",
              " '1311': 36,\n",
              " '1312': 37,\n",
              " '1313': 38,\n",
              " '1314': 39,\n",
              " '1321': 40,\n",
              " '1322': 41,\n",
              " '1323': 42,\n",
              " '1324': 43,\n",
              " '1331': 44,\n",
              " '1332': 45,\n",
              " '1333': 46,\n",
              " '1334': 47,\n",
              " '1341': 48,\n",
              " '1342': 49,\n",
              " '1343': 50,\n",
              " '1344': 51,\n",
              " '1411': 52,\n",
              " '1412': 53,\n",
              " '1413': 54,\n",
              " '1414': 55,\n",
              " '1421': 56,\n",
              " '1422': 57,\n",
              " '1423': 58,\n",
              " '1424': 59,\n",
              " '1431': 60,\n",
              " '1432': 61,\n",
              " '1433': 62,\n",
              " '1434': 63,\n",
              " '1441': 64,\n",
              " '1442': 65,\n",
              " '1443': 66,\n",
              " '1444': 67,\n",
              " '2111': 68,\n",
              " '2112': 69,\n",
              " '2113': 70,\n",
              " '2114': 71,\n",
              " '2121': 72,\n",
              " '2122': 73,\n",
              " '2123': 74,\n",
              " '2124': 75,\n",
              " '2131': 76,\n",
              " '2132': 77,\n",
              " '2133': 78,\n",
              " '2134': 79,\n",
              " '2141': 80,\n",
              " '2142': 81,\n",
              " '2143': 82,\n",
              " '2144': 83,\n",
              " '2211': 84,\n",
              " '2212': 85,\n",
              " '2213': 86,\n",
              " '2214': 87,\n",
              " '2221': 88,\n",
              " '2222': 89,\n",
              " '2223': 90,\n",
              " '2224': 91,\n",
              " '2231': 92,\n",
              " '2232': 93,\n",
              " '2233': 94,\n",
              " '2234': 95,\n",
              " '2241': 96,\n",
              " '2242': 97,\n",
              " '2243': 98,\n",
              " '2244': 99,\n",
              " '2311': 100,\n",
              " '2312': 101,\n",
              " '2313': 102,\n",
              " '2314': 103,\n",
              " '2321': 104,\n",
              " '2322': 105,\n",
              " '2323': 106,\n",
              " '2324': 107,\n",
              " '2331': 108,\n",
              " '2332': 109,\n",
              " '2333': 110,\n",
              " '2334': 111,\n",
              " '2341': 112,\n",
              " '2342': 113,\n",
              " '2343': 114,\n",
              " '2344': 115,\n",
              " '2411': 116,\n",
              " '2412': 117,\n",
              " '2413': 118,\n",
              " '2414': 119,\n",
              " '2421': 120,\n",
              " '2422': 121,\n",
              " '2423': 122,\n",
              " '2424': 123,\n",
              " '2431': 124,\n",
              " '2432': 125,\n",
              " '2433': 126,\n",
              " '2434': 127,\n",
              " '2441': 128,\n",
              " '2442': 129,\n",
              " '2443': 130,\n",
              " '2444': 131,\n",
              " '3111': 132,\n",
              " '3112': 133,\n",
              " '3113': 134,\n",
              " '3114': 135,\n",
              " '3121': 136,\n",
              " '3122': 137,\n",
              " '3123': 138,\n",
              " '3124': 139,\n",
              " '3131': 140,\n",
              " '3132': 141,\n",
              " '3133': 142,\n",
              " '3134': 143,\n",
              " '3141': 144,\n",
              " '3142': 145,\n",
              " '3143': 146,\n",
              " '3144': 147,\n",
              " '3211': 148,\n",
              " '3212': 149,\n",
              " '3213': 150,\n",
              " '3214': 151,\n",
              " '3221': 152,\n",
              " '3222': 153,\n",
              " '3223': 154,\n",
              " '3224': 155,\n",
              " '3231': 156,\n",
              " '3232': 157,\n",
              " '3233': 158,\n",
              " '3234': 159,\n",
              " '3241': 160,\n",
              " '3242': 161,\n",
              " '3243': 162,\n",
              " '3244': 163,\n",
              " '3311': 164,\n",
              " '3312': 165,\n",
              " '3313': 166,\n",
              " '3314': 167,\n",
              " '3321': 168,\n",
              " '3322': 169,\n",
              " '3323': 170,\n",
              " '3324': 171,\n",
              " '3331': 172,\n",
              " '3332': 173,\n",
              " '3333': 174,\n",
              " '3334': 175,\n",
              " '3341': 176,\n",
              " '3342': 177,\n",
              " '3343': 178,\n",
              " '3344': 179,\n",
              " '3411': 180,\n",
              " '3412': 181,\n",
              " '3413': 182,\n",
              " '3414': 183,\n",
              " '3421': 184,\n",
              " '3422': 185,\n",
              " '3423': 186,\n",
              " '3424': 187,\n",
              " '3431': 188,\n",
              " '3432': 189,\n",
              " '3433': 190,\n",
              " '3434': 191,\n",
              " '3441': 192,\n",
              " '3442': 193,\n",
              " '3443': 194,\n",
              " '3444': 195,\n",
              " '4111': 196,\n",
              " '4112': 197,\n",
              " '4113': 198,\n",
              " '4114': 199,\n",
              " '4121': 200,\n",
              " '4122': 201,\n",
              " '4123': 202,\n",
              " '4124': 203,\n",
              " '4131': 204,\n",
              " '4132': 205,\n",
              " '4133': 206,\n",
              " '4134': 207,\n",
              " '4141': 208,\n",
              " '4142': 209,\n",
              " '4143': 210,\n",
              " '4144': 211,\n",
              " '4211': 212,\n",
              " '4212': 213,\n",
              " '4213': 214,\n",
              " '4214': 215,\n",
              " '4221': 216,\n",
              " '4222': 217,\n",
              " '4223': 218,\n",
              " '4224': 219,\n",
              " '4231': 220,\n",
              " '4232': 221,\n",
              " '4233': 222,\n",
              " '4234': 223,\n",
              " '4241': 224,\n",
              " '4242': 225,\n",
              " '4243': 226,\n",
              " '4244': 227,\n",
              " '4311': 228,\n",
              " '4312': 229,\n",
              " '4313': 230,\n",
              " '4314': 231,\n",
              " '4321': 232,\n",
              " '4322': 233,\n",
              " '4323': 234,\n",
              " '4324': 235,\n",
              " '4331': 236,\n",
              " '4332': 237,\n",
              " '4333': 238,\n",
              " '4334': 239,\n",
              " '4341': 240,\n",
              " '4342': 241,\n",
              " '4343': 242,\n",
              " '4344': 243,\n",
              " '4411': 244,\n",
              " '4412': 245,\n",
              " '4413': 246,\n",
              " '4414': 247,\n",
              " '4421': 248,\n",
              " '4422': 249,\n",
              " '4423': 250,\n",
              " '4424': 251,\n",
              " '4431': 252,\n",
              " '4432': 253,\n",
              " '4433': 254,\n",
              " '4434': 255,\n",
              " '4441': 256,\n",
              " '4442': 257,\n",
              " '4443': 258,\n",
              " '4444': 259,\n",
              " '[CLS]': 1,\n",
              " '[MASK]': 3,\n",
              " '[PAD]': 0,\n",
              " '[SEP]': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=3)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "fiveMer_dict = {**a_dict, **new_mer_dict}\n",
        "fiveMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaJii5Ggetdx",
        "outputId": "8d0fc18c-64a7-48c0-84b2-3a06d6bbe9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[CLS]': 1,\n",
              " '[SEP]': 2,\n",
              " '[MASK]': 3,\n",
              " '11111': 4,\n",
              " '11112': 5,\n",
              " '11113': 6,\n",
              " '11114': 7,\n",
              " '11121': 8,\n",
              " '11122': 9,\n",
              " '11123': 10,\n",
              " '11124': 11,\n",
              " '11131': 12,\n",
              " '11132': 13,\n",
              " '11133': 14,\n",
              " '11134': 15,\n",
              " '11141': 16,\n",
              " '11142': 17,\n",
              " '11143': 18,\n",
              " '11144': 19,\n",
              " '11211': 20,\n",
              " '11212': 21,\n",
              " '11213': 22,\n",
              " '11214': 23,\n",
              " '11221': 24,\n",
              " '11222': 25,\n",
              " '11223': 26,\n",
              " '11224': 27,\n",
              " '11231': 28,\n",
              " '11232': 29,\n",
              " '11233': 30,\n",
              " '11234': 31,\n",
              " '11241': 32,\n",
              " '11242': 33,\n",
              " '11243': 34,\n",
              " '11244': 35,\n",
              " '11311': 36,\n",
              " '11312': 37,\n",
              " '11313': 38,\n",
              " '11314': 39,\n",
              " '11321': 40,\n",
              " '11322': 41,\n",
              " '11323': 42,\n",
              " '11324': 43,\n",
              " '11331': 44,\n",
              " '11332': 45,\n",
              " '11333': 46,\n",
              " '11334': 47,\n",
              " '11341': 48,\n",
              " '11342': 49,\n",
              " '11343': 50,\n",
              " '11344': 51,\n",
              " '11411': 52,\n",
              " '11412': 53,\n",
              " '11413': 54,\n",
              " '11414': 55,\n",
              " '11421': 56,\n",
              " '11422': 57,\n",
              " '11423': 58,\n",
              " '11424': 59,\n",
              " '11431': 60,\n",
              " '11432': 61,\n",
              " '11433': 62,\n",
              " '11434': 63,\n",
              " '11441': 64,\n",
              " '11442': 65,\n",
              " '11443': 66,\n",
              " '11444': 67,\n",
              " '12111': 68,\n",
              " '12112': 69,\n",
              " '12113': 70,\n",
              " '12114': 71,\n",
              " '12121': 72,\n",
              " '12122': 73,\n",
              " '12123': 74,\n",
              " '12124': 75,\n",
              " '12131': 76,\n",
              " '12132': 77,\n",
              " '12133': 78,\n",
              " '12134': 79,\n",
              " '12141': 80,\n",
              " '12142': 81,\n",
              " '12143': 82,\n",
              " '12144': 83,\n",
              " '12211': 84,\n",
              " '12212': 85,\n",
              " '12213': 86,\n",
              " '12214': 87,\n",
              " '12221': 88,\n",
              " '12222': 89,\n",
              " '12223': 90,\n",
              " '12224': 91,\n",
              " '12231': 92,\n",
              " '12232': 93,\n",
              " '12233': 94,\n",
              " '12234': 95,\n",
              " '12241': 96,\n",
              " '12242': 97,\n",
              " '12243': 98,\n",
              " '12244': 99,\n",
              " '12311': 100,\n",
              " '12312': 101,\n",
              " '12313': 102,\n",
              " '12314': 103,\n",
              " '12321': 104,\n",
              " '12322': 105,\n",
              " '12323': 106,\n",
              " '12324': 107,\n",
              " '12331': 108,\n",
              " '12332': 109,\n",
              " '12333': 110,\n",
              " '12334': 111,\n",
              " '12341': 112,\n",
              " '12342': 113,\n",
              " '12343': 114,\n",
              " '12344': 115,\n",
              " '12411': 116,\n",
              " '12412': 117,\n",
              " '12413': 118,\n",
              " '12414': 119,\n",
              " '12421': 120,\n",
              " '12422': 121,\n",
              " '12423': 122,\n",
              " '12424': 123,\n",
              " '12431': 124,\n",
              " '12432': 125,\n",
              " '12433': 126,\n",
              " '12434': 127,\n",
              " '12441': 128,\n",
              " '12442': 129,\n",
              " '12443': 130,\n",
              " '12444': 131,\n",
              " '13111': 132,\n",
              " '13112': 133,\n",
              " '13113': 134,\n",
              " '13114': 135,\n",
              " '13121': 136,\n",
              " '13122': 137,\n",
              " '13123': 138,\n",
              " '13124': 139,\n",
              " '13131': 140,\n",
              " '13132': 141,\n",
              " '13133': 142,\n",
              " '13134': 143,\n",
              " '13141': 144,\n",
              " '13142': 145,\n",
              " '13143': 146,\n",
              " '13144': 147,\n",
              " '13211': 148,\n",
              " '13212': 149,\n",
              " '13213': 150,\n",
              " '13214': 151,\n",
              " '13221': 152,\n",
              " '13222': 153,\n",
              " '13223': 154,\n",
              " '13224': 155,\n",
              " '13231': 156,\n",
              " '13232': 157,\n",
              " '13233': 158,\n",
              " '13234': 159,\n",
              " '13241': 160,\n",
              " '13242': 161,\n",
              " '13243': 162,\n",
              " '13244': 163,\n",
              " '13311': 164,\n",
              " '13312': 165,\n",
              " '13313': 166,\n",
              " '13314': 167,\n",
              " '13321': 168,\n",
              " '13322': 169,\n",
              " '13323': 170,\n",
              " '13324': 171,\n",
              " '13331': 172,\n",
              " '13332': 173,\n",
              " '13333': 174,\n",
              " '13334': 175,\n",
              " '13341': 176,\n",
              " '13342': 177,\n",
              " '13343': 178,\n",
              " '13344': 179,\n",
              " '13411': 180,\n",
              " '13412': 181,\n",
              " '13413': 182,\n",
              " '13414': 183,\n",
              " '13421': 184,\n",
              " '13422': 185,\n",
              " '13423': 186,\n",
              " '13424': 187,\n",
              " '13431': 188,\n",
              " '13432': 189,\n",
              " '13433': 190,\n",
              " '13434': 191,\n",
              " '13441': 192,\n",
              " '13442': 193,\n",
              " '13443': 194,\n",
              " '13444': 195,\n",
              " '14111': 196,\n",
              " '14112': 197,\n",
              " '14113': 198,\n",
              " '14114': 199,\n",
              " '14121': 200,\n",
              " '14122': 201,\n",
              " '14123': 202,\n",
              " '14124': 203,\n",
              " '14131': 204,\n",
              " '14132': 205,\n",
              " '14133': 206,\n",
              " '14134': 207,\n",
              " '14141': 208,\n",
              " '14142': 209,\n",
              " '14143': 210,\n",
              " '14144': 211,\n",
              " '14211': 212,\n",
              " '14212': 213,\n",
              " '14213': 214,\n",
              " '14214': 215,\n",
              " '14221': 216,\n",
              " '14222': 217,\n",
              " '14223': 218,\n",
              " '14224': 219,\n",
              " '14231': 220,\n",
              " '14232': 221,\n",
              " '14233': 222,\n",
              " '14234': 223,\n",
              " '14241': 224,\n",
              " '14242': 225,\n",
              " '14243': 226,\n",
              " '14244': 227,\n",
              " '14311': 228,\n",
              " '14312': 229,\n",
              " '14313': 230,\n",
              " '14314': 231,\n",
              " '14321': 232,\n",
              " '14322': 233,\n",
              " '14323': 234,\n",
              " '14324': 235,\n",
              " '14331': 236,\n",
              " '14332': 237,\n",
              " '14333': 238,\n",
              " '14334': 239,\n",
              " '14341': 240,\n",
              " '14342': 241,\n",
              " '14343': 242,\n",
              " '14344': 243,\n",
              " '14411': 244,\n",
              " '14412': 245,\n",
              " '14413': 246,\n",
              " '14414': 247,\n",
              " '14421': 248,\n",
              " '14422': 249,\n",
              " '14423': 250,\n",
              " '14424': 251,\n",
              " '14431': 252,\n",
              " '14432': 253,\n",
              " '14433': 254,\n",
              " '14434': 255,\n",
              " '14441': 256,\n",
              " '14442': 257,\n",
              " '14443': 258,\n",
              " '14444': 259,\n",
              " '21111': 260,\n",
              " '21112': 261,\n",
              " '21113': 262,\n",
              " '21114': 263,\n",
              " '21121': 264,\n",
              " '21122': 265,\n",
              " '21123': 266,\n",
              " '21124': 267,\n",
              " '21131': 268,\n",
              " '21132': 269,\n",
              " '21133': 270,\n",
              " '21134': 271,\n",
              " '21141': 272,\n",
              " '21142': 273,\n",
              " '21143': 274,\n",
              " '21144': 275,\n",
              " '21211': 276,\n",
              " '21212': 277,\n",
              " '21213': 278,\n",
              " '21214': 279,\n",
              " '21221': 280,\n",
              " '21222': 281,\n",
              " '21223': 282,\n",
              " '21224': 283,\n",
              " '21231': 284,\n",
              " '21232': 285,\n",
              " '21233': 286,\n",
              " '21234': 287,\n",
              " '21241': 288,\n",
              " '21242': 289,\n",
              " '21243': 290,\n",
              " '21244': 291,\n",
              " '21311': 292,\n",
              " '21312': 293,\n",
              " '21313': 294,\n",
              " '21314': 295,\n",
              " '21321': 296,\n",
              " '21322': 297,\n",
              " '21323': 298,\n",
              " '21324': 299,\n",
              " '21331': 300,\n",
              " '21332': 301,\n",
              " '21333': 302,\n",
              " '21334': 303,\n",
              " '21341': 304,\n",
              " '21342': 305,\n",
              " '21343': 306,\n",
              " '21344': 307,\n",
              " '21411': 308,\n",
              " '21412': 309,\n",
              " '21413': 310,\n",
              " '21414': 311,\n",
              " '21421': 312,\n",
              " '21422': 313,\n",
              " '21423': 314,\n",
              " '21424': 315,\n",
              " '21431': 316,\n",
              " '21432': 317,\n",
              " '21433': 318,\n",
              " '21434': 319,\n",
              " '21441': 320,\n",
              " '21442': 321,\n",
              " '21443': 322,\n",
              " '21444': 323,\n",
              " '22111': 324,\n",
              " '22112': 325,\n",
              " '22113': 326,\n",
              " '22114': 327,\n",
              " '22121': 328,\n",
              " '22122': 329,\n",
              " '22123': 330,\n",
              " '22124': 331,\n",
              " '22131': 332,\n",
              " '22132': 333,\n",
              " '22133': 334,\n",
              " '22134': 335,\n",
              " '22141': 336,\n",
              " '22142': 337,\n",
              " '22143': 338,\n",
              " '22144': 339,\n",
              " '22211': 340,\n",
              " '22212': 341,\n",
              " '22213': 342,\n",
              " '22214': 343,\n",
              " '22221': 344,\n",
              " '22222': 345,\n",
              " '22223': 346,\n",
              " '22224': 347,\n",
              " '22231': 348,\n",
              " '22232': 349,\n",
              " '22233': 350,\n",
              " '22234': 351,\n",
              " '22241': 352,\n",
              " '22242': 353,\n",
              " '22243': 354,\n",
              " '22244': 355,\n",
              " '22311': 356,\n",
              " '22312': 357,\n",
              " '22313': 358,\n",
              " '22314': 359,\n",
              " '22321': 360,\n",
              " '22322': 361,\n",
              " '22323': 362,\n",
              " '22324': 363,\n",
              " '22331': 364,\n",
              " '22332': 365,\n",
              " '22333': 366,\n",
              " '22334': 367,\n",
              " '22341': 368,\n",
              " '22342': 369,\n",
              " '22343': 370,\n",
              " '22344': 371,\n",
              " '22411': 372,\n",
              " '22412': 373,\n",
              " '22413': 374,\n",
              " '22414': 375,\n",
              " '22421': 376,\n",
              " '22422': 377,\n",
              " '22423': 378,\n",
              " '22424': 379,\n",
              " '22431': 380,\n",
              " '22432': 381,\n",
              " '22433': 382,\n",
              " '22434': 383,\n",
              " '22441': 384,\n",
              " '22442': 385,\n",
              " '22443': 386,\n",
              " '22444': 387,\n",
              " '23111': 388,\n",
              " '23112': 389,\n",
              " '23113': 390,\n",
              " '23114': 391,\n",
              " '23121': 392,\n",
              " '23122': 393,\n",
              " '23123': 394,\n",
              " '23124': 395,\n",
              " '23131': 396,\n",
              " '23132': 397,\n",
              " '23133': 398,\n",
              " '23134': 399,\n",
              " '23141': 400,\n",
              " '23142': 401,\n",
              " '23143': 402,\n",
              " '23144': 403,\n",
              " '23211': 404,\n",
              " '23212': 405,\n",
              " '23213': 406,\n",
              " '23214': 407,\n",
              " '23221': 408,\n",
              " '23222': 409,\n",
              " '23223': 410,\n",
              " '23224': 411,\n",
              " '23231': 412,\n",
              " '23232': 413,\n",
              " '23233': 414,\n",
              " '23234': 415,\n",
              " '23241': 416,\n",
              " '23242': 417,\n",
              " '23243': 418,\n",
              " '23244': 419,\n",
              " '23311': 420,\n",
              " '23312': 421,\n",
              " '23313': 422,\n",
              " '23314': 423,\n",
              " '23321': 424,\n",
              " '23322': 425,\n",
              " '23323': 426,\n",
              " '23324': 427,\n",
              " '23331': 428,\n",
              " '23332': 429,\n",
              " '23333': 430,\n",
              " '23334': 431,\n",
              " '23341': 432,\n",
              " '23342': 433,\n",
              " '23343': 434,\n",
              " '23344': 435,\n",
              " '23411': 436,\n",
              " '23412': 437,\n",
              " '23413': 438,\n",
              " '23414': 439,\n",
              " '23421': 440,\n",
              " '23422': 441,\n",
              " '23423': 442,\n",
              " '23424': 443,\n",
              " '23431': 444,\n",
              " '23432': 445,\n",
              " '23433': 446,\n",
              " '23434': 447,\n",
              " '23441': 448,\n",
              " '23442': 449,\n",
              " '23443': 450,\n",
              " '23444': 451,\n",
              " '24111': 452,\n",
              " '24112': 453,\n",
              " '24113': 454,\n",
              " '24114': 455,\n",
              " '24121': 456,\n",
              " '24122': 457,\n",
              " '24123': 458,\n",
              " '24124': 459,\n",
              " '24131': 460,\n",
              " '24132': 461,\n",
              " '24133': 462,\n",
              " '24134': 463,\n",
              " '24141': 464,\n",
              " '24142': 465,\n",
              " '24143': 466,\n",
              " '24144': 467,\n",
              " '24211': 468,\n",
              " '24212': 469,\n",
              " '24213': 470,\n",
              " '24214': 471,\n",
              " '24221': 472,\n",
              " '24222': 473,\n",
              " '24223': 474,\n",
              " '24224': 475,\n",
              " '24231': 476,\n",
              " '24232': 477,\n",
              " '24233': 478,\n",
              " '24234': 479,\n",
              " '24241': 480,\n",
              " '24242': 481,\n",
              " '24243': 482,\n",
              " '24244': 483,\n",
              " '24311': 484,\n",
              " '24312': 485,\n",
              " '24313': 486,\n",
              " '24314': 487,\n",
              " '24321': 488,\n",
              " '24322': 489,\n",
              " '24323': 490,\n",
              " '24324': 491,\n",
              " '24331': 492,\n",
              " '24332': 493,\n",
              " '24333': 494,\n",
              " '24334': 495,\n",
              " '24341': 496,\n",
              " '24342': 497,\n",
              " '24343': 498,\n",
              " '24344': 499,\n",
              " '24411': 500,\n",
              " '24412': 501,\n",
              " '24413': 502,\n",
              " '24414': 503,\n",
              " '24421': 504,\n",
              " '24422': 505,\n",
              " '24423': 506,\n",
              " '24424': 507,\n",
              " '24431': 508,\n",
              " '24432': 509,\n",
              " '24433': 510,\n",
              " '24434': 511,\n",
              " '24441': 512,\n",
              " '24442': 513,\n",
              " '24443': 514,\n",
              " '24444': 515,\n",
              " '31111': 516,\n",
              " '31112': 517,\n",
              " '31113': 518,\n",
              " '31114': 519,\n",
              " '31121': 520,\n",
              " '31122': 521,\n",
              " '31123': 522,\n",
              " '31124': 523,\n",
              " '31131': 524,\n",
              " '31132': 525,\n",
              " '31133': 526,\n",
              " '31134': 527,\n",
              " '31141': 528,\n",
              " '31142': 529,\n",
              " '31143': 530,\n",
              " '31144': 531,\n",
              " '31211': 532,\n",
              " '31212': 533,\n",
              " '31213': 534,\n",
              " '31214': 535,\n",
              " '31221': 536,\n",
              " '31222': 537,\n",
              " '31223': 538,\n",
              " '31224': 539,\n",
              " '31231': 540,\n",
              " '31232': 541,\n",
              " '31233': 542,\n",
              " '31234': 543,\n",
              " '31241': 544,\n",
              " '31242': 545,\n",
              " '31243': 546,\n",
              " '31244': 547,\n",
              " '31311': 548,\n",
              " '31312': 549,\n",
              " '31313': 550,\n",
              " '31314': 551,\n",
              " '31321': 552,\n",
              " '31322': 553,\n",
              " '31323': 554,\n",
              " '31324': 555,\n",
              " '31331': 556,\n",
              " '31332': 557,\n",
              " '31333': 558,\n",
              " '31334': 559,\n",
              " '31341': 560,\n",
              " '31342': 561,\n",
              " '31343': 562,\n",
              " '31344': 563,\n",
              " '31411': 564,\n",
              " '31412': 565,\n",
              " '31413': 566,\n",
              " '31414': 567,\n",
              " '31421': 568,\n",
              " '31422': 569,\n",
              " '31423': 570,\n",
              " '31424': 571,\n",
              " '31431': 572,\n",
              " '31432': 573,\n",
              " '31433': 574,\n",
              " '31434': 575,\n",
              " '31441': 576,\n",
              " '31442': 577,\n",
              " '31443': 578,\n",
              " '31444': 579,\n",
              " '32111': 580,\n",
              " '32112': 581,\n",
              " '32113': 582,\n",
              " '32114': 583,\n",
              " '32121': 584,\n",
              " '32122': 585,\n",
              " '32123': 586,\n",
              " '32124': 587,\n",
              " '32131': 588,\n",
              " '32132': 589,\n",
              " '32133': 590,\n",
              " '32134': 591,\n",
              " '32141': 592,\n",
              " '32142': 593,\n",
              " '32143': 594,\n",
              " '32144': 595,\n",
              " '32211': 596,\n",
              " '32212': 597,\n",
              " '32213': 598,\n",
              " '32214': 599,\n",
              " '32221': 600,\n",
              " '32222': 601,\n",
              " '32223': 602,\n",
              " '32224': 603,\n",
              " '32231': 604,\n",
              " '32232': 605,\n",
              " '32233': 606,\n",
              " '32234': 607,\n",
              " '32241': 608,\n",
              " '32242': 609,\n",
              " '32243': 610,\n",
              " '32244': 611,\n",
              " '32311': 612,\n",
              " '32312': 613,\n",
              " '32313': 614,\n",
              " '32314': 615,\n",
              " '32321': 616,\n",
              " '32322': 617,\n",
              " '32323': 618,\n",
              " '32324': 619,\n",
              " '32331': 620,\n",
              " '32332': 621,\n",
              " '32333': 622,\n",
              " '32334': 623,\n",
              " '32341': 624,\n",
              " '32342': 625,\n",
              " '32343': 626,\n",
              " '32344': 627,\n",
              " '32411': 628,\n",
              " '32412': 629,\n",
              " '32413': 630,\n",
              " '32414': 631,\n",
              " '32421': 632,\n",
              " '32422': 633,\n",
              " '32423': 634,\n",
              " '32424': 635,\n",
              " '32431': 636,\n",
              " '32432': 637,\n",
              " '32433': 638,\n",
              " '32434': 639,\n",
              " '32441': 640,\n",
              " '32442': 641,\n",
              " '32443': 642,\n",
              " '32444': 643,\n",
              " '33111': 644,\n",
              " '33112': 645,\n",
              " '33113': 646,\n",
              " '33114': 647,\n",
              " '33121': 648,\n",
              " '33122': 649,\n",
              " '33123': 650,\n",
              " '33124': 651,\n",
              " '33131': 652,\n",
              " '33132': 653,\n",
              " '33133': 654,\n",
              " '33134': 655,\n",
              " '33141': 656,\n",
              " '33142': 657,\n",
              " '33143': 658,\n",
              " '33144': 659,\n",
              " '33211': 660,\n",
              " '33212': 661,\n",
              " '33213': 662,\n",
              " '33214': 663,\n",
              " '33221': 664,\n",
              " '33222': 665,\n",
              " '33223': 666,\n",
              " '33224': 667,\n",
              " '33231': 668,\n",
              " '33232': 669,\n",
              " '33233': 670,\n",
              " '33234': 671,\n",
              " '33241': 672,\n",
              " '33242': 673,\n",
              " '33243': 674,\n",
              " '33244': 675,\n",
              " '33311': 676,\n",
              " '33312': 677,\n",
              " '33313': 678,\n",
              " '33314': 679,\n",
              " '33321': 680,\n",
              " '33322': 681,\n",
              " '33323': 682,\n",
              " '33324': 683,\n",
              " '33331': 684,\n",
              " '33332': 685,\n",
              " '33333': 686,\n",
              " '33334': 687,\n",
              " '33341': 688,\n",
              " '33342': 689,\n",
              " '33343': 690,\n",
              " '33344': 691,\n",
              " '33411': 692,\n",
              " '33412': 693,\n",
              " '33413': 694,\n",
              " '33414': 695,\n",
              " '33421': 696,\n",
              " '33422': 697,\n",
              " '33423': 698,\n",
              " '33424': 699,\n",
              " '33431': 700,\n",
              " '33432': 701,\n",
              " '33433': 702,\n",
              " '33434': 703,\n",
              " '33441': 704,\n",
              " '33442': 705,\n",
              " '33443': 706,\n",
              " '33444': 707,\n",
              " '34111': 708,\n",
              " '34112': 709,\n",
              " '34113': 710,\n",
              " '34114': 711,\n",
              " '34121': 712,\n",
              " '34122': 713,\n",
              " '34123': 714,\n",
              " '34124': 715,\n",
              " '34131': 716,\n",
              " '34132': 717,\n",
              " '34133': 718,\n",
              " '34134': 719,\n",
              " '34141': 720,\n",
              " '34142': 721,\n",
              " '34143': 722,\n",
              " '34144': 723,\n",
              " '34211': 724,\n",
              " '34212': 725,\n",
              " '34213': 726,\n",
              " '34214': 727,\n",
              " '34221': 728,\n",
              " '34222': 729,\n",
              " '34223': 730,\n",
              " '34224': 731,\n",
              " '34231': 732,\n",
              " '34232': 733,\n",
              " '34233': 734,\n",
              " '34234': 735,\n",
              " '34241': 736,\n",
              " '34242': 737,\n",
              " '34243': 738,\n",
              " '34244': 739,\n",
              " '34311': 740,\n",
              " '34312': 741,\n",
              " '34313': 742,\n",
              " '34314': 743,\n",
              " '34321': 744,\n",
              " '34322': 745,\n",
              " '34323': 746,\n",
              " '34324': 747,\n",
              " '34331': 748,\n",
              " '34332': 749,\n",
              " '34333': 750,\n",
              " '34334': 751,\n",
              " '34341': 752,\n",
              " '34342': 753,\n",
              " '34343': 754,\n",
              " '34344': 755,\n",
              " '34411': 756,\n",
              " '34412': 757,\n",
              " '34413': 758,\n",
              " '34414': 759,\n",
              " '34421': 760,\n",
              " '34422': 761,\n",
              " '34423': 762,\n",
              " '34424': 763,\n",
              " '34431': 764,\n",
              " '34432': 765,\n",
              " '34433': 766,\n",
              " '34434': 767,\n",
              " '34441': 768,\n",
              " '34442': 769,\n",
              " '34443': 770,\n",
              " '34444': 771,\n",
              " '41111': 772,\n",
              " '41112': 773,\n",
              " '41113': 774,\n",
              " '41114': 775,\n",
              " '41121': 776,\n",
              " '41122': 777,\n",
              " '41123': 778,\n",
              " '41124': 779,\n",
              " '41131': 780,\n",
              " '41132': 781,\n",
              " '41133': 782,\n",
              " '41134': 783,\n",
              " '41141': 784,\n",
              " '41142': 785,\n",
              " '41143': 786,\n",
              " '41144': 787,\n",
              " '41211': 788,\n",
              " '41212': 789,\n",
              " '41213': 790,\n",
              " '41214': 791,\n",
              " '41221': 792,\n",
              " '41222': 793,\n",
              " '41223': 794,\n",
              " '41224': 795,\n",
              " '41231': 796,\n",
              " '41232': 797,\n",
              " '41233': 798,\n",
              " '41234': 799,\n",
              " '41241': 800,\n",
              " '41242': 801,\n",
              " '41243': 802,\n",
              " '41244': 803,\n",
              " '41311': 804,\n",
              " '41312': 805,\n",
              " '41313': 806,\n",
              " '41314': 807,\n",
              " '41321': 808,\n",
              " '41322': 809,\n",
              " '41323': 810,\n",
              " '41324': 811,\n",
              " '41331': 812,\n",
              " '41332': 813,\n",
              " '41333': 814,\n",
              " '41334': 815,\n",
              " '41341': 816,\n",
              " '41342': 817,\n",
              " '41343': 818,\n",
              " '41344': 819,\n",
              " '41411': 820,\n",
              " '41412': 821,\n",
              " '41413': 822,\n",
              " '41414': 823,\n",
              " '41421': 824,\n",
              " '41422': 825,\n",
              " '41423': 826,\n",
              " '41424': 827,\n",
              " '41431': 828,\n",
              " '41432': 829,\n",
              " '41433': 830,\n",
              " '41434': 831,\n",
              " '41441': 832,\n",
              " '41442': 833,\n",
              " '41443': 834,\n",
              " '41444': 835,\n",
              " '42111': 836,\n",
              " '42112': 837,\n",
              " '42113': 838,\n",
              " '42114': 839,\n",
              " '42121': 840,\n",
              " '42122': 841,\n",
              " '42123': 842,\n",
              " '42124': 843,\n",
              " '42131': 844,\n",
              " '42132': 845,\n",
              " '42133': 846,\n",
              " '42134': 847,\n",
              " '42141': 848,\n",
              " '42142': 849,\n",
              " '42143': 850,\n",
              " '42144': 851,\n",
              " '42211': 852,\n",
              " '42212': 853,\n",
              " '42213': 854,\n",
              " '42214': 855,\n",
              " '42221': 856,\n",
              " '42222': 857,\n",
              " '42223': 858,\n",
              " '42224': 859,\n",
              " '42231': 860,\n",
              " '42232': 861,\n",
              " '42233': 862,\n",
              " '42234': 863,\n",
              " '42241': 864,\n",
              " '42242': 865,\n",
              " '42243': 866,\n",
              " '42244': 867,\n",
              " '42311': 868,\n",
              " '42312': 869,\n",
              " '42313': 870,\n",
              " '42314': 871,\n",
              " '42321': 872,\n",
              " '42322': 873,\n",
              " '42323': 874,\n",
              " '42324': 875,\n",
              " '42331': 876,\n",
              " '42332': 877,\n",
              " '42333': 878,\n",
              " '42334': 879,\n",
              " '42341': 880,\n",
              " '42342': 881,\n",
              " '42343': 882,\n",
              " '42344': 883,\n",
              " '42411': 884,\n",
              " '42412': 885,\n",
              " '42413': 886,\n",
              " '42414': 887,\n",
              " '42421': 888,\n",
              " '42422': 889,\n",
              " '42423': 890,\n",
              " '42424': 891,\n",
              " '42431': 892,\n",
              " '42432': 893,\n",
              " '42433': 894,\n",
              " '42434': 895,\n",
              " '42441': 896,\n",
              " '42442': 897,\n",
              " '42443': 898,\n",
              " '42444': 899,\n",
              " '43111': 900,\n",
              " '43112': 901,\n",
              " '43113': 902,\n",
              " '43114': 903,\n",
              " '43121': 904,\n",
              " '43122': 905,\n",
              " '43123': 906,\n",
              " '43124': 907,\n",
              " '43131': 908,\n",
              " '43132': 909,\n",
              " '43133': 910,\n",
              " '43134': 911,\n",
              " '43141': 912,\n",
              " '43142': 913,\n",
              " '43143': 914,\n",
              " '43144': 915,\n",
              " '43211': 916,\n",
              " '43212': 917,\n",
              " '43213': 918,\n",
              " '43214': 919,\n",
              " '43221': 920,\n",
              " '43222': 921,\n",
              " '43223': 922,\n",
              " '43224': 923,\n",
              " '43231': 924,\n",
              " '43232': 925,\n",
              " '43233': 926,\n",
              " '43234': 927,\n",
              " '43241': 928,\n",
              " '43242': 929,\n",
              " '43243': 930,\n",
              " '43244': 931,\n",
              " '43311': 932,\n",
              " '43312': 933,\n",
              " '43313': 934,\n",
              " '43314': 935,\n",
              " '43321': 936,\n",
              " '43322': 937,\n",
              " '43323': 938,\n",
              " '43324': 939,\n",
              " '43331': 940,\n",
              " '43332': 941,\n",
              " '43333': 942,\n",
              " '43334': 943,\n",
              " '43341': 944,\n",
              " '43342': 945,\n",
              " '43343': 946,\n",
              " '43344': 947,\n",
              " '43411': 948,\n",
              " '43412': 949,\n",
              " '43413': 950,\n",
              " '43414': 951,\n",
              " '43421': 952,\n",
              " '43422': 953,\n",
              " '43423': 954,\n",
              " '43424': 955,\n",
              " '43431': 956,\n",
              " '43432': 957,\n",
              " '43433': 958,\n",
              " '43434': 959,\n",
              " '43441': 960,\n",
              " '43442': 961,\n",
              " '43443': 962,\n",
              " '43444': 963,\n",
              " '44111': 964,\n",
              " '44112': 965,\n",
              " '44113': 966,\n",
              " '44114': 967,\n",
              " '44121': 968,\n",
              " '44122': 969,\n",
              " '44123': 970,\n",
              " '44124': 971,\n",
              " '44131': 972,\n",
              " '44132': 973,\n",
              " '44133': 974,\n",
              " '44134': 975,\n",
              " '44141': 976,\n",
              " '44142': 977,\n",
              " '44143': 978,\n",
              " '44144': 979,\n",
              " '44211': 980,\n",
              " '44212': 981,\n",
              " '44213': 982,\n",
              " '44214': 983,\n",
              " '44221': 984,\n",
              " '44222': 985,\n",
              " '44223': 986,\n",
              " '44224': 987,\n",
              " '44231': 988,\n",
              " '44232': 989,\n",
              " '44233': 990,\n",
              " '44234': 991,\n",
              " '44241': 992,\n",
              " '44242': 993,\n",
              " '44243': 994,\n",
              " '44244': 995,\n",
              " '44311': 996,\n",
              " '44312': 997,\n",
              " '44313': 998,\n",
              " '44314': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=4)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "sixMer_dict = {**a_dict, **new_mer_dict}\n",
        "sixMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz-vfmDKexl0",
        "outputId": "270788ed-e09a-47b0-80cb-a95df8cfab7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[CLS]': 1,\n",
              " '[SEP]': 2,\n",
              " '[MASK]': 3,\n",
              " '111111': 4,\n",
              " '111112': 5,\n",
              " '111113': 6,\n",
              " '111114': 7,\n",
              " '111121': 8,\n",
              " '111122': 9,\n",
              " '111123': 10,\n",
              " '111124': 11,\n",
              " '111131': 12,\n",
              " '111132': 13,\n",
              " '111133': 14,\n",
              " '111134': 15,\n",
              " '111141': 16,\n",
              " '111142': 17,\n",
              " '111143': 18,\n",
              " '111144': 19,\n",
              " '111211': 20,\n",
              " '111212': 21,\n",
              " '111213': 22,\n",
              " '111214': 23,\n",
              " '111221': 24,\n",
              " '111222': 25,\n",
              " '111223': 26,\n",
              " '111224': 27,\n",
              " '111231': 28,\n",
              " '111232': 29,\n",
              " '111233': 30,\n",
              " '111234': 31,\n",
              " '111241': 32,\n",
              " '111242': 33,\n",
              " '111243': 34,\n",
              " '111244': 35,\n",
              " '111311': 36,\n",
              " '111312': 37,\n",
              " '111313': 38,\n",
              " '111314': 39,\n",
              " '111321': 40,\n",
              " '111322': 41,\n",
              " '111323': 42,\n",
              " '111324': 43,\n",
              " '111331': 44,\n",
              " '111332': 45,\n",
              " '111333': 46,\n",
              " '111334': 47,\n",
              " '111341': 48,\n",
              " '111342': 49,\n",
              " '111343': 50,\n",
              " '111344': 51,\n",
              " '111411': 52,\n",
              " '111412': 53,\n",
              " '111413': 54,\n",
              " '111414': 55,\n",
              " '111421': 56,\n",
              " '111422': 57,\n",
              " '111423': 58,\n",
              " '111424': 59,\n",
              " '111431': 60,\n",
              " '111432': 61,\n",
              " '111433': 62,\n",
              " '111434': 63,\n",
              " '111441': 64,\n",
              " '111442': 65,\n",
              " '111443': 66,\n",
              " '111444': 67,\n",
              " '112111': 68,\n",
              " '112112': 69,\n",
              " '112113': 70,\n",
              " '112114': 71,\n",
              " '112121': 72,\n",
              " '112122': 73,\n",
              " '112123': 74,\n",
              " '112124': 75,\n",
              " '112131': 76,\n",
              " '112132': 77,\n",
              " '112133': 78,\n",
              " '112134': 79,\n",
              " '112141': 80,\n",
              " '112142': 81,\n",
              " '112143': 82,\n",
              " '112144': 83,\n",
              " '112211': 84,\n",
              " '112212': 85,\n",
              " '112213': 86,\n",
              " '112214': 87,\n",
              " '112221': 88,\n",
              " '112222': 89,\n",
              " '112223': 90,\n",
              " '112224': 91,\n",
              " '112231': 92,\n",
              " '112232': 93,\n",
              " '112233': 94,\n",
              " '112234': 95,\n",
              " '112241': 96,\n",
              " '112242': 97,\n",
              " '112243': 98,\n",
              " '112244': 99,\n",
              " '112311': 100,\n",
              " '112312': 101,\n",
              " '112313': 102,\n",
              " '112314': 103,\n",
              " '112321': 104,\n",
              " '112322': 105,\n",
              " '112323': 106,\n",
              " '112324': 107,\n",
              " '112331': 108,\n",
              " '112332': 109,\n",
              " '112333': 110,\n",
              " '112334': 111,\n",
              " '112341': 112,\n",
              " '112342': 113,\n",
              " '112343': 114,\n",
              " '112344': 115,\n",
              " '112411': 116,\n",
              " '112412': 117,\n",
              " '112413': 118,\n",
              " '112414': 119,\n",
              " '112421': 120,\n",
              " '112422': 121,\n",
              " '112423': 122,\n",
              " '112424': 123,\n",
              " '112431': 124,\n",
              " '112432': 125,\n",
              " '112433': 126,\n",
              " '112434': 127,\n",
              " '112441': 128,\n",
              " '112442': 129,\n",
              " '112443': 130,\n",
              " '112444': 131,\n",
              " '113111': 132,\n",
              " '113112': 133,\n",
              " '113113': 134,\n",
              " '113114': 135,\n",
              " '113121': 136,\n",
              " '113122': 137,\n",
              " '113123': 138,\n",
              " '113124': 139,\n",
              " '113131': 140,\n",
              " '113132': 141,\n",
              " '113133': 142,\n",
              " '113134': 143,\n",
              " '113141': 144,\n",
              " '113142': 145,\n",
              " '113143': 146,\n",
              " '113144': 147,\n",
              " '113211': 148,\n",
              " '113212': 149,\n",
              " '113213': 150,\n",
              " '113214': 151,\n",
              " '113221': 152,\n",
              " '113222': 153,\n",
              " '113223': 154,\n",
              " '113224': 155,\n",
              " '113231': 156,\n",
              " '113232': 157,\n",
              " '113233': 158,\n",
              " '113234': 159,\n",
              " '113241': 160,\n",
              " '113242': 161,\n",
              " '113243': 162,\n",
              " '113244': 163,\n",
              " '113311': 164,\n",
              " '113312': 165,\n",
              " '113313': 166,\n",
              " '113314': 167,\n",
              " '113321': 168,\n",
              " '113322': 169,\n",
              " '113323': 170,\n",
              " '113324': 171,\n",
              " '113331': 172,\n",
              " '113332': 173,\n",
              " '113333': 174,\n",
              " '113334': 175,\n",
              " '113341': 176,\n",
              " '113342': 177,\n",
              " '113343': 178,\n",
              " '113344': 179,\n",
              " '113411': 180,\n",
              " '113412': 181,\n",
              " '113413': 182,\n",
              " '113414': 183,\n",
              " '113421': 184,\n",
              " '113422': 185,\n",
              " '113423': 186,\n",
              " '113424': 187,\n",
              " '113431': 188,\n",
              " '113432': 189,\n",
              " '113433': 190,\n",
              " '113434': 191,\n",
              " '113441': 192,\n",
              " '113442': 193,\n",
              " '113443': 194,\n",
              " '113444': 195,\n",
              " '114111': 196,\n",
              " '114112': 197,\n",
              " '114113': 198,\n",
              " '114114': 199,\n",
              " '114121': 200,\n",
              " '114122': 201,\n",
              " '114123': 202,\n",
              " '114124': 203,\n",
              " '114131': 204,\n",
              " '114132': 205,\n",
              " '114133': 206,\n",
              " '114134': 207,\n",
              " '114141': 208,\n",
              " '114142': 209,\n",
              " '114143': 210,\n",
              " '114144': 211,\n",
              " '114211': 212,\n",
              " '114212': 213,\n",
              " '114213': 214,\n",
              " '114214': 215,\n",
              " '114221': 216,\n",
              " '114222': 217,\n",
              " '114223': 218,\n",
              " '114224': 219,\n",
              " '114231': 220,\n",
              " '114232': 221,\n",
              " '114233': 222,\n",
              " '114234': 223,\n",
              " '114241': 224,\n",
              " '114242': 225,\n",
              " '114243': 226,\n",
              " '114244': 227,\n",
              " '114311': 228,\n",
              " '114312': 229,\n",
              " '114313': 230,\n",
              " '114314': 231,\n",
              " '114321': 232,\n",
              " '114322': 233,\n",
              " '114323': 234,\n",
              " '114324': 235,\n",
              " '114331': 236,\n",
              " '114332': 237,\n",
              " '114333': 238,\n",
              " '114334': 239,\n",
              " '114341': 240,\n",
              " '114342': 241,\n",
              " '114343': 242,\n",
              " '114344': 243,\n",
              " '114411': 244,\n",
              " '114412': 245,\n",
              " '114413': 246,\n",
              " '114414': 247,\n",
              " '114421': 248,\n",
              " '114422': 249,\n",
              " '114423': 250,\n",
              " '114424': 251,\n",
              " '114431': 252,\n",
              " '114432': 253,\n",
              " '114433': 254,\n",
              " '114434': 255,\n",
              " '114441': 256,\n",
              " '114442': 257,\n",
              " '114443': 258,\n",
              " '114444': 259,\n",
              " '121111': 260,\n",
              " '121112': 261,\n",
              " '121113': 262,\n",
              " '121114': 263,\n",
              " '121121': 264,\n",
              " '121122': 265,\n",
              " '121123': 266,\n",
              " '121124': 267,\n",
              " '121131': 268,\n",
              " '121132': 269,\n",
              " '121133': 270,\n",
              " '121134': 271,\n",
              " '121141': 272,\n",
              " '121142': 273,\n",
              " '121143': 274,\n",
              " '121144': 275,\n",
              " '121211': 276,\n",
              " '121212': 277,\n",
              " '121213': 278,\n",
              " '121214': 279,\n",
              " '121221': 280,\n",
              " '121222': 281,\n",
              " '121223': 282,\n",
              " '121224': 283,\n",
              " '121231': 284,\n",
              " '121232': 285,\n",
              " '121233': 286,\n",
              " '121234': 287,\n",
              " '121241': 288,\n",
              " '121242': 289,\n",
              " '121243': 290,\n",
              " '121244': 291,\n",
              " '121311': 292,\n",
              " '121312': 293,\n",
              " '121313': 294,\n",
              " '121314': 295,\n",
              " '121321': 296,\n",
              " '121322': 297,\n",
              " '121323': 298,\n",
              " '121324': 299,\n",
              " '121331': 300,\n",
              " '121332': 301,\n",
              " '121333': 302,\n",
              " '121334': 303,\n",
              " '121341': 304,\n",
              " '121342': 305,\n",
              " '121343': 306,\n",
              " '121344': 307,\n",
              " '121411': 308,\n",
              " '121412': 309,\n",
              " '121413': 310,\n",
              " '121414': 311,\n",
              " '121421': 312,\n",
              " '121422': 313,\n",
              " '121423': 314,\n",
              " '121424': 315,\n",
              " '121431': 316,\n",
              " '121432': 317,\n",
              " '121433': 318,\n",
              " '121434': 319,\n",
              " '121441': 320,\n",
              " '121442': 321,\n",
              " '121443': 322,\n",
              " '121444': 323,\n",
              " '122111': 324,\n",
              " '122112': 325,\n",
              " '122113': 326,\n",
              " '122114': 327,\n",
              " '122121': 328,\n",
              " '122122': 329,\n",
              " '122123': 330,\n",
              " '122124': 331,\n",
              " '122131': 332,\n",
              " '122132': 333,\n",
              " '122133': 334,\n",
              " '122134': 335,\n",
              " '122141': 336,\n",
              " '122142': 337,\n",
              " '122143': 338,\n",
              " '122144': 339,\n",
              " '122211': 340,\n",
              " '122212': 341,\n",
              " '122213': 342,\n",
              " '122214': 343,\n",
              " '122221': 344,\n",
              " '122222': 345,\n",
              " '122223': 346,\n",
              " '122224': 347,\n",
              " '122231': 348,\n",
              " '122232': 349,\n",
              " '122233': 350,\n",
              " '122234': 351,\n",
              " '122241': 352,\n",
              " '122242': 353,\n",
              " '122243': 354,\n",
              " '122244': 355,\n",
              " '122311': 356,\n",
              " '122312': 357,\n",
              " '122313': 358,\n",
              " '122314': 359,\n",
              " '122321': 360,\n",
              " '122322': 361,\n",
              " '122323': 362,\n",
              " '122324': 363,\n",
              " '122331': 364,\n",
              " '122332': 365,\n",
              " '122333': 366,\n",
              " '122334': 367,\n",
              " '122341': 368,\n",
              " '122342': 369,\n",
              " '122343': 370,\n",
              " '122344': 371,\n",
              " '122411': 372,\n",
              " '122412': 373,\n",
              " '122413': 374,\n",
              " '122414': 375,\n",
              " '122421': 376,\n",
              " '122422': 377,\n",
              " '122423': 378,\n",
              " '122424': 379,\n",
              " '122431': 380,\n",
              " '122432': 381,\n",
              " '122433': 382,\n",
              " '122434': 383,\n",
              " '122441': 384,\n",
              " '122442': 385,\n",
              " '122443': 386,\n",
              " '122444': 387,\n",
              " '123111': 388,\n",
              " '123112': 389,\n",
              " '123113': 390,\n",
              " '123114': 391,\n",
              " '123121': 392,\n",
              " '123122': 393,\n",
              " '123123': 394,\n",
              " '123124': 395,\n",
              " '123131': 396,\n",
              " '123132': 397,\n",
              " '123133': 398,\n",
              " '123134': 399,\n",
              " '123141': 400,\n",
              " '123142': 401,\n",
              " '123143': 402,\n",
              " '123144': 403,\n",
              " '123211': 404,\n",
              " '123212': 405,\n",
              " '123213': 406,\n",
              " '123214': 407,\n",
              " '123221': 408,\n",
              " '123222': 409,\n",
              " '123223': 410,\n",
              " '123224': 411,\n",
              " '123231': 412,\n",
              " '123232': 413,\n",
              " '123233': 414,\n",
              " '123234': 415,\n",
              " '123241': 416,\n",
              " '123242': 417,\n",
              " '123243': 418,\n",
              " '123244': 419,\n",
              " '123311': 420,\n",
              " '123312': 421,\n",
              " '123313': 422,\n",
              " '123314': 423,\n",
              " '123321': 424,\n",
              " '123322': 425,\n",
              " '123323': 426,\n",
              " '123324': 427,\n",
              " '123331': 428,\n",
              " '123332': 429,\n",
              " '123333': 430,\n",
              " '123334': 431,\n",
              " '123341': 432,\n",
              " '123342': 433,\n",
              " '123343': 434,\n",
              " '123344': 435,\n",
              " '123411': 436,\n",
              " '123412': 437,\n",
              " '123413': 438,\n",
              " '123414': 439,\n",
              " '123421': 440,\n",
              " '123422': 441,\n",
              " '123423': 442,\n",
              " '123424': 443,\n",
              " '123431': 444,\n",
              " '123432': 445,\n",
              " '123433': 446,\n",
              " '123434': 447,\n",
              " '123441': 448,\n",
              " '123442': 449,\n",
              " '123443': 450,\n",
              " '123444': 451,\n",
              " '124111': 452,\n",
              " '124112': 453,\n",
              " '124113': 454,\n",
              " '124114': 455,\n",
              " '124121': 456,\n",
              " '124122': 457,\n",
              " '124123': 458,\n",
              " '124124': 459,\n",
              " '124131': 460,\n",
              " '124132': 461,\n",
              " '124133': 462,\n",
              " '124134': 463,\n",
              " '124141': 464,\n",
              " '124142': 465,\n",
              " '124143': 466,\n",
              " '124144': 467,\n",
              " '124211': 468,\n",
              " '124212': 469,\n",
              " '124213': 470,\n",
              " '124214': 471,\n",
              " '124221': 472,\n",
              " '124222': 473,\n",
              " '124223': 474,\n",
              " '124224': 475,\n",
              " '124231': 476,\n",
              " '124232': 477,\n",
              " '124233': 478,\n",
              " '124234': 479,\n",
              " '124241': 480,\n",
              " '124242': 481,\n",
              " '124243': 482,\n",
              " '124244': 483,\n",
              " '124311': 484,\n",
              " '124312': 485,\n",
              " '124313': 486,\n",
              " '124314': 487,\n",
              " '124321': 488,\n",
              " '124322': 489,\n",
              " '124323': 490,\n",
              " '124324': 491,\n",
              " '124331': 492,\n",
              " '124332': 493,\n",
              " '124333': 494,\n",
              " '124334': 495,\n",
              " '124341': 496,\n",
              " '124342': 497,\n",
              " '124343': 498,\n",
              " '124344': 499,\n",
              " '124411': 500,\n",
              " '124412': 501,\n",
              " '124413': 502,\n",
              " '124414': 503,\n",
              " '124421': 504,\n",
              " '124422': 505,\n",
              " '124423': 506,\n",
              " '124424': 507,\n",
              " '124431': 508,\n",
              " '124432': 509,\n",
              " '124433': 510,\n",
              " '124434': 511,\n",
              " '124441': 512,\n",
              " '124442': 513,\n",
              " '124443': 514,\n",
              " '124444': 515,\n",
              " '131111': 516,\n",
              " '131112': 517,\n",
              " '131113': 518,\n",
              " '131114': 519,\n",
              " '131121': 520,\n",
              " '131122': 521,\n",
              " '131123': 522,\n",
              " '131124': 523,\n",
              " '131131': 524,\n",
              " '131132': 525,\n",
              " '131133': 526,\n",
              " '131134': 527,\n",
              " '131141': 528,\n",
              " '131142': 529,\n",
              " '131143': 530,\n",
              " '131144': 531,\n",
              " '131211': 532,\n",
              " '131212': 533,\n",
              " '131213': 534,\n",
              " '131214': 535,\n",
              " '131221': 536,\n",
              " '131222': 537,\n",
              " '131223': 538,\n",
              " '131224': 539,\n",
              " '131231': 540,\n",
              " '131232': 541,\n",
              " '131233': 542,\n",
              " '131234': 543,\n",
              " '131241': 544,\n",
              " '131242': 545,\n",
              " '131243': 546,\n",
              " '131244': 547,\n",
              " '131311': 548,\n",
              " '131312': 549,\n",
              " '131313': 550,\n",
              " '131314': 551,\n",
              " '131321': 552,\n",
              " '131322': 553,\n",
              " '131323': 554,\n",
              " '131324': 555,\n",
              " '131331': 556,\n",
              " '131332': 557,\n",
              " '131333': 558,\n",
              " '131334': 559,\n",
              " '131341': 560,\n",
              " '131342': 561,\n",
              " '131343': 562,\n",
              " '131344': 563,\n",
              " '131411': 564,\n",
              " '131412': 565,\n",
              " '131413': 566,\n",
              " '131414': 567,\n",
              " '131421': 568,\n",
              " '131422': 569,\n",
              " '131423': 570,\n",
              " '131424': 571,\n",
              " '131431': 572,\n",
              " '131432': 573,\n",
              " '131433': 574,\n",
              " '131434': 575,\n",
              " '131441': 576,\n",
              " '131442': 577,\n",
              " '131443': 578,\n",
              " '131444': 579,\n",
              " '132111': 580,\n",
              " '132112': 581,\n",
              " '132113': 582,\n",
              " '132114': 583,\n",
              " '132121': 584,\n",
              " '132122': 585,\n",
              " '132123': 586,\n",
              " '132124': 587,\n",
              " '132131': 588,\n",
              " '132132': 589,\n",
              " '132133': 590,\n",
              " '132134': 591,\n",
              " '132141': 592,\n",
              " '132142': 593,\n",
              " '132143': 594,\n",
              " '132144': 595,\n",
              " '132211': 596,\n",
              " '132212': 597,\n",
              " '132213': 598,\n",
              " '132214': 599,\n",
              " '132221': 600,\n",
              " '132222': 601,\n",
              " '132223': 602,\n",
              " '132224': 603,\n",
              " '132231': 604,\n",
              " '132232': 605,\n",
              " '132233': 606,\n",
              " '132234': 607,\n",
              " '132241': 608,\n",
              " '132242': 609,\n",
              " '132243': 610,\n",
              " '132244': 611,\n",
              " '132311': 612,\n",
              " '132312': 613,\n",
              " '132313': 614,\n",
              " '132314': 615,\n",
              " '132321': 616,\n",
              " '132322': 617,\n",
              " '132323': 618,\n",
              " '132324': 619,\n",
              " '132331': 620,\n",
              " '132332': 621,\n",
              " '132333': 622,\n",
              " '132334': 623,\n",
              " '132341': 624,\n",
              " '132342': 625,\n",
              " '132343': 626,\n",
              " '132344': 627,\n",
              " '132411': 628,\n",
              " '132412': 629,\n",
              " '132413': 630,\n",
              " '132414': 631,\n",
              " '132421': 632,\n",
              " '132422': 633,\n",
              " '132423': 634,\n",
              " '132424': 635,\n",
              " '132431': 636,\n",
              " '132432': 637,\n",
              " '132433': 638,\n",
              " '132434': 639,\n",
              " '132441': 640,\n",
              " '132442': 641,\n",
              " '132443': 642,\n",
              " '132444': 643,\n",
              " '133111': 644,\n",
              " '133112': 645,\n",
              " '133113': 646,\n",
              " '133114': 647,\n",
              " '133121': 648,\n",
              " '133122': 649,\n",
              " '133123': 650,\n",
              " '133124': 651,\n",
              " '133131': 652,\n",
              " '133132': 653,\n",
              " '133133': 654,\n",
              " '133134': 655,\n",
              " '133141': 656,\n",
              " '133142': 657,\n",
              " '133143': 658,\n",
              " '133144': 659,\n",
              " '133211': 660,\n",
              " '133212': 661,\n",
              " '133213': 662,\n",
              " '133214': 663,\n",
              " '133221': 664,\n",
              " '133222': 665,\n",
              " '133223': 666,\n",
              " '133224': 667,\n",
              " '133231': 668,\n",
              " '133232': 669,\n",
              " '133233': 670,\n",
              " '133234': 671,\n",
              " '133241': 672,\n",
              " '133242': 673,\n",
              " '133243': 674,\n",
              " '133244': 675,\n",
              " '133311': 676,\n",
              " '133312': 677,\n",
              " '133313': 678,\n",
              " '133314': 679,\n",
              " '133321': 680,\n",
              " '133322': 681,\n",
              " '133323': 682,\n",
              " '133324': 683,\n",
              " '133331': 684,\n",
              " '133332': 685,\n",
              " '133333': 686,\n",
              " '133334': 687,\n",
              " '133341': 688,\n",
              " '133342': 689,\n",
              " '133343': 690,\n",
              " '133344': 691,\n",
              " '133411': 692,\n",
              " '133412': 693,\n",
              " '133413': 694,\n",
              " '133414': 695,\n",
              " '133421': 696,\n",
              " '133422': 697,\n",
              " '133423': 698,\n",
              " '133424': 699,\n",
              " '133431': 700,\n",
              " '133432': 701,\n",
              " '133433': 702,\n",
              " '133434': 703,\n",
              " '133441': 704,\n",
              " '133442': 705,\n",
              " '133443': 706,\n",
              " '133444': 707,\n",
              " '134111': 708,\n",
              " '134112': 709,\n",
              " '134113': 710,\n",
              " '134114': 711,\n",
              " '134121': 712,\n",
              " '134122': 713,\n",
              " '134123': 714,\n",
              " '134124': 715,\n",
              " '134131': 716,\n",
              " '134132': 717,\n",
              " '134133': 718,\n",
              " '134134': 719,\n",
              " '134141': 720,\n",
              " '134142': 721,\n",
              " '134143': 722,\n",
              " '134144': 723,\n",
              " '134211': 724,\n",
              " '134212': 725,\n",
              " '134213': 726,\n",
              " '134214': 727,\n",
              " '134221': 728,\n",
              " '134222': 729,\n",
              " '134223': 730,\n",
              " '134224': 731,\n",
              " '134231': 732,\n",
              " '134232': 733,\n",
              " '134233': 734,\n",
              " '134234': 735,\n",
              " '134241': 736,\n",
              " '134242': 737,\n",
              " '134243': 738,\n",
              " '134244': 739,\n",
              " '134311': 740,\n",
              " '134312': 741,\n",
              " '134313': 742,\n",
              " '134314': 743,\n",
              " '134321': 744,\n",
              " '134322': 745,\n",
              " '134323': 746,\n",
              " '134324': 747,\n",
              " '134331': 748,\n",
              " '134332': 749,\n",
              " '134333': 750,\n",
              " '134334': 751,\n",
              " '134341': 752,\n",
              " '134342': 753,\n",
              " '134343': 754,\n",
              " '134344': 755,\n",
              " '134411': 756,\n",
              " '134412': 757,\n",
              " '134413': 758,\n",
              " '134414': 759,\n",
              " '134421': 760,\n",
              " '134422': 761,\n",
              " '134423': 762,\n",
              " '134424': 763,\n",
              " '134431': 764,\n",
              " '134432': 765,\n",
              " '134433': 766,\n",
              " '134434': 767,\n",
              " '134441': 768,\n",
              " '134442': 769,\n",
              " '134443': 770,\n",
              " '134444': 771,\n",
              " '141111': 772,\n",
              " '141112': 773,\n",
              " '141113': 774,\n",
              " '141114': 775,\n",
              " '141121': 776,\n",
              " '141122': 777,\n",
              " '141123': 778,\n",
              " '141124': 779,\n",
              " '141131': 780,\n",
              " '141132': 781,\n",
              " '141133': 782,\n",
              " '141134': 783,\n",
              " '141141': 784,\n",
              " '141142': 785,\n",
              " '141143': 786,\n",
              " '141144': 787,\n",
              " '141211': 788,\n",
              " '141212': 789,\n",
              " '141213': 790,\n",
              " '141214': 791,\n",
              " '141221': 792,\n",
              " '141222': 793,\n",
              " '141223': 794,\n",
              " '141224': 795,\n",
              " '141231': 796,\n",
              " '141232': 797,\n",
              " '141233': 798,\n",
              " '141234': 799,\n",
              " '141241': 800,\n",
              " '141242': 801,\n",
              " '141243': 802,\n",
              " '141244': 803,\n",
              " '141311': 804,\n",
              " '141312': 805,\n",
              " '141313': 806,\n",
              " '141314': 807,\n",
              " '141321': 808,\n",
              " '141322': 809,\n",
              " '141323': 810,\n",
              " '141324': 811,\n",
              " '141331': 812,\n",
              " '141332': 813,\n",
              " '141333': 814,\n",
              " '141334': 815,\n",
              " '141341': 816,\n",
              " '141342': 817,\n",
              " '141343': 818,\n",
              " '141344': 819,\n",
              " '141411': 820,\n",
              " '141412': 821,\n",
              " '141413': 822,\n",
              " '141414': 823,\n",
              " '141421': 824,\n",
              " '141422': 825,\n",
              " '141423': 826,\n",
              " '141424': 827,\n",
              " '141431': 828,\n",
              " '141432': 829,\n",
              " '141433': 830,\n",
              " '141434': 831,\n",
              " '141441': 832,\n",
              " '141442': 833,\n",
              " '141443': 834,\n",
              " '141444': 835,\n",
              " '142111': 836,\n",
              " '142112': 837,\n",
              " '142113': 838,\n",
              " '142114': 839,\n",
              " '142121': 840,\n",
              " '142122': 841,\n",
              " '142123': 842,\n",
              " '142124': 843,\n",
              " '142131': 844,\n",
              " '142132': 845,\n",
              " '142133': 846,\n",
              " '142134': 847,\n",
              " '142141': 848,\n",
              " '142142': 849,\n",
              " '142143': 850,\n",
              " '142144': 851,\n",
              " '142211': 852,\n",
              " '142212': 853,\n",
              " '142213': 854,\n",
              " '142214': 855,\n",
              " '142221': 856,\n",
              " '142222': 857,\n",
              " '142223': 858,\n",
              " '142224': 859,\n",
              " '142231': 860,\n",
              " '142232': 861,\n",
              " '142233': 862,\n",
              " '142234': 863,\n",
              " '142241': 864,\n",
              " '142242': 865,\n",
              " '142243': 866,\n",
              " '142244': 867,\n",
              " '142311': 868,\n",
              " '142312': 869,\n",
              " '142313': 870,\n",
              " '142314': 871,\n",
              " '142321': 872,\n",
              " '142322': 873,\n",
              " '142323': 874,\n",
              " '142324': 875,\n",
              " '142331': 876,\n",
              " '142332': 877,\n",
              " '142333': 878,\n",
              " '142334': 879,\n",
              " '142341': 880,\n",
              " '142342': 881,\n",
              " '142343': 882,\n",
              " '142344': 883,\n",
              " '142411': 884,\n",
              " '142412': 885,\n",
              " '142413': 886,\n",
              " '142414': 887,\n",
              " '142421': 888,\n",
              " '142422': 889,\n",
              " '142423': 890,\n",
              " '142424': 891,\n",
              " '142431': 892,\n",
              " '142432': 893,\n",
              " '142433': 894,\n",
              " '142434': 895,\n",
              " '142441': 896,\n",
              " '142442': 897,\n",
              " '142443': 898,\n",
              " '142444': 899,\n",
              " '143111': 900,\n",
              " '143112': 901,\n",
              " '143113': 902,\n",
              " '143114': 903,\n",
              " '143121': 904,\n",
              " '143122': 905,\n",
              " '143123': 906,\n",
              " '143124': 907,\n",
              " '143131': 908,\n",
              " '143132': 909,\n",
              " '143133': 910,\n",
              " '143134': 911,\n",
              " '143141': 912,\n",
              " '143142': 913,\n",
              " '143143': 914,\n",
              " '143144': 915,\n",
              " '143211': 916,\n",
              " '143212': 917,\n",
              " '143213': 918,\n",
              " '143214': 919,\n",
              " '143221': 920,\n",
              " '143222': 921,\n",
              " '143223': 922,\n",
              " '143224': 923,\n",
              " '143231': 924,\n",
              " '143232': 925,\n",
              " '143233': 926,\n",
              " '143234': 927,\n",
              " '143241': 928,\n",
              " '143242': 929,\n",
              " '143243': 930,\n",
              " '143244': 931,\n",
              " '143311': 932,\n",
              " '143312': 933,\n",
              " '143313': 934,\n",
              " '143314': 935,\n",
              " '143321': 936,\n",
              " '143322': 937,\n",
              " '143323': 938,\n",
              " '143324': 939,\n",
              " '143331': 940,\n",
              " '143332': 941,\n",
              " '143333': 942,\n",
              " '143334': 943,\n",
              " '143341': 944,\n",
              " '143342': 945,\n",
              " '143343': 946,\n",
              " '143344': 947,\n",
              " '143411': 948,\n",
              " '143412': 949,\n",
              " '143413': 950,\n",
              " '143414': 951,\n",
              " '143421': 952,\n",
              " '143422': 953,\n",
              " '143423': 954,\n",
              " '143424': 955,\n",
              " '143431': 956,\n",
              " '143432': 957,\n",
              " '143433': 958,\n",
              " '143434': 959,\n",
              " '143441': 960,\n",
              " '143442': 961,\n",
              " '143443': 962,\n",
              " '143444': 963,\n",
              " '144111': 964,\n",
              " '144112': 965,\n",
              " '144113': 966,\n",
              " '144114': 967,\n",
              " '144121': 968,\n",
              " '144122': 969,\n",
              " '144123': 970,\n",
              " '144124': 971,\n",
              " '144131': 972,\n",
              " '144132': 973,\n",
              " '144133': 974,\n",
              " '144134': 975,\n",
              " '144141': 976,\n",
              " '144142': 977,\n",
              " '144143': 978,\n",
              " '144144': 979,\n",
              " '144211': 980,\n",
              " '144212': 981,\n",
              " '144213': 982,\n",
              " '144214': 983,\n",
              " '144221': 984,\n",
              " '144222': 985,\n",
              " '144223': 986,\n",
              " '144224': 987,\n",
              " '144231': 988,\n",
              " '144232': 989,\n",
              " '144233': 990,\n",
              " '144234': 991,\n",
              " '144241': 992,\n",
              " '144242': 993,\n",
              " '144243': 994,\n",
              " '144244': 995,\n",
              " '144311': 996,\n",
              " '144312': 997,\n",
              " '144313': 998,\n",
              " '144314': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mer_dict=make_dict(n=5)\n",
        "\n",
        "\n",
        "new_mer_dict ={}\n",
        "for i, key in enumerate(mer_dict):\n",
        "  new_mer_dict[key] = int(mer_dict[key])\n",
        "\n",
        "a_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "sevenMer_dict = {**a_dict, **new_mer_dict}\n",
        "sevenMer_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQaW-_fqe2u0",
        "outputId": "8e7886ff-7088-4e39-80e4-270b09db5250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'[PAD]': 0,\n",
              " '[CLS]': 1,\n",
              " '[SEP]': 2,\n",
              " '[MASK]': 3,\n",
              " '1111111': 4,\n",
              " '1111112': 5,\n",
              " '1111113': 6,\n",
              " '1111114': 7,\n",
              " '1111121': 8,\n",
              " '1111122': 9,\n",
              " '1111123': 10,\n",
              " '1111124': 11,\n",
              " '1111131': 12,\n",
              " '1111132': 13,\n",
              " '1111133': 14,\n",
              " '1111134': 15,\n",
              " '1111141': 16,\n",
              " '1111142': 17,\n",
              " '1111143': 18,\n",
              " '1111144': 19,\n",
              " '1111211': 20,\n",
              " '1111212': 21,\n",
              " '1111213': 22,\n",
              " '1111214': 23,\n",
              " '1111221': 24,\n",
              " '1111222': 25,\n",
              " '1111223': 26,\n",
              " '1111224': 27,\n",
              " '1111231': 28,\n",
              " '1111232': 29,\n",
              " '1111233': 30,\n",
              " '1111234': 31,\n",
              " '1111241': 32,\n",
              " '1111242': 33,\n",
              " '1111243': 34,\n",
              " '1111244': 35,\n",
              " '1111311': 36,\n",
              " '1111312': 37,\n",
              " '1111313': 38,\n",
              " '1111314': 39,\n",
              " '1111321': 40,\n",
              " '1111322': 41,\n",
              " '1111323': 42,\n",
              " '1111324': 43,\n",
              " '1111331': 44,\n",
              " '1111332': 45,\n",
              " '1111333': 46,\n",
              " '1111334': 47,\n",
              " '1111341': 48,\n",
              " '1111342': 49,\n",
              " '1111343': 50,\n",
              " '1111344': 51,\n",
              " '1111411': 52,\n",
              " '1111412': 53,\n",
              " '1111413': 54,\n",
              " '1111414': 55,\n",
              " '1111421': 56,\n",
              " '1111422': 57,\n",
              " '1111423': 58,\n",
              " '1111424': 59,\n",
              " '1111431': 60,\n",
              " '1111432': 61,\n",
              " '1111433': 62,\n",
              " '1111434': 63,\n",
              " '1111441': 64,\n",
              " '1111442': 65,\n",
              " '1111443': 66,\n",
              " '1111444': 67,\n",
              " '1112111': 68,\n",
              " '1112112': 69,\n",
              " '1112113': 70,\n",
              " '1112114': 71,\n",
              " '1112121': 72,\n",
              " '1112122': 73,\n",
              " '1112123': 74,\n",
              " '1112124': 75,\n",
              " '1112131': 76,\n",
              " '1112132': 77,\n",
              " '1112133': 78,\n",
              " '1112134': 79,\n",
              " '1112141': 80,\n",
              " '1112142': 81,\n",
              " '1112143': 82,\n",
              " '1112144': 83,\n",
              " '1112211': 84,\n",
              " '1112212': 85,\n",
              " '1112213': 86,\n",
              " '1112214': 87,\n",
              " '1112221': 88,\n",
              " '1112222': 89,\n",
              " '1112223': 90,\n",
              " '1112224': 91,\n",
              " '1112231': 92,\n",
              " '1112232': 93,\n",
              " '1112233': 94,\n",
              " '1112234': 95,\n",
              " '1112241': 96,\n",
              " '1112242': 97,\n",
              " '1112243': 98,\n",
              " '1112244': 99,\n",
              " '1112311': 100,\n",
              " '1112312': 101,\n",
              " '1112313': 102,\n",
              " '1112314': 103,\n",
              " '1112321': 104,\n",
              " '1112322': 105,\n",
              " '1112323': 106,\n",
              " '1112324': 107,\n",
              " '1112331': 108,\n",
              " '1112332': 109,\n",
              " '1112333': 110,\n",
              " '1112334': 111,\n",
              " '1112341': 112,\n",
              " '1112342': 113,\n",
              " '1112343': 114,\n",
              " '1112344': 115,\n",
              " '1112411': 116,\n",
              " '1112412': 117,\n",
              " '1112413': 118,\n",
              " '1112414': 119,\n",
              " '1112421': 120,\n",
              " '1112422': 121,\n",
              " '1112423': 122,\n",
              " '1112424': 123,\n",
              " '1112431': 124,\n",
              " '1112432': 125,\n",
              " '1112433': 126,\n",
              " '1112434': 127,\n",
              " '1112441': 128,\n",
              " '1112442': 129,\n",
              " '1112443': 130,\n",
              " '1112444': 131,\n",
              " '1113111': 132,\n",
              " '1113112': 133,\n",
              " '1113113': 134,\n",
              " '1113114': 135,\n",
              " '1113121': 136,\n",
              " '1113122': 137,\n",
              " '1113123': 138,\n",
              " '1113124': 139,\n",
              " '1113131': 140,\n",
              " '1113132': 141,\n",
              " '1113133': 142,\n",
              " '1113134': 143,\n",
              " '1113141': 144,\n",
              " '1113142': 145,\n",
              " '1113143': 146,\n",
              " '1113144': 147,\n",
              " '1113211': 148,\n",
              " '1113212': 149,\n",
              " '1113213': 150,\n",
              " '1113214': 151,\n",
              " '1113221': 152,\n",
              " '1113222': 153,\n",
              " '1113223': 154,\n",
              " '1113224': 155,\n",
              " '1113231': 156,\n",
              " '1113232': 157,\n",
              " '1113233': 158,\n",
              " '1113234': 159,\n",
              " '1113241': 160,\n",
              " '1113242': 161,\n",
              " '1113243': 162,\n",
              " '1113244': 163,\n",
              " '1113311': 164,\n",
              " '1113312': 165,\n",
              " '1113313': 166,\n",
              " '1113314': 167,\n",
              " '1113321': 168,\n",
              " '1113322': 169,\n",
              " '1113323': 170,\n",
              " '1113324': 171,\n",
              " '1113331': 172,\n",
              " '1113332': 173,\n",
              " '1113333': 174,\n",
              " '1113334': 175,\n",
              " '1113341': 176,\n",
              " '1113342': 177,\n",
              " '1113343': 178,\n",
              " '1113344': 179,\n",
              " '1113411': 180,\n",
              " '1113412': 181,\n",
              " '1113413': 182,\n",
              " '1113414': 183,\n",
              " '1113421': 184,\n",
              " '1113422': 185,\n",
              " '1113423': 186,\n",
              " '1113424': 187,\n",
              " '1113431': 188,\n",
              " '1113432': 189,\n",
              " '1113433': 190,\n",
              " '1113434': 191,\n",
              " '1113441': 192,\n",
              " '1113442': 193,\n",
              " '1113443': 194,\n",
              " '1113444': 195,\n",
              " '1114111': 196,\n",
              " '1114112': 197,\n",
              " '1114113': 198,\n",
              " '1114114': 199,\n",
              " '1114121': 200,\n",
              " '1114122': 201,\n",
              " '1114123': 202,\n",
              " '1114124': 203,\n",
              " '1114131': 204,\n",
              " '1114132': 205,\n",
              " '1114133': 206,\n",
              " '1114134': 207,\n",
              " '1114141': 208,\n",
              " '1114142': 209,\n",
              " '1114143': 210,\n",
              " '1114144': 211,\n",
              " '1114211': 212,\n",
              " '1114212': 213,\n",
              " '1114213': 214,\n",
              " '1114214': 215,\n",
              " '1114221': 216,\n",
              " '1114222': 217,\n",
              " '1114223': 218,\n",
              " '1114224': 219,\n",
              " '1114231': 220,\n",
              " '1114232': 221,\n",
              " '1114233': 222,\n",
              " '1114234': 223,\n",
              " '1114241': 224,\n",
              " '1114242': 225,\n",
              " '1114243': 226,\n",
              " '1114244': 227,\n",
              " '1114311': 228,\n",
              " '1114312': 229,\n",
              " '1114313': 230,\n",
              " '1114314': 231,\n",
              " '1114321': 232,\n",
              " '1114322': 233,\n",
              " '1114323': 234,\n",
              " '1114324': 235,\n",
              " '1114331': 236,\n",
              " '1114332': 237,\n",
              " '1114333': 238,\n",
              " '1114334': 239,\n",
              " '1114341': 240,\n",
              " '1114342': 241,\n",
              " '1114343': 242,\n",
              " '1114344': 243,\n",
              " '1114411': 244,\n",
              " '1114412': 245,\n",
              " '1114413': 246,\n",
              " '1114414': 247,\n",
              " '1114421': 248,\n",
              " '1114422': 249,\n",
              " '1114423': 250,\n",
              " '1114424': 251,\n",
              " '1114431': 252,\n",
              " '1114432': 253,\n",
              " '1114433': 254,\n",
              " '1114434': 255,\n",
              " '1114441': 256,\n",
              " '1114442': 257,\n",
              " '1114443': 258,\n",
              " '1114444': 259,\n",
              " '1121111': 260,\n",
              " '1121112': 261,\n",
              " '1121113': 262,\n",
              " '1121114': 263,\n",
              " '1121121': 264,\n",
              " '1121122': 265,\n",
              " '1121123': 266,\n",
              " '1121124': 267,\n",
              " '1121131': 268,\n",
              " '1121132': 269,\n",
              " '1121133': 270,\n",
              " '1121134': 271,\n",
              " '1121141': 272,\n",
              " '1121142': 273,\n",
              " '1121143': 274,\n",
              " '1121144': 275,\n",
              " '1121211': 276,\n",
              " '1121212': 277,\n",
              " '1121213': 278,\n",
              " '1121214': 279,\n",
              " '1121221': 280,\n",
              " '1121222': 281,\n",
              " '1121223': 282,\n",
              " '1121224': 283,\n",
              " '1121231': 284,\n",
              " '1121232': 285,\n",
              " '1121233': 286,\n",
              " '1121234': 287,\n",
              " '1121241': 288,\n",
              " '1121242': 289,\n",
              " '1121243': 290,\n",
              " '1121244': 291,\n",
              " '1121311': 292,\n",
              " '1121312': 293,\n",
              " '1121313': 294,\n",
              " '1121314': 295,\n",
              " '1121321': 296,\n",
              " '1121322': 297,\n",
              " '1121323': 298,\n",
              " '1121324': 299,\n",
              " '1121331': 300,\n",
              " '1121332': 301,\n",
              " '1121333': 302,\n",
              " '1121334': 303,\n",
              " '1121341': 304,\n",
              " '1121342': 305,\n",
              " '1121343': 306,\n",
              " '1121344': 307,\n",
              " '1121411': 308,\n",
              " '1121412': 309,\n",
              " '1121413': 310,\n",
              " '1121414': 311,\n",
              " '1121421': 312,\n",
              " '1121422': 313,\n",
              " '1121423': 314,\n",
              " '1121424': 315,\n",
              " '1121431': 316,\n",
              " '1121432': 317,\n",
              " '1121433': 318,\n",
              " '1121434': 319,\n",
              " '1121441': 320,\n",
              " '1121442': 321,\n",
              " '1121443': 322,\n",
              " '1121444': 323,\n",
              " '1122111': 324,\n",
              " '1122112': 325,\n",
              " '1122113': 326,\n",
              " '1122114': 327,\n",
              " '1122121': 328,\n",
              " '1122122': 329,\n",
              " '1122123': 330,\n",
              " '1122124': 331,\n",
              " '1122131': 332,\n",
              " '1122132': 333,\n",
              " '1122133': 334,\n",
              " '1122134': 335,\n",
              " '1122141': 336,\n",
              " '1122142': 337,\n",
              " '1122143': 338,\n",
              " '1122144': 339,\n",
              " '1122211': 340,\n",
              " '1122212': 341,\n",
              " '1122213': 342,\n",
              " '1122214': 343,\n",
              " '1122221': 344,\n",
              " '1122222': 345,\n",
              " '1122223': 346,\n",
              " '1122224': 347,\n",
              " '1122231': 348,\n",
              " '1122232': 349,\n",
              " '1122233': 350,\n",
              " '1122234': 351,\n",
              " '1122241': 352,\n",
              " '1122242': 353,\n",
              " '1122243': 354,\n",
              " '1122244': 355,\n",
              " '1122311': 356,\n",
              " '1122312': 357,\n",
              " '1122313': 358,\n",
              " '1122314': 359,\n",
              " '1122321': 360,\n",
              " '1122322': 361,\n",
              " '1122323': 362,\n",
              " '1122324': 363,\n",
              " '1122331': 364,\n",
              " '1122332': 365,\n",
              " '1122333': 366,\n",
              " '1122334': 367,\n",
              " '1122341': 368,\n",
              " '1122342': 369,\n",
              " '1122343': 370,\n",
              " '1122344': 371,\n",
              " '1122411': 372,\n",
              " '1122412': 373,\n",
              " '1122413': 374,\n",
              " '1122414': 375,\n",
              " '1122421': 376,\n",
              " '1122422': 377,\n",
              " '1122423': 378,\n",
              " '1122424': 379,\n",
              " '1122431': 380,\n",
              " '1122432': 381,\n",
              " '1122433': 382,\n",
              " '1122434': 383,\n",
              " '1122441': 384,\n",
              " '1122442': 385,\n",
              " '1122443': 386,\n",
              " '1122444': 387,\n",
              " '1123111': 388,\n",
              " '1123112': 389,\n",
              " '1123113': 390,\n",
              " '1123114': 391,\n",
              " '1123121': 392,\n",
              " '1123122': 393,\n",
              " '1123123': 394,\n",
              " '1123124': 395,\n",
              " '1123131': 396,\n",
              " '1123132': 397,\n",
              " '1123133': 398,\n",
              " '1123134': 399,\n",
              " '1123141': 400,\n",
              " '1123142': 401,\n",
              " '1123143': 402,\n",
              " '1123144': 403,\n",
              " '1123211': 404,\n",
              " '1123212': 405,\n",
              " '1123213': 406,\n",
              " '1123214': 407,\n",
              " '1123221': 408,\n",
              " '1123222': 409,\n",
              " '1123223': 410,\n",
              " '1123224': 411,\n",
              " '1123231': 412,\n",
              " '1123232': 413,\n",
              " '1123233': 414,\n",
              " '1123234': 415,\n",
              " '1123241': 416,\n",
              " '1123242': 417,\n",
              " '1123243': 418,\n",
              " '1123244': 419,\n",
              " '1123311': 420,\n",
              " '1123312': 421,\n",
              " '1123313': 422,\n",
              " '1123314': 423,\n",
              " '1123321': 424,\n",
              " '1123322': 425,\n",
              " '1123323': 426,\n",
              " '1123324': 427,\n",
              " '1123331': 428,\n",
              " '1123332': 429,\n",
              " '1123333': 430,\n",
              " '1123334': 431,\n",
              " '1123341': 432,\n",
              " '1123342': 433,\n",
              " '1123343': 434,\n",
              " '1123344': 435,\n",
              " '1123411': 436,\n",
              " '1123412': 437,\n",
              " '1123413': 438,\n",
              " '1123414': 439,\n",
              " '1123421': 440,\n",
              " '1123422': 441,\n",
              " '1123423': 442,\n",
              " '1123424': 443,\n",
              " '1123431': 444,\n",
              " '1123432': 445,\n",
              " '1123433': 446,\n",
              " '1123434': 447,\n",
              " '1123441': 448,\n",
              " '1123442': 449,\n",
              " '1123443': 450,\n",
              " '1123444': 451,\n",
              " '1124111': 452,\n",
              " '1124112': 453,\n",
              " '1124113': 454,\n",
              " '1124114': 455,\n",
              " '1124121': 456,\n",
              " '1124122': 457,\n",
              " '1124123': 458,\n",
              " '1124124': 459,\n",
              " '1124131': 460,\n",
              " '1124132': 461,\n",
              " '1124133': 462,\n",
              " '1124134': 463,\n",
              " '1124141': 464,\n",
              " '1124142': 465,\n",
              " '1124143': 466,\n",
              " '1124144': 467,\n",
              " '1124211': 468,\n",
              " '1124212': 469,\n",
              " '1124213': 470,\n",
              " '1124214': 471,\n",
              " '1124221': 472,\n",
              " '1124222': 473,\n",
              " '1124223': 474,\n",
              " '1124224': 475,\n",
              " '1124231': 476,\n",
              " '1124232': 477,\n",
              " '1124233': 478,\n",
              " '1124234': 479,\n",
              " '1124241': 480,\n",
              " '1124242': 481,\n",
              " '1124243': 482,\n",
              " '1124244': 483,\n",
              " '1124311': 484,\n",
              " '1124312': 485,\n",
              " '1124313': 486,\n",
              " '1124314': 487,\n",
              " '1124321': 488,\n",
              " '1124322': 489,\n",
              " '1124323': 490,\n",
              " '1124324': 491,\n",
              " '1124331': 492,\n",
              " '1124332': 493,\n",
              " '1124333': 494,\n",
              " '1124334': 495,\n",
              " '1124341': 496,\n",
              " '1124342': 497,\n",
              " '1124343': 498,\n",
              " '1124344': 499,\n",
              " '1124411': 500,\n",
              " '1124412': 501,\n",
              " '1124413': 502,\n",
              " '1124414': 503,\n",
              " '1124421': 504,\n",
              " '1124422': 505,\n",
              " '1124423': 506,\n",
              " '1124424': 507,\n",
              " '1124431': 508,\n",
              " '1124432': 509,\n",
              " '1124433': 510,\n",
              " '1124434': 511,\n",
              " '1124441': 512,\n",
              " '1124442': 513,\n",
              " '1124443': 514,\n",
              " '1124444': 515,\n",
              " '1131111': 516,\n",
              " '1131112': 517,\n",
              " '1131113': 518,\n",
              " '1131114': 519,\n",
              " '1131121': 520,\n",
              " '1131122': 521,\n",
              " '1131123': 522,\n",
              " '1131124': 523,\n",
              " '1131131': 524,\n",
              " '1131132': 525,\n",
              " '1131133': 526,\n",
              " '1131134': 527,\n",
              " '1131141': 528,\n",
              " '1131142': 529,\n",
              " '1131143': 530,\n",
              " '1131144': 531,\n",
              " '1131211': 532,\n",
              " '1131212': 533,\n",
              " '1131213': 534,\n",
              " '1131214': 535,\n",
              " '1131221': 536,\n",
              " '1131222': 537,\n",
              " '1131223': 538,\n",
              " '1131224': 539,\n",
              " '1131231': 540,\n",
              " '1131232': 541,\n",
              " '1131233': 542,\n",
              " '1131234': 543,\n",
              " '1131241': 544,\n",
              " '1131242': 545,\n",
              " '1131243': 546,\n",
              " '1131244': 547,\n",
              " '1131311': 548,\n",
              " '1131312': 549,\n",
              " '1131313': 550,\n",
              " '1131314': 551,\n",
              " '1131321': 552,\n",
              " '1131322': 553,\n",
              " '1131323': 554,\n",
              " '1131324': 555,\n",
              " '1131331': 556,\n",
              " '1131332': 557,\n",
              " '1131333': 558,\n",
              " '1131334': 559,\n",
              " '1131341': 560,\n",
              " '1131342': 561,\n",
              " '1131343': 562,\n",
              " '1131344': 563,\n",
              " '1131411': 564,\n",
              " '1131412': 565,\n",
              " '1131413': 566,\n",
              " '1131414': 567,\n",
              " '1131421': 568,\n",
              " '1131422': 569,\n",
              " '1131423': 570,\n",
              " '1131424': 571,\n",
              " '1131431': 572,\n",
              " '1131432': 573,\n",
              " '1131433': 574,\n",
              " '1131434': 575,\n",
              " '1131441': 576,\n",
              " '1131442': 577,\n",
              " '1131443': 578,\n",
              " '1131444': 579,\n",
              " '1132111': 580,\n",
              " '1132112': 581,\n",
              " '1132113': 582,\n",
              " '1132114': 583,\n",
              " '1132121': 584,\n",
              " '1132122': 585,\n",
              " '1132123': 586,\n",
              " '1132124': 587,\n",
              " '1132131': 588,\n",
              " '1132132': 589,\n",
              " '1132133': 590,\n",
              " '1132134': 591,\n",
              " '1132141': 592,\n",
              " '1132142': 593,\n",
              " '1132143': 594,\n",
              " '1132144': 595,\n",
              " '1132211': 596,\n",
              " '1132212': 597,\n",
              " '1132213': 598,\n",
              " '1132214': 599,\n",
              " '1132221': 600,\n",
              " '1132222': 601,\n",
              " '1132223': 602,\n",
              " '1132224': 603,\n",
              " '1132231': 604,\n",
              " '1132232': 605,\n",
              " '1132233': 606,\n",
              " '1132234': 607,\n",
              " '1132241': 608,\n",
              " '1132242': 609,\n",
              " '1132243': 610,\n",
              " '1132244': 611,\n",
              " '1132311': 612,\n",
              " '1132312': 613,\n",
              " '1132313': 614,\n",
              " '1132314': 615,\n",
              " '1132321': 616,\n",
              " '1132322': 617,\n",
              " '1132323': 618,\n",
              " '1132324': 619,\n",
              " '1132331': 620,\n",
              " '1132332': 621,\n",
              " '1132333': 622,\n",
              " '1132334': 623,\n",
              " '1132341': 624,\n",
              " '1132342': 625,\n",
              " '1132343': 626,\n",
              " '1132344': 627,\n",
              " '1132411': 628,\n",
              " '1132412': 629,\n",
              " '1132413': 630,\n",
              " '1132414': 631,\n",
              " '1132421': 632,\n",
              " '1132422': 633,\n",
              " '1132423': 634,\n",
              " '1132424': 635,\n",
              " '1132431': 636,\n",
              " '1132432': 637,\n",
              " '1132433': 638,\n",
              " '1132434': 639,\n",
              " '1132441': 640,\n",
              " '1132442': 641,\n",
              " '1132443': 642,\n",
              " '1132444': 643,\n",
              " '1133111': 644,\n",
              " '1133112': 645,\n",
              " '1133113': 646,\n",
              " '1133114': 647,\n",
              " '1133121': 648,\n",
              " '1133122': 649,\n",
              " '1133123': 650,\n",
              " '1133124': 651,\n",
              " '1133131': 652,\n",
              " '1133132': 653,\n",
              " '1133133': 654,\n",
              " '1133134': 655,\n",
              " '1133141': 656,\n",
              " '1133142': 657,\n",
              " '1133143': 658,\n",
              " '1133144': 659,\n",
              " '1133211': 660,\n",
              " '1133212': 661,\n",
              " '1133213': 662,\n",
              " '1133214': 663,\n",
              " '1133221': 664,\n",
              " '1133222': 665,\n",
              " '1133223': 666,\n",
              " '1133224': 667,\n",
              " '1133231': 668,\n",
              " '1133232': 669,\n",
              " '1133233': 670,\n",
              " '1133234': 671,\n",
              " '1133241': 672,\n",
              " '1133242': 673,\n",
              " '1133243': 674,\n",
              " '1133244': 675,\n",
              " '1133311': 676,\n",
              " '1133312': 677,\n",
              " '1133313': 678,\n",
              " '1133314': 679,\n",
              " '1133321': 680,\n",
              " '1133322': 681,\n",
              " '1133323': 682,\n",
              " '1133324': 683,\n",
              " '1133331': 684,\n",
              " '1133332': 685,\n",
              " '1133333': 686,\n",
              " '1133334': 687,\n",
              " '1133341': 688,\n",
              " '1133342': 689,\n",
              " '1133343': 690,\n",
              " '1133344': 691,\n",
              " '1133411': 692,\n",
              " '1133412': 693,\n",
              " '1133413': 694,\n",
              " '1133414': 695,\n",
              " '1133421': 696,\n",
              " '1133422': 697,\n",
              " '1133423': 698,\n",
              " '1133424': 699,\n",
              " '1133431': 700,\n",
              " '1133432': 701,\n",
              " '1133433': 702,\n",
              " '1133434': 703,\n",
              " '1133441': 704,\n",
              " '1133442': 705,\n",
              " '1133443': 706,\n",
              " '1133444': 707,\n",
              " '1134111': 708,\n",
              " '1134112': 709,\n",
              " '1134113': 710,\n",
              " '1134114': 711,\n",
              " '1134121': 712,\n",
              " '1134122': 713,\n",
              " '1134123': 714,\n",
              " '1134124': 715,\n",
              " '1134131': 716,\n",
              " '1134132': 717,\n",
              " '1134133': 718,\n",
              " '1134134': 719,\n",
              " '1134141': 720,\n",
              " '1134142': 721,\n",
              " '1134143': 722,\n",
              " '1134144': 723,\n",
              " '1134211': 724,\n",
              " '1134212': 725,\n",
              " '1134213': 726,\n",
              " '1134214': 727,\n",
              " '1134221': 728,\n",
              " '1134222': 729,\n",
              " '1134223': 730,\n",
              " '1134224': 731,\n",
              " '1134231': 732,\n",
              " '1134232': 733,\n",
              " '1134233': 734,\n",
              " '1134234': 735,\n",
              " '1134241': 736,\n",
              " '1134242': 737,\n",
              " '1134243': 738,\n",
              " '1134244': 739,\n",
              " '1134311': 740,\n",
              " '1134312': 741,\n",
              " '1134313': 742,\n",
              " '1134314': 743,\n",
              " '1134321': 744,\n",
              " '1134322': 745,\n",
              " '1134323': 746,\n",
              " '1134324': 747,\n",
              " '1134331': 748,\n",
              " '1134332': 749,\n",
              " '1134333': 750,\n",
              " '1134334': 751,\n",
              " '1134341': 752,\n",
              " '1134342': 753,\n",
              " '1134343': 754,\n",
              " '1134344': 755,\n",
              " '1134411': 756,\n",
              " '1134412': 757,\n",
              " '1134413': 758,\n",
              " '1134414': 759,\n",
              " '1134421': 760,\n",
              " '1134422': 761,\n",
              " '1134423': 762,\n",
              " '1134424': 763,\n",
              " '1134431': 764,\n",
              " '1134432': 765,\n",
              " '1134433': 766,\n",
              " '1134434': 767,\n",
              " '1134441': 768,\n",
              " '1134442': 769,\n",
              " '1134443': 770,\n",
              " '1134444': 771,\n",
              " '1141111': 772,\n",
              " '1141112': 773,\n",
              " '1141113': 774,\n",
              " '1141114': 775,\n",
              " '1141121': 776,\n",
              " '1141122': 777,\n",
              " '1141123': 778,\n",
              " '1141124': 779,\n",
              " '1141131': 780,\n",
              " '1141132': 781,\n",
              " '1141133': 782,\n",
              " '1141134': 783,\n",
              " '1141141': 784,\n",
              " '1141142': 785,\n",
              " '1141143': 786,\n",
              " '1141144': 787,\n",
              " '1141211': 788,\n",
              " '1141212': 789,\n",
              " '1141213': 790,\n",
              " '1141214': 791,\n",
              " '1141221': 792,\n",
              " '1141222': 793,\n",
              " '1141223': 794,\n",
              " '1141224': 795,\n",
              " '1141231': 796,\n",
              " '1141232': 797,\n",
              " '1141233': 798,\n",
              " '1141234': 799,\n",
              " '1141241': 800,\n",
              " '1141242': 801,\n",
              " '1141243': 802,\n",
              " '1141244': 803,\n",
              " '1141311': 804,\n",
              " '1141312': 805,\n",
              " '1141313': 806,\n",
              " '1141314': 807,\n",
              " '1141321': 808,\n",
              " '1141322': 809,\n",
              " '1141323': 810,\n",
              " '1141324': 811,\n",
              " '1141331': 812,\n",
              " '1141332': 813,\n",
              " '1141333': 814,\n",
              " '1141334': 815,\n",
              " '1141341': 816,\n",
              " '1141342': 817,\n",
              " '1141343': 818,\n",
              " '1141344': 819,\n",
              " '1141411': 820,\n",
              " '1141412': 821,\n",
              " '1141413': 822,\n",
              " '1141414': 823,\n",
              " '1141421': 824,\n",
              " '1141422': 825,\n",
              " '1141423': 826,\n",
              " '1141424': 827,\n",
              " '1141431': 828,\n",
              " '1141432': 829,\n",
              " '1141433': 830,\n",
              " '1141434': 831,\n",
              " '1141441': 832,\n",
              " '1141442': 833,\n",
              " '1141443': 834,\n",
              " '1141444': 835,\n",
              " '1142111': 836,\n",
              " '1142112': 837,\n",
              " '1142113': 838,\n",
              " '1142114': 839,\n",
              " '1142121': 840,\n",
              " '1142122': 841,\n",
              " '1142123': 842,\n",
              " '1142124': 843,\n",
              " '1142131': 844,\n",
              " '1142132': 845,\n",
              " '1142133': 846,\n",
              " '1142134': 847,\n",
              " '1142141': 848,\n",
              " '1142142': 849,\n",
              " '1142143': 850,\n",
              " '1142144': 851,\n",
              " '1142211': 852,\n",
              " '1142212': 853,\n",
              " '1142213': 854,\n",
              " '1142214': 855,\n",
              " '1142221': 856,\n",
              " '1142222': 857,\n",
              " '1142223': 858,\n",
              " '1142224': 859,\n",
              " '1142231': 860,\n",
              " '1142232': 861,\n",
              " '1142233': 862,\n",
              " '1142234': 863,\n",
              " '1142241': 864,\n",
              " '1142242': 865,\n",
              " '1142243': 866,\n",
              " '1142244': 867,\n",
              " '1142311': 868,\n",
              " '1142312': 869,\n",
              " '1142313': 870,\n",
              " '1142314': 871,\n",
              " '1142321': 872,\n",
              " '1142322': 873,\n",
              " '1142323': 874,\n",
              " '1142324': 875,\n",
              " '1142331': 876,\n",
              " '1142332': 877,\n",
              " '1142333': 878,\n",
              " '1142334': 879,\n",
              " '1142341': 880,\n",
              " '1142342': 881,\n",
              " '1142343': 882,\n",
              " '1142344': 883,\n",
              " '1142411': 884,\n",
              " '1142412': 885,\n",
              " '1142413': 886,\n",
              " '1142414': 887,\n",
              " '1142421': 888,\n",
              " '1142422': 889,\n",
              " '1142423': 890,\n",
              " '1142424': 891,\n",
              " '1142431': 892,\n",
              " '1142432': 893,\n",
              " '1142433': 894,\n",
              " '1142434': 895,\n",
              " '1142441': 896,\n",
              " '1142442': 897,\n",
              " '1142443': 898,\n",
              " '1142444': 899,\n",
              " '1143111': 900,\n",
              " '1143112': 901,\n",
              " '1143113': 902,\n",
              " '1143114': 903,\n",
              " '1143121': 904,\n",
              " '1143122': 905,\n",
              " '1143123': 906,\n",
              " '1143124': 907,\n",
              " '1143131': 908,\n",
              " '1143132': 909,\n",
              " '1143133': 910,\n",
              " '1143134': 911,\n",
              " '1143141': 912,\n",
              " '1143142': 913,\n",
              " '1143143': 914,\n",
              " '1143144': 915,\n",
              " '1143211': 916,\n",
              " '1143212': 917,\n",
              " '1143213': 918,\n",
              " '1143214': 919,\n",
              " '1143221': 920,\n",
              " '1143222': 921,\n",
              " '1143223': 922,\n",
              " '1143224': 923,\n",
              " '1143231': 924,\n",
              " '1143232': 925,\n",
              " '1143233': 926,\n",
              " '1143234': 927,\n",
              " '1143241': 928,\n",
              " '1143242': 929,\n",
              " '1143243': 930,\n",
              " '1143244': 931,\n",
              " '1143311': 932,\n",
              " '1143312': 933,\n",
              " '1143313': 934,\n",
              " '1143314': 935,\n",
              " '1143321': 936,\n",
              " '1143322': 937,\n",
              " '1143323': 938,\n",
              " '1143324': 939,\n",
              " '1143331': 940,\n",
              " '1143332': 941,\n",
              " '1143333': 942,\n",
              " '1143334': 943,\n",
              " '1143341': 944,\n",
              " '1143342': 945,\n",
              " '1143343': 946,\n",
              " '1143344': 947,\n",
              " '1143411': 948,\n",
              " '1143412': 949,\n",
              " '1143413': 950,\n",
              " '1143414': 951,\n",
              " '1143421': 952,\n",
              " '1143422': 953,\n",
              " '1143423': 954,\n",
              " '1143424': 955,\n",
              " '1143431': 956,\n",
              " '1143432': 957,\n",
              " '1143433': 958,\n",
              " '1143434': 959,\n",
              " '1143441': 960,\n",
              " '1143442': 961,\n",
              " '1143443': 962,\n",
              " '1143444': 963,\n",
              " '1144111': 964,\n",
              " '1144112': 965,\n",
              " '1144113': 966,\n",
              " '1144114': 967,\n",
              " '1144121': 968,\n",
              " '1144122': 969,\n",
              " '1144123': 970,\n",
              " '1144124': 971,\n",
              " '1144131': 972,\n",
              " '1144132': 973,\n",
              " '1144133': 974,\n",
              " '1144134': 975,\n",
              " '1144141': 976,\n",
              " '1144142': 977,\n",
              " '1144143': 978,\n",
              " '1144144': 979,\n",
              " '1144211': 980,\n",
              " '1144212': 981,\n",
              " '1144213': 982,\n",
              " '1144214': 983,\n",
              " '1144221': 984,\n",
              " '1144222': 985,\n",
              " '1144223': 986,\n",
              " '1144224': 987,\n",
              " '1144231': 988,\n",
              " '1144232': 989,\n",
              " '1144233': 990,\n",
              " '1144234': 991,\n",
              " '1144241': 992,\n",
              " '1144242': 993,\n",
              " '1144243': 994,\n",
              " '1144244': 995,\n",
              " '1144311': 996,\n",
              " '1144312': 997,\n",
              " '1144313': 998,\n",
              " '1144314': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [7, 4, 6, 5, 4, 6, 5, 4, 5, 4, 7, 4, 4, 7, 6, 6, 7, 7, 7, 6, 7, 6]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hKjI8RlyAnuT",
        "outputId": "38285d7a-9930-4203-93e1-2a0511274b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[7, 4]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [7, 4, 6, 5, 4, 6, 5, 4, 5, 4, 7, 4, 4, 7, 6, 6, 7, 7, 7, 6, 7, 6]\n",
        "lst = [i-3 for i in lst]\n",
        "print(lst)\n",
        "def sliding_window(elements, window_size):\n",
        "  elements = [i-3 for i in elements]\n",
        "  new_list=[]\n",
        "  if len(elements) == window_size:\n",
        "    return elements\n",
        "  for i in range(len(elements) - window_size + 1):\n",
        "    tmp=elements[i:i+window_size]\n",
        "    new_str=''\n",
        "    for e in tmp:\n",
        "      new_str += str(e)\n",
        "    new_list.append(new_str)\n",
        "    print(new_str)\n",
        "  return new_list\n",
        "slided_list=sliding_window(lst, 3)\n",
        "slided_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR1dh0O99uxn",
        "outputId": "05b63687-6f12-42a5-d891-193ed57bacdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 1, 3, 2, 1, 3, 2, 1, 2, 1, 4, 1, 1, 4, 3, 3, 4, 4, 4, 3, 4, 3]\n",
            "413\n",
            "132\n",
            "321\n",
            "213\n",
            "132\n",
            "321\n",
            "212\n",
            "121\n",
            "214\n",
            "141\n",
            "411\n",
            "114\n",
            "143\n",
            "433\n",
            "334\n",
            "344\n",
            "444\n",
            "443\n",
            "434\n",
            "343\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['413',\n",
              " '132',\n",
              " '321',\n",
              " '213',\n",
              " '132',\n",
              " '321',\n",
              " '212',\n",
              " '121',\n",
              " '214',\n",
              " '141',\n",
              " '411',\n",
              " '114',\n",
              " '143',\n",
              " '433',\n",
              " '334',\n",
              " '344',\n",
              " '444',\n",
              " '443',\n",
              " '434',\n",
              " '343']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MeshRNADataset(Dataset):\n",
        "  def __init__(self, orig_diseases, orig_mirna, orig_labels, edge_d, edge_attr_d, node_d, hair_x, delete_file=None, is_conv1d=False, num_mer=1, mer_dict=None):\n",
        "    super(MeshRNADataset, self).__init__()\n",
        "    \n",
        "    self.orig_mirna = orig_mirna\n",
        "    self.orig_labels = orig_labels\n",
        "    self.vocab_size = 5\n",
        "    self.MAX_LEN = 28\n",
        "    self.MAX_LEN_D = 6\n",
        "    # self.disease2token_list = disease2token_list\n",
        "    self.delete_file = delete_file\n",
        "    self.edge_d = edge_d\n",
        "    self.edge_attr_d = edge_attr_d\n",
        "    self.node_d = node_d\n",
        "    self.hair_x = hair_x\n",
        "    self.is_conv1d = is_conv1d\n",
        "    self.num_mer = num_mer\n",
        "    self.mer_dict = mer_dict\n",
        "    self.is_mlp = False\n",
        "    if self.delete_file is not None:\n",
        "      self.orig_diseases = [e for e in orig_diseases if e not in self.delete_file]\n",
        "      self.orig_diseases = np.array(self.orig_diseases)\n",
        "    else:\n",
        "      self.orig_diseases = orig_diseases\n",
        "    self.letter2Number = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, \"A\": 4, \"C\": 5, \"G\": 6, \"U\": 7}\n",
        "\n",
        "  def convert_letter_to_number(self, lettter):\n",
        "    letter_to_number = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, \"A\": 4, \"C\": 5, \"G\": 6, \"U\": 7}\n",
        "\n",
        "    return letter_to_number[lettter]\n",
        "\n",
        "  def convert_letter_to_number_Conv1D(self, lettter):\n",
        "    letter_to_number = {'[PAD]': 0, \"A\": 1, \"C\": 2, \"G\": 3, \"U\": 4}\n",
        "\n",
        "    return letter_to_number[lettter]\n",
        "\n",
        "  def make_data_with_unified_length(self, token_list, max_len, covert_dict, is_conv1d=False):\n",
        "      \n",
        "      if is_conv1d:\n",
        "        max_len = max_len + 1  # add [CLS]\n",
        "        token_list = token_list\n",
        "      else:\n",
        "        max_len = max_len + 2  # add [CLS] and [SEP]\n",
        "        token_list = [covert_dict['[CLS]']] + token_list + [covert_dict['[SEP]']]\n",
        "      \n",
        "      n_pad = max_len - len(token_list)\n",
        "      token_list.extend([0] * n_pad)\n",
        "\n",
        "      return token_list\n",
        "\n",
        "  def mirna2nmers(self, token_list, num_mer):\n",
        "    token_list = [i-3 for i in token_list]\n",
        "    new_list=[]\n",
        "    if len(token_list) == num_mer:\n",
        "      return token_list\n",
        "    for i in range(len(token_list) - num_mer + 1):\n",
        "      tmp=token_list[i:i+num_mer]\n",
        "      new_str=''\n",
        "      for e in tmp:\n",
        "        new_str += str(e)\n",
        "      new_list.append(self.mer_dict[new_str])\n",
        "      # print(new_str)\n",
        "    return new_list\n",
        "\n",
        "\n",
        "\n",
        "  def mirna2list(self, data, is_conv1d=False):\n",
        "    out = []\n",
        "    if is_conv1d:\n",
        "      for j in range(len(data)):\n",
        "        coverted_rna = self.convert_letter_to_number_Conv1D(data[j])\n",
        "        out.append(coverted_rna)\n",
        "    else:\n",
        "      for j in range(len(data)):\n",
        "        coverted_rna = self.convert_letter_to_number(data[j])\n",
        "        out.append(coverted_rna)\n",
        "    \n",
        "    if self.num_mer>1:\n",
        "      # print('before out ',out)\n",
        "      out = self.mirna2nmers(out, self.num_mer)\n",
        "      # print('after out ',out)\n",
        "\n",
        "    data = self.make_data_with_unified_length(out, self.MAX_LEN, self.letter2Number, is_conv1d=is_conv1d)\n",
        "    \n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "\n",
        "  def disease2token(self, disease):\n",
        "  \n",
        "    disease = disease.replace(\", \", \" \")\n",
        "    disease_list = disease.split(\" \")\n",
        "    \n",
        "    # print(disease_list)\n",
        "    tokenized=[self.disease2token_list[i] for i in disease_list]\n",
        "    max_len = self.MAX_LEN_D + 2\n",
        "    tokenized = [self.disease2token_list['[CLS]']] + tokenized + [self.disease2token_list['[SEP]']]\n",
        "    n_pad = max_len - len(tokenized)\n",
        "    tokenized.extend([0] * n_pad)\n",
        "    return np.array(tokenized)\n",
        "\n",
        "  def disease2list(self, disease, if_key=False):\n",
        "    if if_key:\n",
        "      disease = disease.replace(\",\", \"\")\n",
        "      disease = disease.replace(\"-\", \" \")\n",
        "      disease = disease.replace(\"[\", \"\")\n",
        "      disease = disease.replace(\"]\", \"\")\n",
        "      disease = disease.replace(\"Kideny\", \"Kidney\")\n",
        "      disease = disease.replace(\"Spondylarthritis\", \"Spondyloarthritis\")\n",
        "    disease_list = disease.split(\" \")\n",
        "    return disease_list\n",
        "\n",
        "\n",
        "  def disease2emb(self, disease, if_key=False):\n",
        "\n",
        "    disease_list = self.disease2list(disease, if_key)\n",
        "    vector = np.zeros(300)\n",
        "    for word in disease_list:\n",
        "      value = word2vec_mesh.get(word)\n",
        "      if value is not None:\n",
        "        vector += word2vec_mesh[word]\n",
        "      else:\n",
        "        vector = np.zeros(300)\n",
        "    out = vector/len(disease_list)\n",
        "    return out\n",
        "\n",
        "  def getAttnMask(self, disease):\n",
        "    att = [int(token_id > 0) for token_id in disease]\n",
        "    return np.array(att)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      orig_disease = str(self.orig_diseases[index])\n",
        "      edge_idx = self.edge_d[orig_disease]\n",
        "\n",
        "      \n",
        "      # print(type(pre_hair_x))\n",
        "     \n",
        "      if edge_idx == []:\n",
        "        edge_idx = torch.tensor([[0, 0]], dtype=torch.long)\n",
        "      else:\n",
        "        edge_idx = torch.tensor(edge_idx, dtype=torch.long)\n",
        "      edge_idx = edge_idx.transpose(0, 1)\n",
        "      node_value = self.node_d[orig_disease]\n",
        "      x = []\n",
        "      for e in node_value:\n",
        "        content = self.disease2emb(e)\n",
        "        x.append(content)\n",
        "      x = torch.tensor(x)\n",
        "\n",
        "      key2emb = self.disease2emb(orig_disease,if_key=True)\n",
        "      key2emb = np.array(key2emb)\n",
        "      data = Data(x=x, edge_index=edge_idx, key2emb=key2emb)\n",
        "\n",
        "      orig_disease = self.orig_diseases[index]\n",
        "      orig_mirna = self.orig_mirna[index]\n",
        "      label = self.orig_labels[index]\n",
        "      # mirna = np.array([self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)])\n",
        "      if self.is_conv1d:\n",
        "        mirna = np.array([self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)])\n",
        "        mirna = torch.from_numpy(mirna).float()\n",
        "      else:\n",
        "        # mirna = self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)\n",
        "        # mirna = np.expand_dims(mirna, axis=0)\n",
        "        mirna = np.array([self.mirna2list(orig_mirna, is_conv1d=self.is_conv1d)])\n",
        "        mirna = np.expand_dims(mirna, axis=0)\n",
        "        mirna = torch.from_numpy(mirna)\n",
        "      \n",
        "      if self.is_mlp:\n",
        "        pre_hair_x = torch.from_numpy(np.array([*self.hair_x[index], *self.mirna2list(orig_mirna)])).float()\n",
        "      else:\n",
        "        pre_hair_x = torch.from_numpy(np.array([[*self.hair_x[index]]])).float()\n",
        "\n",
        "\n",
        "      data = Data(x=x, edge_index=edge_idx, key2emb=key2emb, mirna=mirna, label=label, pre_hair_x=pre_hair_x, orig_disease=orig_disease, orig_mirna=orig_mirna)\n",
        "      return data #(disease, mirna, label, data, orig_disease, orig_mirna)\n",
        "      \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.orig_diseases)\n"
      ],
      "metadata": {
        "id": "9sdJEv72MIke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "dataloader = PYG_DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, data in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  # print('disease.shape',data.disease.shape)\n",
        "  # print('disease', data.disease)\n",
        "  print('pre_hair_x.shape',data.pre_hair_x.shape)\n",
        "  # print('pre_hair_x', data.pre_hair_x)\n",
        "  \n",
        "  print('mirna.shape', data.mirna.shape)\n",
        "  print('mirna', data.mirna)\n",
        "  print('label.shape', torch.tensor(data.label).shape)\n",
        "  # print('label', data.label)\n",
        "  print('orig_disease', data.orig_disease)\n",
        "  print('orig_mirna', data.orig_mirna)\n",
        "  # print('data.batch',data.batch)\n",
        "  print('data.batch.shape',data.batch.shape)\n",
        "  print('data.edge_index.shape',data.edge_index.shape) # [2, num_edges]\n",
        "  # print('data.x',data.x)\n",
        "  print('data.x.shape',data.x.shape) # [num_nodes, num_node_features]\n",
        "  print('data.num_features ',data.num_features)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0oqThhlEDCV",
        "outputId": "1a5094c8-5812-432d-954e-9603577de78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "pre_hair_x.shape torch.Size([2, 41])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "mirna tensor([[[1, 7, 4, 6, 5, 4, 6, 5, 4, 5, 4, 7, 4, 4, 7, 6, 6, 7, 7, 7, 6, 7, 6,\n",
            "          2, 0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[1, 5, 4, 6, 6, 5, 5, 4, 7, 4, 7, 7, 6, 7, 6, 5, 7, 6, 5, 5, 7, 5, 4,\n",
            "          2, 0, 0, 0, 0, 0, 0]]])\n",
            "label.shape torch.Size([2])\n",
            "orig_disease ['Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell']\n",
            "orig_mirna ['UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA']\n",
            "data.batch.shape torch.Size([40])\n",
            "data.edge_index.shape torch.Size([2, 38])\n",
            "data.x.shape torch.Size([40, 300])\n",
            "data.num_features  300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# two mer\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=2, mer_dict=twoMer_dict)\n",
        "dataloader = PYG_DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, data in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  # print('disease.shape',data.disease.shape)\n",
        "  # print('disease', data.disease)\n",
        "  print('pre_hair_x.shape',data.pre_hair_x.shape)\n",
        "  # print('pre_hair_x', data.pre_hair_x)\n",
        "  \n",
        "  print('mirna.shape', data.mirna.shape)\n",
        "  print('mirna', data.mirna)\n",
        "  print('label.shape', torch.tensor(data.label).shape)\n",
        "  # print('label', data.label)\n",
        "  print('orig_disease', data.orig_disease)\n",
        "  print('orig_mirna', data.orig_mirna)\n",
        "  # print('data.batch',data.batch)\n",
        "  print('data.batch.shape',data.batch.shape)\n",
        "  print('data.edge_index.shape',data.edge_index.shape) # [2, num_edges]\n",
        "  # print('data.x',data.x)\n",
        "  print('data.x.shape',data.x.shape) # [num_nodes, num_node_features]\n",
        "  print('data.num_features ',data.num_features)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtARt-icAjgo",
        "outputId": "51c28d05-e77a-4a1f-b1dd-42cfb50ec52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "pre_hair_x.shape torch.Size([2, 41])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "mirna tensor([[[ 1, 16,  6, 13,  8,  6, 13,  8,  5,  8,  7, 16,  4,  7, 18, 14, 15,\n",
            "          19, 19, 18, 15, 18,  2,  0,  0,  0,  0,  0,  0,  0]],\n",
            "\n",
            "        [[ 1,  8,  6, 14, 13,  9,  8,  7, 16,  7, 19, 18, 15, 18, 13, 11, 18,\n",
            "          13,  9, 11, 17,  8,  2,  0,  0,  0,  0,  0,  0,  0]]])\n",
            "label.shape torch.Size([2])\n",
            "orig_disease ['Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell']\n",
            "orig_mirna ['UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA']\n",
            "data.batch.shape torch.Size([40])\n",
            "data.edge_index.shape torch.Size([2, 38])\n",
            "data.x.shape torch.Size([40, 300])\n",
            "data.num_features  300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# three mer\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=3, mer_dict=threeMer_dict)\n",
        "dataloader = PYG_DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, data in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  # print('disease.shape',data.disease.shape)\n",
        "  # print('disease', data.disease)\n",
        "  print('pre_hair_x.shape',data.pre_hair_x.shape)\n",
        "  # print('pre_hair_x', data.pre_hair_x)\n",
        "  \n",
        "  print('mirna.shape', data.mirna.shape)\n",
        "  print('mirna', data.mirna)\n",
        "  print('label.shape', torch.tensor(data.label).shape)\n",
        "  # print('label', data.label)\n",
        "  print('orig_disease', data.orig_disease)\n",
        "  print('orig_mirna', data.orig_mirna)\n",
        "  # print('data.batch',data.batch)\n",
        "  print('data.batch.shape',data.batch.shape)\n",
        "  print('data.edge_index.shape',data.edge_index.shape) # [2, num_edges]\n",
        "  # print('data.x',data.x)\n",
        "  print('data.x.shape',data.x.shape) # [num_nodes, num_node_features]\n",
        "  print('data.num_features ',data.num_features)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RBjFa_eD8YT",
        "outputId": "2a45942e-dc0d-4af1-9f99-30a035e7175b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before out  [7, 4, 6, 5, 4, 6, 5, 4, 5, 4, 7, 4, 4, 7, 6, 6, 7, 7, 7, 6, 7, 6]\n",
            "after out  [54, 13, 40, 22, 13, 40, 21, 8, 23, 16, 52, 7, 18, 62, 47, 51, 67, 66, 63, 50]\n",
            "before out  [5, 4, 6, 6, 5, 5, 4, 7, 4, 7, 7, 6, 7, 6, 5, 7, 6, 5, 5, 7, 5, 4]\n",
            "after out  [22, 14, 45, 41, 24, 23, 16, 55, 19, 66, 63, 50, 61, 43, 34, 61, 41, 27, 33, 56]\n",
            "0\n",
            "pre_hair_x.shape torch.Size([2, 41])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "mirna tensor([[[ 1, 54, 13, 40, 22, 13, 40, 21,  8, 23, 16, 52,  7, 18, 62, 47, 51,\n",
            "          67, 66, 63, 50,  2,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
            "\n",
            "        [[ 1, 22, 14, 45, 41, 24, 23, 16, 55, 19, 66, 63, 50, 61, 43, 34, 61,\n",
            "          41, 27, 33, 56,  2,  0,  0,  0,  0,  0,  0,  0,  0]]])\n",
            "label.shape torch.Size([2])\n",
            "orig_disease ['Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell']\n",
            "orig_mirna ['UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA']\n",
            "data.batch.shape torch.Size([40])\n",
            "data.edge_index.shape torch.Size([2, 38])\n",
            "data.x.shape torch.Size([40, 300])\n",
            "data.num_features  300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "dataloader = PYG_DataLoader(dataset, batch_size=2)\n",
        "\n",
        "for idx, data in enumerate(dataloader):\n",
        "  print(idx)\n",
        "  # print('disease.shape',data.disease.shape)\n",
        "  # print('disease', data.disease)\n",
        "  print('pre_hair_x.shape',data.pre_hair_x.shape)\n",
        "  # print('pre_hair_x', data.pre_hair_x)\n",
        "  \n",
        "  print('mirna.shape', data.mirna.shape)\n",
        "  # print('mirna', data.mirna)\n",
        "  print('label.shape', torch.tensor(data.label).shape)\n",
        "  print('label', data.label)\n",
        "  print('orig_disease', data.orig_disease)\n",
        "  print('orig_mirna', data.orig_mirna)\n",
        "  # print('data.batch',data.batch)\n",
        "  print('data.batch.shape',data.batch.shape)\n",
        "  print('data.edge_index.shape',data.edge_index.shape) # [2, num_edges]\n",
        "  # print('data.x',data.x)\n",
        "  print('data.x.shape',data.x.shape) # [num_nodes, num_node_features]\n",
        "  print('data.num_features ',data.num_features)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "id": "kZBFgv7oSl5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fbbf44-5c86-4698-cea0-ca81edbb408f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "pre_hair_x.shape torch.Size([2, 41])\n",
            "mirna.shape torch.Size([2, 1, 30])\n",
            "label.shape torch.Size([2])\n",
            "label [0, 0]\n",
            "orig_disease ['Leukemia, Lymphocytic, Chronic, B-Cell', 'Leukemia, Lymphocytic, Chronic, B-Cell']\n",
            "orig_mirna ['UAGCAGCACAUAAUGGUUUGUG', 'CAGGCCAUAUUGUGCUGCCUCA']\n",
            "data.batch.shape torch.Size([40])\n",
            "data.edge_index.shape torch.Size([2, 38])\n",
            "data.x.shape torch.Size([40, 300])\n",
            "data.num_features  300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "sRML2fsmTwTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "t3gerb3kSfvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN"
      ],
      "metadata": {
        "id": "CjFShjSvUA_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Am1a4ZyTx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_size = 300\n",
        "num_features = 300\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        # torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GCNConv(num_features, 256)\n",
        "        self.bn_1 = BatchNorm(256)\n",
        "        self.conv1 = GCNConv(256, 128)\n",
        "        self.bn_2 = BatchNorm(128)\n",
        "        self.conv2 = GCNConv(128, 128)\n",
        "        self.bn_3 = BatchNorm(128)\n",
        "        self.conv3 = GCNConv(128, 64)\n",
        "        self.bn_4 = BatchNorm(64)\n",
        "        # Output layer\n",
        "        self.out = Linear(128, 64)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        hidden = self.bn_1(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        hidden = self.bn_2(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = self.bn_3(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = self.bn_4(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        # Global Pooling (stack different aggregations)\n",
        "        # hidden = gap(hidden, batch_index)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index),  gap(hidden, batch_index)], dim=1)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "vb9wsUKBKhC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN()\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "0eT-BzrwJYG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313f70b1-1751-4091-8595-5242cec4e71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (initial_conv): GCNConv(300, 256)\n",
            "  (bn_1): BatchNorm(256)\n",
            "  (conv1): GCNConv(256, 128)\n",
            "  (bn_2): BatchNorm(128)\n",
            "  (conv2): GCNConv(128, 128)\n",
            "  (bn_3): BatchNorm(128)\n",
            "  (conv3): GCNConv(128, 64)\n",
            "  (bn_4): BatchNorm(64)\n",
            "  (out): Linear(in_features=128, out_features=64, bias=True)\n",
            ")\n",
            "Number of parameters:  144128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = MeshDataset(edge_d, edge_attr_d, node_d, orig_x_disease)\n",
        "# graph_list = [dataset[i] for i in range(len(dataset))]"
      ],
      "metadata": {
        "id": "375SP01qK2sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = PYG_DataLoader(dataset, batch_size=8)\n",
        "sampled_data = next(iter(dataloader))\n",
        "pred = model(sampled_data.x.float(), sampled_data.edge_index, sampled_data.batch) \n",
        "# print(sampled_data.batch)\n",
        "print(sampled_data.batch.shape)\n",
        "print(sampled_data.edge_index.shape) # [2, num_edges]\n",
        "print(sampled_data.x.shape) # [num_nodes, num_node_features]\n",
        "print(sampled_data.num_features)\n",
        "print(pred.shape)\n",
        "# print(pred)"
      ],
      "metadata": {
        "id": "XyY4lr-uKvzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8021f12a-9b6a-4fdc-9996-57f46db7515f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([181])\n",
            "torch.Size([2, 173])\n",
            "torch.Size([181, 300])\n",
            "300\n",
            "torch.Size([8, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5xrK4zqxTxa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "jOLFOzfeUHsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jyUv2rGAUJ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU_module(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GRU_module, self).__init__()\n",
        "    self.hidden_size = 64\n",
        "    self.embedding_dim = 30\n",
        "    self.num_layer = 2\n",
        "    self.bidirectional = True\n",
        "    self.bi_num = 2 if self.bidirectional else 1\n",
        "    self.dropout = 0.5\n",
        "\n",
        "    self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layer, bidirectional=True, dropout=self.dropout)\n",
        "\n",
        "    self.FC = nn.Linear(128, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_0, c_0 = self.init_hidden_state(x.size(0))\n",
        "    x = torch.swapaxes(x, 0, 1)\n",
        "    x = x.to(torch.float32)\n",
        "    x, h_n = self.gru(x, h_0)\n",
        "    out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=-1)\n",
        "    out = self.FC(out)\n",
        "    return out\n",
        "  \n",
        "  def init_hidden_state(self, batch_size):\n",
        "    h_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    c_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    return h_0, c_0"
      ],
      "metadata": {
        "id": "smaSyHlLWiV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MzrMG3gsb52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "      self.gru_module = GRU_module()\n",
        "      self.gcn = GCN()\n",
        "      self.mlp_sec = nn.Sequential(\n",
        "          nn.Linear(41, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mlp = nn.Sequential(\n",
        "          # nn.Linear(192, 256),\n",
        "          # nn.BatchNorm1d(256),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(256, 128),\n",
        "          # nn.BatchNorm1d(128),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(128, 32),\n",
        "          # nn.BatchNorm1d(32),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(32, 1),\n",
        "          nn.Linear(192, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(128, 64),\n",
        "          # nn.BatchNorm1d(64),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1)\n",
        "      )\n",
        "      \n",
        "\n",
        "    def forward(self, data):\n",
        "      d_d = self.gcn(data.x.float(), data.edge_index, data.batch)\n",
        "      d_sec = self.mlp_sec(data.pre_hair_x)\n",
        "      d_mr = self.gru_module(data.mirna)\n",
        "      # print('x_d.shape',x_d.shape)\n",
        "      # print('x_mr.shape',x_mr.shape)\n",
        "      # print('d_d.shape',d_d.shape)\n",
        "      # print('d_mr.shape',d_mr.shape)\n",
        "      x = torch.cat((d_mr, d_sec, d_d), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.mlp(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "vcdHMse9qnog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = PYG_DataLoader(dataset, batch_size=8)\n",
        "sampled_data = next(iter(dataloader))\n",
        "model = Classificaion_module().cuda()\n",
        "sampled_data = sampled_data.cuda()\n",
        "\n",
        "pred = model(sampled_data) \n",
        "print(pred.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PCH18NwU2rx",
        "outputId": "ffcce094-88c3-466f-b54c-7a18634d6e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1])\n",
            "tensor([[ 0.5518],\n",
            "        [ 0.3503],\n",
            "        [ 0.7641],\n",
            "        [ 0.3930],\n",
            "        [ 0.6114],\n",
            "        [-0.3658],\n",
            "        [-0.0327],\n",
            "        [ 0.7592]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nZ8CMxQ7Spbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "ZWE0MkCwSpky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN"
      ],
      "metadata": {
        "id": "FHvZOL2LSpkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BifFtPZeSpkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_size = 300\n",
        "num_features = 300\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        # torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GCNConv(num_features, 256)\n",
        "        self.bn_1 = BatchNorm(256)\n",
        "        self.conv1 = GCNConv(256, 128)\n",
        "        self.bn_2 = BatchNorm(128)\n",
        "        self.conv2 = GCNConv(128, 128)\n",
        "        self.bn_3 = BatchNorm(128)\n",
        "        self.conv3 = GCNConv(128, 64)\n",
        "        self.bn_4 = BatchNorm(64)\n",
        "        # Output layer\n",
        "        self.out = Linear(128, 64)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        hidden = self.bn_1(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        hidden = self.bn_2(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = self.bn_3(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = self.bn_4(hidden)\n",
        "        hidden = torch.relu(hidden)\n",
        "        \n",
        "        # Global Pooling (stack different aggregations)\n",
        "        # hidden = gap(hidden, batch_index)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index),  gap(hidden, batch_index)], dim=1)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "Uuc8U4KTSpk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN()\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c96cb41-2429-4f32-f72e-f4eba07e9721",
        "id": "WhDRNXUfSpk0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (initial_conv): GCNConv(300, 256)\n",
            "  (bn_1): BatchNorm(256)\n",
            "  (conv1): GCNConv(256, 128)\n",
            "  (bn_2): BatchNorm(128)\n",
            "  (conv2): GCNConv(128, 128)\n",
            "  (bn_3): BatchNorm(128)\n",
            "  (conv3): GCNConv(128, 64)\n",
            "  (bn_4): BatchNorm(64)\n",
            "  (out): Linear(in_features=128, out_features=64, bias=True)\n",
            ")\n",
            "Number of parameters:  144128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = MeshDataset(edge_d, edge_attr_d, node_d, orig_x_disease)\n",
        "# graph_list = [dataset[i] for i in range(len(dataset))]"
      ],
      "metadata": {
        "id": "0ewLV994Spk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = PYG_DataLoader(dataset, batch_size=8)\n",
        "sampled_data = next(iter(dataloader))\n",
        "pred = model(sampled_data.x.float(), sampled_data.edge_index, sampled_data.batch) \n",
        "# print(sampled_data.batch)\n",
        "print(sampled_data.batch.shape)\n",
        "print(sampled_data.edge_index.shape) # [2, num_edges]\n",
        "print(sampled_data.x.shape) # [num_nodes, num_node_features]\n",
        "print(sampled_data.num_features)\n",
        "print(pred.shape)\n",
        "# print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "0ff924c6-c2e6-4a2b-c8af-0f45fa28c7a2",
        "id": "F2RneODtSpk1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-b3751157866c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPYG_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msampled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(sampled_data.batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "vQwHh3zNSpk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU_module(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GRU_module, self).__init__()\n",
        "    self.hidden_size = 64\n",
        "    self.embedding_dim = 30\n",
        "    self.num_layer = 2\n",
        "    self.bidirectional = True\n",
        "    self.bi_num = 2 if self.bidirectional else 1\n",
        "    self.dropout = 0.5\n",
        "\n",
        "    self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layer, bidirectional=True, dropout=self.dropout)\n",
        "\n",
        "    self.FC = nn.Linear(128, 64)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_0, c_0 = self.init_hidden_state(x.size(0))\n",
        "    x = torch.swapaxes(x, 0, 1)\n",
        "    x = x.to(torch.float32)\n",
        "    x, h_n = self.gru(x, h_0)\n",
        "    out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=-1)\n",
        "    out = self.FC(out)\n",
        "    return out\n",
        "  \n",
        "  def init_hidden_state(self, batch_size):\n",
        "    h_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    c_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to('cuda:0')\n",
        "    return h_0, c_0"
      ],
      "metadata": {
        "id": "desLw2p4Spk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "snFJJ8-bSpk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classificaion_module(nn.Module):\n",
        "    def __init__(self, tanhshrink_ratio=3):\n",
        "      super(Classificaion_module, self).__init__()\n",
        "      self.tanhshrink_ratio = tanhshrink_ratio\n",
        "      self.gru_module = GRU_module()\n",
        "      self.gcn = GCN()\n",
        "      self.mlp_sec = nn.Sequential(\n",
        "          nn.Linear(41, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      self.mlp = nn.Sequential(\n",
        "          # nn.Linear(192, 256),\n",
        "          # nn.BatchNorm1d(256),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(256, 128),\n",
        "          # nn.BatchNorm1d(128),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(128, 32),\n",
        "          # nn.BatchNorm1d(32),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(32, 1),\n",
        "          nn.Linear(192, 128),\n",
        "          nn.BatchNorm1d(128),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(128, 64),\n",
        "          # nn.BatchNorm1d(64),\n",
        "          # nn.ReLU(),\n",
        "          nn.Linear(128, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(),\n",
        "          # nn.Linear(32, 3)\n",
        "          nn.Linear(32, 2)\n",
        "      )\n",
        "      \n",
        "\n",
        "    def forward(self, data):\n",
        "      d_d = self.gcn(data.x.float(), data.edge_index, data.batch)\n",
        "      d_sec = self.mlp_sec(data.pre_hair_x)\n",
        "      d_mr = self.gru_module(data.mirna)\n",
        "      # print('x_d.shape',x_d.shape)\n",
        "      # print('x_mr.shape',x_mr.shape)\n",
        "      # print('d_d.shape',d_d.shape)\n",
        "      # print('d_mr.shape',d_mr.shape)\n",
        "      x = torch.cat((d_mr, d_sec, d_d), dim=1)\n",
        "      # print(x.shape)\n",
        "      x = self.mlp(x)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "5mF10CtoSpk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = PYG_DataLoader(dataset, batch_size=8)\n",
        "sampled_data = next(iter(dataloader))\n",
        "model = Classificaion_module().cuda()\n",
        "sampled_data = sampled_data.cuda()\n",
        "\n",
        "pred = model(sampled_data) \n",
        "print(pred.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d7a50f-f6e7-4d27-ae51-da801a6b4c08",
        "id": "GVD2HDH8Spk4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3])\n",
            "tensor([[ 0.0764, -0.1788, -0.0608],\n",
            "        [ 0.0938, -0.2538, -0.4597],\n",
            "        [ 0.2337, -0.6542,  0.4704],\n",
            "        [ 0.1186, -0.4740,  0.6490],\n",
            "        [ 0.0960, -0.4351,  0.6372],\n",
            "        [ 0.2298, -0.3791,  0.2051],\n",
            "        [ 0.4324, -0.1657,  0.3645],\n",
            "        [ 0.1528,  0.0500,  0.5265]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "nfY8fndjWr8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Circulation"
      ],
      "metadata": {
        "id": "gcGbCwOi9z6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 mer Regression"
      ],
      "metadata": {
        "id": "JP8RxcACSs6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e55J3nyzU4i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "_8H4CoiaqzHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afee768-f113-4fa2-f389-c62e243b7b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534f77da-2710-4785-9b75-cf5f73aabfd2",
        "id": "EvqV7tJaqzHI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "GpNlVVxVqzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c04368-b045-4e84-d78b-551e4547427e",
        "id": "Lcj2KO77qzHJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "Og92KpLhqzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "yQ--rb-1qzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(200)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad55804-400b-41da-9a95-96e55b255928",
        "id": "42BCuA6zqzHJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3296254351735115 | L1 Loss:0.4196109399199486 | R2:0.004306853342364324 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[0] Loss:0.3399969957768917 | L1 Loss:0.4142574042081833 | R2:-0.005120984381598881 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[1] Loss:0.3143345592543483 | L1 Loss:0.4138961136341095 | R2:0.0516866560065313 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[1] Loss:0.3359823912382126 | L1 Loss:0.415238182246685 | R2:0.028397673702505177 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[2] Loss:0.2969454349949956 | L1 Loss:0.40616719983518124 | R2:0.10757993295167004 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[2] Loss:0.33121492378413675 | L1 Loss:0.41696353554725646 | R2:0.041810402047088656 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[3] Loss:0.28797789569944143 | L1 Loss:0.39978105388581753 | R2:0.1313539077847189 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[3] Loss:0.31714696139097215 | L1 Loss:0.40736897587776183 | R2:0.08653116915736966 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[4] Loss:0.2937034321948886 | L1 Loss:0.40461462922394276 | R2:0.113826035565008 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[4] Loss:0.3242771830409765 | L1 Loss:0.41933524906635283 | R2:0.04744910593617909 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[5] Loss:0.27269609179347754 | L1 Loss:0.3959722276777029 | R2:0.1786145008893203 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[5] Loss:0.3054047837853432 | L1 Loss:0.4115792691707611 | R2:0.09612486841188633 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[6] Loss:0.2629864355549216 | L1 Loss:0.3821997959166765 | R2:0.20345157847539547 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[6] Loss:0.3083438850939274 | L1 Loss:0.41495600044727327 | R2:0.07976274955566673 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[7] Loss:0.2622698312625289 | L1 Loss:0.3837854042649269 | R2:0.20870071678327284 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[7] Loss:0.2978216990828514 | L1 Loss:0.40575005412101744 | R2:0.12398512256158152 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[8] Loss:0.25785098411142826 | L1 Loss:0.38414562307298183 | R2:0.21577799775997064 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[8] Loss:0.300031990557909 | L1 Loss:0.40817771255970003 | R2:0.1055713022667798 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[9] Loss:0.25377621315419674 | L1 Loss:0.38065260648727417 | R2:0.22640803218554112 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[9] Loss:0.28417536839842794 | L1 Loss:0.40035504996776583 | R2:0.13522632031163423 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[10] Loss:0.24295682180672884 | L1 Loss:0.38055297918617725 | R2:0.2591536399172148 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[10] Loss:0.29019022211432455 | L1 Loss:0.40589720010757446 | R2:0.09739797992916994 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[11] Loss:0.24426393117755651 | L1 Loss:0.38592609390616417 | R2:0.25491830884262795 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[11] Loss:0.2799809895455837 | L1 Loss:0.40241540521383284 | R2:0.15542278667731962 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[12] Loss:0.23835653997957706 | L1 Loss:0.37922370806336403 | R2:0.26928882872058146 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[12] Loss:0.2792442053556442 | L1 Loss:0.39435062408447263 | R2:0.10728962079104962 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[13] Loss:0.2424359880387783 | L1 Loss:0.38143178820610046 | R2:0.24806449577857342 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[13] Loss:0.27201291769742963 | L1 Loss:0.39950857758522035 | R2:0.14556016846978292 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[14] Loss:0.23185630980879068 | L1 Loss:0.37471180595457554 | R2:0.28077000185373385 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[14] Loss:0.280552351474762 | L1 Loss:0.4065214693546295 | R2:0.001068354337432198 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[15] Loss:0.2315006796270609 | L1 Loss:0.3757195584475994 | R2:0.2857899475555966 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[15] Loss:0.27349670976400375 | L1 Loss:0.4017858624458313 | R2:0.0910774761933673 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[16] Loss:0.23442580178380013 | L1 Loss:0.37330151349306107 | R2:0.2756775900172537 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[16] Loss:0.271575428545475 | L1 Loss:0.3973183065652847 | R2:0.10213952155980517 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[17] Loss:0.23832957539707422 | L1 Loss:0.37962526082992554 | R2:0.26644706043292454 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[17] Loss:0.2581130817532539 | L1 Loss:0.3892859399318695 | R2:0.1366913506702897 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[18] Loss:0.2252620654180646 | L1 Loss:0.36971089988946915 | R2:0.29782009840603435 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[18] Loss:0.25816304683685304 | L1 Loss:0.38664981424808503 | R2:0.13117403123650523 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[19] Loss:0.22248970065265894 | L1 Loss:0.3725962471216917 | R2:0.31365009357179846 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[19] Loss:0.2748323306441307 | L1 Loss:0.39930634796619413 | R2:0.1041482920841507 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[20] Loss:0.2294869478791952 | L1 Loss:0.37842169031500816 | R2:0.28856061295789803 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[20] Loss:0.281589449942112 | L1 Loss:0.40861374139785767 | R2:0.023142854489275666 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[21] Loss:0.23103732708841562 | L1 Loss:0.3829188942909241 | R2:0.2856165818180657 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[21] Loss:0.2690607443451881 | L1 Loss:0.39305061995983126 | R2:0.08667728732899094 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[22] Loss:0.24547186587005854 | L1 Loss:0.3909059502184391 | R2:0.23924756745583653 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[22] Loss:0.2624188005924225 | L1 Loss:0.39211089313030245 | R2:0.10458993343551137 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[23] Loss:0.23790911585092545 | L1 Loss:0.38295865431427956 | R2:0.2617881200893775 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[23] Loss:0.2647839620709419 | L1 Loss:0.39463455975055695 | R2:0.15198788727625023 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[24] Loss:0.24132018350064754 | L1 Loss:0.39316723495721817 | R2:0.24776358732056522 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[24] Loss:0.26334845572710036 | L1 Loss:0.3941794693470001 | R2:0.09772666703984267 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[25] Loss:0.22987259645015 | L1 Loss:0.3790876865386963 | R2:0.28652839107631456 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[25] Loss:0.26227190643548964 | L1 Loss:0.40021399557590487 | R2:0.08008073602707766 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[26] Loss:0.24183739628642797 | L1 Loss:0.3920405451208353 | R2:0.23765859873878595 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[26] Loss:0.268300823867321 | L1 Loss:0.41182430684566496 | R2:0.010512351905261008 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[27] Loss:0.2251952588558197 | L1 Loss:0.3748347070068121 | R2:0.29423226600542474 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[27] Loss:0.26570150405168536 | L1 Loss:0.4065246254205704 | R2:0.028971547628056628 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[28] Loss:0.23244967777282 | L1 Loss:0.3765881359577179 | R2:0.2660640227456178 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[28] Loss:0.271737776696682 | L1 Loss:0.398888373374939 | R2:0.09963462926091057 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[29] Loss:0.2331772744655609 | L1 Loss:0.3747181501239538 | R2:0.27426905609038815 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[29] Loss:0.2651686668395996 | L1 Loss:0.405582982301712 | R2:0.08302151768155897 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[30] Loss:0.24139620643109083 | L1 Loss:0.3819538988173008 | R2:0.24108155477045826 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[30] Loss:0.2734318867325783 | L1 Loss:0.40986343324184416 | R2:-0.004964306572344634 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[31] Loss:0.22780936723574996 | L1 Loss:0.3753809332847595 | R2:0.2854785165042214 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[31] Loss:0.274230495095253 | L1 Loss:0.41357086002826693 | R2:0.041123485363399895 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[32] Loss:0.23221099749207497 | L1 Loss:0.3794390391558409 | R2:0.2706887874344653 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[32] Loss:0.2897728428244591 | L1 Loss:0.4258828967809677 | R2:0.00649740149563256 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[33] Loss:0.22870124829933047 | L1 Loss:0.368757514283061 | R2:0.282621146889236 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[33] Loss:0.28204554319381714 | L1 Loss:0.42664546370506284 | R2:-0.06803385151243116 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[34] Loss:0.22271250188350677 | L1 Loss:0.3727548122406006 | R2:0.2961497383729045 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[34] Loss:0.26404029577970506 | L1 Loss:0.4068407893180847 | R2:0.04906420115152148 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[35] Loss:0.22022556513547897 | L1 Loss:0.3634347654879093 | R2:0.304470478624362 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[35] Loss:0.28075208216905595 | L1 Loss:0.4158394157886505 | R2:0.006670166058237825 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[36] Loss:0.21331898029893637 | L1 Loss:0.3586149029433727 | R2:0.32613995675526414 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[36] Loss:0.27628555446863173 | L1 Loss:0.4116987258195877 | R2:-0.07713206332205838 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[37] Loss:0.22395383194088936 | L1 Loss:0.3718669358640909 | R2:0.2849060231712543 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[37] Loss:0.27426848858594893 | L1 Loss:0.40535837709903716 | R2:-0.07466467306935928 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[38] Loss:0.22197094559669495 | L1 Loss:0.37064291909337044 | R2:0.29377532149024455 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[38] Loss:0.2826380431652069 | L1 Loss:0.4136990547180176 | R2:-0.06753780633289175 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[39] Loss:0.227225411683321 | L1 Loss:0.37717052549123764 | R2:0.28823232302888524 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[39] Loss:0.2721856489777565 | L1 Loss:0.40044246017932894 | R2:-0.09161919819238165 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[40] Loss:0.22847493272274733 | L1 Loss:0.3744868282228708 | R2:0.27771521249068365 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[40] Loss:0.2670182138681412 | L1 Loss:0.39979431331157683 | R2:-0.009362157344484113 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[41] Loss:0.22439967561513186 | L1 Loss:0.3682191763073206 | R2:0.28049293428880706 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[41] Loss:0.2697265326976776 | L1 Loss:0.38873541355133057 | R2:-0.05638511088399918 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[42] Loss:0.21095296368002892 | L1 Loss:0.36105410009622574 | R2:0.33259998815702657 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[42] Loss:0.2672200739383698 | L1 Loss:0.3987687200307846 | R2:-0.11377551712659417 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[43] Loss:0.20819106232374907 | L1 Loss:0.3583377208560705 | R2:0.34083894128581615 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[43] Loss:0.2709537610411644 | L1 Loss:0.40547938644886017 | R2:-0.07370187349738293 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[44] Loss:0.21471489779651165 | L1 Loss:0.3637805189937353 | R2:0.31859644567551665 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[44] Loss:0.2700744688510895 | L1 Loss:0.3983111321926117 | R2:-0.12568254012517383 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[45] Loss:0.21828235127031803 | L1 Loss:0.3702671341598034 | R2:0.31035552402715977 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[45] Loss:0.26401531249284743 | L1 Loss:0.3970141768455505 | R2:-0.10906655257899303 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[46] Loss:0.23570883041247725 | L1 Loss:0.3774340096861124 | R2:0.26202607803096 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[46] Loss:0.27074802219867705 | L1 Loss:0.4105977416038513 | R2:-0.07055387245564 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[47] Loss:0.21979546546936035 | L1 Loss:0.3666223995387554 | R2:0.31504673026160124 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[47] Loss:0.2702622011303902 | L1 Loss:0.4073637545108795 | R2:-0.02564580855016123 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[48] Loss:0.23268951289355755 | L1 Loss:0.3780253864824772 | R2:0.26695375759840523 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[48] Loss:0.258462493121624 | L1 Loss:0.40360223650932314 | R2:0.005987690275444724 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[49] Loss:0.22431340720504522 | L1 Loss:0.37152416445314884 | R2:0.28599920196187856 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[49] Loss:0.2512748628854752 | L1 Loss:0.39842215180397034 | R2:-0.021987711257619936 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[50] Loss:0.23467267956584692 | L1 Loss:0.3783695660531521 | R2:0.25453843184973896 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[50] Loss:0.2653921008110046 | L1 Loss:0.4089005708694458 | R2:-0.14555884741167216 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[51] Loss:0.23279014602303505 | L1 Loss:0.38057032600045204 | R2:0.272414827144478 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[51] Loss:0.2615174427628517 | L1 Loss:0.40635415017604826 | R2:-0.13936179922914177 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[52] Loss:0.23098673205822706 | L1 Loss:0.3801701832562685 | R2:0.2656996522094219 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[52] Loss:0.2533126771450043 | L1 Loss:0.39890651404857635 | R2:0.0077312580889282055 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[53] Loss:0.23562518134713173 | L1 Loss:0.3808254189789295 | R2:0.25015929707130374 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[53] Loss:0.2559247761964798 | L1 Loss:0.4035760223865509 | R2:-0.011587994770466592 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[54] Loss:0.21239279676228762 | L1 Loss:0.3617365211248398 | R2:0.3337208734252466 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[54] Loss:0.2564786016941071 | L1 Loss:0.3983342111110687 | R2:-0.06896619376278981 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[55] Loss:0.21691265376284719 | L1 Loss:0.36822385154664516 | R2:0.3202619087834465 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[55] Loss:0.2541067719459534 | L1 Loss:0.40342149436473845 | R2:-0.015397077056755127 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[56] Loss:0.2278960170224309 | L1 Loss:0.3735077492892742 | R2:0.2907006123934304 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[56] Loss:0.26805727928876877 | L1 Loss:0.4154013395309448 | R2:-0.2274305781620028 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[57] Loss:0.212089697830379 | L1 Loss:0.3629046902060509 | R2:0.33499237677262117 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[57] Loss:0.28426468223333357 | L1 Loss:0.42025556564331057 | R2:-0.34292053793629845 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[58] Loss:0.21166426688432693 | L1 Loss:0.3605423625558615 | R2:0.3307137243150681 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[58] Loss:0.261237707734108 | L1 Loss:0.4003205537796021 | R2:-0.1845250247321605 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[59] Loss:0.21599331684410572 | L1 Loss:0.3668721802532673 | R2:0.3200838034101315 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[59] Loss:0.2576099023222923 | L1 Loss:0.4057181984186172 | R2:-0.05147472471323864 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[60] Loss:0.205868701916188 | L1 Loss:0.35170053504407406 | R2:0.35518557391041705 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[60] Loss:0.25136862099170687 | L1 Loss:0.3936184287071228 | R2:-0.11312533326154846 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[61] Loss:0.20340612484142184 | L1 Loss:0.35342616960406303 | R2:0.35662638867750773 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[61] Loss:0.2657588556408882 | L1 Loss:0.4063467293977737 | R2:-0.10981203689485883 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[62] Loss:0.20570515422150493 | L1 Loss:0.35104035399854183 | R2:0.3392602648872997 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[62] Loss:0.25053214430809023 | L1 Loss:0.39411253333091734 | R2:-0.06003244132654313 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[63] Loss:0.1995795746333897 | L1 Loss:0.3496712725609541 | R2:0.3685625932952281 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[63] Loss:0.25038124322891236 | L1 Loss:0.39395267367362974 | R2:-0.036676883487197554 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[64] Loss:0.20868712291121483 | L1 Loss:0.3539031073451042 | R2:0.3395649493891166 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[64] Loss:0.2577328622341156 | L1 Loss:0.3975537300109863 | R2:-0.14937111836409317 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[65] Loss:0.2136736772954464 | L1 Loss:0.36257123574614525 | R2:0.32361682877904296 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[65] Loss:0.25377798825502396 | L1 Loss:0.382728374004364 | R2:-0.07785515042393829 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[66] Loss:0.21051744557917118 | L1 Loss:0.3581484016031027 | R2:0.32879318626742576 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[66] Loss:0.2505942076444626 | L1 Loss:0.39073833227157595 | R2:-0.0861768933951681 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[67] Loss:0.1943911616690457 | L1 Loss:0.3413666561245918 | R2:0.38894697106745846 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[67] Loss:0.2570167511701584 | L1 Loss:0.39376962184906006 | R2:-0.1356044149917036 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[68] Loss:0.20128227770328522 | L1 Loss:0.3469811361283064 | R2:0.3671721369758901 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[68] Loss:0.25984683334827424 | L1 Loss:0.39620618522167206 | R2:-0.09241658076598211 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[69] Loss:0.1894498597830534 | L1 Loss:0.33645425364375114 | R2:0.4050616633702305 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[69] Loss:0.2488752484321594 | L1 Loss:0.39263927936553955 | R2:-0.09111123512520111 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[70] Loss:0.2017202554270625 | L1 Loss:0.3481245916336775 | R2:0.3509953086191981 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[70] Loss:0.2517008751630783 | L1 Loss:0.39777455627918246 | R2:-0.11226274830039315 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[71] Loss:0.18262996058911085 | L1 Loss:0.3323684874922037 | R2:0.4257844286537448 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[71] Loss:0.2683027505874634 | L1 Loss:0.4097653716802597 | R2:-0.1558729754653579 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[72] Loss:0.19318900164216757 | L1 Loss:0.3369259536266327 | R2:0.39260802239742304 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[72] Loss:0.265778674185276 | L1 Loss:0.4040511131286621 | R2:-0.16685866831238885 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[73] Loss:0.19559209886938334 | L1 Loss:0.34410815965384245 | R2:0.3846119808122382 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[73] Loss:0.26013476103544236 | L1 Loss:0.39638465642929077 | R2:-0.17931568911510673 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[74] Loss:0.1945292390882969 | L1 Loss:0.3368349075317383 | R2:0.3862529909429629 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[74] Loss:0.26986427903175353 | L1 Loss:0.40348453223705294 | R2:-0.21809415680553315 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[75] Loss:0.21415710635483265 | L1 Loss:0.35693821124732494 | R2:0.3161651031759636 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[75] Loss:0.2437156081199646 | L1 Loss:0.38310587108135224 | R2:0.003726530049651 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[76] Loss:0.196671427693218 | L1 Loss:0.34415117278695107 | R2:0.3753090787941018 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[76] Loss:0.24763444662094117 | L1 Loss:0.38684681355953215 | R2:-0.06223439099339072 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[77] Loss:0.1975696124136448 | L1 Loss:0.346567215397954 | R2:0.36940125524873646 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[77] Loss:0.23443384021520614 | L1 Loss:0.38022182881832123 | R2:0.00813537164130813 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[78] Loss:0.1974705383181572 | L1 Loss:0.34409172274172306 | R2:0.3693030230505549 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[78] Loss:0.2693298816680908 | L1 Loss:0.40393536984920503 | R2:-0.14019022129454792 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[79] Loss:0.20165390986949205 | L1 Loss:0.350618664175272 | R2:0.3625076542802345 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[79] Loss:0.2635445982217789 | L1 Loss:0.4032857120037079 | R2:-0.16941591961066038 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[80] Loss:0.20601312536746264 | L1 Loss:0.35261095501482487 | R2:0.34753056447998953 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[80] Loss:0.2619158074259758 | L1 Loss:0.40158946216106417 | R2:-0.18754863848339326 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[81] Loss:0.21830840315669775 | L1 Loss:0.36211929097771645 | R2:0.30580061324687935 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[81] Loss:0.26258105635643003 | L1 Loss:0.3958979308605194 | R2:-0.17994746985828197 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[82] Loss:0.2127960417419672 | L1 Loss:0.35531700029969215 | R2:0.3279051330010263 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[82] Loss:0.25107661485671995 | L1 Loss:0.38423742949962614 | R2:-0.06437815875554742 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[83] Loss:0.2111806431785226 | L1 Loss:0.35864052549004555 | R2:0.3268654139059052 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[83] Loss:0.2535670563578606 | L1 Loss:0.3881229192018509 | R2:-0.09084909213454688 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[84] Loss:0.2155171036720276 | L1 Loss:0.3569618947803974 | R2:0.31598209011135975 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[84] Loss:0.254886668920517 | L1 Loss:0.3952104240655899 | R2:-0.09690358423805216 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[85] Loss:0.20077538769692183 | L1 Loss:0.34786084294319153 | R2:0.3620312555184897 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[85] Loss:0.24448743909597398 | L1 Loss:0.38449235558509826 | R2:-0.013188205676146047 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[86] Loss:0.2088598357513547 | L1 Loss:0.35828652046620846 | R2:0.33022336196962765 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[86] Loss:0.24035694003105162 | L1 Loss:0.37806341648101804 | R2:-0.020476607523511083 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[87] Loss:0.19139389786869287 | L1 Loss:0.3408808037638664 | R2:0.3927362281349456 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[87] Loss:0.24533158540725708 | L1 Loss:0.386537104845047 | R2:-0.04492937823891587 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[88] Loss:0.20398839749395847 | L1 Loss:0.35482815839350224 | R2:0.34970052034285387 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[88] Loss:0.2582665517926216 | L1 Loss:0.404383584856987 | R2:-0.1846661308046175 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[89] Loss:0.213503603823483 | L1 Loss:0.35998948477208614 | R2:0.31187729668575465 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[89] Loss:0.24587473273277283 | L1 Loss:0.3882334679365158 | R2:-0.06265782428036223 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[90] Loss:0.20700987288728356 | L1 Loss:0.3534387703984976 | R2:0.33997199362509567 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[90] Loss:0.243800613284111 | L1 Loss:0.39091330766677856 | R2:-0.10877391077261889 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[91] Loss:0.2042462876997888 | L1 Loss:0.3537631183862686 | R2:0.34193952023923946 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[91] Loss:0.23420182168483733 | L1 Loss:0.3895130425691605 | R2:-0.019269746347735907 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[92] Loss:0.1984350960701704 | L1 Loss:0.34375216625630856 | R2:0.3616865842666307 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[92] Loss:0.23587813079357148 | L1 Loss:0.383453431725502 | R2:0.036931613282132146 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[93] Loss:0.20795916393399239 | L1 Loss:0.34815334901213646 | R2:0.3358503093853673 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[93] Loss:0.22628291547298432 | L1 Loss:0.37377047836780547 | R2:0.0958298972810395 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[94] Loss:0.2034721989184618 | L1 Loss:0.3491386380046606 | R2:0.3564579777487163 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[94] Loss:0.2453520879149437 | L1 Loss:0.3861427128314972 | R2:-0.029003150580230487 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[95] Loss:0.20897466596215963 | L1 Loss:0.35696629248559475 | R2:0.33754819991091534 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[95] Loss:0.22419714778661728 | L1 Loss:0.3739503473043442 | R2:0.08412093932389389 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[96] Loss:0.19860492832958698 | L1 Loss:0.3430492617189884 | R2:0.36869745262881226 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[96] Loss:0.2203592985868454 | L1 Loss:0.36857505440711974 | R2:0.0519221594710523 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[97] Loss:0.19473350420594215 | L1 Loss:0.33861696906387806 | R2:0.38025076499921756 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[97] Loss:0.2336642012000084 | L1 Loss:0.37998571395874026 | R2:-0.049116207253799796 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[98] Loss:0.1931802136823535 | L1 Loss:0.3407818730920553 | R2:0.38584319189833954 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[98] Loss:0.22590822726488113 | L1 Loss:0.37769546210765836 | R2:0.017969774833465735 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[99] Loss:0.20335367880761623 | L1 Loss:0.3451413344591856 | R2:0.34329006921586713 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[99] Loss:0.23410993367433547 | L1 Loss:0.3809837013483047 | R2:0.0039027324963213593 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[100] Loss:0.19997838232666254 | L1 Loss:0.3470175825059414 | R2:0.3568162304317195 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[100] Loss:0.23551167994737626 | L1 Loss:0.3847134649753571 | R2:0.006022513524105965 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[101] Loss:0.20859671104699373 | L1 Loss:0.35756104439496994 | R2:0.3344288909986627 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[101] Loss:0.21633267253637314 | L1 Loss:0.36574083268642427 | R2:0.07684114381755429 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[102] Loss:0.19151919335126877 | L1 Loss:0.3379427623003721 | R2:0.38389230565404453 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[102] Loss:0.23616336062550544 | L1 Loss:0.3798920810222626 | R2:0.08143824646042001 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[103] Loss:0.1985283773392439 | L1 Loss:0.3386306017637253 | R2:0.36091069973345435 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[103] Loss:0.23001716881990433 | L1 Loss:0.3821467012166977 | R2:0.055718100687387785 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[104] Loss:0.1996227614581585 | L1 Loss:0.3420384917408228 | R2:0.3558549557818321 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[104] Loss:0.22829024121165276 | L1 Loss:0.3756955459713936 | R2:0.07786801979046051 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[105] Loss:0.19678079709410667 | L1 Loss:0.34562043845653534 | R2:0.37363181906719567 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[105] Loss:0.2280684605240822 | L1 Loss:0.3825815230607986 | R2:0.04119041797058927 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[106] Loss:0.1952175348997116 | L1 Loss:0.34161724895238876 | R2:0.3715319786541941 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[106] Loss:0.23322077989578247 | L1 Loss:0.3875553965568542 | R2:0.002568950187461905 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[107] Loss:0.1861212132498622 | L1 Loss:0.3393206521868706 | R2:0.4005975440689154 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[107] Loss:0.22569470703601838 | L1 Loss:0.3731609761714935 | R2:0.06957503084341465 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[108] Loss:0.1943103726953268 | L1 Loss:0.34589115530252457 | R2:0.37378917555582275 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[108] Loss:0.21942102313041686 | L1 Loss:0.37012421786785127 | R2:0.10664841376227327 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[109] Loss:0.19378329813480377 | L1 Loss:0.3371041268110275 | R2:0.37928719093353785 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[109] Loss:0.23090270310640335 | L1 Loss:0.3793043941259384 | R2:-0.013879065849950667 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[110] Loss:0.18840699549764395 | L1 Loss:0.3336411714553833 | R2:0.3920688604798911 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[110] Loss:0.22893106192350388 | L1 Loss:0.381673538684845 | R2:-0.0018053934828994756 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[111] Loss:0.1970996968448162 | L1 Loss:0.34783639945089817 | R2:0.3726652568162353 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[111] Loss:0.2368098847568035 | L1 Loss:0.39199924767017363 | R2:-0.03782692312321543 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[112] Loss:0.18471235781908035 | L1 Loss:0.33122288808226585 | R2:0.4162587926769461 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[112] Loss:0.21994187980890273 | L1 Loss:0.3751146733760834 | R2:0.10767615798575769 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[113] Loss:0.18646399024873972 | L1 Loss:0.3259563725441694 | R2:0.4042111876390423 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[113] Loss:0.21186127364635468 | L1 Loss:0.367697548866272 | R2:0.16303259114678456 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[114] Loss:0.20005754474550486 | L1 Loss:0.34442429803311825 | R2:0.3621566933504592 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[114] Loss:0.23009276986122132 | L1 Loss:0.38087025880813596 | R2:-0.018835777519424425 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[115] Loss:0.19168833084404469 | L1 Loss:0.3412946090102196 | R2:0.39101540582453265 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[115] Loss:0.22625109702348709 | L1 Loss:0.37757526636123656 | R2:0.04732472843259163 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[116] Loss:0.1947271851822734 | L1 Loss:0.3386636935174465 | R2:0.38783411002103263 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[116] Loss:0.22525091767311095 | L1 Loss:0.3822179913520813 | R2:0.035997491339270216 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[117] Loss:0.19692614022642374 | L1 Loss:0.3382932413369417 | R2:0.37787500001780433 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[117] Loss:0.23948462903499604 | L1 Loss:0.38696602582931516 | R2:-0.021349425818744595 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[118] Loss:0.19224785640835762 | L1 Loss:0.33546600677073 | R2:0.3894215311012318 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[118] Loss:0.2276524730026722 | L1 Loss:0.37790735363960265 | R2:0.018598620507714214 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[119] Loss:0.19793555233627558 | L1 Loss:0.3433750905096531 | R2:0.37206734453056045 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[119] Loss:0.23132158815860748 | L1 Loss:0.3768292307853699 | R2:0.04853685246290738 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[120] Loss:0.1981717413291335 | L1 Loss:0.3417010623961687 | R2:0.3641549200110938 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[120] Loss:0.22598531544208528 | L1 Loss:0.3725235790014267 | R2:0.091565692667074 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[121] Loss:0.20282546430826187 | L1 Loss:0.3499372135847807 | R2:0.355117852855969 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[121] Loss:0.23119453340768814 | L1 Loss:0.37741456627845765 | R2:-0.008635193421057341 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[122] Loss:0.19842910300940275 | L1 Loss:0.3430321402847767 | R2:0.3721195092960222 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[122] Loss:0.2298460692167282 | L1 Loss:0.37132092416286466 | R2:-0.009237534803722403 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[123] Loss:0.19274466391652822 | L1 Loss:0.33854122646152973 | R2:0.3875435489973296 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[123] Loss:0.22485439628362655 | L1 Loss:0.37357168793678286 | R2:0.10608725726078902 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[124] Loss:0.20474448800086975 | L1 Loss:0.347599521279335 | R2:0.3435821080618364 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[124] Loss:0.23702210262417794 | L1 Loss:0.3821491912007332 | R2:0.031620667625892815 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[125] Loss:0.18921236135065556 | L1 Loss:0.33569007739424706 | R2:0.39463216242339527 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[125] Loss:0.22740356996655464 | L1 Loss:0.3726381778717041 | R2:0.11115763987386398 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[126] Loss:0.1950305625796318 | L1 Loss:0.340190589427948 | R2:0.3810478750572015 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[126] Loss:0.23481981158256532 | L1 Loss:0.3829813450574875 | R2:-0.03470803212414388 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[127] Loss:0.19137737713754177 | L1 Loss:0.33423832803964615 | R2:0.3908656755990181 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[127] Loss:0.21895873323082923 | L1 Loss:0.36273048222064974 | R2:0.1358479347609456 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[128] Loss:0.18327741138637066 | L1 Loss:0.3226562365889549 | R2:0.41885035913971086 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[128] Loss:0.21254067420959472 | L1 Loss:0.3563309073448181 | R2:0.1625444561657073 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[129] Loss:0.18373161274939775 | L1 Loss:0.3286452814936638 | R2:0.41611060957105345 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[129] Loss:0.2255066603422165 | L1 Loss:0.3752483606338501 | R2:0.07049257878143797 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[130] Loss:0.19543044548481703 | L1 Loss:0.3372851237654686 | R2:0.38340180509607574 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[130] Loss:0.21693962067365646 | L1 Loss:0.37174178957939147 | R2:0.09646047557298698 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[131] Loss:0.18372905626893044 | L1 Loss:0.33077922835946083 | R2:0.41565830334697806 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[131] Loss:0.22562415301799774 | L1 Loss:0.37564446330070494 | R2:0.06236663004401586 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[132] Loss:0.18417619168758392 | L1 Loss:0.331869650632143 | R2:0.41631361375191894 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[132] Loss:0.2311454251408577 | L1 Loss:0.373234298825264 | R2:-0.02666844813831557 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[133] Loss:0.1845894455909729 | L1 Loss:0.32744927145540714 | R2:0.4176530350539305 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[133] Loss:0.22062019258737564 | L1 Loss:0.3691971778869629 | R2:0.07998789479114868 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[134] Loss:0.19198098313063383 | L1 Loss:0.33815633319318295 | R2:0.3869598049339224 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[134] Loss:0.22831363528966903 | L1 Loss:0.37233637273311615 | R2:0.057759307919250255 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[135] Loss:0.18784878263249993 | L1 Loss:0.3274166341871023 | R2:0.399712825418289 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[135] Loss:0.22752808928489685 | L1 Loss:0.3753012716770172 | R2:0.05369439849720705 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[136] Loss:0.19079369492828846 | L1 Loss:0.33819805830717087 | R2:0.39738740148450225 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[136] Loss:0.2300588980317116 | L1 Loss:0.37848607301712034 | R2:0.02292747870852572 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[137] Loss:0.18962874775752425 | L1 Loss:0.3372882381081581 | R2:0.40032293979927275 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[137] Loss:0.2359587885439396 | L1 Loss:0.38218180537223817 | R2:0.0959324065513063 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[138] Loss:0.186689720954746 | L1 Loss:0.32947084680199623 | R2:0.40895032945326937 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[138] Loss:0.22860134169459342 | L1 Loss:0.3719244062900543 | R2:0.06848758388208402 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[139] Loss:0.1923976168036461 | L1 Loss:0.334061935544014 | R2:0.3782239015839277 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[139] Loss:0.21293383464217186 | L1 Loss:0.3644735336303711 | R2:0.19649911075023888 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[140] Loss:0.193254463840276 | L1 Loss:0.33341892063617706 | R2:0.37967311898706335 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[140] Loss:0.22190464437007903 | L1 Loss:0.37044805884361265 | R2:0.13539568717108558 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[141] Loss:0.18776019755750895 | L1 Loss:0.3351184483617544 | R2:0.40583628971034696 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[141] Loss:0.21279566287994384 | L1 Loss:0.35993788540363314 | R2:0.09954830924388762 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[142] Loss:0.1874199598096311 | L1 Loss:0.32861958630383015 | R2:0.4033240114857506 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[142] Loss:0.20303244218230249 | L1 Loss:0.35075069665908815 | R2:0.16308228894210827 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[143] Loss:0.20272202184423804 | L1 Loss:0.3428029790520668 | R2:0.35726093465702635 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[143] Loss:0.22336710691452027 | L1 Loss:0.37304768413305284 | R2:0.06976524059365205 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[144] Loss:0.18757406901568174 | L1 Loss:0.33542306534945965 | R2:0.4015868688282777 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[144] Loss:0.2253894180059433 | L1 Loss:0.3731694847345352 | R2:0.044475212083524604 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[145] Loss:0.18346030730754137 | L1 Loss:0.33255874924361706 | R2:0.42014506178754485 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[145] Loss:0.213186876475811 | L1 Loss:0.36350185573101046 | R2:0.14365034322311146 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[146] Loss:0.19062191247940063 | L1 Loss:0.33814193680882454 | R2:0.39139158752844116 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[146] Loss:0.21026307791471482 | L1 Loss:0.3604844182729721 | R2:0.1304352859236447 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[147] Loss:0.19089407054707408 | L1 Loss:0.34105752035975456 | R2:0.3974646001671842 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[147] Loss:0.22730296701192856 | L1 Loss:0.3670147180557251 | R2:0.0376269254204864 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[148] Loss:0.19226012006402016 | L1 Loss:0.3437026161700487 | R2:0.3824750852686296 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[148] Loss:0.21355398148298263 | L1 Loss:0.3631781965494156 | R2:0.1150930854901949 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[149] Loss:0.1907387189567089 | L1 Loss:0.3360300660133362 | R2:0.39786706877412326 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[149] Loss:0.21916665583848954 | L1 Loss:0.3653746247291565 | R2:0.10495697120701974 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[150] Loss:0.19221737701445818 | L1 Loss:0.3316049613058567 | R2:0.3858329516293657 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[150] Loss:0.21754493564367294 | L1 Loss:0.36446482837200167 | R2:0.08919348775790077 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[151] Loss:0.18675271747633815 | L1 Loss:0.33461362682282925 | R2:0.4047843527236208 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[151] Loss:0.21233559399843216 | L1 Loss:0.369433668255806 | R2:0.14201542796118688 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[152] Loss:0.18307252321392298 | L1 Loss:0.3313378430902958 | R2:0.4155744708238577 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[152] Loss:0.21902773082256316 | L1 Loss:0.36963153779506686 | R2:0.17305597200515405 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[153] Loss:0.18687562132254243 | L1 Loss:0.33769870921969414 | R2:0.4079662302314321 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[153] Loss:0.22244053706526756 | L1 Loss:0.37052049934864045 | R2:0.050241380773021135 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[154] Loss:0.18658420536667109 | L1 Loss:0.33064040914177895 | R2:0.40576767647230555 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[154] Loss:0.2200247436761856 | L1 Loss:0.3699145019054413 | R2:0.13126165306272247 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[155] Loss:0.18738224636763334 | L1 Loss:0.3345951046794653 | R2:0.40848460750958726 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[155] Loss:0.2210286259651184 | L1 Loss:0.3679726004600525 | R2:0.11847417602135857 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[156] Loss:0.19002343714237213 | L1 Loss:0.33476847037672997 | R2:0.39598838587403085 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[156] Loss:0.22158498615026473 | L1 Loss:0.36862511932849884 | R2:0.07202382739873787 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[157] Loss:0.19114490551874042 | L1 Loss:0.3329754192382097 | R2:0.395323591524833 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[157] Loss:0.21913149505853652 | L1 Loss:0.37149629294872283 | R2:0.09227603699662149 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[158] Loss:0.19394862418994308 | L1 Loss:0.34199741296470165 | R2:0.3825668941561816 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[158] Loss:0.20870010256767274 | L1 Loss:0.36106460094451903 | R2:0.20665463651278512 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[159] Loss:0.19453924428671598 | L1 Loss:0.34027754329144955 | R2:0.3755767320700601 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[159] Loss:0.22263813018798828 | L1 Loss:0.36897504329681396 | R2:0.14199851075668796 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[160] Loss:0.18930571153759956 | L1 Loss:0.3331070505082607 | R2:0.3923113671081905 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[160] Loss:0.21337335407733918 | L1 Loss:0.3598228573799133 | R2:0.14708441247391862 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[161] Loss:0.19407886173576117 | L1 Loss:0.3359682969748974 | R2:0.3769460144677124 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[161] Loss:0.21402304768562316 | L1 Loss:0.36425513625144956 | R2:0.18412128424542584 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[162] Loss:0.18474552175030112 | L1 Loss:0.33136823400855064 | R2:0.4103845617100675 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[162] Loss:0.21879338547587396 | L1 Loss:0.366450771689415 | R2:0.12074756057585141 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[163] Loss:0.18617603462189436 | L1 Loss:0.3315091449767351 | R2:0.40512052324151715 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[163] Loss:0.2119999498128891 | L1 Loss:0.3663319796323776 | R2:0.15582060343597132 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[164] Loss:0.19218685571104288 | L1 Loss:0.33625063486397266 | R2:0.38642443577689134 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[164] Loss:0.21109917461872102 | L1 Loss:0.3617849797010422 | R2:0.15182024661054108 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[165] Loss:0.19321262696757913 | L1 Loss:0.33810629695653915 | R2:0.38452358626582284 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[165] Loss:0.20747668594121932 | L1 Loss:0.35543409585952757 | R2:0.16455826127748452 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[166] Loss:0.1817279290407896 | L1 Loss:0.3270806074142456 | R2:0.4213476722467173 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[166] Loss:0.22476999163627626 | L1 Loss:0.3748910754919052 | R2:0.07533828205785631 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[167] Loss:0.1884494535624981 | L1 Loss:0.3342558601871133 | R2:0.40072794396962697 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[167] Loss:0.22028395533561707 | L1 Loss:0.3707153260707855 | R2:0.09489142531875458 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[168] Loss:0.1782925925217569 | L1 Loss:0.32780327275395393 | R2:0.43693934167644766 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[168] Loss:0.20808248072862626 | L1 Loss:0.3600267797708511 | R2:0.18901707337288376 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[169] Loss:0.1812161742709577 | L1 Loss:0.3266574013978243 | R2:0.4274682434498479 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[169] Loss:0.21525256037712098 | L1 Loss:0.3631667077541351 | R2:0.1811059242060348 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[170] Loss:0.18612039182335138 | L1 Loss:0.33492714539170265 | R2:0.41241713507349304 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[170] Loss:0.2189696177840233 | L1 Loss:0.3732584238052368 | R2:0.16067879920793976 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[171] Loss:0.18338393094018102 | L1 Loss:0.3328200411051512 | R2:0.4096295199667962 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[171] Loss:0.21001278832554818 | L1 Loss:0.36295250058174133 | R2:0.2145722210029589 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[172] Loss:0.18653934728354216 | L1 Loss:0.3326473478227854 | R2:0.4031800838238798 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[172] Loss:0.20618061423301698 | L1 Loss:0.35478740334510805 | R2:0.2141284356599666 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[173] Loss:0.1814790042117238 | L1 Loss:0.32954167015850544 | R2:0.42093333667323873 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[173] Loss:0.22227704375982285 | L1 Loss:0.37103596031665803 | R2:0.11984134607387142 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[174] Loss:0.18369599198922515 | L1 Loss:0.33076145127415657 | R2:0.41829012083633293 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[174] Loss:0.20522329807281495 | L1 Loss:0.346773961186409 | R2:0.25662598601705594 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[175] Loss:0.17496788688004017 | L1 Loss:0.3254005163908005 | R2:0.44646829611404326 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[175] Loss:0.2046014204621315 | L1 Loss:0.3537438154220581 | R2:0.1898494957846142 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[176] Loss:0.18233685102313757 | L1 Loss:0.3293995186686516 | R2:0.4153992075669038 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[176] Loss:0.2099607393145561 | L1 Loss:0.3553656667470932 | R2:0.19483988319276105 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[177] Loss:0.18579118559136987 | L1 Loss:0.33109762892127037 | R2:0.4090373089029269 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[177] Loss:0.2126417115330696 | L1 Loss:0.3624517098069191 | R2:0.14602966291063332 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[178] Loss:0.17675346555188298 | L1 Loss:0.32816330529749393 | R2:0.44145039887271814 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[178] Loss:0.20341480076313018 | L1 Loss:0.3540895700454712 | R2:0.18960991057689713 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[179] Loss:0.18607442500069737 | L1 Loss:0.33564145117998123 | R2:0.4090190813499485 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[179] Loss:0.20826613456010817 | L1 Loss:0.3617632299661636 | R2:0.16825421746575206 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[180] Loss:0.17700938694179058 | L1 Loss:0.3269849009811878 | R2:0.43910581717666486 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[180] Loss:0.20579032599925995 | L1 Loss:0.35707288682460786 | R2:0.2078690706662482 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[181] Loss:0.18729256745427847 | L1 Loss:0.33234808407723904 | R2:0.40025936452380706 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[181] Loss:0.2036367490887642 | L1 Loss:0.35255122780799864 | R2:0.2085483766225747 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[182] Loss:0.183368728030473 | L1 Loss:0.3285383749753237 | R2:0.4166945874484417 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[182] Loss:0.19964195787906647 | L1 Loss:0.35375154614448545 | R2:0.25553085613620485 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[183] Loss:0.18258208828046918 | L1 Loss:0.328390846028924 | R2:0.42176766347385664 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[183] Loss:0.20379514694213868 | L1 Loss:0.3541657507419586 | R2:0.21587892454510565 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[184] Loss:0.18134581111371517 | L1 Loss:0.3309138659387827 | R2:0.4265964584111469 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[184] Loss:0.20516592189669608 | L1 Loss:0.3557006925344467 | R2:0.2005712160386087 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[185] Loss:0.18306965846568346 | L1 Loss:0.3275331426411867 | R2:0.4228023021693738 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[185] Loss:0.21541527584195136 | L1 Loss:0.37118864357471465 | R2:0.12490597155659663 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[186] Loss:0.1821247790940106 | L1 Loss:0.3290718551725149 | R2:0.42230960854476374 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[186] Loss:0.20722756087779998 | L1 Loss:0.36314005553722384 | R2:0.1721427163402931 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[187] Loss:0.1758320932276547 | L1 Loss:0.32290216349065304 | R2:0.4421936697190945 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[187] Loss:0.21043796390295028 | L1 Loss:0.36659455895423887 | R2:0.1756048249560999 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[188] Loss:0.18342440063133836 | L1 Loss:0.3297082222998142 | R2:0.41677620715248037 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[188] Loss:0.20967302173376084 | L1 Loss:0.36570564806461336 | R2:0.18990985088967177 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[189] Loss:0.18584318924695253 | L1 Loss:0.33170194178819656 | R2:0.41144733090816377 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[189] Loss:0.19840391278266906 | L1 Loss:0.34775558710098264 | R2:0.23099858916138763 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[190] Loss:0.18692354625090957 | L1 Loss:0.3325460199266672 | R2:0.4054826155335315 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[190] Loss:0.20754344016313553 | L1 Loss:0.361069992184639 | R2:0.13002271368864043 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[191] Loss:0.18612125795334578 | L1 Loss:0.32419850304722786 | R2:0.40919380872920047 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[191] Loss:0.2010717511177063 | L1 Loss:0.3537627667188644 | R2:0.22694388395667375 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[192] Loss:0.17816891567781568 | L1 Loss:0.32217102125287056 | R2:0.43425379881905646 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[192] Loss:0.19679106324911116 | L1 Loss:0.3466374784708023 | R2:0.2529261356249368 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[193] Loss:0.18012942327186465 | L1 Loss:0.3256035465747118 | R2:0.4302346806336814 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[193] Loss:0.21063371300697326 | L1 Loss:0.36181644201278684 | R2:0.20320335121673888 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[194] Loss:0.1803723550401628 | L1 Loss:0.33082581125199795 | R2:0.43170750608376884 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[194] Loss:0.2029554843902588 | L1 Loss:0.35831978619098664 | R2:0.19043254912978522 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[195] Loss:0.17602221900597215 | L1 Loss:0.32932309061288834 | R2:0.44522627675376913 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[195] Loss:0.20126714259386064 | L1 Loss:0.3555022329092026 | R2:0.16726062799734892 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[196] Loss:0.18095772666856647 | L1 Loss:0.328933609649539 | R2:0.4249267902675653 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[196] Loss:0.20269514471292496 | L1 Loss:0.34920883774757383 | R2:0.2059382453809857 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[197] Loss:0.17957892687991261 | L1 Loss:0.3290159907191992 | R2:0.4290867611413406 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[197] Loss:0.20503466576337814 | L1 Loss:0.35741124153137205 | R2:0.18892322343200596 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[198] Loss:0.1850218614563346 | L1 Loss:0.3286630157381296 | R2:0.41200940458803353 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[198] Loss:0.21269240975379944 | L1 Loss:0.36066338419914246 | R2:0.1605981951899212 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[199] Loss:0.18422106187790632 | L1 Loss:0.3279115743935108 | R2:0.41744674786828145 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[199] Loss:0.20528821647167206 | L1 Loss:0.35545015037059785 | R2:0.16170699863503457 | ACC: 77.6667%(233/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "# train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "# val_acc_record = []\n",
        "# val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "# test_acc_record = []\n",
        "# test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(100)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W_RlROkfEuC",
        "outputId": "60ebede9-8852-4341-979e-9eb2d8649db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.1792503520846367 | L1 Loss:0.318916454911232 | R2:0.42235851608198155 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[0] Loss:0.21110317707061768 | L1 Loss:0.3559573829174042 | R2:0.18391239896399875 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[1] Loss:0.1840020762756467 | L1 Loss:0.32465980388224125 | R2:0.41161924329369515 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[1] Loss:0.2065364971756935 | L1 Loss:0.35401942431926725 | R2:0.15988851982220903 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[2] Loss:0.17866325238719583 | L1 Loss:0.32482163794338703 | R2:0.43223496792760685 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[2] Loss:0.2095300853252411 | L1 Loss:0.359379968047142 | R2:0.2299105260451027 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[3] Loss:0.17630019085481763 | L1 Loss:0.31945849768817425 | R2:0.43553953278292723 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[3] Loss:0.20743122547864914 | L1 Loss:0.35229367911815646 | R2:0.11278857833492931 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[4] Loss:0.18008623318746686 | L1 Loss:0.323214091360569 | R2:0.42624346042780575 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[4] Loss:0.21846072301268576 | L1 Loss:0.36248535215854644 | R2:0.07804853616694803 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[5] Loss:0.17974242335185409 | L1 Loss:0.32750021666288376 | R2:0.42557991821321983 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[5] Loss:0.22195070162415503 | L1 Loss:0.3649343058466911 | R2:0.09605945955017078 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[6] Loss:0.18273593485355377 | L1 Loss:0.3272496536374092 | R2:0.4222010523941729 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[6] Loss:0.20576176941394805 | L1 Loss:0.35026370286941527 | R2:0.1551067959393352 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[7] Loss:0.1784199788235128 | L1 Loss:0.3243572488427162 | R2:0.43554980922336234 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[7] Loss:0.20897321924567222 | L1 Loss:0.3564242720603943 | R2:0.14067471799416822 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[8] Loss:0.18410741072148085 | L1 Loss:0.323081842623651 | R2:0.4157203550255115 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[8] Loss:0.21542673334479331 | L1 Loss:0.3628082424402237 | R2:0.14898286318799775 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[9] Loss:0.18615863006561995 | L1 Loss:0.326682073995471 | R2:0.4098010578351009 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[9] Loss:0.20952247381210326 | L1 Loss:0.3554060488939285 | R2:0.1507902009019523 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[10] Loss:0.1895186686888337 | L1 Loss:0.3320170268416405 | R2:0.3965252314631127 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[10] Loss:0.20827405378222466 | L1 Loss:0.35358245521783827 | R2:0.1647979997176608 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[11] Loss:0.17695728037506342 | L1 Loss:0.32363252714276314 | R2:0.4386407742113987 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[11] Loss:0.19596533328294755 | L1 Loss:0.3492030829191208 | R2:0.21612519249628587 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[12] Loss:0.176099656149745 | L1 Loss:0.32350850105285645 | R2:0.440533562186496 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[12] Loss:0.20965696200728418 | L1 Loss:0.3614507704973221 | R2:0.10914760149384387 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[13] Loss:0.17777728009968996 | L1 Loss:0.3247948419302702 | R2:0.4323909385749336 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[13] Loss:0.20599443167448045 | L1 Loss:0.3481693476438522 | R2:0.17519756445486187 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[14] Loss:0.17783154733479023 | L1 Loss:0.3250787351280451 | R2:0.43435099201794036 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[14] Loss:0.2070453368127346 | L1 Loss:0.35766723603010175 | R2:0.06911781518857163 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[15] Loss:0.178551958873868 | L1 Loss:0.3213099855929613 | R2:0.43235247902438245 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[15] Loss:0.21285016238689422 | L1 Loss:0.35633803009986875 | R2:0.10888925443913564 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[16] Loss:0.1809648834168911 | L1 Loss:0.3282248508185148 | R2:0.4258157538909007 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[16] Loss:0.20077619925141335 | L1 Loss:0.35251061618328094 | R2:0.19919109703786247 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[17] Loss:0.18468352686613798 | L1 Loss:0.32958369702100754 | R2:0.41475763080251743 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[17] Loss:0.20432633012533188 | L1 Loss:0.3546117275953293 | R2:0.14778372119386737 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[18] Loss:0.17828907119110227 | L1 Loss:0.32793210074305534 | R2:0.43143220163459367 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[18] Loss:0.20851651579141617 | L1 Loss:0.3518746465444565 | R2:0.14043266426066198 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[19] Loss:0.1848644227720797 | L1 Loss:0.32837252877652645 | R2:0.4066012611580352 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[19] Loss:0.20208663642406463 | L1 Loss:0.3436281353235245 | R2:0.15226512639074968 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[20] Loss:0.18773270724341273 | L1 Loss:0.3337366674095392 | R2:0.40126082142825076 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[20] Loss:0.2095115527510643 | L1 Loss:0.3558940201997757 | R2:0.1226699754022325 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[21] Loss:0.17670774087309837 | L1 Loss:0.32301178108900785 | R2:0.44057688213063223 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[21] Loss:0.20183204412460326 | L1 Loss:0.3499909371137619 | R2:0.17318146419453498 | ACC: 78.3333%(235/300)\n",
            "Training Epoch[22] Loss:0.18502218509092927 | L1 Loss:0.33364298194646835 | R2:0.4128286142341515 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[22] Loss:0.20018587708473207 | L1 Loss:0.34746154844760896 | R2:0.1401446199224845 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[23] Loss:0.18525605322793126 | L1 Loss:0.3330986350774765 | R2:0.4070659070973019 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[23] Loss:0.21135660111904145 | L1 Loss:0.35934823751449585 | R2:0.11933094538780513 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[24] Loss:0.17554004024714231 | L1 Loss:0.3197202943265438 | R2:0.44528622313343613 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[24] Loss:0.2009635090827942 | L1 Loss:0.35149946212768557 | R2:0.18452063020183487 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[25] Loss:0.1782108903862536 | L1 Loss:0.3177125118672848 | R2:0.4303607956258005 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[25] Loss:0.20367996841669084 | L1 Loss:0.35322930216789244 | R2:0.19537572872859416 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[26] Loss:0.17451817030087113 | L1 Loss:0.32327608950436115 | R2:0.44033920960930384 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[26] Loss:0.20284675508737565 | L1 Loss:0.35205980837345124 | R2:0.14616432082952063 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[27] Loss:0.18048398150131106 | L1 Loss:0.32734891399741173 | R2:0.4283510509624742 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[27] Loss:0.20876239687204362 | L1 Loss:0.3576627016067505 | R2:0.1329060678214363 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[28] Loss:0.18352648429572582 | L1 Loss:0.3277815841138363 | R2:0.41957073427646757 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[28] Loss:0.20607564598321915 | L1 Loss:0.3524254381656647 | R2:0.17903731313732077 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[29] Loss:0.18280667625367641 | L1 Loss:0.3289712369441986 | R2:0.41847665323404487 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[29] Loss:0.21622442156076432 | L1 Loss:0.36262232065200806 | R2:0.07461733090991504 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[30] Loss:0.1754690152592957 | L1 Loss:0.3179233381524682 | R2:0.43773848749860933 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[30] Loss:0.2033611461520195 | L1 Loss:0.3521135985851288 | R2:0.16653529750946974 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[31] Loss:0.1723999590612948 | L1 Loss:0.3167451322078705 | R2:0.45106284055684054 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[31] Loss:0.2132283106446266 | L1 Loss:0.36213861107826234 | R2:0.11478616369377179 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[32] Loss:0.18008282640948892 | L1 Loss:0.3232492208480835 | R2:0.4251427107583384 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[32] Loss:0.20886130183935164 | L1 Loss:0.35951625406742094 | R2:0.13959012524368383 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[33] Loss:0.18500963784754276 | L1 Loss:0.3261072598397732 | R2:0.40993594585588394 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[33] Loss:0.1975571945309639 | L1 Loss:0.34264416694641114 | R2:0.17461879554579585 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[34] Loss:0.18587292591109872 | L1 Loss:0.3324496429413557 | R2:0.4043770851532639 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[34] Loss:0.20503010973334312 | L1 Loss:0.35009179413318636 | R2:0.16957537281673624 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[35] Loss:0.18099709693342447 | L1 Loss:0.32528912648558617 | R2:0.42540767677903235 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[35] Loss:0.21700752079486846 | L1 Loss:0.3551895976066589 | R2:0.09475931922631275 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[36] Loss:0.18470871169120073 | L1 Loss:0.33009472116827965 | R2:0.41757105589123195 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[36] Loss:0.21213798373937606 | L1 Loss:0.35957007110118866 | R2:0.11922514012162826 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[37] Loss:0.18226983211934566 | L1 Loss:0.32995800487697124 | R2:0.41951669885207754 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[37] Loss:0.20510198175907135 | L1 Loss:0.35071728229522703 | R2:0.11500398741507283 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[38] Loss:0.1822764789685607 | L1 Loss:0.3278390225023031 | R2:0.4196625053470753 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[38] Loss:0.20665836334228516 | L1 Loss:0.3552266061306 | R2:0.145863834394039 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[39] Loss:0.17391219548881054 | L1 Loss:0.3207577746361494 | R2:0.4545609796768617 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[39] Loss:0.20560189709067345 | L1 Loss:0.3461966246366501 | R2:0.15009940647076242 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[40] Loss:0.17884299252182245 | L1 Loss:0.3257527593523264 | R2:0.42970712507529474 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[40] Loss:0.20686361491680144 | L1 Loss:0.34811781346797943 | R2:0.1254526836839903 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[41] Loss:0.17724998947232962 | L1 Loss:0.324887590482831 | R2:0.43478116845829434 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[41] Loss:0.19353825077414513 | L1 Loss:0.3398220747709274 | R2:0.21331601810711356 | ACC: 79.6667%(239/300)\n",
            "Training Epoch[42] Loss:0.18505253922194242 | L1 Loss:0.3279789201915264 | R2:0.4140527366681491 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[42] Loss:0.21375333815813063 | L1 Loss:0.36244540214538573 | R2:0.13526628575664904 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[43] Loss:0.18834547325968742 | L1 Loss:0.3304793331772089 | R2:0.4027734144964321 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[43] Loss:0.20471108183264733 | L1 Loss:0.3502654224634171 | R2:0.18701352884640485 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[44] Loss:0.17634939355775714 | L1 Loss:0.321502136066556 | R2:0.44437061965947416 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[44] Loss:0.1982812002301216 | L1 Loss:0.3454268991947174 | R2:0.20710504656244916 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[45] Loss:0.1702778385952115 | L1 Loss:0.3190623577684164 | R2:0.46095459539254696 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[45] Loss:0.1900847226381302 | L1 Loss:0.34214303493499754 | R2:0.26476217812429415 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[46] Loss:0.17985394783318043 | L1 Loss:0.328423535451293 | R2:0.4265188721330769 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[46] Loss:0.1964576631784439 | L1 Loss:0.35207530558109285 | R2:0.22118168587762504 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[47] Loss:0.1768862484022975 | L1 Loss:0.3257296923547983 | R2:0.4413474989515658 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[47] Loss:0.19261846467852592 | L1 Loss:0.34639483094215395 | R2:0.2538642481993701 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[48] Loss:0.17500933399423957 | L1 Loss:0.3293854631483555 | R2:0.44572891102548257 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[48] Loss:0.20316227823495864 | L1 Loss:0.3579286694526672 | R2:0.1510934076227338 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[49] Loss:0.18343781447038054 | L1 Loss:0.32873720675706863 | R2:0.4165878305073733 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[49] Loss:0.19190754145383834 | L1 Loss:0.3393350958824158 | R2:0.29365482372071067 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[50] Loss:0.1768465177156031 | L1 Loss:0.321490490809083 | R2:0.43887683579347236 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[50] Loss:0.20592711120843887 | L1 Loss:0.35891175270080566 | R2:0.18095932043208313 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[51] Loss:0.17611486930400133 | L1 Loss:0.3215442821383476 | R2:0.44281463275706723 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[51] Loss:0.20112770348787307 | L1 Loss:0.35072739124298097 | R2:0.20405987330710657 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[52] Loss:0.17609345074743032 | L1 Loss:0.31952260807156563 | R2:0.4406331878362737 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[52] Loss:0.19519261717796327 | L1 Loss:0.3464746832847595 | R2:0.2521102686614045 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[53] Loss:0.17582480469718575 | L1 Loss:0.3241795878857374 | R2:0.43769618610236904 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[53] Loss:0.20440751165151597 | L1 Loss:0.35565756261348724 | R2:0.1925306085507214 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[54] Loss:0.17232108674943447 | L1 Loss:0.3209190424531698 | R2:0.45505143732566067 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[54] Loss:0.19506292045116425 | L1 Loss:0.34566419422626493 | R2:0.23675253218746173 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[55] Loss:0.17859783116728067 | L1 Loss:0.3225435186177492 | R2:0.43844960491167506 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[55] Loss:0.20172756612300874 | L1 Loss:0.3532158821821213 | R2:0.1765064633349957 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[56] Loss:0.18072075443342328 | L1 Loss:0.3270661346614361 | R2:0.42956214332027653 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[56] Loss:0.19959060177206994 | L1 Loss:0.34960522651672366 | R2:0.18803878459544218 | ACC: 79.0000%(237/300)\n",
            "Training Epoch[57] Loss:0.17732193693518639 | L1 Loss:0.3263854794204235 | R2:0.44090299984838033 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[57] Loss:0.1904282584786415 | L1 Loss:0.34173092246055603 | R2:0.24240284471085843 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[58] Loss:0.1743134050630033 | L1 Loss:0.3217397481203079 | R2:0.44934648080557776 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[58] Loss:0.20557164251804352 | L1 Loss:0.3528894007205963 | R2:0.1665777314923536 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[59] Loss:0.18023984460160136 | L1 Loss:0.3211241690441966 | R2:0.42539466663074493 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[59] Loss:0.19249816834926606 | L1 Loss:0.34386648833751676 | R2:0.2534698455228471 | ACC: 79.3333%(238/300)\n",
            "Training Epoch[60] Loss:0.17371304286643863 | L1 Loss:0.32056113332509995 | R2:0.4494904911848139 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[60] Loss:0.19640713110566138 | L1 Loss:0.3510263502597809 | R2:0.23570012674129076 | ACC: 79.3333%(238/300)\n",
            "Training Epoch[61] Loss:0.17999977059662342 | L1 Loss:0.3303717616945505 | R2:0.4288033342211836 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[61] Loss:0.19771444499492646 | L1 Loss:0.3448794215917587 | R2:0.21187492646957956 | ACC: 79.3333%(238/300)\n",
            "Training Epoch[62] Loss:0.17830810835584998 | L1 Loss:0.329617103561759 | R2:0.43745713465820074 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[62] Loss:0.19380818158388138 | L1 Loss:0.342907577753067 | R2:0.2458246383776745 | ACC: 79.0000%(237/300)\n",
            "Training Epoch[63] Loss:0.17406408349052072 | L1 Loss:0.32259461283683777 | R2:0.4444860757410975 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[63] Loss:0.20286732986569406 | L1 Loss:0.34890347719192505 | R2:0.20818484509718913 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[64] Loss:0.17496386962011456 | L1 Loss:0.32219001837074757 | R2:0.4404108454513088 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[64] Loss:0.20028179436922072 | L1 Loss:0.3444271057844162 | R2:0.2317402931592511 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[65] Loss:0.1757985968142748 | L1 Loss:0.32175373286008835 | R2:0.4467621549573842 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[65] Loss:0.20342337787151338 | L1 Loss:0.35154229700565337 | R2:0.19488463994910102 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[66] Loss:0.17110839067026973 | L1 Loss:0.3173175659030676 | R2:0.4600728989090472 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[66] Loss:0.2007280983030796 | L1 Loss:0.350221061706543 | R2:0.20514242393755286 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[67] Loss:0.17430580826476216 | L1 Loss:0.3208610527217388 | R2:0.4467523077211939 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[67] Loss:0.20538988783955575 | L1 Loss:0.3497692734003067 | R2:0.1889971825765382 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[68] Loss:0.17407739209011197 | L1 Loss:0.3198283277451992 | R2:0.44393334362648323 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[68] Loss:0.2047651544213295 | L1 Loss:0.3461913108825684 | R2:0.15832830108945833 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[69] Loss:0.1784143685363233 | L1 Loss:0.3234889106824994 | R2:0.4334865039333596 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[69] Loss:0.20743943974375725 | L1 Loss:0.3514179319143295 | R2:0.19508069800689892 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[70] Loss:0.17546380683779716 | L1 Loss:0.3200639896094799 | R2:0.4401113243960698 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[70] Loss:0.20529073476791382 | L1 Loss:0.3526106297969818 | R2:0.17940453710136986 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[71] Loss:0.16881833132356405 | L1 Loss:0.31437971629202366 | R2:0.46698934033461886 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[71] Loss:0.19553587585687637 | L1 Loss:0.34711475372314454 | R2:0.2378300035099396 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[72] Loss:0.16936180694028735 | L1 Loss:0.3171624168753624 | R2:0.46287271333160457 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[72] Loss:0.21291636824607849 | L1 Loss:0.35918877422809603 | R2:0.13865795045813445 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[73] Loss:0.1687407810240984 | L1 Loss:0.31213827710598707 | R2:0.4633470868795918 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[73] Loss:0.19857027381658554 | L1 Loss:0.34973208904266356 | R2:0.21982887784629573 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[74] Loss:0.17316779820248485 | L1 Loss:0.3140950333327055 | R2:0.44876564326921564 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[74] Loss:0.19915825873613358 | L1 Loss:0.3471752434968948 | R2:0.18326946829522486 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[75] Loss:0.18437478598207235 | L1 Loss:0.3291874788701534 | R2:0.4130792260076123 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[75] Loss:0.19902992695569993 | L1 Loss:0.345320463180542 | R2:0.19944537982078536 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[76] Loss:0.17833457002416253 | L1 Loss:0.32420592103153467 | R2:0.43372034133070175 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[76] Loss:0.20482003688812256 | L1 Loss:0.35347605049610137 | R2:0.17259766064589788 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[77] Loss:0.1851993454620242 | L1 Loss:0.3330973759293556 | R2:0.4125112108656147 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[77] Loss:0.19710323065519333 | L1 Loss:0.3471158087253571 | R2:0.24636687659198403 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[78] Loss:0.17454830929636955 | L1 Loss:0.3216309333220124 | R2:0.45000363411423666 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[78] Loss:0.1961127206683159 | L1 Loss:0.3454925864934921 | R2:0.22595117993035627 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[79] Loss:0.17685619881376624 | L1 Loss:0.3234048970043659 | R2:0.44192680253362643 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[79] Loss:0.20504395812749862 | L1 Loss:0.3539068102836609 | R2:0.158867630065943 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[80] Loss:0.17783618858084083 | L1 Loss:0.32364383712410927 | R2:0.43640569725684647 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[80] Loss:0.20692707300186158 | L1 Loss:0.35209537446498873 | R2:0.1370818201562186 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[81] Loss:0.1801477763801813 | L1 Loss:0.33033883944153786 | R2:0.4297374776498367 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[81] Loss:0.20422862619161605 | L1 Loss:0.35064623355865476 | R2:0.1873737669639397 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[82] Loss:0.17525000916793942 | L1 Loss:0.3287323899567127 | R2:0.45033268019734984 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[82] Loss:0.20494810342788697 | L1 Loss:0.3517706662416458 | R2:0.1634053653051306 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[83] Loss:0.17204551678150892 | L1 Loss:0.3200224135071039 | R2:0.4558782656442961 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[83] Loss:0.2056331530213356 | L1 Loss:0.3498408406972885 | R2:0.1500242861438948 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[84] Loss:0.17731552943587303 | L1 Loss:0.3204976096749306 | R2:0.4391947755187475 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[84] Loss:0.20564890056848525 | L1 Loss:0.3544841468334198 | R2:0.17194795240506963 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[85] Loss:0.17268561339005828 | L1 Loss:0.31807006150484085 | R2:0.4514216714357727 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[85] Loss:0.19885349422693252 | L1 Loss:0.3495825231075287 | R2:0.14561428667829768 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[86] Loss:0.18181499326601624 | L1 Loss:0.3258863799273968 | R2:0.4194991714241099 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[86] Loss:0.20749763995409012 | L1 Loss:0.35813920497894286 | R2:0.10917769295377515 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[87] Loss:0.18203379726037383 | L1 Loss:0.3268481455743313 | R2:0.42564727420885984 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[87] Loss:0.19991749674081802 | L1 Loss:0.3498108834028244 | R2:0.20551314777156676 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[88] Loss:0.1807365445420146 | L1 Loss:0.3284481819719076 | R2:0.4265929957345178 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[88] Loss:0.20584897547960282 | L1 Loss:0.34922908544540404 | R2:0.13462379096568683 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[89] Loss:0.17875975230708718 | L1 Loss:0.3265370260924101 | R2:0.43162701944289905 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[89] Loss:0.2163235917687416 | L1 Loss:0.35977144837379454 | R2:0.09930619840120265 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[90] Loss:0.16845851996913552 | L1 Loss:0.31249707844108343 | R2:0.46334840992855564 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[90] Loss:0.2046004965901375 | L1 Loss:0.3501009911298752 | R2:0.14297562412401663 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[91] Loss:0.17278189677745104 | L1 Loss:0.3196899779140949 | R2:0.4511613027494872 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[91] Loss:0.19890255481004715 | L1 Loss:0.346119812130928 | R2:0.1830740329412972 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[92] Loss:0.17160502588376403 | L1 Loss:0.3160167317837477 | R2:0.45148389197205474 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[92] Loss:0.19944948256015776 | L1 Loss:0.3448230504989624 | R2:0.19902904668581112 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[93] Loss:0.17985289683565497 | L1 Loss:0.3233858682215214 | R2:0.4260743704024921 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[93] Loss:0.20377357229590415 | L1 Loss:0.3515474259853363 | R2:0.1426937822373214 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[94] Loss:0.1763316337019205 | L1 Loss:0.32168499659746885 | R2:0.440632960569167 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[94] Loss:0.20449405163526535 | L1 Loss:0.3530675619840622 | R2:0.19989609281224138 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[95] Loss:0.17334074480459094 | L1 Loss:0.319707777351141 | R2:0.4472474284527723 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[95] Loss:0.19859573021531104 | L1 Loss:0.3415185004472733 | R2:0.2156136534374571 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[96] Loss:0.17583625530824065 | L1 Loss:0.32149574533104897 | R2:0.44097731746342483 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[96] Loss:0.19198675453662872 | L1 Loss:0.3385679930448532 | R2:0.2705436558891498 | ACC: 79.6667%(239/300)\n",
            "Training Epoch[97] Loss:0.16840631002560258 | L1 Loss:0.31184957176446915 | R2:0.4669201092737051 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[97] Loss:0.20019753724336625 | L1 Loss:0.3413186401128769 | R2:0.22286971396652108 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[98] Loss:0.1833047354593873 | L1 Loss:0.32605506386607885 | R2:0.4129710512601282 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[98] Loss:0.20372905880212783 | L1 Loss:0.3455280691385269 | R2:0.18344219440071505 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[99] Loss:0.17177737643942237 | L1 Loss:0.3207170767709613 | R2:0.4532812161296565 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[99] Loss:0.20526617839932443 | L1 Loss:0.34973983764648436 | R2:0.218282427824472 | ACC: 76.3333%(229/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3z8ZzJffAjh",
        "outputId": "00793dd8-d48e-443a-9af8-dffe150f7a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.29365482372071067\n",
            "min test_loss_l1_record  0.3385679930448532\n",
            "min test_loss_record  0.1900847226381302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f686e15b-97a5-4f14-8723-f2f97e90569e",
        "id": "qlErKyv6qzHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.25662598601705594\n",
            "min test_loss_l1_record  0.3466374784708023\n",
            "min test_loss_record  0.19679106324911116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15251ed1-ec14-460b-c6e6-73ddc62158b7",
        "id": "hkwfhoAaqzHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.15542278667731962\n",
            "min test_loss_l1_record  0.36857505440711974\n",
            "min test_loss_record  0.2203592985868454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "4675d646-2ebb-4383-aac8-96aea857f27b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guHmxwXFqzHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.15542278667731962\n",
            "min test_loss_l1_record  0.38664981424808503\n",
            "min test_loss_record  0.2512748628854752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hn4kOf5hF0yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 mer Regression"
      ],
      "metadata": {
        "id": "SdrUmqwhF0-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qpx_nuqVF0-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=2, mer_dict=twoMer_dict)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddee3ccb-29b3-423d-be18-afc7be02cda6",
        "id": "1bMApB8BF0-v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4531870-7c12-40e3-fb14-6805c2bb3d9a",
        "id": "wG_7d05rF0-w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "qSyphfCIF0-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3b1c63-e5f7-4178-f5a1-d89a9d25fa04",
        "id": "RVdly8XAF0-w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "mw5tPcRiF0-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "DMOrvDtGF0-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d1e8b1-3525-41c6-c717-c04b3fe1c6a6",
        "id": "daXVHxtzF0-y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.35712728276848793 | L1 Loss:0.42450347542762756 | R2:-0.0503013822851114 | ACC: 61.8000%(309/500)\n",
            "Testing Epoch[0] Loss:0.38265363574028016 | L1 Loss:0.453950634598732 | R2:0.024313772984380477 | ACC: 60.0000%(180/300)\n",
            "Training Epoch[1] Loss:0.3451258987188339 | L1 Loss:0.42804248817265034 | R2:-0.016616637157529472 | ACC: 62.0000%(310/500)\n",
            "Testing Epoch[1] Loss:0.3762075871229172 | L1 Loss:0.4595708578824997 | R2:0.04684039416917958 | ACC: 59.6667%(179/300)\n",
            "Training Epoch[2] Loss:0.33433930203318596 | L1 Loss:0.4281192924827337 | R2:0.012075444385522327 | ACC: 62.2000%(311/500)\n",
            "Testing Epoch[2] Loss:0.36558238714933394 | L1 Loss:0.453710725903511 | R2:0.06894547817521908 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[3] Loss:0.3181684259325266 | L1 Loss:0.4195929765701294 | R2:0.05740387102947139 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[3] Loss:0.3480325698852539 | L1 Loss:0.44362071454524993 | R2:0.11757822885802174 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[4] Loss:0.3251450899988413 | L1 Loss:0.4266798309981823 | R2:0.02737288116614474 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[4] Loss:0.3519950479269028 | L1 Loss:0.44632715582847593 | R2:0.10111105079391314 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[5] Loss:0.3248281730338931 | L1 Loss:0.4282339736819267 | R2:0.026200238268941523 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[5] Loss:0.33688617795705794 | L1 Loss:0.4398005485534668 | R2:0.14368184946380408 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[6] Loss:0.3142901286482811 | L1 Loss:0.4270787164568901 | R2:0.056001724913318576 | ACC: 65.2000%(326/500)\n",
            "Testing Epoch[6] Loss:0.32682485580444337 | L1 Loss:0.4398514211177826 | R2:0.16117358232462803 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[7] Loss:0.30987217742949724 | L1 Loss:0.4265485852956772 | R2:0.06583683473485107 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[7] Loss:0.3308813437819481 | L1 Loss:0.4425396054983139 | R2:0.1529481223087231 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[8] Loss:0.30506337713450193 | L1 Loss:0.42188799008727074 | R2:0.08177148927085451 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[8] Loss:0.3070769250392914 | L1 Loss:0.431857168674469 | R2:0.19676236672817415 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[9] Loss:0.2943236120045185 | L1 Loss:0.4159447457641363 | R2:0.10973471157850705 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[9] Loss:0.30752318948507307 | L1 Loss:0.4336657762527466 | R2:0.20813303140602954 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[10] Loss:0.2751856315881014 | L1 Loss:0.40786405466496944 | R2:0.1673171920734839 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[10] Loss:0.28743869811296463 | L1 Loss:0.4223274737596512 | R2:0.2518062182238198 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[11] Loss:0.29071596544235945 | L1 Loss:0.41658081859350204 | R2:0.12756807957975136 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[11] Loss:0.2947569236159325 | L1 Loss:0.42678092867136 | R2:0.22977544204728395 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[12] Loss:0.27607809100300074 | L1 Loss:0.40427641198039055 | R2:0.16630472251387796 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[12] Loss:0.2855472594499588 | L1 Loss:0.41374036967754363 | R2:0.25705894923284134 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[13] Loss:0.2714220155030489 | L1 Loss:0.3963163495063782 | R2:0.17879976703960482 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[13] Loss:0.30037982389330864 | L1 Loss:0.4174331724643707 | R2:0.2145358116873864 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[14] Loss:0.2574115302413702 | L1 Loss:0.3922482915222645 | R2:0.21081193474414417 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[14] Loss:0.287734717130661 | L1 Loss:0.4170223116874695 | R2:0.2554408454550915 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[15] Loss:0.2661421811208129 | L1 Loss:0.4006745032966137 | R2:0.19142786679712281 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[15] Loss:0.2867853447794914 | L1 Loss:0.422817075252533 | R2:0.24786590831353772 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[16] Loss:0.2623905250802636 | L1 Loss:0.39721265994012356 | R2:0.2060951797885675 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[16] Loss:0.2783077821135521 | L1 Loss:0.40829614996910096 | R2:0.2614929250078726 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[17] Loss:0.27151020988821983 | L1 Loss:0.40256118029356003 | R2:0.17604187986208367 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[17] Loss:0.29620259404182436 | L1 Loss:0.41942200660705564 | R2:0.21412331889194736 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[18] Loss:0.26374446600675583 | L1 Loss:0.3946971334517002 | R2:0.19743137418533824 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[18] Loss:0.2780536487698555 | L1 Loss:0.40656720399856566 | R2:0.25554831612525825 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[19] Loss:0.2612222349271178 | L1 Loss:0.3882561530917883 | R2:0.19960917077485069 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[19] Loss:0.29915482848882674 | L1 Loss:0.4308418184518814 | R2:0.20334672199749493 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[20] Loss:0.26163525972515345 | L1 Loss:0.3886307366192341 | R2:0.20389203952078025 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[20] Loss:0.2918600395321846 | L1 Loss:0.42434016764163973 | R2:0.22112777415947904 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[21] Loss:0.27316385973244905 | L1 Loss:0.3991750031709671 | R2:0.16782128333135563 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[21] Loss:0.3030297741293907 | L1 Loss:0.4305002987384796 | R2:0.18076845215308449 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[22] Loss:0.2603402780368924 | L1 Loss:0.39733666367828846 | R2:0.20910413454131493 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[22] Loss:0.2770263411104679 | L1 Loss:0.40834580063819886 | R2:0.2590125025642293 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[23] Loss:0.2607388449832797 | L1 Loss:0.3953472934663296 | R2:0.20590937555811706 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[23] Loss:0.2809790372848511 | L1 Loss:0.4058805227279663 | R2:0.2522895052725642 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[24] Loss:0.26063035801053047 | L1 Loss:0.39255201630294323 | R2:0.20387661782511907 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[24] Loss:0.2783975012600422 | L1 Loss:0.41013675928115845 | R2:0.2609675813379255 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[25] Loss:0.25768478587269783 | L1 Loss:0.3918056841939688 | R2:0.21179748084797834 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[25] Loss:0.27081177830696107 | L1 Loss:0.4007374033331871 | R2:0.2806389572765936 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[26] Loss:0.2680650223046541 | L1 Loss:0.40387055836617947 | R2:0.1761296005077928 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[26] Loss:0.28775743916630747 | L1 Loss:0.41467646360397337 | R2:0.23486177493370092 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[27] Loss:0.25674255564808846 | L1 Loss:0.3925912529230118 | R2:0.2131704018630252 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[27] Loss:0.26713496297597883 | L1 Loss:0.4039300262928009 | R2:0.2861954696505901 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[28] Loss:0.2667897390201688 | L1 Loss:0.40747919492423534 | R2:0.18359784623956804 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[28] Loss:0.27907632738351823 | L1 Loss:0.40683860778808595 | R2:0.25886704894014817 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[29] Loss:0.2560136541724205 | L1 Loss:0.39328986778855324 | R2:0.22026846536705325 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[29] Loss:0.2880560949444771 | L1 Loss:0.42148680686950685 | R2:0.22478495880040184 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[30] Loss:0.25298944395035505 | L1 Loss:0.3890902269631624 | R2:0.23011174220240777 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[30] Loss:0.28693534433841705 | L1 Loss:0.41531058549880984 | R2:0.2319359968298921 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[31] Loss:0.25668822042644024 | L1 Loss:0.38966859318315983 | R2:0.2218100942758417 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[31] Loss:0.26999851018190385 | L1 Loss:0.4016426742076874 | R2:0.2796912845078652 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[32] Loss:0.26262210961431265 | L1 Loss:0.3985591735690832 | R2:0.20683671007418425 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[32] Loss:0.27637767493724824 | L1 Loss:0.40306379497051237 | R2:0.25710129486182937 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[33] Loss:0.27053820062428713 | L1 Loss:0.402586517855525 | R2:0.18175900523823169 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[33] Loss:0.2886375918984413 | L1 Loss:0.41479639112949374 | R2:0.2374559262252312 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[34] Loss:0.2530476301908493 | L1 Loss:0.39362314343452454 | R2:0.22549079541372583 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[34] Loss:0.260140623152256 | L1 Loss:0.3962205171585083 | R2:0.3197322556999874 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[35] Loss:0.24011939950287342 | L1 Loss:0.38131782598793507 | R2:0.27388245399340905 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[35] Loss:0.26520796343684194 | L1 Loss:0.40390024781227113 | R2:0.3107566293715657 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[36] Loss:0.2543812785297632 | L1 Loss:0.39187297224998474 | R2:0.23123991642927544 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[36] Loss:0.25854303538799284 | L1 Loss:0.40518700480461123 | R2:0.3180203002200697 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[37] Loss:0.2518529677763581 | L1 Loss:0.39195138588547707 | R2:0.2283533871232769 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[37] Loss:0.26878765821456907 | L1 Loss:0.4059825211763382 | R2:0.28729232597635723 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[38] Loss:0.24390358850359917 | L1 Loss:0.3872305266559124 | R2:0.262624054517403 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[38] Loss:0.2646263912320137 | L1 Loss:0.3992371916770935 | R2:0.29741722879189103 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[39] Loss:0.24460526555776596 | L1 Loss:0.38691359013319016 | R2:0.2532611054637173 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[39] Loss:0.2507642224431038 | L1 Loss:0.39822925329208375 | R2:0.332335880277871 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[40] Loss:0.2580412421375513 | L1 Loss:0.39353257417678833 | R2:0.21746011828030265 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[40] Loss:0.2794142261147499 | L1 Loss:0.4121659904718399 | R2:0.2644001560572776 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[41] Loss:0.25855502113699913 | L1 Loss:0.3940358255058527 | R2:0.2156537974390969 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[41] Loss:0.2808402419090271 | L1 Loss:0.40576322078704835 | R2:0.2621268229004861 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[42] Loss:0.2630422553047538 | L1 Loss:0.3918047081679106 | R2:0.1997928799465939 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[42] Loss:0.2709511175751686 | L1 Loss:0.40024579167366026 | R2:0.28274894373603304 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[43] Loss:0.2640325613319874 | L1 Loss:0.3996715433895588 | R2:0.20022461062820357 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[43] Loss:0.28654574006795885 | L1 Loss:0.4217287540435791 | R2:0.2353908721260612 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[44] Loss:0.2524027731269598 | L1 Loss:0.39131755754351616 | R2:0.23643955175121406 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[44] Loss:0.2845935046672821 | L1 Loss:0.4122442752122879 | R2:0.24133279298723243 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[45] Loss:0.2450543688610196 | L1 Loss:0.37846045196056366 | R2:0.24567771241330397 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[45] Loss:0.2879143863916397 | L1 Loss:0.41950013637542727 | R2:0.23663078259646037 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[46] Loss:0.2464720793068409 | L1 Loss:0.3871561326086521 | R2:0.2512979828551821 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[46] Loss:0.2921939827501774 | L1 Loss:0.41195087134838104 | R2:0.21924667140663331 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[47] Loss:0.25529899448156357 | L1 Loss:0.38862740248441696 | R2:0.2158673038982355 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[47] Loss:0.27976188212633135 | L1 Loss:0.40746818482875824 | R2:0.2606932758656434 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[48] Loss:0.24796252138912678 | L1 Loss:0.38094369880855083 | R2:0.25214081115431247 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[48] Loss:0.2824985645711422 | L1 Loss:0.40880052745342255 | R2:0.2376421842647912 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[49] Loss:0.2553832056000829 | L1 Loss:0.39372253976762295 | R2:0.22694990376920732 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[49] Loss:0.28080673813819884 | L1 Loss:0.40207266211509707 | R2:0.25801300131445665 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[50] Loss:0.24719544872641563 | L1 Loss:0.3848668970167637 | R2:0.24104257665433582 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[50] Loss:0.2776591770350933 | L1 Loss:0.39654641300439836 | R2:0.25680101306852615 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[51] Loss:0.23121698666363955 | L1 Loss:0.37628765031695366 | R2:0.29335725618689645 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[51] Loss:0.2750150106847286 | L1 Loss:0.40850523114204407 | R2:0.2693345262259774 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[52] Loss:0.23197573889046907 | L1 Loss:0.3731542993336916 | R2:0.29001691860033396 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[52] Loss:0.28545181900262834 | L1 Loss:0.4233528286218643 | R2:0.25638684403476125 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[53] Loss:0.23427491635084152 | L1 Loss:0.3735836409032345 | R2:0.2859668880535142 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[53] Loss:0.28764312863349917 | L1 Loss:0.4127985090017319 | R2:0.24304027987359794 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[54] Loss:0.21982729900628328 | L1 Loss:0.3649675417691469 | R2:0.33004831702172766 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[54] Loss:0.28296054154634476 | L1 Loss:0.415715754032135 | R2:0.25376158455380526 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[55] Loss:0.23257976863533258 | L1 Loss:0.38077947311103344 | R2:0.2941464482287898 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[55] Loss:0.27776260375976564 | L1 Loss:0.40125393569469453 | R2:0.27964861401951197 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[56] Loss:0.22670786827802658 | L1 Loss:0.3753420878201723 | R2:0.305289735439958 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[56] Loss:0.2912420451641083 | L1 Loss:0.40767593383789064 | R2:0.23616788128715943 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[57] Loss:0.23228014446794987 | L1 Loss:0.37852472998201847 | R2:0.29632384789622185 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[57] Loss:0.26921881139278414 | L1 Loss:0.39924260377883913 | R2:0.285693777696284 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[58] Loss:0.2400051075965166 | L1 Loss:0.3866411577910185 | R2:0.27595384727900085 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[58] Loss:0.2792855933308601 | L1 Loss:0.42244004011154174 | R2:0.25008460882792427 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[59] Loss:0.23595478106290102 | L1 Loss:0.380303293466568 | R2:0.2839338887486598 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[59] Loss:0.27944921106100085 | L1 Loss:0.41080078184604646 | R2:0.2615860130212005 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[60] Loss:0.22478652372956276 | L1 Loss:0.37517863139510155 | R2:0.31322436743574844 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[60] Loss:0.27011832892894744 | L1 Loss:0.40267767012119293 | R2:0.28962989367035485 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[61] Loss:0.21299816202372313 | L1 Loss:0.3648810535669327 | R2:0.3531428170997859 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[61] Loss:0.2721979558467865 | L1 Loss:0.4087196260690689 | R2:0.28608400504225584 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[62] Loss:0.20785643812268972 | L1 Loss:0.3569273632019758 | R2:0.3712136208804662 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[62] Loss:0.2623893991112709 | L1 Loss:0.39826363921165464 | R2:0.3259017056523145 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[63] Loss:0.22647032048553228 | L1 Loss:0.36957645788788795 | R2:0.3107363026538661 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[63] Loss:0.26685668528079987 | L1 Loss:0.40052316784858705 | R2:0.30773427188204544 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[64] Loss:0.22926987893879414 | L1 Loss:0.37606347166001797 | R2:0.3014805543634084 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[64] Loss:0.2843900308012962 | L1 Loss:0.4158184498548508 | R2:0.26052441236282303 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[65] Loss:0.2245524087920785 | L1 Loss:0.3675388414412737 | R2:0.3097288271033199 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[65] Loss:0.26647197008132933 | L1 Loss:0.3996822327375412 | R2:0.31031812201199693 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[66] Loss:0.21383357141166925 | L1 Loss:0.36400260031223297 | R2:0.34639337526572067 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[66] Loss:0.27263218015432356 | L1 Loss:0.39912467300891874 | R2:0.29638620026520485 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[67] Loss:0.2101756976917386 | L1 Loss:0.35852355882525444 | R2:0.35965215099352665 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[67] Loss:0.2631655991077423 | L1 Loss:0.38562377393245695 | R2:0.3195044031756128 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[68] Loss:0.21077935863286257 | L1 Loss:0.3658061549067497 | R2:0.3577846277811071 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[68] Loss:0.25540871173143387 | L1 Loss:0.3876986920833588 | R2:0.3399104037983574 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[69] Loss:0.21329231653362513 | L1 Loss:0.36547980457544327 | R2:0.34840655107782015 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[69] Loss:0.27564917504787445 | L1 Loss:0.40299316346645353 | R2:0.2818848087160171 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[70] Loss:0.21293397806584835 | L1 Loss:0.3665430434048176 | R2:0.3514060033991251 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[70] Loss:0.266998378187418 | L1 Loss:0.39022050201892855 | R2:0.30592178156159555 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[71] Loss:0.21217401325702667 | L1 Loss:0.36223812215030193 | R2:0.35536603160991603 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[71] Loss:0.2626343198120594 | L1 Loss:0.3904921054840088 | R2:0.30550875362510516 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[72] Loss:0.21093472186475992 | L1 Loss:0.36366038396954536 | R2:0.3529709648874244 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[72] Loss:0.25655040740966795 | L1 Loss:0.38566254675388334 | R2:0.31889370016980484 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[73] Loss:0.21746253175660968 | L1 Loss:0.3749359492212534 | R2:0.3370486772629801 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[73] Loss:0.25862891376018526 | L1 Loss:0.38437786400318147 | R2:0.3156517752942019 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[74] Loss:0.2104337103664875 | L1 Loss:0.36307064443826675 | R2:0.3597224749122513 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[74] Loss:0.26841763257980344 | L1 Loss:0.3956190824508667 | R2:0.28138624251287975 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[75] Loss:0.2186462264508009 | L1 Loss:0.3694636709988117 | R2:0.3350795997851525 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[75] Loss:0.2632979989051819 | L1 Loss:0.39253202378749846 | R2:0.3007590241749162 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[76] Loss:0.23404438979923725 | L1 Loss:0.3881307300180197 | R2:0.28323763808897445 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[76] Loss:0.24289465099573135 | L1 Loss:0.37954978048801424 | R2:0.3614556414398888 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[77] Loss:0.2202962851151824 | L1 Loss:0.3703642040491104 | R2:0.3237055520635099 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[77] Loss:0.25434785038232804 | L1 Loss:0.39399489760398865 | R2:0.3356207423279803 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[78] Loss:0.21750670112669468 | L1 Loss:0.363666795194149 | R2:0.3357734132488531 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[78] Loss:0.24821964651346207 | L1 Loss:0.37686448395252226 | R2:0.35199741652853633 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[79] Loss:0.21525147277861834 | L1 Loss:0.37233950942754745 | R2:0.34546939893628553 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[79] Loss:0.25019770562648774 | L1 Loss:0.3767922580242157 | R2:0.3394188024783779 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[80] Loss:0.21287139412015676 | L1 Loss:0.36785558611154556 | R2:0.35258156037731814 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[80] Loss:0.25214712619781493 | L1 Loss:0.38092259466648104 | R2:0.34808327756108515 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[81] Loss:0.2162805274128914 | L1 Loss:0.36957672238349915 | R2:0.34254004692784384 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[81] Loss:0.24386606365442276 | L1 Loss:0.3734748214483261 | R2:0.3714476454701523 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[82] Loss:0.21426111459732056 | L1 Loss:0.3651871792972088 | R2:0.3490002979699512 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[82] Loss:0.2692969173192978 | L1 Loss:0.3911068320274353 | R2:0.2982609809818496 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[83] Loss:0.22254089452326298 | L1 Loss:0.3688960690051317 | R2:0.31800647183560016 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[83] Loss:0.26855795830488205 | L1 Loss:0.3923353761434555 | R2:0.28983946607987043 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[84] Loss:0.21072227600961924 | L1 Loss:0.3606782853603363 | R2:0.3557426321282303 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[84] Loss:0.2592438146471977 | L1 Loss:0.3957802504301071 | R2:0.33337496796322175 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[85] Loss:0.2120028785429895 | L1 Loss:0.35968248918652534 | R2:0.35212883684685947 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[85] Loss:0.24518809765577315 | L1 Loss:0.38163949847221373 | R2:0.35628007204469986 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[86] Loss:0.19347055815160275 | L1 Loss:0.34072582982480526 | R2:0.41208641101288296 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[86] Loss:0.24390020221471786 | L1 Loss:0.3831975430250168 | R2:0.36383992486940164 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[87] Loss:0.19511201418936253 | L1 Loss:0.34390489384531975 | R2:0.4030491710764898 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[87] Loss:0.24643657281994819 | L1 Loss:0.3912543773651123 | R2:0.357440893149181 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[88] Loss:0.21164252515882254 | L1 Loss:0.3585294019430876 | R2:0.35615364186329773 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[88] Loss:0.24801337867975234 | L1 Loss:0.3782269358634949 | R2:0.35697342473104743 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[89] Loss:0.20379496458917856 | L1 Loss:0.35167017579078674 | R2:0.3787445232418538 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[89] Loss:0.25000880658626556 | L1 Loss:0.39397430419921875 | R2:0.3436319792412025 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[90] Loss:0.20781812956556678 | L1 Loss:0.3586392477154732 | R2:0.36552140302287117 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[90] Loss:0.23709974586963653 | L1 Loss:0.3816386193037033 | R2:0.38368928767309096 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[91] Loss:0.20450842939317226 | L1 Loss:0.35439206287264824 | R2:0.3768544884586862 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[91] Loss:0.244834703207016 | L1 Loss:0.3795229196548462 | R2:0.3505855846974118 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[92] Loss:0.21875742822885513 | L1 Loss:0.36507837288081646 | R2:0.3334095319931234 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[92] Loss:0.2505320817232132 | L1 Loss:0.38042758107185365 | R2:0.3482496967231712 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[93] Loss:0.2187868319451809 | L1 Loss:0.3683114964514971 | R2:0.33559687452591724 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[93] Loss:0.2631609871983528 | L1 Loss:0.39466325640678407 | R2:0.3110121986946218 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[94] Loss:0.1999159287661314 | L1 Loss:0.35330211371183395 | R2:0.3946118668457558 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[94] Loss:0.24856275767087938 | L1 Loss:0.38682316839694975 | R2:0.3486382044986442 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[95] Loss:0.21675660833716393 | L1 Loss:0.3661220669746399 | R2:0.33707078406396634 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[95] Loss:0.25726828873157503 | L1 Loss:0.394224089384079 | R2:0.3177307155000698 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[96] Loss:0.20824677869677544 | L1 Loss:0.36483449302613735 | R2:0.36470875656962704 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[96] Loss:0.2587563320994377 | L1 Loss:0.39454489946365356 | R2:0.32149384773967365 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[97] Loss:0.21376734040677547 | L1 Loss:0.3618559781461954 | R2:0.35254377382613267 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[97] Loss:0.26245290115475656 | L1 Loss:0.4010545700788498 | R2:0.3053551330917852 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[98] Loss:0.20798524282872677 | L1 Loss:0.35582523234188557 | R2:0.37041632794286644 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[98] Loss:0.25013670027256013 | L1 Loss:0.3883603274822235 | R2:0.33642428556818443 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[99] Loss:0.2147326460108161 | L1 Loss:0.36382101476192474 | R2:0.343454385134238 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[99] Loss:0.2476937346160412 | L1 Loss:0.38696721792221067 | R2:0.3288612861060443 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[100] Loss:0.2121239947155118 | L1 Loss:0.36250724643468857 | R2:0.3543248550149606 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[100] Loss:0.2530226096510887 | L1 Loss:0.38719741106033323 | R2:0.3212860546454223 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[101] Loss:0.20346728153526783 | L1 Loss:0.35385676473379135 | R2:0.379375628506272 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[101] Loss:0.23422099202871322 | L1 Loss:0.3737641453742981 | R2:0.379554901672826 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[102] Loss:0.21316340938210487 | L1 Loss:0.361549511551857 | R2:0.3462728328512586 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[102] Loss:0.2423267439007759 | L1 Loss:0.3773030132055283 | R2:0.36437166685343625 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[103] Loss:0.20560973603278399 | L1 Loss:0.35074409283697605 | R2:0.373867260123975 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[103] Loss:0.24033602476119995 | L1 Loss:0.3791319340467453 | R2:0.36633351392857516 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[104] Loss:0.2061864174902439 | L1 Loss:0.3542059175670147 | R2:0.3700022644933022 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[104] Loss:0.2458352878689766 | L1 Loss:0.3809629648923874 | R2:0.3435093635714744 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[105] Loss:0.2058912217617035 | L1 Loss:0.35997460037469864 | R2:0.3707946785419805 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[105] Loss:0.2364506706595421 | L1 Loss:0.37757304310798645 | R2:0.36908791006491326 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[106] Loss:0.20520273223519325 | L1 Loss:0.3513717334717512 | R2:0.37765870740841323 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[106] Loss:0.24754871428012848 | L1 Loss:0.380426287651062 | R2:0.34113283869491984 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[107] Loss:0.21202633529901505 | L1 Loss:0.3598206862807274 | R2:0.3584395143278648 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[107] Loss:0.24728454053401946 | L1 Loss:0.3813235372304916 | R2:0.34050747968840617 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[108] Loss:0.21654641348868608 | L1 Loss:0.3623470552265644 | R2:0.3355999699537556 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[108] Loss:0.25240253657102585 | L1 Loss:0.38501821756362914 | R2:0.3263260671005253 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[109] Loss:0.2074525859206915 | L1 Loss:0.35938222147524357 | R2:0.3709439690360317 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[109] Loss:0.24709401726722718 | L1 Loss:0.3770031124353409 | R2:0.34459106511268545 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[110] Loss:0.20479694986715913 | L1 Loss:0.3502341266721487 | R2:0.3780598012254625 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[110] Loss:0.24330573678016662 | L1 Loss:0.3812481939792633 | R2:0.35293029658482744 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[111] Loss:0.201871904078871 | L1 Loss:0.35019924119114876 | R2:0.3903386883493405 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[111] Loss:0.24603745639324187 | L1 Loss:0.380861234664917 | R2:0.3384172947722525 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[112] Loss:0.19685675203800201 | L1 Loss:0.34411317855119705 | R2:0.4006243738892862 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[112] Loss:0.24326880574226378 | L1 Loss:0.37890284657478335 | R2:0.3491089154105106 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[113] Loss:0.19781759660691023 | L1 Loss:0.34302243683487177 | R2:0.39570368772178144 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[113] Loss:0.2529334262013435 | L1 Loss:0.3891117811203003 | R2:0.31194967055911527 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[114] Loss:0.19983206875622272 | L1 Loss:0.34511279314756393 | R2:0.3911531092434325 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[114] Loss:0.2480890303850174 | L1 Loss:0.3859832376241684 | R2:0.3452391964402869 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[115] Loss:0.19673282327130437 | L1 Loss:0.3444270137697458 | R2:0.40327060832137107 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[115] Loss:0.24446629732847214 | L1 Loss:0.38328295946121216 | R2:0.351455855258095 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[116] Loss:0.19122610706835985 | L1 Loss:0.3360765241086483 | R2:0.42406542618820225 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[116] Loss:0.24819750040769578 | L1 Loss:0.3825100749731064 | R2:0.3327689944237743 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[117] Loss:0.20339970290660858 | L1 Loss:0.3440158935263753 | R2:0.38331827477761865 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[117] Loss:0.2513034462928772 | L1 Loss:0.38920783400535586 | R2:0.31928092632091676 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[118] Loss:0.20696550142019987 | L1 Loss:0.3564152978360653 | R2:0.3716958454389508 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[118] Loss:0.2500155732035637 | L1 Loss:0.3898053735494614 | R2:0.3353175134195805 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[119] Loss:0.20536832138895988 | L1 Loss:0.3539302181452513 | R2:0.3753465689180683 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[119] Loss:0.25240987837314605 | L1 Loss:0.3884507268667221 | R2:0.32170452844828035 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[120] Loss:0.20024854503571987 | L1 Loss:0.3430913230404258 | R2:0.38884208404257026 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[120] Loss:0.24073462337255477 | L1 Loss:0.38547201454639435 | R2:0.3574502856190588 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[121] Loss:0.20557301864027977 | L1 Loss:0.3443458303809166 | R2:0.37066297392387526 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[121] Loss:0.2435978904366493 | L1 Loss:0.38564133644104004 | R2:0.3478924144214238 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[122] Loss:0.20481215324252844 | L1 Loss:0.34677837416529655 | R2:0.37866515504655585 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[122] Loss:0.2491702452301979 | L1 Loss:0.3825191259384155 | R2:0.35152248911931117 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[123] Loss:0.20144293969497085 | L1 Loss:0.34346344508230686 | R2:0.38707815480320085 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[123] Loss:0.2508729130029678 | L1 Loss:0.3947069585323334 | R2:0.3446822048539347 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[124] Loss:0.2006372124888003 | L1 Loss:0.34588861651718616 | R2:0.3939988695380321 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[124] Loss:0.25141802430152893 | L1 Loss:0.3890176177024841 | R2:0.333466370334802 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[125] Loss:0.19534555729478598 | L1 Loss:0.33983245864510536 | R2:0.40793777258081065 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[125] Loss:0.24860255867242814 | L1 Loss:0.3820530563592911 | R2:0.3486174687044786 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[126] Loss:0.19882727228105068 | L1 Loss:0.3430273849517107 | R2:0.3931205198142102 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[126] Loss:0.2420426458120346 | L1 Loss:0.38057438731193544 | R2:0.3555851582247991 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[127] Loss:0.2012949576601386 | L1 Loss:0.3477664887905121 | R2:0.3954845934945004 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[127] Loss:0.2403952121734619 | L1 Loss:0.37749647796154023 | R2:0.3584273297767891 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[128] Loss:0.1978538567200303 | L1 Loss:0.3427063897252083 | R2:0.39898823377287074 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[128] Loss:0.2415514811873436 | L1 Loss:0.3779588550329208 | R2:0.352008298109764 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[129] Loss:0.20180271938443184 | L1 Loss:0.34624150954186916 | R2:0.39130991632221396 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[129] Loss:0.2434538185596466 | L1 Loss:0.3761924237012863 | R2:0.35456429985912763 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[130] Loss:0.1982497051358223 | L1 Loss:0.3405882650986314 | R2:0.3999897238169522 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[130] Loss:0.2636104613542557 | L1 Loss:0.40255263447761536 | R2:0.30255917226450557 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[131] Loss:0.18585956515744328 | L1 Loss:0.33269935566931963 | R2:0.4394678931873186 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[131] Loss:0.2287165954709053 | L1 Loss:0.37306526899337766 | R2:0.3878865106574171 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[132] Loss:0.19105552323162556 | L1 Loss:0.3331433888524771 | R2:0.42825723305057917 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[132] Loss:0.23445329070091248 | L1 Loss:0.3731182008981705 | R2:0.3729335427030608 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[133] Loss:0.20317419758066535 | L1 Loss:0.3490862809121609 | R2:0.38585322760289037 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[133] Loss:0.24244148284196854 | L1 Loss:0.38338805437088014 | R2:0.3625879988600092 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[134] Loss:0.1984913949854672 | L1 Loss:0.3439251398667693 | R2:0.4013372846984247 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[134] Loss:0.257585845887661 | L1 Loss:0.3926004558801651 | R2:0.31347074713847645 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[135] Loss:0.1896096090786159 | L1 Loss:0.33649884909391403 | R2:0.42550535599705874 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[135] Loss:0.25247765630483626 | L1 Loss:0.3887371599674225 | R2:0.32829681188923 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[136] Loss:0.19128648191690445 | L1 Loss:0.3379249293357134 | R2:0.41788393522958445 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[136] Loss:0.24263662993907928 | L1 Loss:0.3851175457239151 | R2:0.356951178094186 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[137] Loss:0.19363554753363132 | L1 Loss:0.3409787677228451 | R2:0.41235529317668396 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[137] Loss:0.24591961055994033 | L1 Loss:0.3877794712781906 | R2:0.3440765116543682 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[138] Loss:0.1883803168311715 | L1 Loss:0.33652872312814 | R2:0.43191780007546454 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[138] Loss:0.2470268115401268 | L1 Loss:0.3892462581396103 | R2:0.3342601584490252 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[139] Loss:0.1826926739886403 | L1 Loss:0.32921271305531263 | R2:0.4484107114339341 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[139] Loss:0.2481944367289543 | L1 Loss:0.3922033101320267 | R2:0.33591057284956144 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[140] Loss:0.1897370731458068 | L1 Loss:0.33516258373856544 | R2:0.42783672616614743 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[140] Loss:0.25955072194337847 | L1 Loss:0.401121723651886 | R2:0.3163481060806098 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[141] Loss:0.1907593565993011 | L1 Loss:0.3367714658379555 | R2:0.4263212114859014 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[141] Loss:0.24006879925727845 | L1 Loss:0.3880628734827042 | R2:0.3655097628997243 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[142] Loss:0.191664163954556 | L1 Loss:0.3412829302251339 | R2:0.42480831094705873 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[142] Loss:0.2500614568591118 | L1 Loss:0.3796133667230606 | R2:0.34026973585781023 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[143] Loss:0.20403989171609282 | L1 Loss:0.35390617698431015 | R2:0.38051180254773587 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[143] Loss:0.24860458076000214 | L1 Loss:0.385051628947258 | R2:0.33997545782454075 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[144] Loss:0.1923929937183857 | L1 Loss:0.33671810105443 | R2:0.41785965908018236 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[144] Loss:0.24562777280807496 | L1 Loss:0.38061562478542327 | R2:0.34315736693042076 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[145] Loss:0.19314633961766958 | L1 Loss:0.3355231359601021 | R2:0.4183237677285011 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[145] Loss:0.25476448237895966 | L1 Loss:0.391022652387619 | R2:0.31524339739635393 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[146] Loss:0.20567358005791903 | L1 Loss:0.34807787649333477 | R2:0.37810174026028326 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[146] Loss:0.2521429508924484 | L1 Loss:0.38994528651237487 | R2:0.32844504621413695 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[147] Loss:0.19835012592375278 | L1 Loss:0.34148912876844406 | R2:0.4034415694020138 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[147] Loss:0.24682652503252028 | L1 Loss:0.38838382065296173 | R2:0.3417784091245156 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[148] Loss:0.19222157448530197 | L1 Loss:0.33650924917310476 | R2:0.4189948900696379 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[148] Loss:0.24010253548622132 | L1 Loss:0.3889870852231979 | R2:0.3480996246999237 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[149] Loss:0.19409159012138844 | L1 Loss:0.3359065521508455 | R2:0.41751505883281176 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[149] Loss:0.24870129823684692 | L1 Loss:0.3880093812942505 | R2:0.3309524700535295 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[150] Loss:0.18163454998284578 | L1 Loss:0.3305919338017702 | R2:0.45671302323026686 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[150] Loss:0.24424416273832322 | L1 Loss:0.3780578225851059 | R2:0.3503747601210717 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[151] Loss:0.19073268212378025 | L1 Loss:0.3373576635494828 | R2:0.43023457760516204 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[151] Loss:0.24264786690473555 | L1 Loss:0.38719152808189394 | R2:0.353597629534468 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[152] Loss:0.19113447004929185 | L1 Loss:0.3331052251160145 | R2:0.4262250785676625 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[152] Loss:0.2625256985425949 | L1 Loss:0.4006302982568741 | R2:0.30867602250233916 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[153] Loss:0.18689397256821394 | L1 Loss:0.3347589373588562 | R2:0.4359601048044888 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[153] Loss:0.247143517434597 | L1 Loss:0.39042403995990754 | R2:0.351040261446692 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[154] Loss:0.19539882522076368 | L1 Loss:0.33566946163773537 | R2:0.41431099282805034 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[154] Loss:0.2533540308475494 | L1 Loss:0.3886795938014984 | R2:0.32555984430646967 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[155] Loss:0.19187695905566216 | L1 Loss:0.3347466439008713 | R2:0.4245258230580743 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[155] Loss:0.2448272079229355 | L1 Loss:0.3827551305294037 | R2:0.347769149476596 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[156] Loss:0.183693440631032 | L1 Loss:0.3279997194185853 | R2:0.4453554764630366 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[156] Loss:0.2523806422948837 | L1 Loss:0.3904110938310623 | R2:0.3312033104971455 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[157] Loss:0.18283705692738295 | L1 Loss:0.33077091816812754 | R2:0.446439853196255 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[157] Loss:0.23821583539247512 | L1 Loss:0.37417306303977965 | R2:0.35856853771602964 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[158] Loss:0.1860709683969617 | L1 Loss:0.3289724290370941 | R2:0.438374561578719 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[158] Loss:0.2435038208961487 | L1 Loss:0.38231482207775114 | R2:0.35815757630081607 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[159] Loss:0.19270180305466056 | L1 Loss:0.3361079301685095 | R2:0.416447256050744 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[159] Loss:0.24904343038797377 | L1 Loss:0.3844212919473648 | R2:0.331185947961925 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[160] Loss:0.18649565009400249 | L1 Loss:0.33266746811568737 | R2:0.4393359522420454 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[160] Loss:0.2512571901082993 | L1 Loss:0.3876156181097031 | R2:0.3220086371252139 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[161] Loss:0.18513247417286038 | L1 Loss:0.33148898743093014 | R2:0.44633737795115325 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[161] Loss:0.26063296347856524 | L1 Loss:0.3948113054037094 | R2:0.30065403476181907 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[162] Loss:0.18228077655658126 | L1 Loss:0.3290792182087898 | R2:0.45119894359929374 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[162] Loss:0.2493356093764305 | L1 Loss:0.3855461150407791 | R2:0.3396789399812198 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[163] Loss:0.1802174150943756 | L1 Loss:0.3273008204996586 | R2:0.4607433343336771 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[163] Loss:0.25880234986543654 | L1 Loss:0.3968372493982315 | R2:0.301046294441475 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[164] Loss:0.18245674297213554 | L1 Loss:0.33159823901951313 | R2:0.45219601101549034 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[164] Loss:0.260577030479908 | L1 Loss:0.39837194979190826 | R2:0.3042850269707662 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[165] Loss:0.1845012465491891 | L1 Loss:0.33076195791363716 | R2:0.45150837371974295 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[165] Loss:0.2537012279033661 | L1 Loss:0.39085022509098055 | R2:0.3205260572141226 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[166] Loss:0.18730464577674866 | L1 Loss:0.3394938353449106 | R2:0.43701474983188915 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[166] Loss:0.2461165025830269 | L1 Loss:0.3837438464164734 | R2:0.34074184935473656 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[167] Loss:0.18498808797448874 | L1 Loss:0.32854836992919445 | R2:0.4435347435258013 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[167] Loss:0.238596573472023 | L1 Loss:0.3730347901582718 | R2:0.36580941886423596 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[168] Loss:0.18499419558793306 | L1 Loss:0.33444883208721876 | R2:0.44462539350349306 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[168] Loss:0.24529525488615037 | L1 Loss:0.37814687490463256 | R2:0.3454868180180505 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[169] Loss:0.19413732318207622 | L1 Loss:0.3403170658275485 | R2:0.4213372240363875 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[169] Loss:0.25132151544094083 | L1 Loss:0.3797874301671982 | R2:0.3229238485195539 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[170] Loss:0.19505144702270627 | L1 Loss:0.34292672108858824 | R2:0.4145527988101535 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[170] Loss:0.24915037602186202 | L1 Loss:0.37990556061267855 | R2:0.33008248965546433 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[171] Loss:0.18487863522022963 | L1 Loss:0.3344328682869673 | R2:0.4517105302840051 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[171] Loss:0.2472606360912323 | L1 Loss:0.3799252867698669 | R2:0.34063425201890113 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[172] Loss:0.18941762717440724 | L1 Loss:0.33580133877694607 | R2:0.43097673606151443 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[172] Loss:0.24951565712690355 | L1 Loss:0.38465145230293274 | R2:0.3369523665987818 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[173] Loss:0.18750622309744358 | L1 Loss:0.3295669611543417 | R2:0.4373611106204768 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[173] Loss:0.25506009757518766 | L1 Loss:0.384076189994812 | R2:0.3168835568592286 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[174] Loss:0.19591120351105928 | L1 Loss:0.34166449401527643 | R2:0.41309707615538666 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[174] Loss:0.25997251868247984 | L1 Loss:0.3832638323307037 | R2:0.3094368027156433 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[175] Loss:0.18783252965658903 | L1 Loss:0.3351008761674166 | R2:0.4322399200148533 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[175] Loss:0.2640278622508049 | L1 Loss:0.3832335025072098 | R2:0.29310099218579166 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[176] Loss:0.19392111618071795 | L1 Loss:0.3429928859695792 | R2:0.41593447357861313 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[176] Loss:0.2523650601506233 | L1 Loss:0.3777829498052597 | R2:0.321735745409704 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[177] Loss:0.18041930720210075 | L1 Loss:0.32152828946709633 | R2:0.45053065344345317 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[177] Loss:0.25495700240135194 | L1 Loss:0.3804775208234787 | R2:0.3257002673676494 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[178] Loss:0.19241152331233025 | L1 Loss:0.33770338352769613 | R2:0.418183174636397 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[178] Loss:0.24951454102993012 | L1 Loss:0.37590635418891905 | R2:0.34161558149925214 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[179] Loss:0.18900094646960497 | L1 Loss:0.336855829693377 | R2:0.4290253905202988 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[179] Loss:0.24169953167438507 | L1 Loss:0.37416446805000303 | R2:0.35988453406469134 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[180] Loss:0.19667238369584084 | L1 Loss:0.34494948014616966 | R2:0.40729305414603556 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[180] Loss:0.2571423873305321 | L1 Loss:0.3908173739910126 | R2:0.31343051952535744 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[181] Loss:0.19366493402048945 | L1 Loss:0.34008074551820755 | R2:0.41522021509501916 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[181] Loss:0.2589167281985283 | L1 Loss:0.39237345159053805 | R2:0.30969325394240593 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[182] Loss:0.18899398064240813 | L1 Loss:0.3324219658970833 | R2:0.4307366127270634 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[182] Loss:0.2407685860991478 | L1 Loss:0.37561355233192445 | R2:0.36014059750233207 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[183] Loss:0.18532542092725635 | L1 Loss:0.33185367099940777 | R2:0.43786740004016833 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[183] Loss:0.23743961602449418 | L1 Loss:0.37006141245365143 | R2:0.3667109100646332 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[184] Loss:0.1876161191612482 | L1 Loss:0.33732657693326473 | R2:0.43195828952013204 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[184] Loss:0.24593871384859084 | L1 Loss:0.379234179854393 | R2:0.34164187088758025 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[185] Loss:0.18892899248749018 | L1 Loss:0.3378211837261915 | R2:0.42718886602018763 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[185] Loss:0.23960512429475783 | L1 Loss:0.36576642394065856 | R2:0.35672715930385096 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[186] Loss:0.1800443516112864 | L1 Loss:0.32938831485807896 | R2:0.46055949792466044 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[186] Loss:0.2362503081560135 | L1 Loss:0.3756307363510132 | R2:0.36479448278030774 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[187] Loss:0.18717758683487773 | L1 Loss:0.3323399070650339 | R2:0.43636254095741783 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[187] Loss:0.2412193387746811 | L1 Loss:0.38329219818115234 | R2:0.3547817666376147 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[188] Loss:0.1879910994321108 | L1 Loss:0.3362364526838064 | R2:0.4339305713405613 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[188] Loss:0.2300230734050274 | L1 Loss:0.36632430255413057 | R2:0.3782813239390881 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[189] Loss:0.1837365608662367 | L1 Loss:0.3300611302256584 | R2:0.4519987931703892 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[189] Loss:0.24569966197013854 | L1 Loss:0.37563619315624236 | R2:0.34382108039919385 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[190] Loss:0.17889994103461504 | L1 Loss:0.32455665804445744 | R2:0.4623967895857993 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[190] Loss:0.24252336770296096 | L1 Loss:0.37442231774330137 | R2:0.3529256835048341 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[191] Loss:0.18228886974975467 | L1 Loss:0.331867353990674 | R2:0.45458678275552344 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[191] Loss:0.2472128763794899 | L1 Loss:0.37758137881755827 | R2:0.3418355654782238 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[192] Loss:0.1830992945469916 | L1 Loss:0.32763661071658134 | R2:0.4524501563244926 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[192] Loss:0.24764979183673858 | L1 Loss:0.38342762887477877 | R2:0.3355090797631965 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[193] Loss:0.18510397896170616 | L1 Loss:0.32733086682856083 | R2:0.44115973549672194 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[193] Loss:0.26383905559778215 | L1 Loss:0.3925505667924881 | R2:0.29351375286455933 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[194] Loss:0.18623941158875823 | L1 Loss:0.33331093564629555 | R2:0.4376859285600453 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[194] Loss:0.25680429935455323 | L1 Loss:0.3835541784763336 | R2:0.31066651491391084 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[195] Loss:0.18612050265073776 | L1 Loss:0.3346893712878227 | R2:0.43927354471953894 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[195] Loss:0.247861111164093 | L1 Loss:0.37979416847229003 | R2:0.33727904384014706 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[196] Loss:0.18793175974860787 | L1 Loss:0.33492826391011477 | R2:0.4382612922372607 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[196] Loss:0.26371439546346664 | L1 Loss:0.39950995743274687 | R2:0.29908531967552204 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[197] Loss:0.17797312699258327 | L1 Loss:0.3240987453609705 | R2:0.4611684345778377 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[197] Loss:0.2583121910691261 | L1 Loss:0.3856656730175018 | R2:0.31014865720425266 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[198] Loss:0.17623936152085662 | L1 Loss:0.32176355458796024 | R2:0.47103348669433026 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[198] Loss:0.2601668700575829 | L1 Loss:0.3858312159776688 | R2:0.3079570877039939 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[199] Loss:0.1799565157853067 | L1 Loss:0.32429651357233524 | R2:0.45740037803542594 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[199] Loss:0.25969918817281723 | L1 Loss:0.38363373279571533 | R2:0.3137864701521343 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[200] Loss:0.18402722803875804 | L1 Loss:0.32910347171127796 | R2:0.4438558906734437 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[200] Loss:0.2524081751704216 | L1 Loss:0.3767555505037308 | R2:0.3350761193537064 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[201] Loss:0.18832646729424596 | L1 Loss:0.3319982532411814 | R2:0.4281785157816995 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[201] Loss:0.258272323012352 | L1 Loss:0.38764219880104067 | R2:0.31650978175144107 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[202] Loss:0.18762598372995853 | L1 Loss:0.33237025886774063 | R2:0.43764974136983315 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[202] Loss:0.24813560992479325 | L1 Loss:0.3850217640399933 | R2:0.34421064424855713 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[203] Loss:0.1825117226690054 | L1 Loss:0.32910893205553293 | R2:0.45400553726038206 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[203] Loss:0.23960478305816652 | L1 Loss:0.37824121713638303 | R2:0.3663405274284643 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[204] Loss:0.18209249805659056 | L1 Loss:0.3296411680057645 | R2:0.4495603892352035 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[204] Loss:0.2389364942908287 | L1 Loss:0.3744577616453171 | R2:0.3697811049987993 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[205] Loss:0.1932357382029295 | L1 Loss:0.3396389940753579 | R2:0.4134319692779588 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[205] Loss:0.2433249607682228 | L1 Loss:0.3729908406734467 | R2:0.36166440844282477 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[206] Loss:0.1828266908414662 | L1 Loss:0.3311252761632204 | R2:0.4485498053546552 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[206] Loss:0.25129170417785646 | L1 Loss:0.38705115020275116 | R2:0.3423317891334645 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[207] Loss:0.18240209389477968 | L1 Loss:0.33226701244711876 | R2:0.44797225352182635 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[207] Loss:0.25180253833532334 | L1 Loss:0.388236340880394 | R2:0.3405907483383653 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[208] Loss:0.18522173492237926 | L1 Loss:0.32851169630885124 | R2:0.4449393152301097 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[208] Loss:0.24797750860452653 | L1 Loss:0.3804678976535797 | R2:0.34625550019381635 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[209] Loss:0.18021819274872541 | L1 Loss:0.3275029780343175 | R2:0.4577351016947858 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[209] Loss:0.24775975495576857 | L1 Loss:0.37438054382801056 | R2:0.35110339791956935 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[210] Loss:0.19338290393352509 | L1 Loss:0.3408976010978222 | R2:0.41845344197181145 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[210] Loss:0.2447943091392517 | L1 Loss:0.37296985685825346 | R2:0.35105945149909734 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[211] Loss:0.18864084919914603 | L1 Loss:0.33703329786658287 | R2:0.43207252836389304 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[211] Loss:0.24173667430877685 | L1 Loss:0.3775178849697113 | R2:0.3594855394501296 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[212] Loss:0.18463926296681166 | L1 Loss:0.33363323099911213 | R2:0.44495575809228244 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[212] Loss:0.24165430366992952 | L1 Loss:0.3792677789926529 | R2:0.34725723884920157 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[213] Loss:0.18519061617553234 | L1 Loss:0.3281473070383072 | R2:0.44388634637008945 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[213] Loss:0.23727730959653853 | L1 Loss:0.37409044802188873 | R2:0.36050747084895923 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[214] Loss:0.18254569731652737 | L1 Loss:0.32841052021831274 | R2:0.4483192687057926 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[214] Loss:0.24400218725204467 | L1 Loss:0.3783900886774063 | R2:0.3430401533253161 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[215] Loss:0.1857795799151063 | L1 Loss:0.3298713844269514 | R2:0.43599080239244903 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[215] Loss:0.2571767583489418 | L1 Loss:0.38921641409397123 | R2:0.3121510154502393 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[216] Loss:0.18304387852549553 | L1 Loss:0.3318652082234621 | R2:0.449614660604884 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[216] Loss:0.24812184274196625 | L1 Loss:0.3768489331007004 | R2:0.3406950995237342 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[217] Loss:0.17646826896816492 | L1 Loss:0.32045366056263447 | R2:0.4715524760087521 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[217] Loss:0.25158472955226896 | L1 Loss:0.3849033325910568 | R2:0.33094697575985377 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[218] Loss:0.173613375518471 | L1 Loss:0.3191306935623288 | R2:0.4804295838177892 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[218] Loss:0.2402869716286659 | L1 Loss:0.37515814900398253 | R2:0.36664865318441225 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[219] Loss:0.16747769294306636 | L1 Loss:0.3143099481239915 | R2:0.4947939690806008 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[219] Loss:0.2437485322356224 | L1 Loss:0.37984482049942014 | R2:0.35492281917011487 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[220] Loss:0.17602987168356776 | L1 Loss:0.3174766506999731 | R2:0.46512587378369047 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[220] Loss:0.23770488798618317 | L1 Loss:0.37427926659584043 | R2:0.3707807594487336 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[221] Loss:0.17772222450003028 | L1 Loss:0.32477364130318165 | R2:0.46691881449283956 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[221] Loss:0.24262830317020417 | L1 Loss:0.37640256285667417 | R2:0.35240474390043863 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[222] Loss:0.1789813726209104 | L1 Loss:0.3179021207615733 | R2:0.4584137154674156 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[222] Loss:0.25585713386535647 | L1 Loss:0.3883084386587143 | R2:0.3179114373587478 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[223] Loss:0.17294031754136086 | L1 Loss:0.3192936833947897 | R2:0.47842193332127103 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[223] Loss:0.24883550703525542 | L1 Loss:0.3877509742975235 | R2:0.33228673116923846 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[224] Loss:0.17771624214947224 | L1 Loss:0.3230823017656803 | R2:0.46203378437431625 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[224] Loss:0.2472889631986618 | L1 Loss:0.38184181451797483 | R2:0.3473292358172465 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[225] Loss:0.17444876581430435 | L1 Loss:0.3157772272825241 | R2:0.4782239968658149 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[225] Loss:0.2489650294184685 | L1 Loss:0.383864164352417 | R2:0.3476073692615335 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[226] Loss:0.17567509040236473 | L1 Loss:0.32225634437054396 | R2:0.4714651266373601 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[226] Loss:0.2544562891125679 | L1 Loss:0.38652179539203646 | R2:0.3307208975067012 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[227] Loss:0.17798692546784878 | L1 Loss:0.32179250102490187 | R2:0.461446999384655 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[227] Loss:0.2378923699259758 | L1 Loss:0.3735799044370651 | R2:0.37570642211881294 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[228] Loss:0.17156858323141932 | L1 Loss:0.3130495920777321 | R2:0.47904251409265686 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[228] Loss:0.2625722199678421 | L1 Loss:0.39557007551193235 | R2:0.30615771018145027 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[229] Loss:0.17442902410402894 | L1 Loss:0.31698484532535076 | R2:0.47230151798525455 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[229] Loss:0.25054495632648466 | L1 Loss:0.38220019936561583 | R2:0.335355398756061 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[230] Loss:0.17825777269899845 | L1 Loss:0.3180189710110426 | R2:0.46282845763353553 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[230] Loss:0.2586119547486305 | L1 Loss:0.3884259283542633 | R2:0.3176943213492324 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[231] Loss:0.18063104059547186 | L1 Loss:0.32285863533616066 | R2:0.4572579719443028 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[231] Loss:0.2416409119963646 | L1 Loss:0.38224944174289704 | R2:0.36580816354793183 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[232] Loss:0.18314666952937841 | L1 Loss:0.32407766580581665 | R2:0.448961968862011 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[232] Loss:0.24423460364341737 | L1 Loss:0.376860523223877 | R2:0.3574067290202506 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[233] Loss:0.1746982135809958 | L1 Loss:0.3203620482236147 | R2:0.47832117077458014 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[233] Loss:0.2374865487217903 | L1 Loss:0.36771470308303833 | R2:0.3688911492871216 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[234] Loss:0.18429186986759305 | L1 Loss:0.3255256824195385 | R2:0.4474613468173282 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[234] Loss:0.24548159688711166 | L1 Loss:0.3795980215072632 | R2:0.3509905027498737 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[235] Loss:0.17932937247678638 | L1 Loss:0.3297624755650759 | R2:0.45876808921556417 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[235] Loss:0.24345018789172174 | L1 Loss:0.37155753970146177 | R2:0.3610280360622988 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[236] Loss:0.18141541816294193 | L1 Loss:0.32538590393960476 | R2:0.4535578596152034 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[236] Loss:0.24706340730190277 | L1 Loss:0.3793664962053299 | R2:0.3456890466799746 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[237] Loss:0.19002763601019979 | L1 Loss:0.3392773624509573 | R2:0.4279904582276664 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[237] Loss:0.24579859524965286 | L1 Loss:0.3773406624794006 | R2:0.34068534550371715 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[238] Loss:0.18298555631190538 | L1 Loss:0.33093263767659664 | R2:0.4499431450789214 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[238] Loss:0.25362214595079424 | L1 Loss:0.3832250446081161 | R2:0.3212649947279757 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[239] Loss:0.18440202809870243 | L1 Loss:0.3344382494688034 | R2:0.44170454027787975 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[239] Loss:0.25513914227485657 | L1 Loss:0.38678680658340453 | R2:0.31433695655615984 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[240] Loss:0.17380732903257012 | L1 Loss:0.32800279930233955 | R2:0.4778120166806618 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[240] Loss:0.2414604738354683 | L1 Loss:0.37630575299263 | R2:0.3533497224979651 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[241] Loss:0.17225935170426965 | L1 Loss:0.3245661873370409 | R2:0.4860924914024454 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[241] Loss:0.2482261076569557 | L1 Loss:0.38294683396816254 | R2:0.3299963914728162 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[242] Loss:0.17617195611819625 | L1 Loss:0.32181964069604874 | R2:0.4719035326258044 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[242] Loss:0.24553257524967192 | L1 Loss:0.3800359219312668 | R2:0.34464857203488164 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[243] Loss:0.17628578562289476 | L1 Loss:0.32069359347224236 | R2:0.46944787769102925 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[243] Loss:0.23829825073480607 | L1 Loss:0.37275044023990633 | R2:0.36388537044249847 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[244] Loss:0.16875295853242278 | L1 Loss:0.31267063319683075 | R2:0.4894579314284476 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[244] Loss:0.24422235190868377 | L1 Loss:0.37599450945854185 | R2:0.35085730501002754 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[245] Loss:0.17361873062327504 | L1 Loss:0.31829248182475567 | R2:0.4763937549279399 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[245] Loss:0.23802105337381363 | L1 Loss:0.37528073489665986 | R2:0.367295435894596 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[246] Loss:0.1736361300572753 | L1 Loss:0.3171209543943405 | R2:0.47888894725489733 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[246] Loss:0.2453760400414467 | L1 Loss:0.382653084397316 | R2:0.3525047655979871 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[247] Loss:0.17573129665106535 | L1 Loss:0.32105089351534843 | R2:0.47187693544660975 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[247] Loss:0.235633547604084 | L1 Loss:0.3713808685541153 | R2:0.3718287115562381 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[248] Loss:0.17631771182641387 | L1 Loss:0.3209988521412015 | R2:0.4701559423783851 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[248] Loss:0.2430865064263344 | L1 Loss:0.37681185007095336 | R2:0.36231423953016395 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[249] Loss:0.17942732106894255 | L1 Loss:0.3239268846809864 | R2:0.45950855351231235 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[249] Loss:0.24585501998662948 | L1 Loss:0.377913162112236 | R2:0.3572790732172579 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[250] Loss:0.17380019603297114 | L1 Loss:0.32503500394523144 | R2:0.4790959728592675 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[250] Loss:0.24350854754447937 | L1 Loss:0.3749912023544312 | R2:0.3555208707930456 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[251] Loss:0.1751330359838903 | L1 Loss:0.3238095697015524 | R2:0.47415978758983635 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[251] Loss:0.24374031722545625 | L1 Loss:0.3719082772731781 | R2:0.35663221971040404 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[252] Loss:0.17362464498728514 | L1 Loss:0.3242291435599327 | R2:0.47349312970258517 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[252] Loss:0.24220984429121017 | L1 Loss:0.37660265266895293 | R2:0.3546294054987052 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[253] Loss:0.17290786048397422 | L1 Loss:0.32047336734831333 | R2:0.4771990603906525 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[253] Loss:0.24664034843444824 | L1 Loss:0.38335140943527224 | R2:0.3432735516546808 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[254] Loss:0.16556084295734763 | L1 Loss:0.3126677554100752 | R2:0.5038073956536022 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[254] Loss:0.24082318246364592 | L1 Loss:0.36932254433631895 | R2:0.3573092774427721 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[255] Loss:0.17355510173365474 | L1 Loss:0.31822645012289286 | R2:0.47616499296206566 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[255] Loss:0.2521297797560692 | L1 Loss:0.3809981673955917 | R2:0.33393251161712656 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[256] Loss:0.16977749858051538 | L1 Loss:0.31514424458146095 | R2:0.49011265283347205 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[256] Loss:0.23947882801294326 | L1 Loss:0.37871296107769015 | R2:0.3621671590866306 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[257] Loss:0.17139527387917042 | L1 Loss:0.31501787062734365 | R2:0.4828715056396071 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[257] Loss:0.2502123400568962 | L1 Loss:0.3804272323846817 | R2:0.331579473214924 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[258] Loss:0.16798089677467942 | L1 Loss:0.3116292580962181 | R2:0.4964888018474017 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[258] Loss:0.2524727836251259 | L1 Loss:0.3862458109855652 | R2:0.32866095422099756 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[259] Loss:0.16572854248806834 | L1 Loss:0.3096919134259224 | R2:0.5052171930617024 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[259] Loss:0.24287782311439515 | L1 Loss:0.3773783534765244 | R2:0.35848868432078884 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[260] Loss:0.16252070479094982 | L1 Loss:0.3037475999444723 | R2:0.5139055656719302 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[260] Loss:0.24062999337911606 | L1 Loss:0.3738382160663605 | R2:0.3534279424574636 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[261] Loss:0.1622585654258728 | L1 Loss:0.3083035433664918 | R2:0.5186130711731939 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[261] Loss:0.2292800575494766 | L1 Loss:0.36812554895877836 | R2:0.3884622326964202 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[262] Loss:0.16799434460699558 | L1 Loss:0.31784658785909414 | R2:0.4974996848980577 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[262] Loss:0.24638870880007743 | L1 Loss:0.38289749026298525 | R2:0.3435405496103748 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[263] Loss:0.17044394835829735 | L1 Loss:0.32012877333909273 | R2:0.49260356490738405 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[263] Loss:0.23889582753181457 | L1 Loss:0.3746471732854843 | R2:0.368022664594067 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[264] Loss:0.17250728700309992 | L1 Loss:0.3193969130516052 | R2:0.48461737496702484 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[264] Loss:0.25298247784376143 | L1 Loss:0.38483295142650603 | R2:0.3273981573378994 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[265] Loss:0.17976186471059918 | L1 Loss:0.3269463013857603 | R2:0.45936814221292577 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[265] Loss:0.24674302190542222 | L1 Loss:0.3832588762044907 | R2:0.349601904254402 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[266] Loss:0.1722023794427514 | L1 Loss:0.318433552980423 | R2:0.4856935617588727 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[266] Loss:0.2513222098350525 | L1 Loss:0.3830133855342865 | R2:0.3339130728573697 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[267] Loss:0.16759652830660343 | L1 Loss:0.31390716321766376 | R2:0.4991874897938901 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[267] Loss:0.24147787988185881 | L1 Loss:0.3759272783994675 | R2:0.35414097674977674 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[268] Loss:0.16734414966776967 | L1 Loss:0.31107720639556646 | R2:0.5011341030514709 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[268] Loss:0.24251001179218293 | L1 Loss:0.37510762810707093 | R2:0.359065077785217 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[269] Loss:0.16351275984197855 | L1 Loss:0.31261887680739164 | R2:0.5097100861674279 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[269] Loss:0.23963317573070525 | L1 Loss:0.37916714549064634 | R2:0.3661518049899012 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[270] Loss:0.16861126711592078 | L1 Loss:0.315284944139421 | R2:0.4930458767868508 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[270] Loss:0.2514656141400337 | L1 Loss:0.3819428443908691 | R2:0.32553976960943104 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[271] Loss:0.17206297162920237 | L1 Loss:0.31879943050444126 | R2:0.47991842411159025 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[271] Loss:0.2471551239490509 | L1 Loss:0.38461408019065857 | R2:0.3462431590094236 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[272] Loss:0.17351964069530368 | L1 Loss:0.3201118027791381 | R2:0.4814833035363645 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[272] Loss:0.25577202141284944 | L1 Loss:0.39069860279560087 | R2:0.3179176332788335 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[273] Loss:0.17148313904181123 | L1 Loss:0.3177083060145378 | R2:0.4868519447056615 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[273] Loss:0.24599165320396424 | L1 Loss:0.3817918747663498 | R2:0.3510227482004472 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[274] Loss:0.17642398038879037 | L1 Loss:0.32196429930627346 | R2:0.4711705985305024 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[274] Loss:0.23914434760808945 | L1 Loss:0.3740134447813034 | R2:0.3710626969606899 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[275] Loss:0.16506715957075357 | L1 Loss:0.31529586762189865 | R2:0.5082894775010197 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[275] Loss:0.2334723249077797 | L1 Loss:0.3714865863323212 | R2:0.3769910710072554 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[276] Loss:0.17159798415377736 | L1 Loss:0.3229069411754608 | R2:0.4885070109233656 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[276] Loss:0.2396663710474968 | L1 Loss:0.37388362288475036 | R2:0.3600962653809323 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[277] Loss:0.1727435765787959 | L1 Loss:0.32177718076854944 | R2:0.483138521329584 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[277] Loss:0.23652412593364716 | L1 Loss:0.3733345091342926 | R2:0.36453728630107113 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[278] Loss:0.16834739781916142 | L1 Loss:0.3209013631567359 | R2:0.4974782982055041 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[278] Loss:0.24132243394851685 | L1 Loss:0.3759110927581787 | R2:0.3526104968033535 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[279] Loss:0.1679027802310884 | L1 Loss:0.3151909690350294 | R2:0.49545821263700673 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[279] Loss:0.24521594047546386 | L1 Loss:0.3760951071977615 | R2:0.34147307687315626 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[280] Loss:0.17618455970659852 | L1 Loss:0.32505345437675714 | R2:0.47041182373377993 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[280] Loss:0.2520854502916336 | L1 Loss:0.37946800887584686 | R2:0.32491026132185896 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[281] Loss:0.1710952534340322 | L1 Loss:0.31790053751319647 | R2:0.48856312377828204 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[281] Loss:0.24995331466197968 | L1 Loss:0.38492547571659086 | R2:0.3322754803217668 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[282] Loss:0.16457273811101913 | L1 Loss:0.31231125071644783 | R2:0.503594872124554 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[282] Loss:0.24723111987113952 | L1 Loss:0.37767454981803894 | R2:0.35026238011371646 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[283] Loss:0.1601008358411491 | L1 Loss:0.3066984210163355 | R2:0.5205339033486714 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[283] Loss:0.238442824780941 | L1 Loss:0.37454111576080323 | R2:0.3705178276558015 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[284] Loss:0.16878819279372692 | L1 Loss:0.3108993014320731 | R2:0.49220975722728333 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[284] Loss:0.23275895565748214 | L1 Loss:0.37241199910640715 | R2:0.39076446367784534 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[285] Loss:0.16957888938486576 | L1 Loss:0.31668946892023087 | R2:0.4889896551186659 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[285] Loss:0.23449821919202804 | L1 Loss:0.370668426156044 | R2:0.37801328415750063 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[286] Loss:0.1717085400596261 | L1 Loss:0.3220230434089899 | R2:0.4855686491460462 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[286] Loss:0.2445175364613533 | L1 Loss:0.38522939682006835 | R2:0.35459810989074053 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[287] Loss:0.17216232931241393 | L1 Loss:0.3239321745932102 | R2:0.47985198936437895 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[287] Loss:0.24151325672864915 | L1 Loss:0.38007470667362214 | R2:0.36486884340498305 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[288] Loss:0.17058267956599593 | L1 Loss:0.31961049512028694 | R2:0.48658557931923707 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[288] Loss:0.24312085211277007 | L1 Loss:0.3783010601997375 | R2:0.3506032091826671 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[289] Loss:0.16567411087453365 | L1 Loss:0.30895699840039015 | R2:0.49828414127972587 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[289] Loss:0.24075430035591125 | L1 Loss:0.37472017407417296 | R2:0.3635949224492423 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[290] Loss:0.1649874709546566 | L1 Loss:0.31684993486851454 | R2:0.5021710078360679 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[290] Loss:0.2510918349027634 | L1 Loss:0.3850649058818817 | R2:0.3407993550964561 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[291] Loss:0.16402215557172894 | L1 Loss:0.3113002358004451 | R2:0.5058350686781906 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[291] Loss:0.23920611739158631 | L1 Loss:0.37727233171463015 | R2:0.367345187855864 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[292] Loss:0.16545196110382676 | L1 Loss:0.3145616874098778 | R2:0.4969926001160416 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[292] Loss:0.24283951073884963 | L1 Loss:0.3801587700843811 | R2:0.3483785130010991 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[293] Loss:0.16772699914872646 | L1 Loss:0.3146756235510111 | R2:0.4940721091535603 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[293] Loss:0.24017616510391235 | L1 Loss:0.38029226660728455 | R2:0.3606634520343593 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[294] Loss:0.16216232860460877 | L1 Loss:0.30972294323146343 | R2:0.5138034574103258 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[294] Loss:0.24059400707483292 | L1 Loss:0.36997694969177247 | R2:0.3652942326380112 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[295] Loss:0.16458198288455606 | L1 Loss:0.31309851817786694 | R2:0.502982403629863 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[295] Loss:0.24179345965385438 | L1 Loss:0.37986809611320493 | R2:0.36071276503876976 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[296] Loss:0.1600102768279612 | L1 Loss:0.3090848810970783 | R2:0.5165935622057677 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[296] Loss:0.23743225783109664 | L1 Loss:0.3682925313711166 | R2:0.37225184377117193 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[297] Loss:0.15987819619476795 | L1 Loss:0.3075333693996072 | R2:0.5201081440294207 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[297] Loss:0.24536396861076354 | L1 Loss:0.37043485343456267 | R2:0.3510175820547752 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[298] Loss:0.16715526580810547 | L1 Loss:0.3194516021758318 | R2:0.49800837470511966 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[298] Loss:0.2401372656226158 | L1 Loss:0.3756576508283615 | R2:0.3605459263101009 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[299] Loss:0.16988356038928032 | L1 Loss:0.31720430962741375 | R2:0.4875057255043894 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[299] Loss:0.24008301496505738 | L1 Loss:0.3739988148212433 | R2:0.35984657242301743 | ACC: 70.3333%(211/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05aeeaf5-bb51-4ad9-976a-f1b5fcfa8ffb",
        "id": "4524TUFoF0-y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.39076446367784534\n",
            "min test_loss_l1_record  0.36576642394065856\n",
            "min test_loss_record  0.2287165954709053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb2aee1-a1dc-4127-a3b7-58ac9d5922e7",
        "id": "Ff91DsYHF0-y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.3878865106574171\n",
            "min test_loss_l1_record  0.36576642394065856\n",
            "min test_loss_record  0.2287165954709053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d503e1-4cf5-46e8-9930-074b1d838c2b",
        "id": "GMFMdQ6fF0-z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.38368928767309096\n",
            "min test_loss_l1_record  0.3734748214483261\n",
            "min test_loss_record  0.23709974586963653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "8477e34f-734a-47e0-9630-34dec5840f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdS3mc6IF0-z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.332335880277871\n",
            "min test_loss_l1_record  0.3962205171585083\n",
            "min test_loss_record  0.2507642224431038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcp7thw5PQQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 mer Regression"
      ],
      "metadata": {
        "id": "L2KCQkeGPUdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=3, mer_dict=threeMer_dict)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15050638-bfbb-4e95-fb23-c97a21b8cb75",
        "id": "XArTRw0BPQbg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba448e73-b6ab-4a54-85dc-396178bc30ce",
        "id": "ReWREkwYPQbh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "vdsuNkW-PQbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78311ba5-f354-42f7-9a28-536858b51f43",
        "id": "IyBr-SEbPQbh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "8QW19yCRPQbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "NBs1BAUxPQbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816aaca0-4f0f-4ead-8a0f-481d2ecadb6f",
        "id": "JsT3Sp7LPQbh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3658509301021695 | L1 Loss:0.4449390098452568 | R2:-0.0460129517688935 | ACC: 60.6000%(303/500)\n",
            "Testing Epoch[0] Loss:0.35084871500730513 | L1 Loss:0.4302876353263855 | R2:0.006781337565057921 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[1] Loss:0.35088404454290867 | L1 Loss:0.44466692954301834 | R2:-0.0014409517490993479 | ACC: 60.8000%(304/500)\n",
            "Testing Epoch[1] Loss:0.33953937292099 | L1 Loss:0.4342294454574585 | R2:0.03558569847994834 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[2] Loss:0.33034331630915403 | L1 Loss:0.43558860570192337 | R2:0.05183079081842091 | ACC: 61.4000%(307/500)\n",
            "Testing Epoch[2] Loss:0.3367109686136246 | L1 Loss:0.42653076350688934 | R2:0.05150022883318276 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[3] Loss:0.32981119491159916 | L1 Loss:0.4402302987873554 | R2:0.050044729118823725 | ACC: 62.2000%(311/500)\n",
            "Testing Epoch[3] Loss:0.3352016195654869 | L1 Loss:0.4355527549982071 | R2:0.051564345302434186 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[4] Loss:0.3185337018221617 | L1 Loss:0.43614277243614197 | R2:0.08539112984196241 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[4] Loss:0.3365055963397026 | L1 Loss:0.4386443793773651 | R2:0.04021368472189356 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[5] Loss:0.3146832063794136 | L1 Loss:0.4371297899633646 | R2:0.09273258797871364 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[5] Loss:0.3498437538743019 | L1 Loss:0.45454810559749603 | R2:-0.003327154655526199 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[6] Loss:0.30306857731193304 | L1 Loss:0.4348388984799385 | R2:0.12039024028674639 | ACC: 64.4000%(322/500)\n",
            "Testing Epoch[6] Loss:0.3499484285712242 | L1 Loss:0.44704756438732146 | R2:0.0008344693252546831 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[7] Loss:0.30113047268241644 | L1 Loss:0.43084820732474327 | R2:0.12804863729979893 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[7] Loss:0.3375813812017441 | L1 Loss:0.4474919557571411 | R2:0.033780734073621387 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[8] Loss:0.29392147809267044 | L1 Loss:0.4315553940832615 | R2:0.14215832918558974 | ACC: 62.4000%(312/500)\n",
            "Testing Epoch[8] Loss:0.34448846578598025 | L1 Loss:0.4516393572092056 | R2:0.013078069119455716 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[9] Loss:0.27975091245025396 | L1 Loss:0.4232353921979666 | R2:0.18493735745329581 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[9] Loss:0.34654107242822646 | L1 Loss:0.45005815029144286 | R2:0.0066743322669601925 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[10] Loss:0.2661843877285719 | L1 Loss:0.4139614012092352 | R2:0.21777660247077746 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[10] Loss:0.3298809453845024 | L1 Loss:0.4384824842214584 | R2:0.04745110351833788 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[11] Loss:0.25926152896136045 | L1 Loss:0.40351465344429016 | R2:0.23220103011509347 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[11] Loss:0.33981003761291506 | L1 Loss:0.44640318751335145 | R2:0.020764256053197407 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[12] Loss:0.2639031894505024 | L1 Loss:0.40760647132992744 | R2:0.22631055556831664 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[12] Loss:0.3501648113131523 | L1 Loss:0.4441332519054413 | R2:-0.009306606448340481 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[13] Loss:0.2596624018624425 | L1 Loss:0.40634316205978394 | R2:0.2383841325197934 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[13] Loss:0.32288401275873185 | L1 Loss:0.4271833121776581 | R2:0.0739677353053024 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[14] Loss:0.2522488422691822 | L1 Loss:0.3981329780071974 | R2:0.25094978351593156 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[14] Loss:0.32933868616819384 | L1 Loss:0.42925965785980225 | R2:0.06424070035537273 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[15] Loss:0.2358037307858467 | L1 Loss:0.3839159160852432 | R2:0.299217765400483 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[15] Loss:0.3477453738451004 | L1 Loss:0.4397832348942757 | R2:0.015045743708327597 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[16] Loss:0.23760821484029293 | L1 Loss:0.38245088048279285 | R2:0.29765796472477773 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[16] Loss:0.3438930809497833 | L1 Loss:0.43944231867790223 | R2:0.01890108794131561 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[17] Loss:0.2268849043175578 | L1 Loss:0.3764760009944439 | R2:0.3344438036669402 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[17] Loss:0.3550384745001793 | L1 Loss:0.4478268802165985 | R2:-0.014834269211545747 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[18] Loss:0.22937815636396408 | L1 Loss:0.3760954849421978 | R2:0.3252351086171419 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[18] Loss:0.3357554227113724 | L1 Loss:0.4251658946275711 | R2:0.03582771066752051 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[19] Loss:0.22716096136718988 | L1 Loss:0.3789991308003664 | R2:0.3327552608220833 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[19] Loss:0.34996253103017805 | L1 Loss:0.44016288220882416 | R2:-0.006649953103943329 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[20] Loss:0.23234207183122635 | L1 Loss:0.37744687125086784 | R2:0.32122663600769485 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[20] Loss:0.3429585501551628 | L1 Loss:0.441686674952507 | R2:0.016111524172945903 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[21] Loss:0.22682569548487663 | L1 Loss:0.37830613926053047 | R2:0.3381957249177438 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[21] Loss:0.340870863199234 | L1 Loss:0.440894228219986 | R2:0.014133470623230171 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[22] Loss:0.22125785145908594 | L1 Loss:0.3708833195269108 | R2:0.3525614918139685 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[22] Loss:0.34205379635095595 | L1 Loss:0.44112885296344756 | R2:0.0002905918992894829 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[23] Loss:0.21330876275897026 | L1 Loss:0.36600664258003235 | R2:0.3752477046144556 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[23] Loss:0.3320685252547264 | L1 Loss:0.4320391118526459 | R2:0.03014778192640586 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[24] Loss:0.22016239818185568 | L1 Loss:0.3810769636183977 | R2:0.3587272796020644 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[24] Loss:0.33184787482023237 | L1 Loss:0.42799500524997713 | R2:0.040634961062898343 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[25] Loss:0.2182165766134858 | L1 Loss:0.37290805391967297 | R2:0.3624163137926161 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[25] Loss:0.3280234470963478 | L1 Loss:0.42318577468395235 | R2:0.05001177943709527 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[26] Loss:0.21848670579493046 | L1 Loss:0.3733134735375643 | R2:0.3604065642212948 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[26] Loss:0.33805689960718155 | L1 Loss:0.42910500764846804 | R2:0.017710356627815216 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[27] Loss:0.2139663239941001 | L1 Loss:0.3708596993237734 | R2:0.378878146565904 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[27] Loss:0.33901848196983336 | L1 Loss:0.4394212275743484 | R2:0.015646339423658416 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[28] Loss:0.20979013899341226 | L1 Loss:0.36169232428073883 | R2:0.38266124077982083 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[28] Loss:0.3301857501268387 | L1 Loss:0.42858519554138186 | R2:0.04175249656626053 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[29] Loss:0.21610239893198013 | L1 Loss:0.3714843075722456 | R2:0.36067624653578184 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[29] Loss:0.32774215638637544 | L1 Loss:0.42417768239974973 | R2:0.05888896282378312 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[30] Loss:0.18914331262931228 | L1 Loss:0.3440908268094063 | R2:0.44550285246967897 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[30] Loss:0.33044539391994476 | L1 Loss:0.42318327724933624 | R2:0.04354177313137943 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[31] Loss:0.19763586856424809 | L1 Loss:0.3544715456664562 | R2:0.4207246810682415 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[31] Loss:0.3381372541189194 | L1 Loss:0.4253106087446213 | R2:0.03501826782232995 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[32] Loss:0.19550076313316822 | L1 Loss:0.3522837460041046 | R2:0.4302326015776063 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[32] Loss:0.32330516278743743 | L1 Loss:0.41110258996486665 | R2:0.07157752185416888 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[33] Loss:0.20039940439164639 | L1 Loss:0.35250865295529366 | R2:0.4094930813771316 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[33] Loss:0.3252351552248001 | L1 Loss:0.415213817358017 | R2:0.06948476924775053 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[34] Loss:0.20214616786688566 | L1 Loss:0.35559067875146866 | R2:0.40714425628837825 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[34] Loss:0.34258170425891876 | L1 Loss:0.423704469203949 | R2:0.004915697381784279 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[35] Loss:0.20468015875667334 | L1 Loss:0.3541559800505638 | R2:0.40073199124121217 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[35] Loss:0.3178170397877693 | L1 Loss:0.41663924753665926 | R2:0.08014927560259186 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[36] Loss:0.1979100159369409 | L1 Loss:0.35250305756926537 | R2:0.42178277873310266 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[36] Loss:0.3475732922554016 | L1 Loss:0.44404666125774384 | R2:-0.017338497071374602 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[37] Loss:0.19303433038294315 | L1 Loss:0.350245239213109 | R2:0.43015732722989153 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[37] Loss:0.3267673164606094 | L1 Loss:0.4233066767454147 | R2:0.06293818980592888 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[38] Loss:0.20417962782084942 | L1 Loss:0.36079668812453747 | R2:0.397093037929116 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[38] Loss:0.33447942435741423 | L1 Loss:0.42576900124549866 | R2:0.05162794876679385 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[39] Loss:0.1921601900830865 | L1 Loss:0.34956409595906734 | R2:0.43037496348571586 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[39] Loss:0.3345649421215057 | L1 Loss:0.4329562783241272 | R2:0.03518094305333287 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[40] Loss:0.20121338218450546 | L1 Loss:0.35351793095469475 | R2:0.4020570882670365 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[40] Loss:0.32053461819887163 | L1 Loss:0.4279993772506714 | R2:0.07187048613447988 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[41] Loss:0.21138068195432425 | L1 Loss:0.36842214688658714 | R2:0.3732947975252806 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[41] Loss:0.32464184761047366 | L1 Loss:0.42147189676761626 | R2:0.06195963846554888 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[42] Loss:0.20408468320965767 | L1 Loss:0.35346512123942375 | R2:0.3984145511198891 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[42] Loss:0.296264611184597 | L1 Loss:0.40633673071861265 | R2:0.14845821416443467 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[43] Loss:0.19820874650031328 | L1 Loss:0.35183326713740826 | R2:0.41624738480771173 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[43] Loss:0.3007120579481125 | L1 Loss:0.4088601678609848 | R2:0.14243556862598938 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[44] Loss:0.20358540676534176 | L1 Loss:0.3583832085132599 | R2:0.39355406791530584 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[44] Loss:0.2972114682197571 | L1 Loss:0.4058924436569214 | R2:0.14942691989892015 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[45] Loss:0.20385315734893084 | L1 Loss:0.3538978099822998 | R2:0.39430723565859016 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[45] Loss:0.31302749216556547 | L1 Loss:0.4207155406475067 | R2:0.10681404478292005 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[46] Loss:0.20563179068267345 | L1 Loss:0.35202786698937416 | R2:0.38455272326108303 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[46] Loss:0.3015280723571777 | L1 Loss:0.4080112546682358 | R2:0.1389986661397146 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[47] Loss:0.19336442276835442 | L1 Loss:0.34297602251172066 | R2:0.4198022156668504 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[47] Loss:0.29350702464580536 | L1 Loss:0.4060232937335968 | R2:0.164631662396206 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[48] Loss:0.2051389990374446 | L1 Loss:0.35273345932364464 | R2:0.3876996394174963 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[48] Loss:0.31817715913057326 | L1 Loss:0.4194316416978836 | R2:0.09126396629388972 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[49] Loss:0.19683063589036465 | L1 Loss:0.3456813283264637 | R2:0.4129564824360945 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[49] Loss:0.30416871458292005 | L1 Loss:0.41143046617507933 | R2:0.12387125622249981 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[50] Loss:0.1979372939094901 | L1 Loss:0.3506247941404581 | R2:0.4108047462187825 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[50] Loss:0.3139597848057747 | L1 Loss:0.4225980043411255 | R2:0.10296244899397675 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[51] Loss:0.20405077748000622 | L1 Loss:0.35236469469964504 | R2:0.3926109126758059 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[51] Loss:0.2944795697927475 | L1 Loss:0.4050708502531052 | R2:0.1592683410781571 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[52] Loss:0.18963923957198858 | L1 Loss:0.3414396159350872 | R2:0.4304037538896331 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[52] Loss:0.32564333975315096 | L1 Loss:0.42489827871322633 | R2:0.06279601646530111 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[53] Loss:0.2019773991778493 | L1 Loss:0.3552458602935076 | R2:0.3994558586865476 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[53] Loss:0.30850093364715575 | L1 Loss:0.4190832585096359 | R2:0.10944144928863006 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[54] Loss:0.19267130456864834 | L1 Loss:0.3460867144167423 | R2:0.427745607275591 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[54] Loss:0.31888088434934614 | L1 Loss:0.41907784938812254 | R2:0.07190124931412686 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[55] Loss:0.18603247497230768 | L1 Loss:0.3375701308250427 | R2:0.44592511358342224 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[55] Loss:0.31036687940359114 | L1 Loss:0.4177093982696533 | R2:0.10603791660685621 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[56] Loss:0.18239861726760864 | L1 Loss:0.3344202060252428 | R2:0.4566559825519586 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[56] Loss:0.3004383981227875 | L1 Loss:0.41696362793445585 | R2:0.12414472565024566 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[57] Loss:0.18820566218346357 | L1 Loss:0.3450498152524233 | R2:0.4332266150256682 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[57] Loss:0.3101752907037735 | L1 Loss:0.41479672193527223 | R2:0.10281006898678688 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[58] Loss:0.17661596508696675 | L1 Loss:0.3349466547369957 | R2:0.47016839346838407 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[58] Loss:0.29916943311691285 | L1 Loss:0.4014838457107544 | R2:0.1294611197879226 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[59] Loss:0.1759442025795579 | L1 Loss:0.3285157922655344 | R2:0.47794129752971914 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[59] Loss:0.3148998633027077 | L1 Loss:0.4145734578371048 | R2:0.07934390760853591 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[60] Loss:0.19603016693145037 | L1 Loss:0.3489738553762436 | R2:0.41571998767034246 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[60] Loss:0.3064285010099411 | L1 Loss:0.3995062857866287 | R2:0.10433722101052434 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[61] Loss:0.18116254918277264 | L1 Loss:0.3323360588401556 | R2:0.4661938700570495 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[61] Loss:0.2946359798312187 | L1 Loss:0.40162312388420107 | R2:0.13737248366223367 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[62] Loss:0.18236216204240918 | L1 Loss:0.33534709736704826 | R2:0.4560098578156686 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[62] Loss:0.30226973444223404 | L1 Loss:0.39518006443977355 | R2:0.10694680501189073 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[63] Loss:0.19157158583402634 | L1 Loss:0.3488395856693387 | R2:0.43186798253856246 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[63] Loss:0.2914137154817581 | L1 Loss:0.3975777983665466 | R2:0.14694981025551274 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[64] Loss:0.16852380521595478 | L1 Loss:0.32947417348623276 | R2:0.501249840413425 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[64] Loss:0.2849373698234558 | L1 Loss:0.38695970773696897 | R2:0.16432633568348837 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[65] Loss:0.18842852674424648 | L1 Loss:0.3445188608020544 | R2:0.44310935234548204 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[65] Loss:0.31459277868270874 | L1 Loss:0.4092315584421158 | R2:0.08258290682740459 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[66] Loss:0.18239974975585938 | L1 Loss:0.3437880910933018 | R2:0.4654038845703157 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[66] Loss:0.28077601343393327 | L1 Loss:0.3886865735054016 | R2:0.17584200759345167 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[67] Loss:0.2007292928174138 | L1 Loss:0.35220099799335003 | R2:0.40614696463782785 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[67] Loss:0.30107216984033586 | L1 Loss:0.41106775403022766 | R2:0.11925353295175815 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[68] Loss:0.19913268648087978 | L1 Loss:0.35107458755373955 | R2:0.41740310617234394 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[68] Loss:0.30120921432971953 | L1 Loss:0.40352466106414797 | R2:0.12154527066220347 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[69] Loss:0.19161790795624256 | L1 Loss:0.3473117519170046 | R2:0.4354937833521732 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[69] Loss:0.2908952459692955 | L1 Loss:0.40256455540657043 | R2:0.1522713089548427 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[70] Loss:0.19243044778704643 | L1 Loss:0.34214552491903305 | R2:0.42991696866840556 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[70] Loss:0.3078584149479866 | L1 Loss:0.4109172374010086 | R2:0.10553287150886896 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[71] Loss:0.2079715197905898 | L1 Loss:0.3584292344748974 | R2:0.3856779095496213 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[71] Loss:0.30337619930505755 | L1 Loss:0.41481253802776336 | R2:0.11379987581689824 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[72] Loss:0.20910991169512272 | L1 Loss:0.36364040710031986 | R2:0.38115130642682593 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[72] Loss:0.3057123512029648 | L1 Loss:0.4086408406496048 | R2:0.11678523833370662 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[73] Loss:0.20406616013497114 | L1 Loss:0.3558775447309017 | R2:0.39140182869123125 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[73] Loss:0.29329867511987684 | L1 Loss:0.39522023499011993 | R2:0.15236890960027427 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[74] Loss:0.20263281418010592 | L1 Loss:0.357116024941206 | R2:0.38871213210502703 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[74] Loss:0.29293290972709657 | L1 Loss:0.39822860062122345 | R2:0.1583585725096309 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[75] Loss:0.19439758080989122 | L1 Loss:0.34505858551710844 | R2:0.4249593592254415 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[75] Loss:0.29441339820623397 | L1 Loss:0.40857463479042055 | R2:0.14982681902687714 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[76] Loss:0.1878659725189209 | L1 Loss:0.34612527024000883 | R2:0.4388972785546885 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[76] Loss:0.28631752729415894 | L1 Loss:0.3962049335241318 | R2:0.16932020410835977 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[77] Loss:0.1933856699615717 | L1 Loss:0.350297998636961 | R2:0.4255606636112091 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[77] Loss:0.29189067110419276 | L1 Loss:0.39602373242378236 | R2:0.14986280818158698 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[78] Loss:0.18664640141651034 | L1 Loss:0.33943849615752697 | R2:0.44537086422854766 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[78] Loss:0.28439570814371107 | L1 Loss:0.3890979915857315 | R2:0.1734637443377726 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[79] Loss:0.19408630579710007 | L1 Loss:0.35390251502394676 | R2:0.41536832827137193 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[79] Loss:0.29997460544109344 | L1 Loss:0.4024475634098053 | R2:0.13043517708725916 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[80] Loss:0.1919876867905259 | L1 Loss:0.3465824145823717 | R2:0.4318529852999468 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[80] Loss:0.29294982850551604 | L1 Loss:0.3935501128435135 | R2:0.15591116558069912 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[81] Loss:0.19522039033472538 | L1 Loss:0.3479362428188324 | R2:0.4189668985785254 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[81] Loss:0.2958082765340805 | L1 Loss:0.39362342953681945 | R2:0.1503246801646456 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[82] Loss:0.18896785518154502 | L1 Loss:0.34544635005295277 | R2:0.4364866813934536 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[82] Loss:0.2926975920796394 | L1 Loss:0.399610823392868 | R2:0.15937224368526942 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[83] Loss:0.18542596464976668 | L1 Loss:0.33766711596399546 | R2:0.4420645689329861 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[83] Loss:0.28616982996463775 | L1 Loss:0.39552849233150483 | R2:0.17833238450657274 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[84] Loss:0.182452364359051 | L1 Loss:0.33654225524514914 | R2:0.45168266774311117 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[84] Loss:0.30002063065767287 | L1 Loss:0.4051672399044037 | R2:0.13935042752840948 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[85] Loss:0.17074793064966798 | L1 Loss:0.32719081174582243 | R2:0.48085435360164386 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[85] Loss:0.27328253388404844 | L1 Loss:0.38673574328422544 | R2:0.2042818978048762 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[86] Loss:0.17028693901374936 | L1 Loss:0.32567968033254147 | R2:0.4854169543626575 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[86] Loss:0.28354291468858717 | L1 Loss:0.39520042538642886 | R2:0.17617771355290618 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[87] Loss:0.17561390763148665 | L1 Loss:0.3319061938673258 | R2:0.46561802238667976 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[87] Loss:0.29622125178575515 | L1 Loss:0.4039896190166473 | R2:0.14705594790359933 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[88] Loss:0.18774379696696997 | L1 Loss:0.34640095196664333 | R2:0.43355739021312356 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[88] Loss:0.28610173165798186 | L1 Loss:0.3908690720796585 | R2:0.1771822541206905 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[89] Loss:0.17664513131603599 | L1 Loss:0.331603130325675 | R2:0.46851201192867054 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[89] Loss:0.27592178881168367 | L1 Loss:0.38874891996383665 | R2:0.20843708300971744 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[90] Loss:0.17811262514442205 | L1 Loss:0.3389435410499573 | R2:0.46486767822814384 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[90] Loss:0.2915328897535801 | L1 Loss:0.3987307071685791 | R2:0.15356005914233933 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[91] Loss:0.18102262821048498 | L1 Loss:0.3416649252176285 | R2:0.45571600497780906 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[91] Loss:0.2811098098754883 | L1 Loss:0.3916135400533676 | R2:0.18340795703998994 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[92] Loss:0.16965641034767032 | L1 Loss:0.32406978588551283 | R2:0.4830160366237628 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[92] Loss:0.2928985685110092 | L1 Loss:0.40315237939357756 | R2:0.1497061844572484 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[93] Loss:0.17957535991445184 | L1 Loss:0.3342025000602007 | R2:0.45360273474799384 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[93] Loss:0.29011613577604295 | L1 Loss:0.4040225625038147 | R2:0.14705493457763075 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[94] Loss:0.18237462919205427 | L1 Loss:0.3378789033740759 | R2:0.4452106158866437 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[94] Loss:0.2873131923377514 | L1 Loss:0.4009048014879227 | R2:0.1581393834478573 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[95] Loss:0.1877163085155189 | L1 Loss:0.34787866100668907 | R2:0.4279486329667835 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[95] Loss:0.2634816706180573 | L1 Loss:0.38158464431762695 | R2:0.23299568640361276 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[96] Loss:0.1773622869513929 | L1 Loss:0.3427626518532634 | R2:0.4625701330282126 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[96] Loss:0.27698090821504595 | L1 Loss:0.38862832486629484 | R2:0.19543736919892427 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[97] Loss:0.16618616273626685 | L1 Loss:0.3228533649817109 | R2:0.49651106244084525 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[97] Loss:0.2636971741914749 | L1 Loss:0.37144995033740996 | R2:0.23579258527060895 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[98] Loss:0.1776547715999186 | L1 Loss:0.33562709111720324 | R2:0.45856390077818077 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[98] Loss:0.2862395979464054 | L1 Loss:0.39375286996364595 | R2:0.17135489305351853 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[99] Loss:0.1716110184788704 | L1 Loss:0.32719762809574604 | R2:0.4804225697912585 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[99] Loss:0.2779868520796299 | L1 Loss:0.3825953245162964 | R2:0.1840951770234613 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[100] Loss:0.1731052421964705 | L1 Loss:0.33155266009271145 | R2:0.4703069991939032 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[100] Loss:0.2673952251672745 | L1 Loss:0.38782534599304197 | R2:0.22068166787753868 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[101] Loss:0.16879405453801155 | L1 Loss:0.3263915218412876 | R2:0.48349584731807493 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[101] Loss:0.2635138541460037 | L1 Loss:0.3895664602518082 | R2:0.23960043011256346 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[102] Loss:0.17250762460753322 | L1 Loss:0.3273919764906168 | R2:0.47228561073890635 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[102] Loss:0.2656807444989681 | L1 Loss:0.3823621481657028 | R2:0.23876766588982154 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[103] Loss:0.17086222767829895 | L1 Loss:0.3282779809087515 | R2:0.47945432216524914 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[103] Loss:0.25628153681755067 | L1 Loss:0.37598215490579606 | R2:0.2560247684282846 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[104] Loss:0.16652985475957394 | L1 Loss:0.32971679233014584 | R2:0.49405684365791513 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[104] Loss:0.2700492933392525 | L1 Loss:0.39269883632659913 | R2:0.2077524355571426 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[105] Loss:0.17806224105879664 | L1 Loss:0.33678469248116016 | R2:0.4551940684738191 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[105] Loss:0.27614999786019323 | L1 Loss:0.3965174674987793 | R2:0.19415253287772413 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[106] Loss:0.1808175784535706 | L1 Loss:0.34005577862262726 | R2:0.4476668937336312 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[106] Loss:0.26391604989767076 | L1 Loss:0.3825237721204758 | R2:0.23288432258464087 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[107] Loss:0.1831997218541801 | L1 Loss:0.33763534016907215 | R2:0.4415533608623194 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[107] Loss:0.2634678304195404 | L1 Loss:0.39478408098220824 | R2:0.231579863974408 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[108] Loss:0.17359689204022288 | L1 Loss:0.33564527332782745 | R2:0.4724248557332724 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[108] Loss:0.28217501789331434 | L1 Loss:0.39569444954395294 | R2:0.16854112809975805 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[109] Loss:0.16894133482128382 | L1 Loss:0.3294424433261156 | R2:0.4867240460352683 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[109] Loss:0.26946680396795275 | L1 Loss:0.3898193895816803 | R2:0.21399063688794584 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[110] Loss:0.17075296770781279 | L1 Loss:0.3306458340957761 | R2:0.4836983270766304 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[110] Loss:0.2737113207578659 | L1 Loss:0.4007463365793228 | R2:0.19464268684921443 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[111] Loss:0.1825601765885949 | L1 Loss:0.3370709642767906 | R2:0.45006694743425346 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[111] Loss:0.2708060756325722 | L1 Loss:0.3983972996473312 | R2:0.1962316812563172 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[112] Loss:0.18313982523977757 | L1 Loss:0.3357503153383732 | R2:0.4474309595533895 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[112] Loss:0.2645631656050682 | L1 Loss:0.3956603705883026 | R2:0.2185820996348185 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[113] Loss:0.18398018460720778 | L1 Loss:0.3405301598832011 | R2:0.4467656104938046 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[113] Loss:0.2670131281018257 | L1 Loss:0.39861273765563965 | R2:0.2041253542091071 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[114] Loss:0.17180656734853983 | L1 Loss:0.3277749363332987 | R2:0.48283485051903063 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[114] Loss:0.26924691498279574 | L1 Loss:0.3904190480709076 | R2:0.20173993186312922 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[115] Loss:0.180069531314075 | L1 Loss:0.33503610640764236 | R2:0.46080278241282513 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[115] Loss:0.26551113948225974 | L1 Loss:0.3860348641872406 | R2:0.21700203531351323 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[116] Loss:0.17967578675597906 | L1 Loss:0.3375927098095417 | R2:0.46288968895789173 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[116] Loss:0.2786142185330391 | L1 Loss:0.40328153371810915 | R2:0.17688085627148825 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[117] Loss:0.18020566320046782 | L1 Loss:0.33178974501788616 | R2:0.45611517004994784 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[117] Loss:0.2742320768535137 | L1 Loss:0.3971976935863495 | R2:0.19960770426683386 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[118] Loss:0.1772468937560916 | L1 Loss:0.3317673373967409 | R2:0.46683636865659284 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[118] Loss:0.28233630657196046 | L1 Loss:0.412566664814949 | R2:0.17457325439731042 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[119] Loss:0.17058308329433203 | L1 Loss:0.3302502781152725 | R2:0.4885167309329053 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[119] Loss:0.2663285665214062 | L1 Loss:0.3936684161424637 | R2:0.22144959073450404 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[120] Loss:0.17296785954385996 | L1 Loss:0.33271647058427334 | R2:0.48334312541083113 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[120] Loss:0.26846630573272706 | L1 Loss:0.39141840636730196 | R2:0.21370619600821109 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[121] Loss:0.17418799735605717 | L1 Loss:0.3330668956041336 | R2:0.4817961810504247 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[121] Loss:0.2774085134267807 | L1 Loss:0.4034945249557495 | R2:0.18095888738315452 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[122] Loss:0.1816161242313683 | L1 Loss:0.3343052165582776 | R2:0.45471434425828894 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[122] Loss:0.2940242663025856 | L1 Loss:0.41493648290634155 | R2:0.13166151370870663 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[123] Loss:0.17961809923872352 | L1 Loss:0.33145190961658955 | R2:0.4585849506523654 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[123] Loss:0.2754752352833748 | L1 Loss:0.40413340032100675 | R2:0.19944143093087602 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[124] Loss:0.18514680955559015 | L1 Loss:0.3369801389053464 | R2:0.4416851861209823 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[124] Loss:0.27996123433113096 | L1 Loss:0.4101229250431061 | R2:0.1787595895747332 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[125] Loss:0.18193582724779844 | L1 Loss:0.33476932160556316 | R2:0.4538728765016064 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[125] Loss:0.27997889965772627 | L1 Loss:0.4009111762046814 | R2:0.16868703643241556 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[126] Loss:0.16861249273642898 | L1 Loss:0.3207371383905411 | R2:0.4893828061113786 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[126] Loss:0.2761592671275139 | L1 Loss:0.40216506123542783 | R2:0.19287122465721745 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[127] Loss:0.17000387981534004 | L1 Loss:0.3246875610202551 | R2:0.48582338365025396 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[127] Loss:0.2823226824402809 | L1 Loss:0.4069963306188583 | R2:0.16580390477597481 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[128] Loss:0.1600049356929958 | L1 Loss:0.31332028843462467 | R2:0.5141084441697696 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[128] Loss:0.26394622325897216 | L1 Loss:0.38666000962257385 | R2:0.22170855677015505 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[129] Loss:0.17259307578206062 | L1 Loss:0.3304089792072773 | R2:0.4796116809067642 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[129] Loss:0.2662475183606148 | L1 Loss:0.3944112867116928 | R2:0.22060077117862043 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[130] Loss:0.1682243924587965 | L1 Loss:0.32126477640122175 | R2:0.49238985248677847 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[130] Loss:0.2790759697556496 | L1 Loss:0.4003690183162689 | R2:0.1747068045278902 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[131] Loss:0.16631947830319405 | L1 Loss:0.3213950488716364 | R2:0.48980966632319956 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[131] Loss:0.27506483942270277 | L1 Loss:0.399270498752594 | R2:0.19376770587767542 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[132] Loss:0.1722021414898336 | L1 Loss:0.3274030666798353 | R2:0.4814179839517382 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[132] Loss:0.27453647553920746 | L1 Loss:0.4045360654592514 | R2:0.19203232300389408 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[133] Loss:0.17736156890168786 | L1 Loss:0.3366158977150917 | R2:0.4632192480331826 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[133] Loss:0.2859853357076645 | L1 Loss:0.41190956234931947 | R2:0.15259233930160812 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[134] Loss:0.1725928382948041 | L1 Loss:0.3389114635065198 | R2:0.4838887469184127 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[134] Loss:0.2747182801365852 | L1 Loss:0.40339741110801697 | R2:0.1928294836420735 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[135] Loss:0.16544223483651876 | L1 Loss:0.32578066643327475 | R2:0.504762475683198 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[135] Loss:0.2578806705772877 | L1 Loss:0.38257147967815397 | R2:0.247127858685967 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[136] Loss:0.17026396002620459 | L1 Loss:0.32912195567041636 | R2:0.4888729451506483 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[136] Loss:0.2674638256430626 | L1 Loss:0.3956478923559189 | R2:0.20757108200904834 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[137] Loss:0.18035143101587892 | L1 Loss:0.3355033565312624 | R2:0.45814342033471234 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[137] Loss:0.267519149184227 | L1 Loss:0.39406841397285464 | R2:0.2035444400360194 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[138] Loss:0.17534068645909429 | L1 Loss:0.3322951979935169 | R2:0.47442374432281365 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[138] Loss:0.2630095839500427 | L1 Loss:0.3945199757814407 | R2:0.222057704340062 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[139] Loss:0.17432743543758988 | L1 Loss:0.3336933571845293 | R2:0.47274488659611436 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[139] Loss:0.2697039142251015 | L1 Loss:0.408205983042717 | R2:0.2057746262041702 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[140] Loss:0.18081749696284533 | L1 Loss:0.33726577181369066 | R2:0.45229215580833104 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[140] Loss:0.2636446297168732 | L1 Loss:0.3970518410205841 | R2:0.2271041543044169 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[141] Loss:0.17412504786625504 | L1 Loss:0.3319711787626147 | R2:0.4739399519034027 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[141] Loss:0.2567013785243034 | L1 Loss:0.3939356356859207 | R2:0.24599348473145516 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[142] Loss:0.1717519978992641 | L1 Loss:0.33231412898749113 | R2:0.4849581849773701 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[142] Loss:0.263041689991951 | L1 Loss:0.3938235193490982 | R2:0.22234317110454999 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[143] Loss:0.1742076426744461 | L1 Loss:0.33359165117144585 | R2:0.47738176740993576 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[143] Loss:0.27206271290779116 | L1 Loss:0.4070470631122589 | R2:0.1981286735587056 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[144] Loss:0.16538543067872524 | L1 Loss:0.3279705448076129 | R2:0.501020243114334 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[144] Loss:0.25797862783074377 | L1 Loss:0.3905878305435181 | R2:0.2322702172596797 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[145] Loss:0.1732703554444015 | L1 Loss:0.3337549362331629 | R2:0.4753350139512008 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[145] Loss:0.24999009743332862 | L1 Loss:0.38505960404872897 | R2:0.2538376498843843 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[146] Loss:0.16658308263868093 | L1 Loss:0.32109283842146397 | R2:0.5024665286802059 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[146] Loss:0.25705805569887163 | L1 Loss:0.38955798745155334 | R2:0.23610416129429562 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[147] Loss:0.16827002353966236 | L1 Loss:0.3261144310235977 | R2:0.49234744634684807 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[147] Loss:0.267408062517643 | L1 Loss:0.3964975208044052 | R2:0.20188353094185815 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[148] Loss:0.16831023804843426 | L1 Loss:0.32409793976694345 | R2:0.4959015391396925 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[148] Loss:0.2565894156694412 | L1 Loss:0.39355256259441374 | R2:0.23714635598966655 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[149] Loss:0.16550243832170963 | L1 Loss:0.3240927765145898 | R2:0.5073300001369951 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[149] Loss:0.25257751867175104 | L1 Loss:0.38905464112758636 | R2:0.25076659084798353 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[150] Loss:0.15691385883837938 | L1 Loss:0.31329929642379284 | R2:0.5309489149063029 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[150] Loss:0.25060350745916365 | L1 Loss:0.3840526968240738 | R2:0.26125316906092877 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[151] Loss:0.16470030834898353 | L1 Loss:0.31756072491407394 | R2:0.5084235358029865 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[151] Loss:0.24119879752397538 | L1 Loss:0.37205140888690946 | R2:0.2961461445446113 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[152] Loss:0.16581797832623124 | L1 Loss:0.3248135829344392 | R2:0.5028878143811734 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[152] Loss:0.2565562680363655 | L1 Loss:0.3814192533493042 | R2:0.2523156847261887 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[153] Loss:0.165949075948447 | L1 Loss:0.32687555626034737 | R2:0.5030103109723958 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[153] Loss:0.24509629756212234 | L1 Loss:0.374015212059021 | R2:0.28308144998103 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[154] Loss:0.15624484093859792 | L1 Loss:0.3155889818444848 | R2:0.5332688936748678 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[154] Loss:0.2483590491116047 | L1 Loss:0.38003954887390134 | R2:0.27521396375694407 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[155] Loss:0.16711887205019593 | L1 Loss:0.32641843892633915 | R2:0.500101225512689 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[155] Loss:0.2529383674263954 | L1 Loss:0.38533940017223356 | R2:0.2626576863714858 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[156] Loss:0.1646860744804144 | L1 Loss:0.3236685376614332 | R2:0.5055963294704745 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[156] Loss:0.2593489333987236 | L1 Loss:0.38783423900604247 | R2:0.23724504383391576 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[157] Loss:0.15997436875477433 | L1 Loss:0.3184720017015934 | R2:0.5222162450153163 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[157] Loss:0.2548669032752514 | L1 Loss:0.38764796406030655 | R2:0.2514216494293909 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[158] Loss:0.1622233665548265 | L1 Loss:0.3190986644476652 | R2:0.5109914518994196 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[158] Loss:0.2641179487109184 | L1 Loss:0.3965427964925766 | R2:0.22503668264800814 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[159] Loss:0.16402360098436475 | L1 Loss:0.3229194236919284 | R2:0.5088078788297595 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[159] Loss:0.2611532062292099 | L1 Loss:0.39200291931629183 | R2:0.23110719619835654 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[160] Loss:0.17287743836641312 | L1 Loss:0.3335507195442915 | R2:0.482487150158188 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[160] Loss:0.2560678645968437 | L1 Loss:0.3916851028800011 | R2:0.24932210852470488 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[161] Loss:0.16441888827830553 | L1 Loss:0.3215588703751564 | R2:0.504622345527928 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[161] Loss:0.2719587989151478 | L1 Loss:0.40613779723644255 | R2:0.1982069956794178 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[162] Loss:0.16739325085654855 | L1 Loss:0.3252606187015772 | R2:0.49751417359790473 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[162] Loss:0.25859900265932084 | L1 Loss:0.39560105949640273 | R2:0.24489121253519466 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[163] Loss:0.16316896863281727 | L1 Loss:0.32674949057400227 | R2:0.5081872821000533 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[163] Loss:0.26462955549359324 | L1 Loss:0.3968090653419495 | R2:0.22726852728439972 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[164] Loss:0.15867899311706424 | L1 Loss:0.31858470663428307 | R2:0.5198101742826124 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[164] Loss:0.254941338300705 | L1 Loss:0.39141717851161956 | R2:0.24621835616280538 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[165] Loss:0.15467563131824136 | L1 Loss:0.3146770466119051 | R2:0.536809295770226 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[165] Loss:0.2620449230074883 | L1 Loss:0.3869359701871872 | R2:0.22669539573624284 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[166] Loss:0.15834813192486763 | L1 Loss:0.314158184453845 | R2:0.5263719293139102 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[166] Loss:0.25788061022758485 | L1 Loss:0.3885663509368896 | R2:0.24983990940942774 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[167] Loss:0.16561819659546018 | L1 Loss:0.3261886481195688 | R2:0.5039547965779446 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[167] Loss:0.2515183128416538 | L1 Loss:0.3853957787156105 | R2:0.2614768060641782 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[168] Loss:0.1595526272431016 | L1 Loss:0.3204280771315098 | R2:0.5173537254865539 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[168] Loss:0.25273440033197403 | L1 Loss:0.3828351259231567 | R2:0.259434858660513 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[169] Loss:0.16188144125044346 | L1 Loss:0.3191790832206607 | R2:0.5154589524658466 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[169] Loss:0.24508965015411377 | L1 Loss:0.3759054273366928 | R2:0.28131609622087905 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[170] Loss:0.16068493528291583 | L1 Loss:0.31640635430812836 | R2:0.5193960099262518 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[170] Loss:0.24798078387975692 | L1 Loss:0.381306579709053 | R2:0.2667755827433086 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[171] Loss:0.16200097557157278 | L1 Loss:0.31915684416890144 | R2:0.51379420997771 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[171] Loss:0.243796256929636 | L1 Loss:0.3722487330436707 | R2:0.28755673535889226 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[172] Loss:0.1596978921443224 | L1 Loss:0.3170723728835583 | R2:0.5193055721957167 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[172] Loss:0.24895762354135514 | L1 Loss:0.3830529123544693 | R2:0.27498959927879085 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[173] Loss:0.16120766522362828 | L1 Loss:0.3150483975186944 | R2:0.5193298336007667 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[173] Loss:0.2461216516792774 | L1 Loss:0.37791032791137696 | R2:0.2837837672289846 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[174] Loss:0.16368794161826372 | L1 Loss:0.32094500213861465 | R2:0.5099935976001053 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[174] Loss:0.25202602744102476 | L1 Loss:0.3916207730770111 | R2:0.26125999096728947 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[175] Loss:0.16229587839916348 | L1 Loss:0.32349099311977625 | R2:0.5139573142379043 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[175] Loss:0.2446863941848278 | L1 Loss:0.3853074461221695 | R2:0.29324789012602703 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[176] Loss:0.15899919671937823 | L1 Loss:0.3155413456261158 | R2:0.5241628628186905 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[176] Loss:0.247617993876338 | L1 Loss:0.37854711413383485 | R2:0.2825443176153552 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[177] Loss:0.15490602003410459 | L1 Loss:0.312423468567431 | R2:0.538156832645611 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[177] Loss:0.24275663495063782 | L1 Loss:0.37453944236040115 | R2:0.29663897293870223 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[178] Loss:0.15310736373066902 | L1 Loss:0.3092527659609914 | R2:0.539091924893875 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[178] Loss:0.24318690374493598 | L1 Loss:0.38422329872846606 | R2:0.28966360456409773 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[179] Loss:0.15948767075315118 | L1 Loss:0.31787895783782005 | R2:0.5218580834037219 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[179] Loss:0.2338936746120453 | L1 Loss:0.3755982667207718 | R2:0.32317593863854566 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[180] Loss:0.15976045047864318 | L1 Loss:0.31862602196633816 | R2:0.5248848693161448 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[180] Loss:0.2467706985771656 | L1 Loss:0.37980220913887025 | R2:0.2868000142915784 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[181] Loss:0.1582257291302085 | L1 Loss:0.31477836705744267 | R2:0.5272643015549323 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[181] Loss:0.231414457783103 | L1 Loss:0.3683756247162819 | R2:0.32983059338716525 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[182] Loss:0.16137186717242002 | L1 Loss:0.3238045694306493 | R2:0.5196282058081773 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[182] Loss:0.23521999418735504 | L1 Loss:0.3683731064200401 | R2:0.3125088453912272 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[183] Loss:0.15554668195545673 | L1 Loss:0.31410294957458973 | R2:0.5359919042482508 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[183] Loss:0.24237975478172302 | L1 Loss:0.37828543931245806 | R2:0.29702461112074774 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[184] Loss:0.16038413671776652 | L1 Loss:0.3235838767141104 | R2:0.5227254130285846 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[184] Loss:0.2477539774030447 | L1 Loss:0.376170402765274 | R2:0.2781866672258465 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[185] Loss:0.15654293773695827 | L1 Loss:0.31963901594281197 | R2:0.5299798216029765 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[185] Loss:0.24335254430770875 | L1 Loss:0.36677940040826795 | R2:0.2972929258459182 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[186] Loss:0.1635700287297368 | L1 Loss:0.3208348620682955 | R2:0.5099276505823079 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[186] Loss:0.24383722022175788 | L1 Loss:0.3710871905088425 | R2:0.2871412590521003 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[187] Loss:0.16540973540395498 | L1 Loss:0.3298952914774418 | R2:0.5085041841922391 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[187] Loss:0.24699587374925613 | L1 Loss:0.3748196721076965 | R2:0.2791481788813358 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[188] Loss:0.16590013215318322 | L1 Loss:0.32631273940205574 | R2:0.5073413977018156 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[188] Loss:0.24446378014981746 | L1 Loss:0.37488973289728167 | R2:0.2874130054410016 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[189] Loss:0.16576602682471275 | L1 Loss:0.3280255999416113 | R2:0.5048591025800759 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[189] Loss:0.24670865945518017 | L1 Loss:0.3704839497804642 | R2:0.28132139200088685 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[190] Loss:0.17044473672285676 | L1 Loss:0.33047695830464363 | R2:0.4859773298975836 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[190] Loss:0.2529087632894516 | L1 Loss:0.3835224360227585 | R2:0.2633728741009211 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[191] Loss:0.178339549805969 | L1 Loss:0.33869148790836334 | R2:0.4685731091101946 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[191] Loss:0.23777021020650863 | L1 Loss:0.36655662804841993 | R2:0.3122677842446821 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[192] Loss:0.16331004071980715 | L1 Loss:0.3239586725831032 | R2:0.5107538645888211 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[192] Loss:0.2480569839477539 | L1 Loss:0.38242752850055695 | R2:0.27789390065381825 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[193] Loss:0.15749584278091788 | L1 Loss:0.31226243637502193 | R2:0.5307139838306535 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[193] Loss:0.24892271012067796 | L1 Loss:0.37914775907993314 | R2:0.2778879977072056 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[194] Loss:0.15555652137845755 | L1 Loss:0.31107028760015965 | R2:0.5367257399989717 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[194] Loss:0.24652978628873826 | L1 Loss:0.3846168726682663 | R2:0.28204713299010375 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[195] Loss:0.1579936919733882 | L1 Loss:0.3141303760930896 | R2:0.5231914848148665 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[195] Loss:0.252901928126812 | L1 Loss:0.3911543309688568 | R2:0.26191706115488667 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[196] Loss:0.1507499790750444 | L1 Loss:0.3094745296984911 | R2:0.5456746793012296 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[196] Loss:0.2504035301506519 | L1 Loss:0.3785190165042877 | R2:0.27107940047901374 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[197] Loss:0.15312690939754248 | L1 Loss:0.312952752225101 | R2:0.5347544466989044 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[197] Loss:0.24591590315103531 | L1 Loss:0.3743111178278923 | R2:0.28323312507725795 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[198] Loss:0.15104149375110865 | L1 Loss:0.3077531373128295 | R2:0.5458884485357404 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[198] Loss:0.24379687383770943 | L1 Loss:0.3767728865146637 | R2:0.2825261951594074 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[199] Loss:0.1531323455274105 | L1 Loss:0.3095678724348545 | R2:0.5387370552871562 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[199] Loss:0.2486696369946003 | L1 Loss:0.37988185286521914 | R2:0.2750803191000638 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[200] Loss:0.15200113924220204 | L1 Loss:0.3127989796921611 | R2:0.5441675055205837 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[200] Loss:0.25069202929735185 | L1 Loss:0.3853923618793488 | R2:0.26003454960381306 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[201] Loss:0.15191254578530788 | L1 Loss:0.3131076693534851 | R2:0.5454704189821327 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[201] Loss:0.2433897376060486 | L1 Loss:0.37776012867689135 | R2:0.2949787269876917 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[202] Loss:0.15312065090984106 | L1 Loss:0.3135258201509714 | R2:0.5370290329963885 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[202] Loss:0.24024396315217017 | L1 Loss:0.3739009127020836 | R2:0.3013940427254277 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[203] Loss:0.15460763685405254 | L1 Loss:0.3142132479697466 | R2:0.5318225381686642 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[203] Loss:0.2591770961880684 | L1 Loss:0.38688207864761354 | R2:0.2523137523993368 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[204] Loss:0.1502168462611735 | L1 Loss:0.3097766786813736 | R2:0.5504805423659485 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[204] Loss:0.25668462812900544 | L1 Loss:0.3894002318382263 | R2:0.24547795616099913 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[205] Loss:0.15296080382540822 | L1 Loss:0.30509203393012285 | R2:0.5399875003485977 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[205] Loss:0.2515634924173355 | L1 Loss:0.38331169784069063 | R2:0.2667694454222045 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[206] Loss:0.14986357395537198 | L1 Loss:0.3081318326294422 | R2:0.5500996069005288 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[206] Loss:0.24420310258865358 | L1 Loss:0.37728560268878936 | R2:0.2893592068821766 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[207] Loss:0.15833184868097305 | L1 Loss:0.31906918436288834 | R2:0.5279436104339956 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[207] Loss:0.23814228922128677 | L1 Loss:0.37544798403978347 | R2:0.30475018967725137 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[208] Loss:0.1625864221714437 | L1 Loss:0.32254065573215485 | R2:0.5124864418660956 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[208] Loss:0.24821870401501656 | L1 Loss:0.38152415454387667 | R2:0.2838642979933208 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[209] Loss:0.1557398703880608 | L1 Loss:0.31280445493757725 | R2:0.5345556977862124 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[209] Loss:0.2408003732562065 | L1 Loss:0.37576756477355955 | R2:0.29928582142105215 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[210] Loss:0.1542654479853809 | L1 Loss:0.3100885422900319 | R2:0.5375975969646218 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[210] Loss:0.2505046121776104 | L1 Loss:0.3812216863036156 | R2:0.27863215974309913 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[211] Loss:0.15205688308924437 | L1 Loss:0.30966469272971153 | R2:0.5475450179566518 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[211] Loss:0.25632738322019577 | L1 Loss:0.38784644901752474 | R2:0.25709430179075154 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[212] Loss:0.15296327229589224 | L1 Loss:0.3124171197414398 | R2:0.543137124427661 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[212] Loss:0.2509033501148224 | L1 Loss:0.3808490693569183 | R2:0.26947377026453473 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[213] Loss:0.15608780458569527 | L1 Loss:0.3136851340532303 | R2:0.5309680642281004 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[213] Loss:0.2628213241696358 | L1 Loss:0.38978568017482756 | R2:0.23998529311955621 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[214] Loss:0.1594634372740984 | L1 Loss:0.3175660204142332 | R2:0.5229061033558154 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[214] Loss:0.24349376037716866 | L1 Loss:0.37839633524417876 | R2:0.29328569919105585 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[215] Loss:0.1597039089538157 | L1 Loss:0.3177433768287301 | R2:0.5216196402366625 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[215] Loss:0.24656106159090996 | L1 Loss:0.38480291068553923 | R2:0.28120211626017777 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[216] Loss:0.1524626105092466 | L1 Loss:0.3098508743569255 | R2:0.5438355833486488 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[216] Loss:0.23875851854681968 | L1 Loss:0.3748526185750961 | R2:0.2982789962769192 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[217] Loss:0.156001643743366 | L1 Loss:0.30969179421663284 | R2:0.5331289666382134 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[217] Loss:0.24443473145365716 | L1 Loss:0.3723759323358536 | R2:0.2806878371430407 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[218] Loss:0.15126773482188582 | L1 Loss:0.3110495936125517 | R2:0.5462988070139133 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[218] Loss:0.2506142184138298 | L1 Loss:0.3789088577032089 | R2:0.26650100963215806 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[219] Loss:0.1561799724586308 | L1 Loss:0.31605739891529083 | R2:0.5336461781057087 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[219] Loss:0.26003020256757736 | L1 Loss:0.3880842298269272 | R2:0.23404995420072972 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[220] Loss:0.15130620170384645 | L1 Loss:0.3082565199583769 | R2:0.548505617599922 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[220] Loss:0.2593932509422302 | L1 Loss:0.39036323726177213 | R2:0.24321954257194972 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[221] Loss:0.14676532475277781 | L1 Loss:0.301130760461092 | R2:0.5592513807407189 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[221] Loss:0.2494744800031185 | L1 Loss:0.38198795914649963 | R2:0.26444638619387667 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[222] Loss:0.15141087560914457 | L1 Loss:0.30810435116291046 | R2:0.54699509903429 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[222] Loss:0.24310350567102432 | L1 Loss:0.38027561604976656 | R2:0.27786471256276346 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[223] Loss:0.14690282940864563 | L1 Loss:0.30568411480635405 | R2:0.5597713931863001 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[223] Loss:0.2490723744034767 | L1 Loss:0.3769758462905884 | R2:0.2675340549551726 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[224] Loss:0.15094236843287945 | L1 Loss:0.31168494559824467 | R2:0.5467899677237293 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[224] Loss:0.2397265374660492 | L1 Loss:0.3761110842227936 | R2:0.2925501488604327 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[225] Loss:0.15346243139356375 | L1 Loss:0.3106965981423855 | R2:0.5405368781064405 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[225] Loss:0.23998789936304094 | L1 Loss:0.38062377870082853 | R2:0.294526013687077 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[226] Loss:0.14915798930451274 | L1 Loss:0.30428917799144983 | R2:0.5511626527151068 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[226] Loss:0.25283452197909356 | L1 Loss:0.3801752060651779 | R2:0.2580391095122401 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[227] Loss:0.14916490530595183 | L1 Loss:0.3042190168052912 | R2:0.5560072175143818 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[227] Loss:0.23975315690040588 | L1 Loss:0.37260920703411105 | R2:0.297682869295229 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[228] Loss:0.1472646244801581 | L1 Loss:0.3004508903250098 | R2:0.5618537863435344 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[228] Loss:0.24101224094629287 | L1 Loss:0.3726951152086258 | R2:0.2929010181859599 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[229] Loss:0.14829677669331431 | L1 Loss:0.3056505937129259 | R2:0.558442658681506 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[229] Loss:0.24249995052814483 | L1 Loss:0.37652509808540346 | R2:0.290700801236944 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[230] Loss:0.15405130060389638 | L1 Loss:0.3064994290471077 | R2:0.5383978704925478 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[230] Loss:0.2370057262480259 | L1 Loss:0.3699733853340149 | R2:0.3042033041948458 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[231] Loss:0.15181371569633484 | L1 Loss:0.31433601304888725 | R2:0.5480193639083419 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[231] Loss:0.2365123972296715 | L1 Loss:0.37132543325424194 | R2:0.31341115850027096 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[232] Loss:0.15038133831694722 | L1 Loss:0.3026063023135066 | R2:0.5516314301705423 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[232] Loss:0.241311401873827 | L1 Loss:0.37728914320468904 | R2:0.29570835206038426 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[233] Loss:0.15868808701634407 | L1 Loss:0.3180638216435909 | R2:0.5300965071930609 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[233] Loss:0.2361999660730362 | L1 Loss:0.37559324204921724 | R2:0.31535192815870106 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[234] Loss:0.15211530029773712 | L1 Loss:0.31063693296164274 | R2:0.5417193288844993 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[234] Loss:0.24676607176661491 | L1 Loss:0.38050234913825987 | R2:0.2859245633262361 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[235] Loss:0.15780885238200426 | L1 Loss:0.31793964747339487 | R2:0.5294573854278379 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[235] Loss:0.25043305158615115 | L1 Loss:0.3848196804523468 | R2:0.26661890841830094 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[236] Loss:0.16010740911588073 | L1 Loss:0.317866132594645 | R2:0.5222915143964625 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[236] Loss:0.2419773444533348 | L1 Loss:0.3729812055826187 | R2:0.2933736113536488 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[237] Loss:0.1630967678502202 | L1 Loss:0.3251691563054919 | R2:0.5132212202296588 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[237] Loss:0.2455932840704918 | L1 Loss:0.38376844525337217 | R2:0.29043265457701206 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[238] Loss:0.15959996776655316 | L1 Loss:0.3210837272927165 | R2:0.5224606883575421 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[238] Loss:0.2505459450185299 | L1 Loss:0.3842297852039337 | R2:0.2671082411917979 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[239] Loss:0.16207461198791862 | L1 Loss:0.318189961835742 | R2:0.5167842332372331 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[239] Loss:0.24687332510948182 | L1 Loss:0.3840229660272598 | R2:0.2806497636306227 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[240] Loss:0.15643439628183842 | L1 Loss:0.3140620496124029 | R2:0.5368540654253523 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[240] Loss:0.24689487591385842 | L1 Loss:0.3815372854471207 | R2:0.2829847911498452 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[241] Loss:0.159177890047431 | L1 Loss:0.31352502666413784 | R2:0.5260670212357809 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[241] Loss:0.24551553651690483 | L1 Loss:0.3762762188911438 | R2:0.2859798631967917 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[242] Loss:0.15625871811062098 | L1 Loss:0.31218055821955204 | R2:0.5345107416340553 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[242] Loss:0.24822071939706802 | L1 Loss:0.38092000484466554 | R2:0.2771155558326153 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[243] Loss:0.1504730093292892 | L1 Loss:0.3056091070175171 | R2:0.5502820291213697 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[243] Loss:0.2409911721944809 | L1 Loss:0.3767640233039856 | R2:0.2932207078448711 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[244] Loss:0.15175962774083018 | L1 Loss:0.30908585619181395 | R2:0.5497638544982788 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[244] Loss:0.240899445861578 | L1 Loss:0.3698553889989853 | R2:0.2944247886341476 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[245] Loss:0.14854847267270088 | L1 Loss:0.3050930146127939 | R2:0.5607345676728701 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[245] Loss:0.23878944739699365 | L1 Loss:0.37221788465976713 | R2:0.3040607472521081 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[246] Loss:0.15286143077537417 | L1 Loss:0.3117044633254409 | R2:0.5428005834473763 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[246] Loss:0.2426069900393486 | L1 Loss:0.37746378779411316 | R2:0.2935212921256863 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[247] Loss:0.14795778784900904 | L1 Loss:0.30856817588210106 | R2:0.558491954203076 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[247] Loss:0.2395511046051979 | L1 Loss:0.3793632760643959 | R2:0.30348821285523736 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[248] Loss:0.14669337961822748 | L1 Loss:0.30609993170946836 | R2:0.5624494694672084 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[248] Loss:0.2506494700908661 | L1 Loss:0.3842676967382431 | R2:0.2658870094667001 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[249] Loss:0.1531531922519207 | L1 Loss:0.30922385584563017 | R2:0.5426583358197152 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[249] Loss:0.2525862842798233 | L1 Loss:0.38717980682849884 | R2:0.2643284952333528 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[250] Loss:0.1545039233751595 | L1 Loss:0.3139151344075799 | R2:0.5397325014845621 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[250] Loss:0.24741073176264763 | L1 Loss:0.37979083955287934 | R2:0.2784382776039139 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[251] Loss:0.14898864179849625 | L1 Loss:0.3030610512942076 | R2:0.5568448947429674 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[251] Loss:0.24563156962394714 | L1 Loss:0.37924729883670805 | R2:0.27600775536177646 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[252] Loss:0.14502108050510287 | L1 Loss:0.30209824070334435 | R2:0.5680563605711937 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[252] Loss:0.24074818715453147 | L1 Loss:0.37023474276065826 | R2:0.3002011700209989 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[253] Loss:0.14591394318267703 | L1 Loss:0.30213665682822466 | R2:0.5655312489083667 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[253] Loss:0.24877118840813636 | L1 Loss:0.3777481570839882 | R2:0.27033805067605343 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[254] Loss:0.1521415775641799 | L1 Loss:0.3080735160037875 | R2:0.5496121683484348 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[254] Loss:0.24680862426757813 | L1 Loss:0.37377881407737734 | R2:0.277530607479264 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[255] Loss:0.1488526971079409 | L1 Loss:0.30964169278740883 | R2:0.5559482580342453 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[255] Loss:0.2415624141693115 | L1 Loss:0.3714634537696838 | R2:0.29292253921260836 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[256] Loss:0.14922219584695995 | L1 Loss:0.30843193642795086 | R2:0.5534692590718958 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[256] Loss:0.24382010474801064 | L1 Loss:0.3806683361530304 | R2:0.28414440651125805 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[257] Loss:0.15315293706953526 | L1 Loss:0.30915969144552946 | R2:0.5405810815458713 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[257] Loss:0.24930120781064033 | L1 Loss:0.3848655462265015 | R2:0.27389608001418686 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[258] Loss:0.145608427003026 | L1 Loss:0.30314754135906696 | R2:0.5613021505536485 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[258] Loss:0.2452917903661728 | L1 Loss:0.3791656821966171 | R2:0.28132663648631917 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[259] Loss:0.14843975426629186 | L1 Loss:0.3041542051360011 | R2:0.5588441578027857 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[259] Loss:0.24414838403463363 | L1 Loss:0.3751594305038452 | R2:0.2916411808660349 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[260] Loss:0.16025632619857788 | L1 Loss:0.3140772292390466 | R2:0.5233926208727254 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[260] Loss:0.23577408194541932 | L1 Loss:0.36946102827787397 | R2:0.3218286227243932 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[261] Loss:0.1562679558992386 | L1 Loss:0.3098590010777116 | R2:0.5335688915698894 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[261] Loss:0.24343119487166404 | L1 Loss:0.37585773319005966 | R2:0.3016305067984451 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[262] Loss:0.16153435595333576 | L1 Loss:0.3165729772299528 | R2:0.5186751011840894 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[262] Loss:0.24223498329520227 | L1 Loss:0.37350085079669954 | R2:0.2931167316202532 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[263] Loss:0.16145850904285908 | L1 Loss:0.31880346965044737 | R2:0.5162679742263815 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[263] Loss:0.24675434455275536 | L1 Loss:0.3758811354637146 | R2:0.281896295903525 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[264] Loss:0.16742837615311146 | L1 Loss:0.322524631395936 | R2:0.49993952209184955 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[264] Loss:0.25647222325205804 | L1 Loss:0.37934800535440444 | R2:0.25514590029611556 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[265] Loss:0.15298254694789648 | L1 Loss:0.31273820623755455 | R2:0.5465388725651604 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[265] Loss:0.2439665149897337 | L1 Loss:0.3708927810192108 | R2:0.2909104903286522 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[266] Loss:0.15402345871552825 | L1 Loss:0.3102791476994753 | R2:0.5418621296416262 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[266] Loss:0.2407042771577835 | L1 Loss:0.37730189561843874 | R2:0.29768992459074745 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[267] Loss:0.15411845222115517 | L1 Loss:0.3104240745306015 | R2:0.5409489978708963 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[267] Loss:0.24192575737833977 | L1 Loss:0.37250772416591643 | R2:0.29785846683406847 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[268] Loss:0.15942988777533174 | L1 Loss:0.31769835390150547 | R2:0.5276658414233282 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[268] Loss:0.24144987165927886 | L1 Loss:0.37390087693929674 | R2:0.28003923118651797 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[269] Loss:0.154998027253896 | L1 Loss:0.3106314968317747 | R2:0.5382527989066628 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[269] Loss:0.23664629980921745 | L1 Loss:0.3727883532643318 | R2:0.3064589613340058 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[270] Loss:0.15305766323581338 | L1 Loss:0.30841171368956566 | R2:0.5449906315359121 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[270] Loss:0.23873024806380272 | L1 Loss:0.373946824669838 | R2:0.2882512728923591 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[271] Loss:0.16070510353893042 | L1 Loss:0.31831102818250656 | R2:0.524141408452624 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[271] Loss:0.23754413723945617 | L1 Loss:0.37177008092403413 | R2:0.3005899250549159 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[272] Loss:0.1551839835010469 | L1 Loss:0.3103318838402629 | R2:0.5392600906368263 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[272] Loss:0.24372925981879234 | L1 Loss:0.3718969255685806 | R2:0.28582969653739804 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[273] Loss:0.149910110514611 | L1 Loss:0.30600418243557215 | R2:0.5556833722503878 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[273] Loss:0.25028786808252335 | L1 Loss:0.37916603982448577 | R2:0.25884976495450895 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[274] Loss:0.14670702069997787 | L1 Loss:0.30123690236359835 | R2:0.5620063783354216 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[274] Loss:0.25052097588777544 | L1 Loss:0.38194645643234254 | R2:0.266763285418053 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[275] Loss:0.14747278718277812 | L1 Loss:0.3035078989341855 | R2:0.56150508169713 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[275] Loss:0.24588324204087258 | L1 Loss:0.37505117654800413 | R2:0.27853867651513364 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[276] Loss:0.1462446223013103 | L1 Loss:0.3039672803133726 | R2:0.5666722086830003 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[276] Loss:0.2556095473468304 | L1 Loss:0.38282923996448515 | R2:0.24473254376210501 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[277] Loss:0.14445226220414042 | L1 Loss:0.29802415799349546 | R2:0.5726583924042681 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[277] Loss:0.23898092582821845 | L1 Loss:0.369822108745575 | R2:0.29964884822684484 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[278] Loss:0.14430419355630875 | L1 Loss:0.29929917864501476 | R2:0.5718978901137344 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[278] Loss:0.23734356462955475 | L1 Loss:0.37146641612052916 | R2:0.30273669635899203 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[279] Loss:0.14460094505921006 | L1 Loss:0.2993996534496546 | R2:0.5698241131914911 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[279] Loss:0.23293521404266357 | L1 Loss:0.36961560547351835 | R2:0.3137795336378044 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[280] Loss:0.14630342973396182 | L1 Loss:0.30186612252146006 | R2:0.5597657852554825 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[280] Loss:0.2345612660050392 | L1 Loss:0.36823171824216844 | R2:0.3154332984971344 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[281] Loss:0.14644867880269885 | L1 Loss:0.30028958059847355 | R2:0.5629144964003125 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[281] Loss:0.23138370215892792 | L1 Loss:0.370674604177475 | R2:0.31881991256783915 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[282] Loss:0.15173322474583983 | L1 Loss:0.3059890801087022 | R2:0.5529415776116848 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[282] Loss:0.2404950074851513 | L1 Loss:0.3757522523403168 | R2:0.2974173626652767 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[283] Loss:0.15364865958690643 | L1 Loss:0.30832653027027845 | R2:0.5419308961274355 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[283] Loss:0.2421969562768936 | L1 Loss:0.3750931218266487 | R2:0.29457211224303437 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[284] Loss:0.15232910076156259 | L1 Loss:0.30479274317622185 | R2:0.5462209733849026 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[284] Loss:0.24334969818592073 | L1 Loss:0.3641464740037918 | R2:0.29427766320814186 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[285] Loss:0.15253823157399893 | L1 Loss:0.3044977756217122 | R2:0.5476099294103821 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[285] Loss:0.23176471069455146 | L1 Loss:0.3654336273670197 | R2:0.3210076284929503 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[286] Loss:0.15080922981724143 | L1 Loss:0.30119036603718996 | R2:0.5545747017759409 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[286] Loss:0.23533736914396286 | L1 Loss:0.3666894644498825 | R2:0.31184578508981703 | ACC: 78.3333%(235/300)\n",
            "Training Epoch[287] Loss:0.15299822390079498 | L1 Loss:0.30523611046373844 | R2:0.5496988070805069 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[287] Loss:0.23998311012983323 | L1 Loss:0.3734691500663757 | R2:0.3030478077495156 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[288] Loss:0.15044771507382393 | L1 Loss:0.30176566634327173 | R2:0.5533670171205824 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[288] Loss:0.2457003928720951 | L1 Loss:0.36555296629667283 | R2:0.2836417834164971 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[289] Loss:0.14880328392609954 | L1 Loss:0.3027755878865719 | R2:0.5556046668431938 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[289] Loss:0.2366085931658745 | L1 Loss:0.36223327815532685 | R2:0.2997785984139008 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[290] Loss:0.15157929668202996 | L1 Loss:0.3087413152679801 | R2:0.5488292406461981 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[290] Loss:0.2415015548467636 | L1 Loss:0.375426259636879 | R2:0.28697154432811406 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[291] Loss:0.14514409005641937 | L1 Loss:0.2994165066629648 | R2:0.5683078570788808 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[291] Loss:0.2277846410870552 | L1 Loss:0.3614021807909012 | R2:0.33054598511520766 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[292] Loss:0.14461194584146142 | L1 Loss:0.3016188610345125 | R2:0.5695752712599726 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[292] Loss:0.24179226607084275 | L1 Loss:0.36842006295919416 | R2:0.2870447213829542 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[293] Loss:0.1460720542818308 | L1 Loss:0.30258722975850105 | R2:0.5674703716528511 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[293] Loss:0.24107232317328453 | L1 Loss:0.3710275486111641 | R2:0.28956420548569406 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[294] Loss:0.1436962173320353 | L1 Loss:0.3025814816355705 | R2:0.5740546556556688 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[294] Loss:0.23649544045329093 | L1 Loss:0.3588025107979774 | R2:0.30120925868238707 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[295] Loss:0.13763180514797568 | L1 Loss:0.2934314217418432 | R2:0.5893452424940733 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[295] Loss:0.23724170476198198 | L1 Loss:0.3718401104211807 | R2:0.30802249031574613 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[296] Loss:0.14155511278659105 | L1 Loss:0.29992202762514353 | R2:0.5740451312493908 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[296] Loss:0.2378321632742882 | L1 Loss:0.3708949401974678 | R2:0.30716543521105566 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[297] Loss:0.14616762846708298 | L1 Loss:0.30480269342660904 | R2:0.565418784731543 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[297] Loss:0.24551779478788377 | L1 Loss:0.37880952209234237 | R2:0.28531604681245265 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[298] Loss:0.14659150410443544 | L1 Loss:0.2987291179597378 | R2:0.5674102870448688 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[298] Loss:0.23970983177423477 | L1 Loss:0.3718569964170456 | R2:0.29791873528712376 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[299] Loss:0.13779548648744822 | L1 Loss:0.2925040675327182 | R2:0.5894489539751527 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[299] Loss:0.2442450925707817 | L1 Loss:0.37161301672458646 | R2:0.2796959799643303 | ACC: 70.0000%(210/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43feaf9-398f-48c2-ec01-fdd0fd588a70",
        "id": "PlCqSIK1PQbi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.33054598511520766\n",
            "min test_loss_l1_record  0.3588025107979774\n",
            "min test_loss_record  0.2277846410870552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7613a295-f2f9-4aa6-d579-8e8f71d9f707",
        "id": "1G404R7qPQbi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.32983059338716525\n",
            "min test_loss_l1_record  0.36655662804841993\n",
            "min test_loss_record  0.231414457783103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8c6528-6854-4874-f2c0-88dbec8bf1a9",
        "id": "qTYOf-TWPQbi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.23579258527060895\n",
            "min test_loss_l1_record  0.37144995033740996\n",
            "min test_loss_record  0.2634816706180573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "2dba7b77-fea3-4d2f-8bf9-e666178e052a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njQfMSwHPQbi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.164631662396206\n",
            "min test_loss_l1_record  0.4058924436569214\n",
            "min test_loss_record  0.29350702464580536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 mer Regression"
      ],
      "metadata": {
        "id": "tPr4Yh3FfRTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "14QcUA2rfVrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=4, mer_dict=fourMer_dict)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc6021f-03f9-4681-f249-355175e49483",
        "id": "BoPz35a4fbbl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f445ea3-7ab3-4503-e4f1-53ca4f640d02",
        "id": "pXKKTVPvfbbm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "jVAJpcwgfbbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629acc5b-4b39-4526-83d5-9224c978d98a",
        "id": "a2qPSXw4fbbm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "kqxQ9673fbbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "hUchiTZ9fbbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11ad9ff-9757-4754-f917-c4633e0a027a",
        "id": "PD2fJWoefbbn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:148: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3582678157836199 | L1 Loss:0.4460401125252247 | R2:-0.03236157772161233 | ACC: 62.8000%(314/500)\n",
            "Testing Epoch[0] Loss:0.36821635365486144 | L1 Loss:0.449246221780777 | R2:-0.03765480296273173 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[1] Loss:0.3511079791933298 | L1 Loss:0.4484448507428169 | R2:-0.017632126721049066 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[1] Loss:0.35735647231340406 | L1 Loss:0.45602996945381163 | R2:-0.015689714278640944 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[2] Loss:0.33470977284014225 | L1 Loss:0.43966568633913994 | R2:0.02442458738233317 | ACC: 62.8000%(314/500)\n",
            "Testing Epoch[2] Loss:0.35113009810447693 | L1 Loss:0.44734145402908326 | R2:0.006552859053093774 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[3] Loss:0.3263947032392025 | L1 Loss:0.43440649099648 | R2:0.057498734314020486 | ACC: 61.8000%(309/500)\n",
            "Testing Epoch[3] Loss:0.3462304830551147 | L1 Loss:0.4453787267208099 | R2:0.024619510351400385 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[4] Loss:0.32517777662724257 | L1 Loss:0.4340764842927456 | R2:0.0647069674968575 | ACC: 64.2000%(321/500)\n",
            "Testing Epoch[4] Loss:0.3393755957484245 | L1 Loss:0.44691436290740966 | R2:0.04002814805297263 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[5] Loss:0.3031761310994625 | L1 Loss:0.42220883071422577 | R2:0.12292041744858906 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[5] Loss:0.3232667356729507 | L1 Loss:0.42628733813762665 | R2:0.08786659707641578 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[6] Loss:0.2931325053796172 | L1 Loss:0.4234308134764433 | R2:0.15103394991210645 | ACC: 65.2000%(326/500)\n",
            "Testing Epoch[6] Loss:0.3369717448949814 | L1 Loss:0.4444917857646942 | R2:0.040372843684711855 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[7] Loss:0.28812030144035816 | L1 Loss:0.41676487028598785 | R2:0.16206929323393926 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[7] Loss:0.35524768829345704 | L1 Loss:0.44476469457149503 | R2:-0.009829354858933848 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[8] Loss:0.29587466455996037 | L1 Loss:0.41891358233988285 | R2:0.1408542410586887 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[8] Loss:0.3620214194059372 | L1 Loss:0.45320590436458585 | R2:-0.021370717457106204 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[9] Loss:0.2798066232353449 | L1 Loss:0.40873780101537704 | R2:0.18804502430353376 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[9] Loss:0.3437281608581543 | L1 Loss:0.44467914700508115 | R2:0.025647541195783895 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[10] Loss:0.2744156578555703 | L1 Loss:0.40230435505509377 | R2:0.2019670256446865 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[10] Loss:0.34609298706054686 | L1 Loss:0.4509422183036804 | R2:0.016856077412188464 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[11] Loss:0.274110471829772 | L1 Loss:0.40356010384857655 | R2:0.19681397452778046 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[11] Loss:0.37394209057092664 | L1 Loss:0.47045986652374266 | R2:-0.06073172715970328 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[12] Loss:0.2718638079240918 | L1 Loss:0.40601539239287376 | R2:0.2053983723127435 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[12] Loss:0.3719097703695297 | L1 Loss:0.47718815207481385 | R2:-0.05674974280369901 | ACC: 60.0000%(180/300)\n",
            "Training Epoch[13] Loss:0.2542589697986841 | L1 Loss:0.39092654548585415 | R2:0.2575715174160715 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[13] Loss:0.39208871126174927 | L1 Loss:0.48906596302986144 | R2:-0.1136402220297243 | ACC: 59.3333%(178/300)\n",
            "Training Epoch[14] Loss:0.2576611777767539 | L1 Loss:0.39424981363117695 | R2:0.24177672996900293 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[14] Loss:0.39084545373916624 | L1 Loss:0.48446919620037077 | R2:-0.1110062458424427 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[15] Loss:0.25071049574762583 | L1 Loss:0.3895773459225893 | R2:0.26187688378062524 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[15] Loss:0.3838328331708908 | L1 Loss:0.4731724143028259 | R2:-0.09100470216395089 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[16] Loss:0.24343675933778286 | L1 Loss:0.3840447776019573 | R2:0.2803467483624138 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[16] Loss:0.3656145319342613 | L1 Loss:0.46038057208061217 | R2:-0.041996969784151975 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[17] Loss:0.2410316914319992 | L1 Loss:0.3837617300450802 | R2:0.2817800618462716 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[17] Loss:0.3572156310081482 | L1 Loss:0.45423394739627837 | R2:-0.0072061686858121195 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[18] Loss:0.24374394305050373 | L1 Loss:0.3877813443541527 | R2:0.2732510401913855 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[18] Loss:0.34567348212003707 | L1 Loss:0.45598588287830355 | R2:0.01724911944515094 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[19] Loss:0.2417633282020688 | L1 Loss:0.38796100579202175 | R2:0.2881871504380501 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[19] Loss:0.3599086329340935 | L1 Loss:0.4568156063556671 | R2:-0.011680955532738757 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[20] Loss:0.23132087755948305 | L1 Loss:0.3805954195559025 | R2:0.31763618149705475 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[20] Loss:0.3390460446476936 | L1 Loss:0.44957763850688937 | R2:0.0417584989755343 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[21] Loss:0.23376386798918247 | L1 Loss:0.38148541934788227 | R2:0.3098597457093658 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[21] Loss:0.34323849231004716 | L1 Loss:0.453997939825058 | R2:0.03768560110990997 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[22] Loss:0.24081476591527462 | L1 Loss:0.38738097064197063 | R2:0.28795217061172307 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[22] Loss:0.3371288225054741 | L1 Loss:0.45176264345645906 | R2:0.046158171638246606 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[23] Loss:0.24243685323745012 | L1 Loss:0.3868755903095007 | R2:0.28435235010856563 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[23] Loss:0.34017058163881303 | L1 Loss:0.44923326671123504 | R2:0.03062490848253576 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[24] Loss:0.23284027446061373 | L1 Loss:0.37996355444192886 | R2:0.3153833687160148 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[24] Loss:0.32434054613113406 | L1 Loss:0.441127347946167 | R2:0.07628158311893245 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[25] Loss:0.2300383960828185 | L1 Loss:0.37406822480261326 | R2:0.31338701318695217 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[25] Loss:0.31802814453840256 | L1 Loss:0.4261197566986084 | R2:0.10279121186663291 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[26] Loss:0.2301235650666058 | L1 Loss:0.3791895769536495 | R2:0.3244957299962148 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[26] Loss:0.32305379658937455 | L1 Loss:0.42948334813117983 | R2:0.08742531617330238 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[27] Loss:0.23698751721531153 | L1 Loss:0.38410246931016445 | R2:0.2976652244269172 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[27] Loss:0.3425080716609955 | L1 Loss:0.4358182042837143 | R2:0.03547753403233787 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[28] Loss:0.25331807136535645 | L1 Loss:0.3957408145070076 | R2:0.2446693334465885 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[28] Loss:0.3319093570113182 | L1 Loss:0.43237747848033903 | R2:0.07100339979121503 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[29] Loss:0.2448288220912218 | L1 Loss:0.3862897716462612 | R2:0.270068076341634 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[29] Loss:0.329522916674614 | L1 Loss:0.4346428483724594 | R2:0.07802760003750864 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[30] Loss:0.25089817214757204 | L1 Loss:0.38982329703867435 | R2:0.25945921473715633 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[30] Loss:0.32440380454063417 | L1 Loss:0.430254864692688 | R2:0.09566666503461389 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[31] Loss:0.24111806321889162 | L1 Loss:0.3879513405263424 | R2:0.27729412909811063 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[31] Loss:0.3483850359916687 | L1 Loss:0.4363322466611862 | R2:0.022009667342797367 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[32] Loss:0.23470182344317436 | L1 Loss:0.3887187298387289 | R2:0.3056651462657465 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[32] Loss:0.31577817648649215 | L1 Loss:0.42149183750152586 | R2:0.12001224256809721 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[33] Loss:0.24158765003085136 | L1 Loss:0.38980946503579617 | R2:0.29017715536385036 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[33] Loss:0.32029383927583693 | L1 Loss:0.4358491122722626 | R2:0.11973538986012913 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[34] Loss:0.26341425348073244 | L1 Loss:0.40521662682294846 | R2:0.2171037066513068 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[34] Loss:0.33833535462617875 | L1 Loss:0.442900151014328 | R2:0.05945653508984487 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[35] Loss:0.24536128249019384 | L1 Loss:0.39467303827404976 | R2:0.27369243047576675 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[35] Loss:0.31229942440986636 | L1 Loss:0.42851944267749786 | R2:0.13955262059895357 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[36] Loss:0.2496830876916647 | L1 Loss:0.39028983376920223 | R2:0.2558132871256049 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[36] Loss:0.3282368049025536 | L1 Loss:0.4448168039321899 | R2:0.0891835215511622 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[37] Loss:0.23931328486651182 | L1 Loss:0.38722814805805683 | R2:0.29241238235006894 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[37] Loss:0.31157744079828265 | L1 Loss:0.4340484976768494 | R2:0.13726170064568427 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[38] Loss:0.24495582468807697 | L1 Loss:0.3923394773155451 | R2:0.2718201319240498 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[38] Loss:0.32343567311763766 | L1 Loss:0.44216732680797577 | R2:0.08806740179150982 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[39] Loss:0.24303291831165552 | L1 Loss:0.3976469673216343 | R2:0.28041532144516446 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[39] Loss:0.32650636732578275 | L1 Loss:0.4475513011217117 | R2:0.08663677452996936 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[40] Loss:0.24961897172033787 | L1 Loss:0.39487817138433456 | R2:0.25506672554320875 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[40] Loss:0.32452979534864423 | L1 Loss:0.452000030875206 | R2:0.08840547166273438 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[41] Loss:0.26229177322238684 | L1 Loss:0.40359135158360004 | R2:0.2195382842025249 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[41] Loss:0.3503889158368111 | L1 Loss:0.4680275619029999 | R2:0.012152516259001867 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[42] Loss:0.2656388161703944 | L1 Loss:0.4109503850340843 | R2:0.21610575634425438 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[42] Loss:0.35478078573942184 | L1 Loss:0.4700521737337112 | R2:0.017467871968706063 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[43] Loss:0.25133526138961315 | L1 Loss:0.39618518203496933 | R2:0.26615745678891956 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[43] Loss:0.3341087490320206 | L1 Loss:0.44673005044460296 | R2:0.06505506689305722 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[44] Loss:0.25733449403196573 | L1 Loss:0.39560611732304096 | R2:0.24463945892416897 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[44] Loss:0.33415089547634125 | L1 Loss:0.4533480525016785 | R2:0.06760279209387587 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[45] Loss:0.23854690324515104 | L1 Loss:0.3881806433200836 | R2:0.29385727670268613 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[45] Loss:0.3158166468143463 | L1 Loss:0.4325735569000244 | R2:0.1248024405633427 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[46] Loss:0.24583460669964552 | L1 Loss:0.39611177891492844 | R2:0.27521109266163213 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[46] Loss:0.31378342807292936 | L1 Loss:0.4292610824108124 | R2:0.1342208332418543 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[47] Loss:0.25562746450304985 | L1 Loss:0.40239174850285053 | R2:0.2507044475991811 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[47] Loss:0.2987877234816551 | L1 Loss:0.4185855120420456 | R2:0.18352771365944429 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[48] Loss:0.24748555291444063 | L1 Loss:0.39395915903151035 | R2:0.27936712258611845 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[48] Loss:0.3281696528196335 | L1 Loss:0.4352529078722 | R2:0.09160164268437569 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[49] Loss:0.24283062107861042 | L1 Loss:0.3952125087380409 | R2:0.28461959366527456 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[49] Loss:0.31503738909959794 | L1 Loss:0.42432841956615447 | R2:0.13102194652177926 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[50] Loss:0.2500474378466606 | L1 Loss:0.40356286987662315 | R2:0.2512153571677233 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[50] Loss:0.3107763335108757 | L1 Loss:0.42252337336540224 | R2:0.1386968884321502 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[51] Loss:0.2390863811597228 | L1 Loss:0.39060911536216736 | R2:0.293539109051187 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[51] Loss:0.2991791650652885 | L1 Loss:0.4139092803001404 | R2:0.16739568947551065 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[52] Loss:0.23147162981331348 | L1 Loss:0.38826336711645126 | R2:0.3190948539565278 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[52] Loss:0.31753425747156144 | L1 Loss:0.4306763827800751 | R2:0.1146612236328907 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[53] Loss:0.23730321414768696 | L1 Loss:0.38628842309117317 | R2:0.29588838038872445 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[53] Loss:0.3117046892642975 | L1 Loss:0.41779369711875913 | R2:0.13003746966308485 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[54] Loss:0.22768496721982956 | L1 Loss:0.3809558916836977 | R2:0.3325077710523599 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[54] Loss:0.3047410473227501 | L1 Loss:0.41781440675258635 | R2:0.14925785437457478 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[55] Loss:0.22988422866910696 | L1 Loss:0.3842250593006611 | R2:0.3285944690261971 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[55] Loss:0.31644883304834365 | L1 Loss:0.4298196315765381 | R2:0.11631914990059784 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[56] Loss:0.23414232023060322 | L1 Loss:0.38843329809606075 | R2:0.3160640410594003 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[56] Loss:0.3051035821437836 | L1 Loss:0.4194556176662445 | R2:0.1491084320039862 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[57] Loss:0.23631736123934388 | L1 Loss:0.3875274173915386 | R2:0.3030422955236668 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[57] Loss:0.310506509244442 | L1 Loss:0.43122309148311616 | R2:0.1335013083183035 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[58] Loss:0.24102714285254478 | L1 Loss:0.39214911311864853 | R2:0.29457090789639395 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[58] Loss:0.3191025674343109 | L1 Loss:0.42886724770069123 | R2:0.11716660632766593 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[59] Loss:0.22912460565567017 | L1 Loss:0.3815661910921335 | R2:0.33353531095767663 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[59] Loss:0.2997241646051407 | L1 Loss:0.42119444012641905 | R2:0.16680195574523363 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[60] Loss:0.2413299805484712 | L1 Loss:0.3947073370218277 | R2:0.2900857682632508 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[60] Loss:0.32574803531169894 | L1 Loss:0.44113922119140625 | R2:0.0840599610940771 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[61] Loss:0.2404748983681202 | L1 Loss:0.39107657223939896 | R2:0.2903369452071139 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[61] Loss:0.31782353967428206 | L1 Loss:0.43834332525730135 | R2:0.12708855998700244 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[62] Loss:0.2357867769896984 | L1 Loss:0.38611968234181404 | R2:0.30221422858618774 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[62] Loss:0.30984946340322495 | L1 Loss:0.4276097327470779 | R2:0.14408776255659095 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[63] Loss:0.2518750140443444 | L1 Loss:0.3999280910938978 | R2:0.2602594784838875 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[63] Loss:0.3152156129479408 | L1 Loss:0.42212864458560945 | R2:0.12388852064763109 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[64] Loss:0.23625603131949902 | L1 Loss:0.377626184374094 | R2:0.31396350195139744 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[64] Loss:0.3129329666495323 | L1 Loss:0.42618141174316404 | R2:0.130746710562746 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[65] Loss:0.24047005735337734 | L1 Loss:0.38507483527064323 | R2:0.30008681049158603 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[65] Loss:0.32403508126735686 | L1 Loss:0.4300900101661682 | R2:0.10725687245690547 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[66] Loss:0.24365140963345766 | L1 Loss:0.3900410607457161 | R2:0.29224748436031855 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[66] Loss:0.30144267678260805 | L1 Loss:0.42465361058712003 | R2:0.16339345617294215 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[67] Loss:0.24351761024445295 | L1 Loss:0.3837064951658249 | R2:0.28841621342347684 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[67] Loss:0.307712721824646 | L1 Loss:0.42141404151916506 | R2:0.14816378519334555 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[68] Loss:0.23845758568495512 | L1 Loss:0.38745476491749287 | R2:0.30264204857891985 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[68] Loss:0.34357930421829225 | L1 Loss:0.4437048822641373 | R2:0.03757214474238853 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[69] Loss:0.23398206289857626 | L1 Loss:0.37905397079885006 | R2:0.32050234715371495 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[69] Loss:0.3300650902092457 | L1 Loss:0.42590669095516204 | R2:0.08740989179091667 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[70] Loss:0.2231235597282648 | L1 Loss:0.37272493727505207 | R2:0.34790196475708374 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[70] Loss:0.3285616964101791 | L1 Loss:0.4298690438270569 | R2:0.08141477582788714 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[71] Loss:0.22796680778265 | L1 Loss:0.3725391514599323 | R2:0.33966875445618344 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[71] Loss:0.32504959478974343 | L1 Loss:0.4239561855792999 | R2:0.08756320105858943 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[72] Loss:0.22438566014170647 | L1 Loss:0.37624537758529186 | R2:0.3437691195178806 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[72] Loss:0.3389932088553905 | L1 Loss:0.4283423274755478 | R2:0.04199712153334141 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[73] Loss:0.23152827890589833 | L1 Loss:0.38078748062253 | R2:0.3218375214019432 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[73] Loss:0.3279480129480362 | L1 Loss:0.4221288114786148 | R2:0.08606950353703594 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[74] Loss:0.21845486480742693 | L1 Loss:0.3695526458323002 | R2:0.35613986677153825 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[74] Loss:0.30215524733066557 | L1 Loss:0.41341792345046996 | R2:0.14440019211902375 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[75] Loss:0.2176575344055891 | L1 Loss:0.371055006980896 | R2:0.3635167635356886 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[75] Loss:0.29669261500239374 | L1 Loss:0.405391788482666 | R2:0.17546743462541717 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[76] Loss:0.2226656237617135 | L1 Loss:0.3765900954604149 | R2:0.33991709705643863 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[76] Loss:0.3098099410533905 | L1 Loss:0.42085473239421844 | R2:0.13063423587786183 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[77] Loss:0.22176287323236465 | L1 Loss:0.3678424134850502 | R2:0.35348752911918735 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[77] Loss:0.32740904241800306 | L1 Loss:0.42646163105964663 | R2:0.08381394913352824 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[78] Loss:0.2151755215600133 | L1 Loss:0.3641578033566475 | R2:0.37304374280544 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[78] Loss:0.3286308139562607 | L1 Loss:0.4334739327430725 | R2:0.05558830593692794 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[79] Loss:0.22199588641524315 | L1 Loss:0.3706977292895317 | R2:0.34110766655148694 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[79] Loss:0.3223621562123299 | L1 Loss:0.42431819438934326 | R2:0.08530684056341005 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[80] Loss:0.22719452623277903 | L1 Loss:0.3724815174937248 | R2:0.3281634158743134 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[80] Loss:0.3258840084075928 | L1 Loss:0.4324859261512756 | R2:0.08644673281276098 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[81] Loss:0.22473075333982706 | L1 Loss:0.37356952764093876 | R2:0.34811190873804776 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[81] Loss:0.3200069308280945 | L1 Loss:0.4235101342201233 | R2:0.09236154510987227 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[82] Loss:0.22801364585757256 | L1 Loss:0.3748791981488466 | R2:0.326424480627972 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[82] Loss:0.3184510260820389 | L1 Loss:0.42695139050483705 | R2:0.0959216708117094 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[83] Loss:0.22064852947369218 | L1 Loss:0.36641827039420605 | R2:0.3457252310292624 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[83] Loss:0.3153247147798538 | L1 Loss:0.42503568828105925 | R2:0.09753220830536105 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[84] Loss:0.22548548225313425 | L1 Loss:0.3750036098062992 | R2:0.34006072034934276 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[84] Loss:0.3153219260275364 | L1 Loss:0.42169695198535917 | R2:0.09869860816171201 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[85] Loss:0.2287494894117117 | L1 Loss:0.37794093787670135 | R2:0.3274689244845738 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[85] Loss:0.3112593531608582 | L1 Loss:0.4219282060861588 | R2:0.1083890778390519 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[86] Loss:0.22877861838787794 | L1 Loss:0.38146355375647545 | R2:0.3273417829819528 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[86] Loss:0.3187615960836411 | L1 Loss:0.4206532210111618 | R2:0.09507639852441074 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[87] Loss:0.24080112390220165 | L1 Loss:0.3854731786996126 | R2:0.30114755129419146 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[87] Loss:0.3072132408618927 | L1 Loss:0.41915715038776397 | R2:0.1144772151874057 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[88] Loss:0.22703466936945915 | L1 Loss:0.37399300932884216 | R2:0.3298466801725764 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[88] Loss:0.293369847536087 | L1 Loss:0.4119262218475342 | R2:0.17283270290625247 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[89] Loss:0.20829270035028458 | L1 Loss:0.35613076761364937 | R2:0.38925983438193 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[89] Loss:0.29449716210365295 | L1 Loss:0.4083207458257675 | R2:0.148680250408965 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[90] Loss:0.22255445178598166 | L1 Loss:0.37045430205762386 | R2:0.3419582913306719 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[90] Loss:0.2861447796225548 | L1 Loss:0.4053958773612976 | R2:0.18069532367642044 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[91] Loss:0.22122346051037312 | L1 Loss:0.3699757345020771 | R2:0.34670734709837087 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[91] Loss:0.281775127351284 | L1 Loss:0.3978608787059784 | R2:0.19565320502706382 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[92] Loss:0.23027934320271015 | L1 Loss:0.3793594017624855 | R2:0.32374518777129624 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[92] Loss:0.2949746921658516 | L1 Loss:0.40489460825920104 | R2:0.1660228424598707 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[93] Loss:0.22279871441423893 | L1 Loss:0.37131343968212605 | R2:0.3466917802854111 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[93] Loss:0.2937386706471443 | L1 Loss:0.41107879877090453 | R2:0.16666764099741846 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[94] Loss:0.21815911401063204 | L1 Loss:0.3679226040840149 | R2:0.35487501607405947 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[94] Loss:0.2848632365465164 | L1 Loss:0.40246141254901885 | R2:0.19328799138972136 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[95] Loss:0.2119302600622177 | L1 Loss:0.3638581484556198 | R2:0.37463080574123525 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[95] Loss:0.2949895575642586 | L1 Loss:0.401931169629097 | R2:0.16562450051933894 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[96] Loss:0.20704474113881588 | L1 Loss:0.3562653213739395 | R2:0.38752886190529756 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[96] Loss:0.2850475192070007 | L1 Loss:0.40332594215869905 | R2:0.1950539621749136 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[97] Loss:0.21518447808921337 | L1 Loss:0.3676119390875101 | R2:0.36670927174645757 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[97] Loss:0.28839164823293684 | L1 Loss:0.4015567123889923 | R2:0.18613616906823127 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[98] Loss:0.2093723537400365 | L1 Loss:0.3623080663383007 | R2:0.3886779916614813 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[98] Loss:0.2981456980109215 | L1 Loss:0.4122180461883545 | R2:0.16346592152930833 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[99] Loss:0.22548379376530647 | L1 Loss:0.3752650134265423 | R2:0.3367788883755216 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[99] Loss:0.3168741911649704 | L1 Loss:0.42313456535339355 | R2:0.10570320207714298 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[100] Loss:0.21188298054039478 | L1 Loss:0.3632417097687721 | R2:0.3795982333184917 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[100] Loss:0.3052312985062599 | L1 Loss:0.40959550738334655 | R2:0.15051417289167437 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[101] Loss:0.19806759618222713 | L1 Loss:0.35009622387588024 | R2:0.4191960044264309 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[101] Loss:0.26202561259269713 | L1 Loss:0.3786797493696213 | R2:0.26285249472226146 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[102] Loss:0.21370251942425966 | L1 Loss:0.3625180870294571 | R2:0.38153574348523195 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[102] Loss:0.2742180123925209 | L1 Loss:0.39316056072711947 | R2:0.22148937509821431 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[103] Loss:0.20532841980457306 | L1 Loss:0.3548399321734905 | R2:0.39863341110055817 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[103] Loss:0.2632760651409626 | L1 Loss:0.3897545874118805 | R2:0.2526609050717431 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[104] Loss:0.20450847875326872 | L1 Loss:0.35619574412703514 | R2:0.4012871565747247 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[104] Loss:0.2613480374217033 | L1 Loss:0.3856748670339584 | R2:0.2554721761193582 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[105] Loss:0.20003034453839064 | L1 Loss:0.35327683575451374 | R2:0.41738784344761376 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[105] Loss:0.2734469875693321 | L1 Loss:0.3945192813873291 | R2:0.2228063627059207 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[106] Loss:0.2082031685858965 | L1 Loss:0.3641647268086672 | R2:0.38870188894375696 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[106] Loss:0.2764588475227356 | L1 Loss:0.3987252712249756 | R2:0.2175797577330635 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[107] Loss:0.20860730204731226 | L1 Loss:0.3648930247873068 | R2:0.38998368578964265 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[107] Loss:0.2876469910144806 | L1 Loss:0.3896428465843201 | R2:0.17259537451793064 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[108] Loss:0.20438965409994125 | L1 Loss:0.35896976105868816 | R2:0.3992767878359374 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[108] Loss:0.29487825334072115 | L1 Loss:0.4028801918029785 | R2:0.15464304715210025 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[109] Loss:0.20465761981904507 | L1 Loss:0.35787997022271156 | R2:0.3931607190809191 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[109] Loss:0.292155085504055 | L1 Loss:0.400776207447052 | R2:0.16915282331760978 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[110] Loss:0.19538512220606208 | L1 Loss:0.346289437264204 | R2:0.42365408220124096 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[110] Loss:0.2747845336794853 | L1 Loss:0.4006223529577255 | R2:0.21249797068911871 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[111] Loss:0.20638609305024147 | L1 Loss:0.35860837437212467 | R2:0.38717736231340333 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[111] Loss:0.24409551918506622 | L1 Loss:0.3783891350030899 | R2:0.31154050936303085 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[112] Loss:0.1993842488154769 | L1 Loss:0.35320672020316124 | R2:0.40809595863375836 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[112] Loss:0.2557834982872009 | L1 Loss:0.38619930744171144 | R2:0.27820546371326377 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[113] Loss:0.19502067007124424 | L1 Loss:0.3467428423464298 | R2:0.4217994432055082 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[113] Loss:0.28195482343435285 | L1 Loss:0.4031591475009918 | R2:0.19953634175152363 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[114] Loss:0.198336617089808 | L1 Loss:0.3517818581312895 | R2:0.41443981292851445 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[114] Loss:0.27762540280818937 | L1 Loss:0.397809574007988 | R2:0.20906145592740732 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[115] Loss:0.20473553333431482 | L1 Loss:0.3583068326115608 | R2:0.38968123407607297 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[115] Loss:0.2696670338511467 | L1 Loss:0.39887785613536836 | R2:0.2254292436930047 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[116] Loss:0.19901965279132128 | L1 Loss:0.3511601649224758 | R2:0.41507191456599796 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[116] Loss:0.28129833936691284 | L1 Loss:0.4013376712799072 | R2:0.18575382891807654 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[117] Loss:0.20703961420804262 | L1 Loss:0.36178396083414555 | R2:0.39671459948123083 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[117] Loss:0.2774971678853035 | L1 Loss:0.4014735847711563 | R2:0.20113799205564709 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[118] Loss:0.20725913811475039 | L1 Loss:0.3596883527934551 | R2:0.39000375275756066 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[118] Loss:0.26024329662323 | L1 Loss:0.3902415126562119 | R2:0.24616964779468464 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[119] Loss:0.20823339745402336 | L1 Loss:0.36029363237321377 | R2:0.3914205656897025 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[119] Loss:0.27337559014558793 | L1 Loss:0.3969595104455948 | R2:0.22670902700314058 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[120] Loss:0.20833540987223387 | L1 Loss:0.3574282433837652 | R2:0.3887885666068519 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[120] Loss:0.2668865993618965 | L1 Loss:0.38502774238586424 | R2:0.23638522277823565 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[121] Loss:0.20792863611131907 | L1 Loss:0.35416144132614136 | R2:0.3948927052744644 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[121] Loss:0.27851845026016236 | L1 Loss:0.3941387712955475 | R2:0.2094905995787275 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[122] Loss:0.19765155762434006 | L1 Loss:0.34507925622165203 | R2:0.42493756606250876 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[122] Loss:0.29656251668930056 | L1 Loss:0.4052336513996124 | R2:0.15440633048199523 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[123] Loss:0.2081662961281836 | L1 Loss:0.3571773711591959 | R2:0.38866432758073094 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[123] Loss:0.250905005633831 | L1 Loss:0.37530637383460996 | R2:0.28853162082499323 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[124] Loss:0.2084015435539186 | L1 Loss:0.35584888234734535 | R2:0.3856210720899048 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[124] Loss:0.27833266407251356 | L1 Loss:0.39661105573177335 | R2:0.19315884220244506 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[125] Loss:0.2055943738669157 | L1 Loss:0.3513291347771883 | R2:0.39679162142506547 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[125] Loss:0.28132950589060784 | L1 Loss:0.40019360184669495 | R2:0.19165609833350708 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[126] Loss:0.2095463126897812 | L1 Loss:0.3537778742611408 | R2:0.37673404945113165 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[126] Loss:0.2743724875152111 | L1 Loss:0.39589454531669616 | R2:0.21417587144905875 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[127] Loss:0.21242714347317815 | L1 Loss:0.3524409793317318 | R2:0.36824213756532415 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[127] Loss:0.28614831417798997 | L1 Loss:0.39843280613422394 | R2:0.17061068114504332 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[128] Loss:0.20554043538868427 | L1 Loss:0.3448542598634958 | R2:0.393937239621317 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[128] Loss:0.27545068562030794 | L1 Loss:0.3922638833522797 | R2:0.20620480837820554 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[129] Loss:0.21742603369057178 | L1 Loss:0.35522112250328064 | R2:0.35903661398437065 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[129] Loss:0.2609980069100857 | L1 Loss:0.38878001272678375 | R2:0.25404655010196536 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[130] Loss:0.2041239608079195 | L1 Loss:0.345837976783514 | R2:0.39594756709506096 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[130] Loss:0.2706702798604965 | L1 Loss:0.39035518765449523 | R2:0.21579928111006486 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[131] Loss:0.21767176501452923 | L1 Loss:0.36004053242504597 | R2:0.3598638074331053 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[131] Loss:0.28050600215792654 | L1 Loss:0.39829844534397124 | R2:0.20150503745014697 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[132] Loss:0.2088161315768957 | L1 Loss:0.34975663386285305 | R2:0.38909341859764923 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[132] Loss:0.27222826778888704 | L1 Loss:0.4030545622110367 | R2:0.21071466995428226 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[133] Loss:0.19419613108038902 | L1 Loss:0.339027464389801 | R2:0.42754737189097963 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[133] Loss:0.2748937591910362 | L1 Loss:0.39416039884090426 | R2:0.19847710738351262 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[134] Loss:0.20754179544746876 | L1 Loss:0.35368715785443783 | R2:0.3901864421167471 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[134] Loss:0.2592406183481216 | L1 Loss:0.3818851739168167 | R2:0.2496474014602918 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[135] Loss:0.1985311871394515 | L1 Loss:0.35135569609701633 | R2:0.4131854114577167 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[135] Loss:0.2674182206392288 | L1 Loss:0.38723184466362 | R2:0.23068273190307959 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[136] Loss:0.20313650369644165 | L1 Loss:0.350629610940814 | R2:0.39397237688819 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[136] Loss:0.2554876387119293 | L1 Loss:0.3765202105045319 | R2:0.27025799962112973 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[137] Loss:0.20283226342871785 | L1 Loss:0.35077198781073093 | R2:0.39810507129626765 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[137] Loss:0.26137034595012665 | L1 Loss:0.38218770623207093 | R2:0.24623698943829148 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[138] Loss:0.20971073396503925 | L1 Loss:0.3560769874602556 | R2:0.38267621069951363 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[138] Loss:0.2714149281382561 | L1 Loss:0.3870340704917908 | R2:0.2134971631168434 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[139] Loss:0.19230580935254693 | L1 Loss:0.3422908056527376 | R2:0.4308638106852503 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[139] Loss:0.2580075815320015 | L1 Loss:0.3817018359899521 | R2:0.26459338462994014 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[140] Loss:0.1922838343307376 | L1 Loss:0.34098094142973423 | R2:0.4275448663765602 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[140] Loss:0.27066330760717394 | L1 Loss:0.391224616765976 | R2:0.22872007192099342 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[141] Loss:0.18786538485437632 | L1 Loss:0.33464594557881355 | R2:0.44455561067562616 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[141] Loss:0.2699827209115028 | L1 Loss:0.39441206157207487 | R2:0.234143187555593 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[142] Loss:0.19946035323664546 | L1 Loss:0.3463256508111954 | R2:0.4057753291264973 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[142] Loss:0.27055289447307584 | L1 Loss:0.39390767812728883 | R2:0.22642758240724023 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[143] Loss:0.2043206738308072 | L1 Loss:0.34845152124762535 | R2:0.4034566343049694 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[143] Loss:0.2550025343894958 | L1 Loss:0.3824098318815231 | R2:0.2685955960671458 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[144] Loss:0.19771659234538674 | L1 Loss:0.3453330509364605 | R2:0.4130201413838288 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[144] Loss:0.2569585070014 | L1 Loss:0.38340332806110383 | R2:0.2618229643796349 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[145] Loss:0.19496925361454487 | L1 Loss:0.3438260080292821 | R2:0.42565178449576135 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[145] Loss:0.27960276156663894 | L1 Loss:0.3986018866300583 | R2:0.19454806274115222 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[146] Loss:0.2019232138991356 | L1 Loss:0.3491239417344332 | R2:0.3990069870198536 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[146] Loss:0.27969684898853303 | L1 Loss:0.39084628224372864 | R2:0.19701974112720047 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[147] Loss:0.20576548017561436 | L1 Loss:0.35054754093289375 | R2:0.3948531898994541 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[147] Loss:0.2590747684240341 | L1 Loss:0.38243223130702975 | R2:0.25991516875802784 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[148] Loss:0.19703259970992804 | L1 Loss:0.34796804189682007 | R2:0.4164704583480372 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[148] Loss:0.25569300949573515 | L1 Loss:0.3819017320871353 | R2:0.27303431517747007 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[149] Loss:0.20702236890792847 | L1 Loss:0.3517070449888706 | R2:0.3970384129213745 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[149] Loss:0.2640457510948181 | L1 Loss:0.39073714017868044 | R2:0.2523252470904306 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[150] Loss:0.2063407301902771 | L1 Loss:0.34794107638299465 | R2:0.3964794553724901 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[150] Loss:0.2452666699886322 | L1 Loss:0.38377741873264315 | R2:0.3049503380914952 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[151] Loss:0.2023988515138626 | L1 Loss:0.3459615148603916 | R2:0.4026522065335853 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[151] Loss:0.2582474909722805 | L1 Loss:0.3853431075811386 | R2:0.26304816832737987 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[152] Loss:0.19996786396950483 | L1 Loss:0.34944734163582325 | R2:0.4108146642043908 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[152] Loss:0.2635651260614395 | L1 Loss:0.389658722281456 | R2:0.24815334979202683 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[153] Loss:0.21114852372556925 | L1 Loss:0.35251786559820175 | R2:0.37909357868355326 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[153] Loss:0.26286435723304746 | L1 Loss:0.3992243081331253 | R2:0.25099461097169 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[154] Loss:0.2038481435738504 | L1 Loss:0.3498187568038702 | R2:0.4037006405556886 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[154] Loss:0.26212331652641296 | L1 Loss:0.3864191323518753 | R2:0.2516623273513833 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[155] Loss:0.20410401467233896 | L1 Loss:0.35216967202723026 | R2:0.3963841363921446 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[155] Loss:0.26126956641674043 | L1 Loss:0.3868944227695465 | R2:0.2532722664139374 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[156] Loss:0.20703736413270235 | L1 Loss:0.3539468087255955 | R2:0.38812525480568943 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[156] Loss:0.2596289813518524 | L1 Loss:0.3955316185951233 | R2:0.2598533391704053 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[157] Loss:0.1938744392246008 | L1 Loss:0.33797892928123474 | R2:0.43056645789551573 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[157] Loss:0.2502927839756012 | L1 Loss:0.3834878921508789 | R2:0.2841990945092693 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[158] Loss:0.19611044647172093 | L1 Loss:0.34316817484796047 | R2:0.42690961720267767 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[158] Loss:0.25109032094478606 | L1 Loss:0.38105120956897737 | R2:0.288824524959708 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[159] Loss:0.19271227112039924 | L1 Loss:0.3394590485841036 | R2:0.43261957130387396 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[159] Loss:0.25503615885972974 | L1 Loss:0.38175460398197175 | R2:0.2700474630031514 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[160] Loss:0.19267016742378473 | L1 Loss:0.3407661225646734 | R2:0.4311000475106255 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[160] Loss:0.2546128466725349 | L1 Loss:0.3746980011463165 | R2:0.27031143762254045 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[161] Loss:0.18584139831364155 | L1 Loss:0.33348470367491245 | R2:0.44868070350391354 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[161] Loss:0.26942815333604814 | L1 Loss:0.3856481909751892 | R2:0.23835514547889694 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[162] Loss:0.18861020915210247 | L1 Loss:0.3374466709792614 | R2:0.4458868362173238 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[162] Loss:0.256733862310648 | L1 Loss:0.38056791126728057 | R2:0.2787142994509576 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[163] Loss:0.19042985048145056 | L1 Loss:0.33886229433119297 | R2:0.4360033333185642 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[163] Loss:0.25071124657988547 | L1 Loss:0.376005756855011 | R2:0.2883440828786133 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[164] Loss:0.19219828583300114 | L1 Loss:0.3365014214068651 | R2:0.4320463142463865 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[164] Loss:0.25027078837156297 | L1 Loss:0.3811891317367554 | R2:0.28730723534784797 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[165] Loss:0.1910183299332857 | L1 Loss:0.340651199221611 | R2:0.4340094517542942 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[165] Loss:0.2622513100504875 | L1 Loss:0.3867086261510849 | R2:0.2614982078768107 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[166] Loss:0.1951920730061829 | L1 Loss:0.3441587146371603 | R2:0.42526925488652345 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[166] Loss:0.2600990951061249 | L1 Loss:0.3869400233030319 | R2:0.26548303234366083 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[167] Loss:0.1924242451786995 | L1 Loss:0.3435795307159424 | R2:0.4319352778845187 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[167] Loss:0.26288327425718305 | L1 Loss:0.39285696148872373 | R2:0.251537160279417 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[168] Loss:0.19470440689474344 | L1 Loss:0.3474201876670122 | R2:0.423802387218694 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[168] Loss:0.26881165206432345 | L1 Loss:0.3855147659778595 | R2:0.2358103364765995 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[169] Loss:0.19141894485801458 | L1 Loss:0.33954618126153946 | R2:0.434021702146573 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[169] Loss:0.2579708263278008 | L1 Loss:0.3901495486497879 | R2:0.26699261250592626 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[170] Loss:0.19743320997804403 | L1 Loss:0.3408391736447811 | R2:0.41710479705984094 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[170] Loss:0.26192216128110885 | L1 Loss:0.38531115353107454 | R2:0.26331368364677343 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[171] Loss:0.202357511036098 | L1 Loss:0.3493974395096302 | R2:0.4031411784228362 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[171] Loss:0.2666554242372513 | L1 Loss:0.3913391649723053 | R2:0.24236806964723484 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[172] Loss:0.19583430234342813 | L1 Loss:0.3440684247761965 | R2:0.4258926026345952 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[172] Loss:0.2653188452124596 | L1 Loss:0.38615832924842836 | R2:0.2460720071740851 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[173] Loss:0.19058344326913357 | L1 Loss:0.3415998946875334 | R2:0.442973645775456 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[173] Loss:0.2674064561724663 | L1 Loss:0.39465509057044984 | R2:0.2444557467811304 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[174] Loss:0.20534863509237766 | L1 Loss:0.3503662310540676 | R2:0.40123111231536357 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[174] Loss:0.2672187894582748 | L1 Loss:0.3934376984834671 | R2:0.24438424490625824 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[175] Loss:0.2012541489675641 | L1 Loss:0.355836171656847 | R2:0.40914767243568895 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[175] Loss:0.27906509339809416 | L1 Loss:0.3973795473575592 | R2:0.20997609947681733 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[176] Loss:0.1968465857207775 | L1 Loss:0.3496243692934513 | R2:0.42333556843476583 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[176] Loss:0.2716908842325211 | L1 Loss:0.3900242120027542 | R2:0.22933166567272814 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[177] Loss:0.19221410900354385 | L1 Loss:0.34141293354332447 | R2:0.43304990575723645 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[177] Loss:0.2738053575158119 | L1 Loss:0.38711889684200285 | R2:0.2194356707747586 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[178] Loss:0.18478267220780253 | L1 Loss:0.33794232830405235 | R2:0.45830235839295397 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[178] Loss:0.260710808634758 | L1 Loss:0.3796197295188904 | R2:0.26123767179708857 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[179] Loss:0.18840323435142636 | L1 Loss:0.33941696770489216 | R2:0.4410625077548803 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[179] Loss:0.2816275849938393 | L1 Loss:0.3928644508123398 | R2:0.19916300324977335 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[180] Loss:0.18234138702973723 | L1 Loss:0.3299027942121029 | R2:0.46088411592909906 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[180] Loss:0.25969956517219545 | L1 Loss:0.3809100776910782 | R2:0.25738929452575326 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[181] Loss:0.19307926297187805 | L1 Loss:0.3408553060144186 | R2:0.43517968476054664 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[181] Loss:0.2692247495055199 | L1 Loss:0.3881947070360184 | R2:0.23029764952309772 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[182] Loss:0.1878855344839394 | L1 Loss:0.3328615166246891 | R2:0.44959532366812127 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[182] Loss:0.26909789592027666 | L1 Loss:0.38995673060417174 | R2:0.22568008991896632 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[183] Loss:0.18193996977061033 | L1 Loss:0.3319209236651659 | R2:0.46837465099942416 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[183] Loss:0.27073114514350893 | L1 Loss:0.38614724576473236 | R2:0.22386959548050314 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[184] Loss:0.18760001100599766 | L1 Loss:0.3350616078823805 | R2:0.44987961263964193 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[184] Loss:0.25239785239100454 | L1 Loss:0.37807875871658325 | R2:0.28766652515901037 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[185] Loss:0.1879714848473668 | L1 Loss:0.3366875611245632 | R2:0.4465460017183722 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[185] Loss:0.26340948343276976 | L1 Loss:0.3845753788948059 | R2:0.26025116233161405 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[186] Loss:0.18889547605067492 | L1 Loss:0.33112167939543724 | R2:0.4430299330200893 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[186] Loss:0.25518576577305796 | L1 Loss:0.3845787763595581 | R2:0.28134253236904033 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[187] Loss:0.19165508821606636 | L1 Loss:0.33703601732850075 | R2:0.4368558065245104 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[187] Loss:0.255207259953022 | L1 Loss:0.3835252344608307 | R2:0.2822656045054813 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[188] Loss:0.1924569709226489 | L1 Loss:0.3398002162575722 | R2:0.43338656912433116 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[188] Loss:0.26212979704141615 | L1 Loss:0.3857422977685928 | R2:0.2645625426509188 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[189] Loss:0.1873934310860932 | L1 Loss:0.3342926986515522 | R2:0.4488680208656027 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[189] Loss:0.2688934519886971 | L1 Loss:0.3914087384939194 | R2:0.23830194078198344 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[190] Loss:0.1855992879718542 | L1 Loss:0.333120483905077 | R2:0.45691514788626936 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[190] Loss:0.2666528709232807 | L1 Loss:0.38777576982975004 | R2:0.25349950168958263 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[191] Loss:0.19039987679570913 | L1 Loss:0.3377545829862356 | R2:0.4447967790918007 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[191] Loss:0.2645556777715683 | L1 Loss:0.38698323965072634 | R2:0.24475390367572789 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[192] Loss:0.19023059401661158 | L1 Loss:0.33967951871454716 | R2:0.4438359892433408 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[192] Loss:0.26417455971241 | L1 Loss:0.3909145385026932 | R2:0.24934741852440373 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[193] Loss:0.19403495965525508 | L1 Loss:0.33995161671191454 | R2:0.437531720566976 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[193] Loss:0.25227615386247637 | L1 Loss:0.37816978991031647 | R2:0.28237858525998866 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[194] Loss:0.18854950973764062 | L1 Loss:0.33589406311511993 | R2:0.45365002236465635 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[194] Loss:0.2612151920795441 | L1 Loss:0.3867200940847397 | R2:0.2551104637743153 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[195] Loss:0.1834899066016078 | L1 Loss:0.3359151128679514 | R2:0.4678680015247263 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[195] Loss:0.26988246142864225 | L1 Loss:0.38907275795936586 | R2:0.23530068294125833 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[196] Loss:0.18186201341450214 | L1 Loss:0.3317079581320286 | R2:0.46826560702488823 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[196] Loss:0.24824644923210143 | L1 Loss:0.37842183411121366 | R2:0.2938376190117985 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[197] Loss:0.1889220355078578 | L1 Loss:0.3352989051491022 | R2:0.4458514391398236 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[197] Loss:0.2510889880359173 | L1 Loss:0.3731678813695908 | R2:0.28224746471999146 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[198] Loss:0.18596888985484838 | L1 Loss:0.3379782363772392 | R2:0.45365345904994936 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[198] Loss:0.2759306564927101 | L1 Loss:0.3903430849313736 | R2:0.21686481122113604 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[199] Loss:0.18375349882990122 | L1 Loss:0.33536585606634617 | R2:0.4626240565315428 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[199] Loss:0.28328226804733275 | L1 Loss:0.39226417541503905 | R2:0.202050521001346 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[200] Loss:0.18485950864851475 | L1 Loss:0.3376022335141897 | R2:0.45021602468948807 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[200] Loss:0.270646557956934 | L1 Loss:0.3869292438030243 | R2:0.22875181288795926 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[201] Loss:0.18081371672451496 | L1 Loss:0.33203642442822456 | R2:0.47190007602717143 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[201] Loss:0.25041617155075074 | L1 Loss:0.3816381901502609 | R2:0.2938624634274832 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[202] Loss:0.18201868515461683 | L1 Loss:0.3308219686150551 | R2:0.46142791675980527 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[202] Loss:0.2564416743814945 | L1 Loss:0.37419231832027433 | R2:0.2784564616685553 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[203] Loss:0.18764757364988327 | L1 Loss:0.3371046632528305 | R2:0.44971752596891634 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[203] Loss:0.2584173008799553 | L1 Loss:0.3885883271694183 | R2:0.2685486717311153 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[204] Loss:0.197598434984684 | L1 Loss:0.34756205044686794 | R2:0.4141951336708062 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[204] Loss:0.27548485845327375 | L1 Loss:0.3877906411886215 | R2:0.2205004295395276 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[205] Loss:0.185898810159415 | L1 Loss:0.33497609198093414 | R2:0.45349608866458624 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[205] Loss:0.28691084012389184 | L1 Loss:0.39665901362895967 | R2:0.1831034197085803 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[206] Loss:0.1877319272607565 | L1 Loss:0.3385213688015938 | R2:0.45026701749368825 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[206] Loss:0.2777897596359253 | L1 Loss:0.3969766885042191 | R2:0.20824573755054282 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[207] Loss:0.19735331181436777 | L1 Loss:0.3499704357236624 | R2:0.42168653186248134 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[207] Loss:0.2560154855251312 | L1 Loss:0.38832909762859347 | R2:0.2810801083810559 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[208] Loss:0.1929266769438982 | L1 Loss:0.34024609811604023 | R2:0.4326671236121241 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[208] Loss:0.26392957642674447 | L1 Loss:0.38272259533405306 | R2:0.2586204714451218 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[209] Loss:0.19162335991859436 | L1 Loss:0.34255334362387657 | R2:0.4371845080870525 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[209] Loss:0.2619045153260231 | L1 Loss:0.3903879851102829 | R2:0.2552651440479231 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[210] Loss:0.18882092367857695 | L1 Loss:0.34077744744718075 | R2:0.44686468718959166 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[210] Loss:0.2691560283303261 | L1 Loss:0.38620799481868745 | R2:0.23951040798448703 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[211] Loss:0.1895281272009015 | L1 Loss:0.3422249648720026 | R2:0.4413720671108195 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[211] Loss:0.2469767674803734 | L1 Loss:0.3729854717850685 | R2:0.3054342758612446 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[212] Loss:0.19731328915804625 | L1 Loss:0.3473766166716814 | R2:0.42064363779838165 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[212] Loss:0.2576349124312401 | L1 Loss:0.3802591174840927 | R2:0.2653588346374322 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[213] Loss:0.202888292260468 | L1 Loss:0.34890616685152054 | R2:0.4025352635149889 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[213] Loss:0.24327703863382338 | L1 Loss:0.3747239738702774 | R2:0.31117376811818664 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[214] Loss:0.19578535435721278 | L1 Loss:0.3443740177899599 | R2:0.42875162312005216 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[214] Loss:0.2392435446381569 | L1 Loss:0.3743401914834976 | R2:0.3265428905837008 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[215] Loss:0.19683362171053886 | L1 Loss:0.34059025906026363 | R2:0.4236789797129548 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[215] Loss:0.2596444010734558 | L1 Loss:0.38525893092155455 | R2:0.2576148315943804 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[216] Loss:0.18598713958635926 | L1 Loss:0.33143357560038567 | R2:0.4484855277906663 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[216] Loss:0.25981886237859725 | L1 Loss:0.38161712884902954 | R2:0.2664402053446756 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[217] Loss:0.18299694685265422 | L1 Loss:0.33051043562591076 | R2:0.4572353873207208 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[217] Loss:0.24297628700733184 | L1 Loss:0.3739590898156166 | R2:0.31919240570519813 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[218] Loss:0.18489642068743706 | L1 Loss:0.33187838084995747 | R2:0.4538412690901322 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[218] Loss:0.26358273774385454 | L1 Loss:0.38715268969535827 | R2:0.25556625866395627 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[219] Loss:0.18395326379686594 | L1 Loss:0.33573450334370136 | R2:0.4538087025767248 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[219] Loss:0.2734586700797081 | L1 Loss:0.3992117702960968 | R2:0.2305054898641059 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[220] Loss:0.18505164235830307 | L1 Loss:0.3349504005163908 | R2:0.4555441978922097 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[220] Loss:0.26757646054029466 | L1 Loss:0.39157322943210604 | R2:0.2507533689674794 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[221] Loss:0.18705361150205135 | L1 Loss:0.3346659764647484 | R2:0.4524940585962933 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[221] Loss:0.25857140123844147 | L1 Loss:0.390063539147377 | R2:0.2696713491950806 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[222] Loss:0.19177124835550785 | L1 Loss:0.33713458105921745 | R2:0.4365864448981569 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[222] Loss:0.2545476764440536 | L1 Loss:0.3761627197265625 | R2:0.28126787057027774 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[223] Loss:0.18952465523034334 | L1 Loss:0.33825426176190376 | R2:0.4449829376615124 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[223] Loss:0.25604349076747895 | L1 Loss:0.3864264994859695 | R2:0.27445691303311504 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[224] Loss:0.19600570667535067 | L1 Loss:0.34000906348228455 | R2:0.422784758574421 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[224] Loss:0.2639669492840767 | L1 Loss:0.38420769572257996 | R2:0.2569099555737521 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[225] Loss:0.19683658378198743 | L1 Loss:0.34019405394792557 | R2:0.42445043387158754 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[225] Loss:0.26244167909026145 | L1 Loss:0.38512908220291137 | R2:0.2681504247483155 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[226] Loss:0.1861759484745562 | L1 Loss:0.3378241043537855 | R2:0.45030386030330916 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[226] Loss:0.2739163562655449 | L1 Loss:0.40159698128700255 | R2:0.23034987811602142 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[227] Loss:0.17998543102294207 | L1 Loss:0.32885522209107876 | R2:0.4670053999056102 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[227] Loss:0.259480881690979 | L1 Loss:0.38814914524555205 | R2:0.28185601762922224 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[228] Loss:0.18578293547034264 | L1 Loss:0.336242888122797 | R2:0.4538713238792499 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[228] Loss:0.26734024882316587 | L1 Loss:0.3926555037498474 | R2:0.24576459706524584 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[229] Loss:0.19100072793662548 | L1 Loss:0.3448798041790724 | R2:0.4334542138218235 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[229] Loss:0.2626967988908291 | L1 Loss:0.3837938606739044 | R2:0.2654200689103091 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[230] Loss:0.19031000696122646 | L1 Loss:0.33904802054166794 | R2:0.4347377112922237 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[230] Loss:0.2813392117619514 | L1 Loss:0.40193945467472075 | R2:0.21289136138518047 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[231] Loss:0.19256331212818623 | L1 Loss:0.34175571985542774 | R2:0.426427433261796 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[231] Loss:0.2680090218782425 | L1 Loss:0.39210290312767027 | R2:0.24544384030046879 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[232] Loss:0.1888464903458953 | L1 Loss:0.3382659796625376 | R2:0.4397555847208073 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[232] Loss:0.26106733828783035 | L1 Loss:0.38364917039871216 | R2:0.2603036427004823 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[233] Loss:0.18837866000831127 | L1 Loss:0.33462347369641066 | R2:0.44688333042065975 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[233] Loss:0.2680231884121895 | L1 Loss:0.38750975131988524 | R2:0.24834289115507696 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[234] Loss:0.18600696418434381 | L1 Loss:0.33666182309389114 | R2:0.4450558926241971 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[234] Loss:0.2730506554245949 | L1 Loss:0.39032473266124723 | R2:0.22941097920612302 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[235] Loss:0.1928867814131081 | L1 Loss:0.34239406883716583 | R2:0.4340480778931114 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[235] Loss:0.27275885343551637 | L1 Loss:0.39639759957790377 | R2:0.2302303563801607 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[236] Loss:0.18513988656923175 | L1 Loss:0.33187297731637955 | R2:0.45989067649643306 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[236] Loss:0.26761857867240907 | L1 Loss:0.39317103326320646 | R2:0.2451208972364888 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[237] Loss:0.1886262777261436 | L1 Loss:0.33782293647527695 | R2:0.4473141119156395 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[237] Loss:0.27810580730438234 | L1 Loss:0.39690700471401213 | R2:0.21004999596321258 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[238] Loss:0.1871571335941553 | L1 Loss:0.3338552862405777 | R2:0.45136833000872123 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[238] Loss:0.2585477143526077 | L1 Loss:0.3930383503437042 | R2:0.2705469524651723 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[239] Loss:0.1910873595625162 | L1 Loss:0.33928281255066395 | R2:0.4387573190084555 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[239] Loss:0.26914269626140597 | L1 Loss:0.39031916856765747 | R2:0.23958256606716252 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[240] Loss:0.18185834866017103 | L1 Loss:0.3291723467409611 | R2:0.46256703150348877 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[240] Loss:0.27024419605731964 | L1 Loss:0.39124582409858705 | R2:0.2402457128048643 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[241] Loss:0.1908264122903347 | L1 Loss:0.34061868488788605 | R2:0.4332255537890497 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[241] Loss:0.2719033770263195 | L1 Loss:0.39284835159778597 | R2:0.23353009823082238 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[242] Loss:0.19415863929316401 | L1 Loss:0.33803137950599194 | R2:0.42624723136560344 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[242] Loss:0.2540672168135643 | L1 Loss:0.3782105207443237 | R2:0.28214723614261594 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[243] Loss:0.18613417213782668 | L1 Loss:0.3307582549750805 | R2:0.4502220598558604 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[243] Loss:0.26598118245601654 | L1 Loss:0.39282670617103577 | R2:0.2510518522272176 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[244] Loss:0.1879685129970312 | L1 Loss:0.3371040504425764 | R2:0.44110694873473344 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[244] Loss:0.2566005915403366 | L1 Loss:0.3817707568407059 | R2:0.27606394431556774 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[245] Loss:0.185830008238554 | L1 Loss:0.3328242674469948 | R2:0.45106205023631996 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[245] Loss:0.24168707206845283 | L1 Loss:0.3745527297258377 | R2:0.3124467789018353 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[246] Loss:0.18459145352244377 | L1 Loss:0.3338001165539026 | R2:0.45644229594758157 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[246] Loss:0.25794084593653677 | L1 Loss:0.383242192864418 | R2:0.2702854324340492 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[247] Loss:0.17909803614020348 | L1 Loss:0.3288270514458418 | R2:0.46981567683798037 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[247] Loss:0.2560164615511894 | L1 Loss:0.3807870954275131 | R2:0.2779042346459713 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[248] Loss:0.1853075702674687 | L1 Loss:0.33775389194488525 | R2:0.4551122577738689 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[248] Loss:0.2588378548622131 | L1 Loss:0.38231665194034575 | R2:0.2680396489471838 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[249] Loss:0.18175011640414596 | L1 Loss:0.3318714778870344 | R2:0.46393210895241077 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[249] Loss:0.260128702968359 | L1 Loss:0.38417231738567353 | R2:0.2611245971908461 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[250] Loss:0.1786944344639778 | L1 Loss:0.3295742589980364 | R2:0.47852349024041846 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[250] Loss:0.250847889482975 | L1 Loss:0.372068327665329 | R2:0.29052817113288326 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[251] Loss:0.18113292194902897 | L1 Loss:0.3347707949578762 | R2:0.4674303145740078 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[251] Loss:0.24796319752931595 | L1 Loss:0.374217164516449 | R2:0.3001971064482176 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[252] Loss:0.18098255340009928 | L1 Loss:0.3321535736322403 | R2:0.46718099860416995 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[252] Loss:0.2507898464798927 | L1 Loss:0.37705841958522796 | R2:0.29650447396342156 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[253] Loss:0.18460333673283458 | L1 Loss:0.33802696131169796 | R2:0.4573794277459541 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[253] Loss:0.2662428870797157 | L1 Loss:0.38401863276958464 | R2:0.25311757834468657 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[254] Loss:0.19158148486167192 | L1 Loss:0.3424994517117739 | R2:0.4401860313254269 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[254] Loss:0.266496092826128 | L1 Loss:0.3877909332513809 | R2:0.2447226446036775 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[255] Loss:0.18670063745230436 | L1 Loss:0.33659168146550655 | R2:0.45964473670719014 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[255] Loss:0.2662177003920078 | L1 Loss:0.39151941537857055 | R2:0.24008156214401977 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[256] Loss:0.19472467247396708 | L1 Loss:0.34525072388350964 | R2:0.4295909962869801 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[256] Loss:0.26738378703594207 | L1 Loss:0.38834662437438966 | R2:0.2408032475910326 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[257] Loss:0.19844854902476072 | L1 Loss:0.34421117790043354 | R2:0.4220113604422002 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[257] Loss:0.2585787333548069 | L1 Loss:0.3907830655574799 | R2:0.2677862382365567 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[258] Loss:0.19457530602812767 | L1 Loss:0.34098572097718716 | R2:0.4332337935421662 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[258] Loss:0.27194293439388273 | L1 Loss:0.3957871824502945 | R2:0.2344688308823927 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[259] Loss:0.19474621769040823 | L1 Loss:0.34483178332448006 | R2:0.42641005716272185 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[259] Loss:0.2560175471007824 | L1 Loss:0.38367055356502533 | R2:0.2818450194482907 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[260] Loss:0.1899285139515996 | L1 Loss:0.3355070427060127 | R2:0.43962852987420725 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[260] Loss:0.2551665253937244 | L1 Loss:0.3766203194856644 | R2:0.2798347899907753 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[261] Loss:0.18421595357358456 | L1 Loss:0.33763954415917397 | R2:0.4567250088650495 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[261] Loss:0.24592687115073203 | L1 Loss:0.37717668414115907 | R2:0.30929969136743185 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[262] Loss:0.17558041960000992 | L1 Loss:0.32664410024881363 | R2:0.4830649343662317 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[262] Loss:0.2456254169344902 | L1 Loss:0.37285479456186293 | R2:0.31082921269785074 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[263] Loss:0.1755991568788886 | L1 Loss:0.32831428200006485 | R2:0.481053351021088 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[263] Loss:0.2607929401099682 | L1 Loss:0.38281938433647156 | R2:0.26388336264254664 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[264] Loss:0.1830304404720664 | L1 Loss:0.333032738417387 | R2:0.45954882918748413 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[264] Loss:0.25215590447187425 | L1 Loss:0.3754614323377609 | R2:0.2918573247114037 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[265] Loss:0.17546719778329134 | L1 Loss:0.3264284133911133 | R2:0.4802614187107418 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[265] Loss:0.2792919635772705 | L1 Loss:0.3991214781999588 | R2:0.2215167475170829 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[266] Loss:0.18238080898299813 | L1 Loss:0.3339004423469305 | R2:0.4656879992436435 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[266] Loss:0.2577475607395172 | L1 Loss:0.3784454435110092 | R2:0.28085726488265655 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[267] Loss:0.18669067416340113 | L1 Loss:0.33796008490025997 | R2:0.45095092015629723 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[267] Loss:0.2520825006067753 | L1 Loss:0.3780409663915634 | R2:0.29073132137492763 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[268] Loss:0.17753221932798624 | L1 Loss:0.32693827897310257 | R2:0.4815408826332436 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[268] Loss:0.26338040381669997 | L1 Loss:0.38663073182106017 | R2:0.2602413279490424 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[269] Loss:0.17349879164248705 | L1 Loss:0.32426239363849163 | R2:0.4882386139129368 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[269] Loss:0.2644264191389084 | L1 Loss:0.3853680491447449 | R2:0.26783749144975194 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[270] Loss:0.17556990310549736 | L1 Loss:0.3304592203348875 | R2:0.4826215901333262 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[270] Loss:0.26872583031654357 | L1 Loss:0.39302046298980714 | R2:0.2469580030114938 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[271] Loss:0.18165928171947598 | L1 Loss:0.33279672265052795 | R2:0.46664685074475776 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[271] Loss:0.28168340772390366 | L1 Loss:0.40324845612049104 | R2:0.20034450497930437 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[272] Loss:0.17890848778188229 | L1 Loss:0.33107578195631504 | R2:0.4781597836999269 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[272] Loss:0.2739177465438843 | L1 Loss:0.3952815383672714 | R2:0.2271049704215528 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[273] Loss:0.17672698898240924 | L1 Loss:0.3265736773610115 | R2:0.48359701737751276 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[273] Loss:0.26563208252191545 | L1 Loss:0.39247700572013855 | R2:0.25279772280314794 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[274] Loss:0.181284143589437 | L1 Loss:0.3361229971051216 | R2:0.4693858511124817 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[274] Loss:0.2586586236953735 | L1 Loss:0.38720203936100006 | R2:0.27101395310046694 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[275] Loss:0.18626594869419932 | L1 Loss:0.33696845546364784 | R2:0.4578585046816527 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[275] Loss:0.2674543023109436 | L1 Loss:0.39026153087615967 | R2:0.25208477398821794 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[276] Loss:0.18519900273531675 | L1 Loss:0.3374688997864723 | R2:0.4605747271291017 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[276] Loss:0.27854430079460146 | L1 Loss:0.39926375448703766 | R2:0.21772969763676767 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[277] Loss:0.17393125081434846 | L1 Loss:0.3250062111765146 | R2:0.49506444828354673 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[277] Loss:0.2754180870950222 | L1 Loss:0.3970142394304276 | R2:0.22427151606379478 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[278] Loss:0.17933573108166456 | L1 Loss:0.3295481652021408 | R2:0.4784196002123449 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[278] Loss:0.2669263184070587 | L1 Loss:0.3859379649162292 | R2:0.2448205156385077 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[279] Loss:0.17560419533401728 | L1 Loss:0.32593131624162197 | R2:0.4920337667438411 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[279] Loss:0.27670913487672805 | L1 Loss:0.39631299674510956 | R2:0.21067304472137222 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[280] Loss:0.18625250505283475 | L1 Loss:0.3317106533795595 | R2:0.4603091847690815 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[280] Loss:0.25197881981730463 | L1 Loss:0.3744072481989861 | R2:0.2872555966977281 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[281] Loss:0.18208796810358763 | L1 Loss:0.3320278860628605 | R2:0.4730349413390366 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[281] Loss:0.26723184436559677 | L1 Loss:0.38855058550834654 | R2:0.2391541397862051 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[282] Loss:0.18281789869070053 | L1 Loss:0.33120143227279186 | R2:0.467461788429584 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[282] Loss:0.2700319170951843 | L1 Loss:0.39697801470756533 | R2:0.23827875972275767 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[283] Loss:0.18278961721807718 | L1 Loss:0.3340811673551798 | R2:0.46558297935841475 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[283] Loss:0.2632407084107399 | L1 Loss:0.3959305495023727 | R2:0.25781765996883826 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[284] Loss:0.18203068245202303 | L1 Loss:0.3324464540928602 | R2:0.4735709231945233 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[284] Loss:0.26457931771874427 | L1 Loss:0.39050177335739134 | R2:0.25668999414189053 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[285] Loss:0.18067755177617073 | L1 Loss:0.3324705008417368 | R2:0.47313960357142204 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[285] Loss:0.26266493648290634 | L1 Loss:0.38765913248062134 | R2:0.26379973104117105 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[286] Loss:0.17546013742685318 | L1 Loss:0.32604029029607773 | R2:0.483384670376524 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[286] Loss:0.24663533866405488 | L1 Loss:0.37894854247570037 | R2:0.3024317854148337 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[287] Loss:0.17226492147892714 | L1 Loss:0.3278122525662184 | R2:0.4990928030922172 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[287] Loss:0.26005263179540633 | L1 Loss:0.3821703404188156 | R2:0.2661134779504598 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[288] Loss:0.1752549116499722 | L1 Loss:0.3240033183246851 | R2:0.4850126208097078 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[288] Loss:0.2551861420273781 | L1 Loss:0.37903850972652436 | R2:0.2808393995477488 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[289] Loss:0.17790065705776215 | L1 Loss:0.32710581086575985 | R2:0.4763030417664858 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[289] Loss:0.24994496405124664 | L1 Loss:0.37238443791866305 | R2:0.298303169032731 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[290] Loss:0.17999087367206812 | L1 Loss:0.32422318309545517 | R2:0.4734031005246976 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[290] Loss:0.25745639204978943 | L1 Loss:0.378595820069313 | R2:0.27694065053684697 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[291] Loss:0.1787758357822895 | L1 Loss:0.32816595397889614 | R2:0.48205067622737274 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[291] Loss:0.26060141026973727 | L1 Loss:0.38123539686203 | R2:0.2575343266806737 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[292] Loss:0.1772890519350767 | L1 Loss:0.32635172083973885 | R2:0.4780274810139558 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[292] Loss:0.24779964685440065 | L1 Loss:0.3776162087917328 | R2:0.3035051596522506 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[293] Loss:0.17961358930915594 | L1 Loss:0.33277497813105583 | R2:0.4764063972897181 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[293] Loss:0.2581919848918915 | L1 Loss:0.37842577397823335 | R2:0.2679290946015146 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[294] Loss:0.18455485673621297 | L1 Loss:0.3355008140206337 | R2:0.4585148441280881 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[294] Loss:0.2626586526632309 | L1 Loss:0.381230229139328 | R2:0.26282311705190287 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[295] Loss:0.19286404084414244 | L1 Loss:0.3416156005114317 | R2:0.43540560092746255 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[295] Loss:0.2688839495182037 | L1 Loss:0.3891333192586899 | R2:0.23886874543704034 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[296] Loss:0.1847298489883542 | L1 Loss:0.3352683559060097 | R2:0.45800041249163226 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[296] Loss:0.2592695981264114 | L1 Loss:0.37991406619548795 | R2:0.26963388049657055 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[297] Loss:0.18972076009958982 | L1 Loss:0.338888481259346 | R2:0.44079058159921036 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[297] Loss:0.261019329726696 | L1 Loss:0.3830542415380478 | R2:0.25944432194512046 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[298] Loss:0.18657818995416164 | L1 Loss:0.33301493525505066 | R2:0.44986525585815074 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[298] Loss:0.26412608474493027 | L1 Loss:0.3895059794187546 | R2:0.26051495495836735 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[299] Loss:0.19070932362228632 | L1 Loss:0.3352994676679373 | R2:0.4412011115367387 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[299] Loss:0.2629132144153118 | L1 Loss:0.39316973388195037 | R2:0.26495311942277233 | ACC: 71.6667%(215/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc83faa-3d2e-46bd-924c-19c1eff670c8",
        "id": "2QI8lzJUfbbn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.3265428905837008\n",
            "min test_loss_l1_record  0.372068327665329\n",
            "min test_loss_record  0.2392435446381569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e14719-3bbe-4376-8539-ed613c54c2c8",
        "id": "tgj1nfSjfbbn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.31154050936303085\n",
            "min test_loss_l1_record  0.3731678813695908\n",
            "min test_loss_record  0.24409551918506622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ebf348-cd33-49d2-bd86-bc405bb8b474",
        "id": "9wHi3uktfbbn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.19565320502706382\n",
            "min test_loss_l1_record  0.3978608787059784\n",
            "min test_loss_record  0.281775127351284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "25e17731-a930-44de-e892-0f54f16385d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu9bqhCZfbbo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.18352771365944429\n",
            "min test_loss_l1_record  0.4185855120420456\n",
            "min test_loss_record  0.2987877234816551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 mer Regression"
      ],
      "metadata": {
        "id": "41AJLEmYizv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-p6G-J9gi2OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=5, mer_dict=fiveMer_dict)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5634d79b-7695-4888-a429-eac3bd0ebab5",
        "id": "G6MKaumwi6cQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b283ca32-e3e4-4d04-812c-c678222fcfb0",
        "id": "oqRZo4wwi6cR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "0B5xW0zbi6cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532b5db6-0b47-4f25-def2-059398da8282",
        "id": "BRmK3OhQi6cS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "HDJOwQjvi6cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "I_7adS-ai6cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24577514-2bf5-4fb1-9fb4-f0092691dbf5",
        "id": "y5hLuBVhi6cT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3490692311897874 | L1 Loss:0.42538656666874886 | R2:0.03433780556830582 | ACC: 62.6000%(313/500)\n",
            "Testing Epoch[0] Loss:0.35888863205909727 | L1 Loss:0.4306080937385559 | R2:-0.03379330572171986 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[1] Loss:0.3433760916814208 | L1 Loss:0.42734514735639095 | R2:0.0454024275602886 | ACC: 62.8000%(314/500)\n",
            "Testing Epoch[1] Loss:0.3466500252485275 | L1 Loss:0.4276060789823532 | R2:0.001749976720321511 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[2] Loss:0.333058126270771 | L1 Loss:0.4223238565027714 | R2:0.07576210011874765 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[2] Loss:0.3508618578314781 | L1 Loss:0.42794216573238375 | R2:-0.013097745232236802 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[3] Loss:0.33207417093217373 | L1 Loss:0.420732244849205 | R2:0.07139829354475945 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[3] Loss:0.35560910403728485 | L1 Loss:0.42989055514335633 | R2:-0.0339585546424838 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[4] Loss:0.3174768630415201 | L1 Loss:0.41659753397107124 | R2:0.11568212500735746 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[4] Loss:0.3489614471793175 | L1 Loss:0.43049748837947843 | R2:-0.015579025926180934 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[5] Loss:0.30722580291330814 | L1 Loss:0.415435878559947 | R2:0.14825791792336657 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[5] Loss:0.3483292281627655 | L1 Loss:0.43717764019966127 | R2:-0.019482311199488244 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[6] Loss:0.3126460947096348 | L1 Loss:0.4165901690721512 | R2:0.1311862517388569 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[6] Loss:0.35513058602809905 | L1 Loss:0.44333783686161043 | R2:-0.0404438839942172 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[7] Loss:0.30510886665433645 | L1 Loss:0.4205485638231039 | R2:0.14840479913775678 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[7] Loss:0.3597627878189087 | L1 Loss:0.4447526425123215 | R2:-0.05642454104346488 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[8] Loss:0.29765830282121897 | L1 Loss:0.415666276589036 | R2:0.1771650227188565 | ACC: 66.0000%(330/500)\n",
            "Testing Epoch[8] Loss:0.34884697794914243 | L1 Loss:0.4454406380653381 | R2:-0.030198770458904067 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[9] Loss:0.27977451123297215 | L1 Loss:0.4021450486034155 | R2:0.22411431958823264 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[9] Loss:0.3410894200205803 | L1 Loss:0.440825155377388 | R2:-0.004131548861576395 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[10] Loss:0.28011810686439276 | L1 Loss:0.39907574839890003 | R2:0.22181384346079536 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[10] Loss:0.34774314761161806 | L1 Loss:0.44804189205169676 | R2:-0.02579240914672276 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[11] Loss:0.27418364956974983 | L1 Loss:0.4025166779756546 | R2:0.2321715385054292 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[11] Loss:0.34721021056175233 | L1 Loss:0.44730871021747587 | R2:-0.024076812395993418 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[12] Loss:0.27389267878606915 | L1 Loss:0.398832892999053 | R2:0.2323243984384307 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[12] Loss:0.3371949762105942 | L1 Loss:0.44584131240844727 | R2:0.005847735681316068 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[13] Loss:0.2659097141586244 | L1 Loss:0.39252003096044064 | R2:0.25748348593029496 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[13] Loss:0.35477698892354964 | L1 Loss:0.4489231377840042 | R2:-0.04685537740907778 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[14] Loss:0.2643414684571326 | L1 Loss:0.38800938054919243 | R2:0.26268708518147066 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[14] Loss:0.35808040350675585 | L1 Loss:0.4487726241350174 | R2:-0.05893732500163259 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[15] Loss:0.2513026390224695 | L1 Loss:0.3859205786138773 | R2:0.3030761607047714 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[15] Loss:0.3625026851892471 | L1 Loss:0.44830599427223206 | R2:-0.07185356929353581 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[16] Loss:0.2546414486132562 | L1 Loss:0.38586558401584625 | R2:0.2883553474299565 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[16] Loss:0.36583160758018496 | L1 Loss:0.4569843471050262 | R2:-0.08601420800060773 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[17] Loss:0.2521815476939082 | L1 Loss:0.38609316665679216 | R2:0.29224822563409475 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[17] Loss:0.36256127059459686 | L1 Loss:0.44814183115959166 | R2:-0.07674432231291786 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[18] Loss:0.25435678847134113 | L1 Loss:0.3840874210000038 | R2:0.28235810975174525 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[18] Loss:0.3582025349140167 | L1 Loss:0.4524846613407135 | R2:-0.05740845165899561 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[19] Loss:0.2701901951804757 | L1 Loss:0.3963906988501549 | R2:0.2237679740511367 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[19] Loss:0.40004434287548063 | L1 Loss:0.4794821828603745 | R2:-0.17791488754960647 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[20] Loss:0.2642810456454754 | L1 Loss:0.39685774222016335 | R2:0.2549552428344187 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[20] Loss:0.3620669037103653 | L1 Loss:0.4641790449619293 | R2:-0.05950808774698395 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[21] Loss:0.2607000470161438 | L1 Loss:0.391260901466012 | R2:0.26806095299535854 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[21] Loss:0.3554852932691574 | L1 Loss:0.4646425753831863 | R2:-0.05057041965027993 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[22] Loss:0.26381334103643894 | L1 Loss:0.392154086381197 | R2:0.25206647473040833 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[22] Loss:0.3716565310955048 | L1 Loss:0.4689393907785416 | R2:-0.0930985528080547 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[23] Loss:0.26217995304614305 | L1 Loss:0.3990628756582737 | R2:0.2599406384019103 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[23] Loss:0.377337771654129 | L1 Loss:0.47151619493961333 | R2:-0.10890995683505185 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[24] Loss:0.2564058704301715 | L1 Loss:0.3911285921931267 | R2:0.2650011023810209 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[24] Loss:0.36146079301834105 | L1 Loss:0.4569491922855377 | R2:-0.053883493355645795 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[25] Loss:0.2590382667258382 | L1 Loss:0.38992898911237717 | R2:0.2608906709969628 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[25] Loss:0.3597796708345413 | L1 Loss:0.45762793719768524 | R2:-0.05205468838826912 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[26] Loss:0.25924558378756046 | L1 Loss:0.3908867221325636 | R2:0.27182974695672046 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[26] Loss:0.3737503156065941 | L1 Loss:0.4636402040719986 | R2:-0.09988762728947194 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[27] Loss:0.2695108847692609 | L1 Loss:0.39943285286426544 | R2:0.2377638505637047 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[27] Loss:0.39641972780227663 | L1 Loss:0.4786438673734665 | R2:-0.16222494482169572 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[28] Loss:0.2624335214495659 | L1 Loss:0.39759718626737595 | R2:0.25812550696779674 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[28] Loss:0.3650820791721344 | L1 Loss:0.46307351291179655 | R2:-0.07225999160684408 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[29] Loss:0.2601785250008106 | L1 Loss:0.3944215290248394 | R2:0.2610014953967505 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[29] Loss:0.37260733246803285 | L1 Loss:0.4740289330482483 | R2:-0.0880347259466755 | ACC: 59.6667%(179/300)\n",
            "Training Epoch[30] Loss:0.25942367780953646 | L1 Loss:0.39061409793794155 | R2:0.26854665288789376 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[30] Loss:0.3610687658190727 | L1 Loss:0.47336799204349517 | R2:-0.05978054835802667 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[31] Loss:0.25022064335644245 | L1 Loss:0.3799275439232588 | R2:0.297517530209861 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[31] Loss:0.3607478320598602 | L1 Loss:0.4681010663509369 | R2:-0.06442226003109873 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[32] Loss:0.2686999598518014 | L1 Loss:0.3914074245840311 | R2:0.24874456212291585 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[32] Loss:0.3852966159582138 | L1 Loss:0.48995682299137117 | R2:-0.12822514715172062 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[33] Loss:0.2629325743764639 | L1 Loss:0.38630895502865314 | R2:0.25767753281991607 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[33] Loss:0.35648120641708375 | L1 Loss:0.468567419052124 | R2:-0.038541214816717094 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[34] Loss:0.272017409093678 | L1 Loss:0.3953120354562998 | R2:0.23499432324641825 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[34] Loss:0.35512625277042387 | L1 Loss:0.4663143754005432 | R2:-0.031227348683921175 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[35] Loss:0.26730153523385525 | L1 Loss:0.4024774357676506 | R2:0.24705517254519582 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[35] Loss:0.3498755842447281 | L1 Loss:0.45599410831928255 | R2:-0.012476852373113235 | ACC: 59.6667%(179/300)\n",
            "Training Epoch[36] Loss:0.2641774173825979 | L1 Loss:0.39338736422359943 | R2:0.2728238412565343 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[36] Loss:0.3533641293644905 | L1 Loss:0.4662493675947189 | R2:-0.028028121519279258 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[37] Loss:0.2569010155275464 | L1 Loss:0.3848944213241339 | R2:0.2855979349016061 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[37] Loss:0.3398724734783173 | L1 Loss:0.45737436413764954 | R2:0.024859193565960037 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[38] Loss:0.2653363710269332 | L1 Loss:0.3881768509745598 | R2:0.2565654663153598 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[38] Loss:0.35228678286075593 | L1 Loss:0.4676547318696976 | R2:-0.022336185551318444 | ACC: 61.3333%(184/300)\n",
            "Training Epoch[39] Loss:0.2581956572830677 | L1 Loss:0.37951023131608963 | R2:0.2796073943580899 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[39] Loss:0.32687768489122393 | L1 Loss:0.45230941772460936 | R2:0.05217188509984466 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[40] Loss:0.26502825319767 | L1 Loss:0.3926256038248539 | R2:0.2574652852760486 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[40] Loss:0.33914892822504045 | L1 Loss:0.4586200058460236 | R2:0.01039327727695145 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[41] Loss:0.26089836563915014 | L1 Loss:0.3870460167527199 | R2:0.26399992741466116 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[41] Loss:0.3528628498315811 | L1 Loss:0.4619386553764343 | R2:-0.032147563781483035 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[42] Loss:0.2528193797916174 | L1 Loss:0.3753527756780386 | R2:0.28608899747190547 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[42] Loss:0.346070696413517 | L1 Loss:0.45940510630607606 | R2:-0.010291058736859959 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[43] Loss:0.25657449662685394 | L1 Loss:0.37860645167529583 | R2:0.27705747069360165 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[43] Loss:0.3346645668148994 | L1 Loss:0.45206049978733065 | R2:0.02722610958672299 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[44] Loss:0.2551857987418771 | L1 Loss:0.3816327825188637 | R2:0.2818343012711222 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[44] Loss:0.3651831790804863 | L1 Loss:0.4629558503627777 | R2:-0.05910666511334781 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[45] Loss:0.25680088717490435 | L1 Loss:0.3782220184803009 | R2:0.2626209839311316 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[45] Loss:0.36152223199605943 | L1 Loss:0.4589426428079605 | R2:-0.04454644281632762 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[46] Loss:0.2528247223235667 | L1 Loss:0.37458572164177895 | R2:0.29197784049614167 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[46] Loss:0.34685736149549484 | L1 Loss:0.4566130369901657 | R2:-0.006487016782759869 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[47] Loss:0.25917310919612646 | L1 Loss:0.38172013126313686 | R2:0.2600247730769164 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[47] Loss:0.34973926693201063 | L1 Loss:0.45321060717105865 | R2:-0.014926629371438993 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[48] Loss:0.2549245422706008 | L1 Loss:0.3849765546619892 | R2:0.28410874878724085 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[48] Loss:0.33666029423475263 | L1 Loss:0.4424884557723999 | R2:0.01471247400013138 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[49] Loss:0.24819683749228716 | L1 Loss:0.3719235882163048 | R2:0.30046924698949723 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[49] Loss:0.34068426191806794 | L1 Loss:0.4438259720802307 | R2:0.006948127363004997 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[50] Loss:0.2424737149849534 | L1 Loss:0.37006350234150887 | R2:0.3116904647309102 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[50] Loss:0.31900152266025544 | L1 Loss:0.4366424709558487 | R2:0.06883479719063668 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[51] Loss:0.2531269690953195 | L1 Loss:0.37557923700660467 | R2:0.27835486859081027 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[51] Loss:0.32641077488660813 | L1 Loss:0.4395292341709137 | R2:0.04707756627336997 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[52] Loss:0.25590009009465575 | L1 Loss:0.3814911935478449 | R2:0.2772153112017563 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[52] Loss:0.339933468401432 | L1 Loss:0.4378392070531845 | R2:0.006203614536476998 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[53] Loss:0.24259968847036362 | L1 Loss:0.37295228242874146 | R2:0.3071819310433729 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[53] Loss:0.305230176448822 | L1 Loss:0.4238404482603073 | R2:0.10977095739563703 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[54] Loss:0.2305903173983097 | L1 Loss:0.3657471798360348 | R2:0.3371285453826473 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[54] Loss:0.31306069791316987 | L1 Loss:0.42805221378803254 | R2:0.08514202605636886 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[55] Loss:0.24802252743393183 | L1 Loss:0.37588012032210827 | R2:0.28347131202072734 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[55] Loss:0.3113949105143547 | L1 Loss:0.4191786378622055 | R2:0.09152287620205916 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[56] Loss:0.24739450588822365 | L1 Loss:0.36805156990885735 | R2:0.2885273231159301 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[56] Loss:0.3009958669543266 | L1 Loss:0.41545167863368987 | R2:0.11945779851372837 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[57] Loss:0.23262205999344587 | L1 Loss:0.3670534770935774 | R2:0.32528368969224863 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[57] Loss:0.3121810004115105 | L1 Loss:0.42165041863918307 | R2:0.08877026840906599 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[58] Loss:0.23219620622694492 | L1 Loss:0.36132048442959785 | R2:0.3293761992542489 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[58] Loss:0.30857920199632644 | L1 Loss:0.4237768679857254 | R2:0.09231714762702933 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[59] Loss:0.22581460792571306 | L1 Loss:0.3579529467970133 | R2:0.3489301804317758 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[59] Loss:0.3004823014140129 | L1 Loss:0.42269313633441924 | R2:0.12208837515247017 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[60] Loss:0.23046007752418518 | L1 Loss:0.36793855763971806 | R2:0.3338781430199532 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[60] Loss:0.33202105164527895 | L1 Loss:0.44009210765361784 | R2:0.01835049255955572 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[61] Loss:0.23362033069133759 | L1 Loss:0.3666956517845392 | R2:0.32789240608695 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[61] Loss:0.32306239753961563 | L1 Loss:0.4356309652328491 | R2:0.05099373940772996 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[62] Loss:0.24164829403162003 | L1 Loss:0.3721464667469263 | R2:0.29964565783015784 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[62] Loss:0.30764491111040115 | L1 Loss:0.4148156374692917 | R2:0.09230543116666512 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[63] Loss:0.23405296355485916 | L1 Loss:0.36826016567647457 | R2:0.33348270085767373 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[63] Loss:0.32778341323137283 | L1 Loss:0.4410830080509186 | R2:0.0330811806511641 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[64] Loss:0.23902408313006163 | L1 Loss:0.3766969721764326 | R2:0.3120959131894874 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[64] Loss:0.3270313084125519 | L1 Loss:0.43834666907787323 | R2:0.04448353624544994 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[65] Loss:0.2269155466929078 | L1 Loss:0.3708711825311184 | R2:0.3392962213248618 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[65] Loss:0.31946505457162855 | L1 Loss:0.4347016900777817 | R2:0.05999172131554005 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[66] Loss:0.2429147306829691 | L1 Loss:0.37877384573221207 | R2:0.31693491536068696 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[66] Loss:0.3368117049336433 | L1 Loss:0.44843277931213377 | R2:0.016697329763415935 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[67] Loss:0.2380252704024315 | L1 Loss:0.37232318334281445 | R2:0.31541679700477754 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[67] Loss:0.2904904283583164 | L1 Loss:0.41331617832183837 | R2:0.14628007223712694 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[68] Loss:0.2279122518375516 | L1 Loss:0.36394759081304073 | R2:0.3556227242885623 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[68] Loss:0.29462726414203644 | L1 Loss:0.4270021080970764 | R2:0.13727088550958247 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[69] Loss:0.23732812609523535 | L1 Loss:0.3697416465729475 | R2:0.32196953942294493 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[69] Loss:0.3201206669211388 | L1 Loss:0.4387703835964203 | R2:0.06631668244387443 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[70] Loss:0.2335853474214673 | L1 Loss:0.36519632302224636 | R2:0.3327589771943824 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[70] Loss:0.29805581122636793 | L1 Loss:0.41686469316482544 | R2:0.12797398496004198 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[71] Loss:0.2460139710456133 | L1 Loss:0.3816973604261875 | R2:0.28685736017985475 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[71] Loss:0.30834914147853854 | L1 Loss:0.43120023012161257 | R2:0.09286298934131332 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[72] Loss:0.24809091910719872 | L1 Loss:0.37342434749007225 | R2:0.2851514174249279 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[72] Loss:0.30326698869466784 | L1 Loss:0.4252346187829971 | R2:0.11248458333161311 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[73] Loss:0.23674528766423464 | L1 Loss:0.3757224027067423 | R2:0.3244065858646519 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[73] Loss:0.3042836606502533 | L1 Loss:0.42550773322582247 | R2:0.1070885183425558 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[74] Loss:0.23124848864972591 | L1 Loss:0.36870805732905865 | R2:0.3273380306665612 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[74] Loss:0.2873847812414169 | L1 Loss:0.4178621590137482 | R2:0.15847138390767154 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[75] Loss:0.22683038283139467 | L1 Loss:0.3670610412955284 | R2:0.3505839970465947 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[75] Loss:0.3070759564638138 | L1 Loss:0.4205998212099075 | R2:0.09570629464215238 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[76] Loss:0.22339898999780416 | L1 Loss:0.36524696089327335 | R2:0.35661427532620504 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[76] Loss:0.28693333715200425 | L1 Loss:0.40446663498878477 | R2:0.15639083327738937 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[77] Loss:0.22836927557364106 | L1 Loss:0.37109882663935423 | R2:0.3410893611903142 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[77] Loss:0.29801456034183504 | L1 Loss:0.41394765079021456 | R2:0.12563363040957132 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[78] Loss:0.21417678147554398 | L1 Loss:0.35243179462850094 | R2:0.38294419099290544 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[78] Loss:0.2966884046792984 | L1 Loss:0.40927609205245974 | R2:0.1371183919128761 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[79] Loss:0.225329858250916 | L1 Loss:0.35918086022138596 | R2:0.35235099683100074 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[79] Loss:0.2843911960721016 | L1 Loss:0.4080319106578827 | R2:0.1696156686639781 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[80] Loss:0.22209296096116304 | L1 Loss:0.36140267364680767 | R2:0.3664036456511896 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[80] Loss:0.2938066780567169 | L1 Loss:0.41622847616672515 | R2:0.14279505916871335 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[81] Loss:0.23809977527707815 | L1 Loss:0.373245058581233 | R2:0.3134860755915452 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[81] Loss:0.30300654768943786 | L1 Loss:0.41448459923267367 | R2:0.10987481025786239 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[82] Loss:0.22442337684333324 | L1 Loss:0.36352143809199333 | R2:0.3662438273556882 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[82] Loss:0.2986011281609535 | L1 Loss:0.42076833844184874 | R2:0.1192696949911285 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[83] Loss:0.23043738212436438 | L1 Loss:0.3644243087619543 | R2:0.34408032316679404 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[83] Loss:0.2868692621588707 | L1 Loss:0.41989031434059143 | R2:0.15272032786211703 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[84] Loss:0.23403925076127052 | L1 Loss:0.37324900180101395 | R2:0.3353277808527959 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[84] Loss:0.29072125554084777 | L1 Loss:0.4129337191581726 | R2:0.1458693706138239 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[85] Loss:0.22558668628335 | L1 Loss:0.36742468550801277 | R2:0.3528521852566534 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[85] Loss:0.28343660831451417 | L1 Loss:0.40936364233493805 | R2:0.16357175007303668 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[86] Loss:0.23370314575731754 | L1 Loss:0.3744362909346819 | R2:0.32626959767711 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[86] Loss:0.26528388261795044 | L1 Loss:0.3974344521760941 | R2:0.22103438688507157 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[87] Loss:0.2258622981607914 | L1 Loss:0.3684992492198944 | R2:0.3501419235982189 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[87] Loss:0.2588698729872704 | L1 Loss:0.39679096937179564 | R2:0.24125977594833423 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[88] Loss:0.23535889480262995 | L1 Loss:0.37150331772863865 | R2:0.32480295883436866 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[88] Loss:0.2596364051103592 | L1 Loss:0.3962262779474258 | R2:0.23872513816434832 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[89] Loss:0.21299317758530378 | L1 Loss:0.35693349316716194 | R2:0.39364078706599237 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[89] Loss:0.2631849631667137 | L1 Loss:0.3951616853475571 | R2:0.2273822818829327 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[90] Loss:0.2319746338762343 | L1 Loss:0.3695848770439625 | R2:0.3389509629956291 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[90] Loss:0.28729164600372314 | L1 Loss:0.4168761849403381 | R2:0.15020188864270506 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[91] Loss:0.22326712403446436 | L1 Loss:0.3637208677828312 | R2:0.3686714887608518 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[91] Loss:0.26407354325056076 | L1 Loss:0.3986508995294571 | R2:0.22655033653995843 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[92] Loss:0.2281035683117807 | L1 Loss:0.3667639382183552 | R2:0.3524202076945769 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[92] Loss:0.272964745759964 | L1 Loss:0.40788922011852263 | R2:0.190773138313496 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[93] Loss:0.22540687583386898 | L1 Loss:0.3650042209774256 | R2:0.3486823399733492 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[93] Loss:0.26460050940513613 | L1 Loss:0.3945860624313354 | R2:0.22200896544360135 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[94] Loss:0.23047416098415852 | L1 Loss:0.37221362069249153 | R2:0.3498873020555716 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[94] Loss:0.2749450609087944 | L1 Loss:0.4080614745616913 | R2:0.19089948622950073 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[95] Loss:0.2295950949192047 | L1 Loss:0.37653124891221523 | R2:0.3480124145806356 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[95] Loss:0.26969707012176514 | L1 Loss:0.40425012707710267 | R2:0.20863455984764948 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[96] Loss:0.22543201688677073 | L1 Loss:0.37066665291786194 | R2:0.35944308491030497 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[96] Loss:0.2657082870602608 | L1 Loss:0.4010118842124939 | R2:0.2173988382251612 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[97] Loss:0.22197393560782075 | L1 Loss:0.3619875330477953 | R2:0.3639708424908007 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[97] Loss:0.26352031975984574 | L1 Loss:0.40124320685863496 | R2:0.22409317227896244 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[98] Loss:0.21783087588846684 | L1 Loss:0.35907130874693394 | R2:0.3823005367921751 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[98] Loss:0.27198527604341505 | L1 Loss:0.40783252120018004 | R2:0.20088458641552087 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[99] Loss:0.22213244810700417 | L1 Loss:0.3676162399351597 | R2:0.3700155297537664 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[99] Loss:0.2746397316455841 | L1 Loss:0.40107212364673617 | R2:0.19467360219971966 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[100] Loss:0.21506400685757399 | L1 Loss:0.35717698372900486 | R2:0.3838222778309849 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[100] Loss:0.25110542923212054 | L1 Loss:0.3917657554149628 | R2:0.26158288790716844 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[101] Loss:0.207780325319618 | L1 Loss:0.3531719073653221 | R2:0.4008031173534562 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[101] Loss:0.2605229541659355 | L1 Loss:0.3966668635606766 | R2:0.23953558964272834 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[102] Loss:0.21404111478477716 | L1 Loss:0.35452427342534065 | R2:0.38455523680417614 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[102] Loss:0.2596439436078072 | L1 Loss:0.3961812347173691 | R2:0.23528986360914156 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[103] Loss:0.2147432672791183 | L1 Loss:0.3568586567416787 | R2:0.3856979722424819 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[103] Loss:0.26500040143728254 | L1 Loss:0.39599794447422026 | R2:0.22964888371108144 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[104] Loss:0.21611527958884835 | L1 Loss:0.3537451922893524 | R2:0.3701597317178018 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[104] Loss:0.2831575572490692 | L1 Loss:0.4224692463874817 | R2:0.17069552550669537 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[105] Loss:0.21311742439866066 | L1 Loss:0.350872615352273 | R2:0.3929804610866087 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[105] Loss:0.266365684568882 | L1 Loss:0.4100982457399368 | R2:0.2172384785064562 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[106] Loss:0.22895485954359174 | L1 Loss:0.3675490338355303 | R2:0.343236513165154 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[106] Loss:0.27885157614946365 | L1 Loss:0.40581254065036776 | R2:0.18681733963444594 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[107] Loss:0.2121700383722782 | L1 Loss:0.35364281199872494 | R2:0.3899342543733352 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[107] Loss:0.26507175117731097 | L1 Loss:0.40332269072532656 | R2:0.21989159921446869 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[108] Loss:0.2159598534926772 | L1 Loss:0.34894079715013504 | R2:0.37600654372151415 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[108] Loss:0.28024877458810804 | L1 Loss:0.41976740062236784 | R2:0.1798987055557556 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[109] Loss:0.20913562690839171 | L1 Loss:0.34036059863865376 | R2:0.39304137901869524 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[109] Loss:0.2595519557595253 | L1 Loss:0.3976772308349609 | R2:0.237970622440349 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[110] Loss:0.2106913710013032 | L1 Loss:0.35144233144819736 | R2:0.39356213292318043 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[110] Loss:0.25824846774339677 | L1 Loss:0.40076352655887604 | R2:0.24213435551925766 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[111] Loss:0.22678693290799856 | L1 Loss:0.36764170974493027 | R2:0.3366998317669039 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[111] Loss:0.2631229817867279 | L1 Loss:0.40180067121982577 | R2:0.22304956856971989 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[112] Loss:0.21901776641607285 | L1 Loss:0.35857838205993176 | R2:0.3561070006298584 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[112] Loss:0.28594369888305665 | L1 Loss:0.42026768028736117 | R2:0.15840326604876948 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[113] Loss:0.2078369027003646 | L1 Loss:0.35276339016854763 | R2:0.40225183998452296 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[113] Loss:0.27418866455554963 | L1 Loss:0.41114687323570254 | R2:0.19337891356207038 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[114] Loss:0.20790891163051128 | L1 Loss:0.34866282902657986 | R2:0.396011736805629 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[114] Loss:0.2615914478898048 | L1 Loss:0.40493946373462675 | R2:0.2282815899804307 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[115] Loss:0.21782011818140745 | L1 Loss:0.3604062870144844 | R2:0.36344631604966626 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[115] Loss:0.27316763550043105 | L1 Loss:0.41878012120723723 | R2:0.2019157255433976 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[116] Loss:0.2248096950352192 | L1 Loss:0.36058441922068596 | R2:0.3444794167364799 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[116] Loss:0.2759370878338814 | L1 Loss:0.4184341937303543 | R2:0.19183440481232175 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[117] Loss:0.21672112587839365 | L1 Loss:0.352237481623888 | R2:0.37467684359767683 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[117] Loss:0.28236215114593505 | L1 Loss:0.42544108629226685 | R2:0.16603146871324784 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[118] Loss:0.20609803404659033 | L1 Loss:0.34714668057858944 | R2:0.3986592787908828 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[118] Loss:0.2754825592041016 | L1 Loss:0.4080767959356308 | R2:0.19688303596300086 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[119] Loss:0.20495538040995598 | L1 Loss:0.3414682224392891 | R2:0.4035289728738054 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[119] Loss:0.27786183059215547 | L1 Loss:0.41378421485424044 | R2:0.17979930083222007 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[120] Loss:0.2110688858665526 | L1 Loss:0.3523681443184614 | R2:0.3898076653728581 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[120] Loss:0.28342013210058215 | L1 Loss:0.4187535285949707 | R2:0.16737829791131426 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[121] Loss:0.215790462680161 | L1 Loss:0.3493728470057249 | R2:0.37127305416235334 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[121] Loss:0.2902213305234909 | L1 Loss:0.41450378596782683 | R2:0.14758920158336497 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[122] Loss:0.2109332988038659 | L1 Loss:0.34779293462634087 | R2:0.3879374138007199 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[122] Loss:0.2761613965034485 | L1 Loss:0.41308889389038084 | R2:0.19196339979698301 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[123] Loss:0.21665639709681273 | L1 Loss:0.3486284427344799 | R2:0.36942059990101145 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[123] Loss:0.2938691794872284 | L1 Loss:0.42260500490665437 | R2:0.13700763622534112 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[124] Loss:0.217395999468863 | L1 Loss:0.35666345432400703 | R2:0.3741184178231227 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[124] Loss:0.2914030358195305 | L1 Loss:0.4160912990570068 | R2:0.13827315207488386 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[125] Loss:0.21583171421661973 | L1 Loss:0.35234731808304787 | R2:0.3693288167794211 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[125] Loss:0.26578470766544343 | L1 Loss:0.3996704339981079 | R2:0.22288505507576226 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[126] Loss:0.21620568726211786 | L1 Loss:0.35700073558837175 | R2:0.370199340913803 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[126] Loss:0.290828512609005 | L1 Loss:0.421083927154541 | R2:0.14005482910546088 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[127] Loss:0.2130439691245556 | L1 Loss:0.35264901630580425 | R2:0.3829929780378887 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[127] Loss:0.27544633150100706 | L1 Loss:0.4049618661403656 | R2:0.1896498226665662 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[128] Loss:0.19729290064424276 | L1 Loss:0.33700073044747114 | R2:0.4273815086284609 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[128] Loss:0.26787438839673994 | L1 Loss:0.40626108050346377 | R2:0.2133908483384234 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[129] Loss:0.21039853617548943 | L1 Loss:0.35160051845014095 | R2:0.3907040612955056 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[129] Loss:0.2785795032978058 | L1 Loss:0.41194660067558286 | R2:0.1750979946221711 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[130] Loss:0.20692108199000359 | L1 Loss:0.34591795690357685 | R2:0.40667051116801467 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[130] Loss:0.28020385205745696 | L1 Loss:0.42023228108882904 | R2:0.1779828128007992 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[131] Loss:0.21553818928077817 | L1 Loss:0.35625570453703403 | R2:0.36899907598507753 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[131] Loss:0.28266899287700653 | L1 Loss:0.42283443808555604 | R2:0.1644249847183827 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[132] Loss:0.20748662250116467 | L1 Loss:0.3421474741771817 | R2:0.40415987525771124 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[132] Loss:0.2759793922305107 | L1 Loss:0.41130274534225464 | R2:0.1866314691690431 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[133] Loss:0.2075282670557499 | L1 Loss:0.350458737462759 | R2:0.3947026255230059 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[133] Loss:0.2821186244487762 | L1 Loss:0.4153221666812897 | R2:0.1638288927906776 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[134] Loss:0.20633585192263126 | L1 Loss:0.3472239561378956 | R2:0.3966660066821781 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[134] Loss:0.2652436465024948 | L1 Loss:0.39911929070949553 | R2:0.22202567385047986 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[135] Loss:0.21755134593695402 | L1 Loss:0.35436839424073696 | R2:0.35983094197419874 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[135] Loss:0.27927336245775225 | L1 Loss:0.41701793372631074 | R2:0.17630784269202307 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[136] Loss:0.21150438953191042 | L1 Loss:0.34632674790918827 | R2:0.3870476213552245 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[136] Loss:0.28625777214765546 | L1 Loss:0.41865459978580477 | R2:0.15514402904458374 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[137] Loss:0.21332863252609968 | L1 Loss:0.3510840553790331 | R2:0.380311103165272 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[137] Loss:0.2834708794951439 | L1 Loss:0.4114496648311615 | R2:0.1647552610979088 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[138] Loss:0.21041119750589132 | L1 Loss:0.34615742042660713 | R2:0.39029497962667836 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[138] Loss:0.26829258501529696 | L1 Loss:0.39933890998363497 | R2:0.21041373817797665 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[139] Loss:0.2071506311185658 | L1 Loss:0.34973341785371304 | R2:0.4021686432683351 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[139] Loss:0.27731793820858003 | L1 Loss:0.4080637514591217 | R2:0.18480410110627835 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[140] Loss:0.21674047270789742 | L1 Loss:0.3526326101273298 | R2:0.3772281143612998 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[140] Loss:0.28244499117136 | L1 Loss:0.41061513125896454 | R2:0.16952187372394087 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[141] Loss:0.21821040753275156 | L1 Loss:0.3596728425472975 | R2:0.3668267015101411 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[141] Loss:0.27880585193634033 | L1 Loss:0.40749413073062896 | R2:0.18388214202663017 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[142] Loss:0.21071171667426825 | L1 Loss:0.349355511367321 | R2:0.39088614527087423 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[142] Loss:0.2655151501297951 | L1 Loss:0.3975419282913208 | R2:0.2167479888149904 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[143] Loss:0.21174044720828533 | L1 Loss:0.35201033391058445 | R2:0.3925714562492822 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[143] Loss:0.2686448395252228 | L1 Loss:0.4066401273012161 | R2:0.20685467388338052 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[144] Loss:0.21417432185262442 | L1 Loss:0.3576518092304468 | R2:0.37629426942323574 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[144] Loss:0.2938821479678154 | L1 Loss:0.41632776856422427 | R2:0.13006189123918147 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[145] Loss:0.2172226393595338 | L1 Loss:0.3601198084652424 | R2:0.373778914187341 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[145] Loss:0.28075943142175674 | L1 Loss:0.4151217550039291 | R2:0.17496801311671298 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[146] Loss:0.2159133506938815 | L1 Loss:0.3548090923577547 | R2:0.36910927026597595 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[146] Loss:0.27944040596485137 | L1 Loss:0.40507722795009615 | R2:0.17832462744964866 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[147] Loss:0.20527801383286715 | L1 Loss:0.34510054625570774 | R2:0.4013881682292283 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[147] Loss:0.25862821787595747 | L1 Loss:0.3978008896112442 | R2:0.24349611996336645 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[148] Loss:0.2016396690160036 | L1 Loss:0.3468174859881401 | R2:0.41958052531157486 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[148] Loss:0.26982502788305285 | L1 Loss:0.4031017512083054 | R2:0.20697273086331788 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[149] Loss:0.20709372591227293 | L1 Loss:0.3474170621484518 | R2:0.4044870942847484 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[149] Loss:0.263371904194355 | L1 Loss:0.3992274791002274 | R2:0.22793290715788653 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[150] Loss:0.1903703585267067 | L1 Loss:0.3405975457280874 | R2:0.4450596610090874 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[150] Loss:0.26367236077785494 | L1 Loss:0.3964280873537064 | R2:0.22928928328802511 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[151] Loss:0.19891771767288446 | L1 Loss:0.3449087329208851 | R2:0.4204134411965013 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[151] Loss:0.26573627144098283 | L1 Loss:0.3924478471279144 | R2:0.22316567107284305 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[152] Loss:0.19316271878778934 | L1 Loss:0.3398068696260452 | R2:0.43084138348406187 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[152] Loss:0.2738727807998657 | L1 Loss:0.40401779413223265 | R2:0.19506830516515014 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[153] Loss:0.20103131840005517 | L1 Loss:0.34263039752840996 | R2:0.4103295858661335 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[153] Loss:0.27060029804706576 | L1 Loss:0.40155660808086396 | R2:0.20013805958294953 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[154] Loss:0.19997038831934333 | L1 Loss:0.3394341208040714 | R2:0.42553240434196243 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[154] Loss:0.26473230570554734 | L1 Loss:0.39570434093475343 | R2:0.21749830894356775 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[155] Loss:0.20114668179303408 | L1 Loss:0.34167399536818266 | R2:0.41554230247232 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[155] Loss:0.2542248621582985 | L1 Loss:0.39205502569675443 | R2:0.25884262188733714 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[156] Loss:0.2038659555837512 | L1 Loss:0.3498523011803627 | R2:0.4063052451490774 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[156] Loss:0.2759822577238083 | L1 Loss:0.40134293735027315 | R2:0.19309135187180448 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[157] Loss:0.1989536015316844 | L1 Loss:0.3431851165369153 | R2:0.42372146741752276 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[157] Loss:0.26737496852874754 | L1 Loss:0.3989880710840225 | R2:0.21262401816129647 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[158] Loss:0.19851404428482056 | L1 Loss:0.3372650248929858 | R2:0.4177185914204452 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[158] Loss:0.28009437918663027 | L1 Loss:0.4062506198883057 | R2:0.1748501776795763 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[159] Loss:0.20356607530266047 | L1 Loss:0.3458485957235098 | R2:0.4086380769229883 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[159] Loss:0.2667178362607956 | L1 Loss:0.4028235971927643 | R2:0.2139239835382797 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[160] Loss:0.19485612493008375 | L1 Loss:0.34086485020816326 | R2:0.43232450385166676 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[160] Loss:0.26936028003692625 | L1 Loss:0.3991799145936966 | R2:0.20685343003454815 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[161] Loss:0.1972650485113263 | L1 Loss:0.3390193562954664 | R2:0.4244566912898653 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[161] Loss:0.2708945319056511 | L1 Loss:0.4030257225036621 | R2:0.20100640652491664 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[162] Loss:0.1971241752617061 | L1 Loss:0.3384901313111186 | R2:0.42580766067472686 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[162] Loss:0.278326253592968 | L1 Loss:0.4101945996284485 | R2:0.1836963654179214 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[163] Loss:0.1948784706182778 | L1 Loss:0.3394958255812526 | R2:0.4303135046976615 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[163] Loss:0.2643068477511406 | L1 Loss:0.4043488085269928 | R2:0.2211399610245131 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[164] Loss:0.20230558048933744 | L1 Loss:0.3422263637185097 | R2:0.41215185441234453 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[164] Loss:0.2630424931645393 | L1 Loss:0.39742390513420106 | R2:0.22607446895455724 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[165] Loss:0.20066853053867817 | L1 Loss:0.347867114469409 | R2:0.4185265861398302 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[165] Loss:0.2632096320390701 | L1 Loss:0.3936740130186081 | R2:0.21954406546755018 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[166] Loss:0.20511289546266198 | L1 Loss:0.34928653482347727 | R2:0.4084012498041719 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[166] Loss:0.2688578963279724 | L1 Loss:0.4019579619169235 | R2:0.20397070458530564 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[167] Loss:0.2017354555428028 | L1 Loss:0.3434148598462343 | R2:0.41413727811262785 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[167] Loss:0.27957265824079514 | L1 Loss:0.40962781608104704 | R2:0.17898925372210525 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[168] Loss:0.20087479846552014 | L1 Loss:0.34450692776590586 | R2:0.4183777894903944 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[168] Loss:0.2744884222745895 | L1 Loss:0.40366370379924776 | R2:0.19138690320176682 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[169] Loss:0.19137493474408984 | L1 Loss:0.3424989515915513 | R2:0.44891003105365085 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[169] Loss:0.26230519711971284 | L1 Loss:0.39584157764911654 | R2:0.23093209804795983 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[170] Loss:0.21001888113096356 | L1 Loss:0.35001750010997057 | R2:0.39021776879078823 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[170] Loss:0.28885636776685714 | L1 Loss:0.41262196004390717 | R2:0.14281903679678445 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[171] Loss:0.19101396948099136 | L1 Loss:0.3352725300937891 | R2:0.4478149712651959 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[171] Loss:0.2602424189448357 | L1 Loss:0.3950215965509415 | R2:0.23451819264888343 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[172] Loss:0.1914798291400075 | L1 Loss:0.3355363579466939 | R2:0.44505539403954175 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[172] Loss:0.272748327255249 | L1 Loss:0.40395718812942505 | R2:0.1987405481127309 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[173] Loss:0.20865804655477405 | L1 Loss:0.34392763022333384 | R2:0.4032519723073949 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[173] Loss:0.2755295023322105 | L1 Loss:0.40728499591350553 | R2:0.18517481049585544 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[174] Loss:0.2013059421442449 | L1 Loss:0.3398760575801134 | R2:0.41484589486291357 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[174] Loss:0.2769032046198845 | L1 Loss:0.410320508480072 | R2:0.1809533844097183 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[175] Loss:0.2041448513045907 | L1 Loss:0.3443525582551956 | R2:0.4069596414316913 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[175] Loss:0.27505175620317457 | L1 Loss:0.40386366844177246 | R2:0.19007319401183737 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[176] Loss:0.19924553204327822 | L1 Loss:0.33968133851885796 | R2:0.42280518353671803 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[176] Loss:0.25647523403167727 | L1 Loss:0.39250054955482483 | R2:0.24641630485524324 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[177] Loss:0.19285756954923272 | L1 Loss:0.33633277006447315 | R2:0.43922566706793287 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[177] Loss:0.2795697093009949 | L1 Loss:0.40484635531902313 | R2:0.17598163929828486 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[178] Loss:0.20073837134987116 | L1 Loss:0.3424009745940566 | R2:0.41351121803833324 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[178] Loss:0.2559204176068306 | L1 Loss:0.39591816663742063 | R2:0.2530773663756579 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[179] Loss:0.19102770648896694 | L1 Loss:0.33480471186339855 | R2:0.43571043912348384 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[179] Loss:0.25905350893735885 | L1 Loss:0.3935893028974533 | R2:0.2397147182259134 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[180] Loss:0.19128372892737389 | L1 Loss:0.33388436771929264 | R2:0.4439125592349426 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[180] Loss:0.2518904209136963 | L1 Loss:0.39106025397777555 | R2:0.2645528050243875 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[181] Loss:0.20468496344983578 | L1 Loss:0.34722739458084106 | R2:0.40726540902592234 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[181] Loss:0.2605617716908455 | L1 Loss:0.3944820284843445 | R2:0.23839016702239918 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[182] Loss:0.1999648567289114 | L1 Loss:0.3427215963602066 | R2:0.41811755461787126 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[182] Loss:0.2744899183511734 | L1 Loss:0.40348765850067136 | R2:0.19094849973787625 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[183] Loss:0.2013024827465415 | L1 Loss:0.349022283218801 | R2:0.4132649214383648 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[183] Loss:0.2697449207305908 | L1 Loss:0.40775598883628844 | R2:0.2111520582508654 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[184] Loss:0.18563978653401136 | L1 Loss:0.32937093172222376 | R2:0.4630329556121845 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[184] Loss:0.25470053106546403 | L1 Loss:0.3960526525974274 | R2:0.25146283630130556 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[185] Loss:0.19645141763612628 | L1 Loss:0.3387784557417035 | R2:0.42638885916718317 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[185] Loss:0.2660370647907257 | L1 Loss:0.4045612424612045 | R2:0.22199373783291731 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[186] Loss:0.19347694562748075 | L1 Loss:0.33911034651100636 | R2:0.43612589297755644 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[186] Loss:0.25309529453516005 | L1 Loss:0.3927650272846222 | R2:0.2572552421543196 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[187] Loss:0.18671912094578147 | L1 Loss:0.3295215703547001 | R2:0.45559263090720503 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[187] Loss:0.2539326295256615 | L1 Loss:0.39353896081447604 | R2:0.2505654919430712 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[188] Loss:0.1926883216947317 | L1 Loss:0.34076141379773617 | R2:0.4401180607290632 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[188] Loss:0.26418785005807877 | L1 Loss:0.3985231190919876 | R2:0.22549934763846222 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[189] Loss:0.19298474956303835 | L1 Loss:0.33210247941315174 | R2:0.4405378227400495 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[189] Loss:0.26235780864953995 | L1 Loss:0.39789125621318816 | R2:0.22603098100908445 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[190] Loss:0.2030230681411922 | L1 Loss:0.3413148056715727 | R2:0.40533445001719304 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[190] Loss:0.2594641536474228 | L1 Loss:0.3927011162042618 | R2:0.24234446060885212 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[191] Loss:0.18558032577857375 | L1 Loss:0.33037128672003746 | R2:0.46556524331185256 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[191] Loss:0.2408668428659439 | L1 Loss:0.3883009165525436 | R2:0.29113141404294013 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[192] Loss:0.19923861045390368 | L1 Loss:0.34230531472712755 | R2:0.42749727366557677 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[192] Loss:0.26468019336462023 | L1 Loss:0.3995349854230881 | R2:0.22446631068110542 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[193] Loss:0.19873918453231454 | L1 Loss:0.34779106453061104 | R2:0.4217870028467311 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[193] Loss:0.2460603803396225 | L1 Loss:0.38940615952014923 | R2:0.2735942019394165 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[194] Loss:0.19195039849728346 | L1 Loss:0.33561136573553085 | R2:0.44249229148278574 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[194] Loss:0.24723923504352568 | L1 Loss:0.3889275431632996 | R2:0.27519898305953566 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[195] Loss:0.20093846740201116 | L1 Loss:0.3434352586045861 | R2:0.4172430290151612 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[195] Loss:0.24515034556388854 | L1 Loss:0.3894814491271973 | R2:0.2794191053893657 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[196] Loss:0.19998615141957998 | L1 Loss:0.34314053133130074 | R2:0.4149825606007215 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[196] Loss:0.25071804970502853 | L1 Loss:0.39026241600513456 | R2:0.26858203161535493 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[197] Loss:0.1972551867365837 | L1 Loss:0.3402510527521372 | R2:0.4249803059064608 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[197] Loss:0.24269712567329407 | L1 Loss:0.3860948860645294 | R2:0.28500707476416387 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[198] Loss:0.1992577463388443 | L1 Loss:0.3433353863656521 | R2:0.42119140086618523 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[198] Loss:0.25551982074975965 | L1 Loss:0.38914177417755125 | R2:0.24622705992271535 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[199] Loss:0.1931354384869337 | L1 Loss:0.3369831517338753 | R2:0.4380282007821331 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[199] Loss:0.2534592807292938 | L1 Loss:0.39313653111457825 | R2:0.25453457283379227 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[200] Loss:0.19852416450157762 | L1 Loss:0.3432691916823387 | R2:0.4328632674071087 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[200] Loss:0.2600811943411827 | L1 Loss:0.39749082922935486 | R2:0.2325619624478504 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[201] Loss:0.18799374578520656 | L1 Loss:0.33118456043303013 | R2:0.4591274005508874 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[201] Loss:0.26138378530740736 | L1 Loss:0.39624164402484896 | R2:0.23204423189409798 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[202] Loss:0.19717549160122871 | L1 Loss:0.3401843532919884 | R2:0.4282972231167403 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[202] Loss:0.24006484895944596 | L1 Loss:0.38481259942054746 | R2:0.29960951269983893 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[203] Loss:0.19405914517119527 | L1 Loss:0.3405987722799182 | R2:0.43732690230153926 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[203] Loss:0.24201486855745316 | L1 Loss:0.38066641092300413 | R2:0.2940459845515052 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[204] Loss:0.19594458304345608 | L1 Loss:0.3373128455132246 | R2:0.4355617538876094 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[204] Loss:0.2535130336880684 | L1 Loss:0.38943805992603303 | R2:0.2561550074153133 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[205] Loss:0.194191362708807 | L1 Loss:0.34124992694705725 | R2:0.43736979275943216 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[205] Loss:0.25014538168907163 | L1 Loss:0.38825672268867495 | R2:0.26792342647836354 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[206] Loss:0.19102102937176824 | L1 Loss:0.33843433670699596 | R2:0.4472552899949233 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[206] Loss:0.25571204274892806 | L1 Loss:0.3910535961389542 | R2:0.25216576327052936 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[207] Loss:0.20379808824509382 | L1 Loss:0.3468615785241127 | R2:0.41188014066801154 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[207] Loss:0.249718602001667 | L1 Loss:0.3882331758737564 | R2:0.2686155324785709 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[208] Loss:0.20310419891029596 | L1 Loss:0.3462465200573206 | R2:0.4123540404879026 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[208] Loss:0.2573426619172096 | L1 Loss:0.39971242249011996 | R2:0.25190791157540626 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[209] Loss:0.19110797345638275 | L1 Loss:0.3363761603832245 | R2:0.44313032453773515 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[209] Loss:0.25941005498170855 | L1 Loss:0.3983410209417343 | R2:0.23721285980139006 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[210] Loss:0.18916873820126057 | L1 Loss:0.33417106233537197 | R2:0.4561065020752894 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[210] Loss:0.2527792274951935 | L1 Loss:0.3971775650978088 | R2:0.2616560609991716 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[211] Loss:0.18300745077431202 | L1 Loss:0.32524398528039455 | R2:0.4721469593136768 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[211] Loss:0.25059419572353364 | L1 Loss:0.38570981919765474 | R2:0.26336943300651994 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[212] Loss:0.18865757388994098 | L1 Loss:0.33267468493431807 | R2:0.45454357696586106 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[212] Loss:0.24753052443265916 | L1 Loss:0.38551467657089233 | R2:0.27318062589225167 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[213] Loss:0.1862701904028654 | L1 Loss:0.3308824496343732 | R2:0.46192138022337814 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[213] Loss:0.24459408670663835 | L1 Loss:0.38737329840660095 | R2:0.2828981734586196 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[214] Loss:0.18832117458805442 | L1 Loss:0.33816133439540863 | R2:0.4589997467751188 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[214] Loss:0.2632888361811638 | L1 Loss:0.39820448160171507 | R2:0.22443765485745154 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[215] Loss:0.19326179288327694 | L1 Loss:0.337828260846436 | R2:0.4471923802699391 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[215] Loss:0.24213114380836487 | L1 Loss:0.390947163105011 | R2:0.29115073946704234 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[216] Loss:0.18635938130319118 | L1 Loss:0.32969221845269203 | R2:0.46732269658595693 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[216] Loss:0.2447773739695549 | L1 Loss:0.39627307653427124 | R2:0.2785482510733933 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[217] Loss:0.19238611217588186 | L1 Loss:0.33184180594980717 | R2:0.44557477046610783 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[217] Loss:0.2546487614512444 | L1 Loss:0.3988377809524536 | R2:0.2578325535767637 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[218] Loss:0.183573457878083 | L1 Loss:0.32733067125082016 | R2:0.47277977458460585 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[218] Loss:0.2536067083477974 | L1 Loss:0.401287716627121 | R2:0.25698129614394966 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[219] Loss:0.18439611420035362 | L1 Loss:0.3293891353532672 | R2:0.4664546998228094 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[219] Loss:0.2576205492019653 | L1 Loss:0.40164474248886106 | R2:0.2449652478216052 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[220] Loss:0.18656355701386929 | L1 Loss:0.33079681172966957 | R2:0.4637846908333723 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[220] Loss:0.24661128371953964 | L1 Loss:0.38920490741729735 | R2:0.2779533036722401 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[221] Loss:0.18930777488276362 | L1 Loss:0.33542306907474995 | R2:0.450445331391909 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[221] Loss:0.2594316557049751 | L1 Loss:0.3939873307943344 | R2:0.2379023994833382 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[222] Loss:0.1829343307763338 | L1 Loss:0.32878332398831844 | R2:0.4756092855903789 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[222] Loss:0.2484305366873741 | L1 Loss:0.38296526968479155 | R2:0.2694375118762847 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[223] Loss:0.18557959469035268 | L1 Loss:0.3303997218608856 | R2:0.4628924199097581 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[223] Loss:0.24799578338861467 | L1 Loss:0.38730678558349607 | R2:0.267697267320009 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[224] Loss:0.18570125102996826 | L1 Loss:0.33297126553952694 | R2:0.4661053525314799 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[224] Loss:0.24485914707183837 | L1 Loss:0.382625886797905 | R2:0.2809526096481273 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[225] Loss:0.18587323557585478 | L1 Loss:0.3318945299834013 | R2:0.46274611711726654 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[225] Loss:0.24112362563610076 | L1 Loss:0.3781096518039703 | R2:0.2931623390020371 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[226] Loss:0.1914770919829607 | L1 Loss:0.33209882862865925 | R2:0.4509868593760774 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[226] Loss:0.24490882009267806 | L1 Loss:0.38991710245609285 | R2:0.27908180021752926 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[227] Loss:0.19308482017368078 | L1 Loss:0.335824241861701 | R2:0.4492543810445538 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[227] Loss:0.2566813603043556 | L1 Loss:0.3937862813472748 | R2:0.2457624864258093 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[228] Loss:0.1789965438656509 | L1 Loss:0.3253105916082859 | R2:0.4878278651342384 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[228] Loss:0.24180277734994887 | L1 Loss:0.3828766971826553 | R2:0.28716688155523373 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[229] Loss:0.18634294252842665 | L1 Loss:0.3316160421818495 | R2:0.4647097308724092 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[229] Loss:0.25326295793056486 | L1 Loss:0.3907024830579758 | R2:0.2543245722271099 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[230] Loss:0.18909511109814048 | L1 Loss:0.33738219924271107 | R2:0.458407415442473 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[230] Loss:0.2506689980626106 | L1 Loss:0.39019562900066374 | R2:0.2654589439453916 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[231] Loss:0.18508624145761132 | L1 Loss:0.32713671028614044 | R2:0.4603814302894351 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[231] Loss:0.2496191903948784 | L1 Loss:0.39266716837883 | R2:0.26593751003064964 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[232] Loss:0.186288399156183 | L1 Loss:0.32835059612989426 | R2:0.4615408500128689 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[232] Loss:0.24129902124404906 | L1 Loss:0.37870652675628663 | R2:0.2946186098529372 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[233] Loss:0.18106643809005618 | L1 Loss:0.32448544166982174 | R2:0.4760223856917677 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[233] Loss:0.2542482316493988 | L1 Loss:0.392242231965065 | R2:0.2545494329761073 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[234] Loss:0.18387377029284835 | L1 Loss:0.3317335266619921 | R2:0.46983741174902705 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[234] Loss:0.24157386124134064 | L1 Loss:0.38623613119125366 | R2:0.29201035633870964 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[235] Loss:0.18760937824845314 | L1 Loss:0.33516168873757124 | R2:0.455353350909382 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[235] Loss:0.2405369684100151 | L1 Loss:0.3834202319383621 | R2:0.29654695157175653 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[236] Loss:0.17795787006616592 | L1 Loss:0.32822143472731113 | R2:0.4845851936046479 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[236] Loss:0.24124365150928498 | L1 Loss:0.38510315120220184 | R2:0.29295630462727196 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[237] Loss:0.1871863780543208 | L1 Loss:0.33229777589440346 | R2:0.4583200673108655 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[237] Loss:0.2521439611911774 | L1 Loss:0.3921837627887726 | R2:0.2595060300197051 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[238] Loss:0.18874304788187146 | L1 Loss:0.3314853757619858 | R2:0.4514392156188114 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[238] Loss:0.2437733843922615 | L1 Loss:0.3876814037561417 | R2:0.2846186145120785 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[239] Loss:0.19419631408527493 | L1 Loss:0.3345474898815155 | R2:0.4403313277497199 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[239] Loss:0.24535758942365646 | L1 Loss:0.387635612487793 | R2:0.28192413893899626 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[240] Loss:0.18389573972672224 | L1 Loss:0.33125470392405987 | R2:0.47130858677543386 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[240] Loss:0.2434364750981331 | L1 Loss:0.39458402395248415 | R2:0.29035168353777807 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[241] Loss:0.1851626904681325 | L1 Loss:0.33331483509391546 | R2:0.4628737674902802 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[241] Loss:0.23868987262248992 | L1 Loss:0.3850684195756912 | R2:0.30006115537590966 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[242] Loss:0.18508947640657425 | L1 Loss:0.3365719560533762 | R2:0.46223551361998666 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[242] Loss:0.23863710910081865 | L1 Loss:0.3830041766166687 | R2:0.3008293459462375 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[243] Loss:0.18613776098936796 | L1 Loss:0.33285186160355806 | R2:0.4585209124044095 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[243] Loss:0.2333717867732048 | L1 Loss:0.38201617300510404 | R2:0.31274611511150796 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[244] Loss:0.1855768682435155 | L1 Loss:0.33514050114899874 | R2:0.46440724061118666 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[244] Loss:0.2431769549846649 | L1 Loss:0.3864552199840546 | R2:0.28398976850131985 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[245] Loss:0.18535444838926196 | L1 Loss:0.3357269298285246 | R2:0.4630268159526433 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[245] Loss:0.25850190222263336 | L1 Loss:0.39683846235275266 | R2:0.23920507071114852 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[246] Loss:0.18210671842098236 | L1 Loss:0.33104328252375126 | R2:0.47626321344580735 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[246] Loss:0.24127310812473296 | L1 Loss:0.3838705211877823 | R2:0.294738225365306 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[247] Loss:0.18461220292374492 | L1 Loss:0.33531446382403374 | R2:0.4700834405320852 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[247] Loss:0.23351785242557527 | L1 Loss:0.3791629016399384 | R2:0.31855165903124594 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[248] Loss:0.18892193865031004 | L1 Loss:0.33382159657776356 | R2:0.45739377526069636 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[248] Loss:0.23794626891613008 | L1 Loss:0.37877429723739625 | R2:0.30317108446467855 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[249] Loss:0.18092971248552203 | L1 Loss:0.3268098486587405 | R2:0.4811513498888842 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[249] Loss:0.23387980610132217 | L1 Loss:0.3813123255968094 | R2:0.31078478035997603 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[250] Loss:0.18376166885718703 | L1 Loss:0.33487116638571024 | R2:0.4678391919177596 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[250] Loss:0.2410352870821953 | L1 Loss:0.3796497225761414 | R2:0.2919340923733348 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[251] Loss:0.18066522106528282 | L1 Loss:0.3301262278109789 | R2:0.4834634797050587 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[251] Loss:0.2376859277486801 | L1 Loss:0.38127659559249877 | R2:0.30280315464673413 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[252] Loss:0.17959364829584956 | L1 Loss:0.3275083713233471 | R2:0.4828968909652798 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[252] Loss:0.2453713595867157 | L1 Loss:0.3841965675354004 | R2:0.2784091623561419 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[253] Loss:0.1778320735320449 | L1 Loss:0.32540551386773586 | R2:0.4838396268514609 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[253] Loss:0.23729157894849778 | L1 Loss:0.3817077100276947 | R2:0.3061383933540388 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[254] Loss:0.18276131711900234 | L1 Loss:0.3263802621513605 | R2:0.4719925237495184 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[254] Loss:0.25039616972208023 | L1 Loss:0.3914080709218979 | R2:0.26245189820817594 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[255] Loss:0.1934647080488503 | L1 Loss:0.3372007552534342 | R2:0.4350791484875322 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[255] Loss:0.2560273498296738 | L1 Loss:0.38708722293376924 | R2:0.2504357093804421 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[256] Loss:0.17580474354326725 | L1 Loss:0.32219140604138374 | R2:0.4946891068658925 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[256] Loss:0.25030056983232496 | L1 Loss:0.39089393615722656 | R2:0.2692719576073177 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[257] Loss:0.18802914209663868 | L1 Loss:0.3341007586568594 | R2:0.45775752012442417 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[257] Loss:0.24559683352708817 | L1 Loss:0.3870707839727402 | R2:0.28543354841053165 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[258] Loss:0.19015858741477132 | L1 Loss:0.33029385190457106 | R2:0.4475623511045207 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[258] Loss:0.244131176173687 | L1 Loss:0.38985291719436643 | R2:0.2844156009020124 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[259] Loss:0.18986570741981268 | L1 Loss:0.33745208755135536 | R2:0.44929939221253795 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[259] Loss:0.2428692787885666 | L1 Loss:0.3896369248628616 | R2:0.291328365493131 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[260] Loss:0.18875200767070055 | L1 Loss:0.3330006543546915 | R2:0.45429703425630114 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[260] Loss:0.2434772729873657 | L1 Loss:0.39068839251995086 | R2:0.2907203318578738 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[261] Loss:0.1786418491974473 | L1 Loss:0.32397692650556564 | R2:0.48403037551621664 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[261] Loss:0.23916712403297424 | L1 Loss:0.37821994423866273 | R2:0.30204840909107544 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[262] Loss:0.18293593730777502 | L1 Loss:0.33531375974416733 | R2:0.468715820750065 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[262] Loss:0.24935173839330674 | L1 Loss:0.38979611098766326 | R2:0.27009774603922704 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[263] Loss:0.1816136110574007 | L1 Loss:0.3277319725602865 | R2:0.47675682834816324 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[263] Loss:0.24310041666030885 | L1 Loss:0.39047608375549314 | R2:0.2862824695270176 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[264] Loss:0.18653120286762714 | L1 Loss:0.3353520706295967 | R2:0.4593862075830508 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[264] Loss:0.25230325311422347 | L1 Loss:0.38951307237148286 | R2:0.2667112788637064 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[265] Loss:0.18245897255837917 | L1 Loss:0.3298798333853483 | R2:0.4697004443711586 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[265] Loss:0.25068858563899993 | L1 Loss:0.4013452440500259 | R2:0.27031551630806244 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[266] Loss:0.18220960954204202 | L1 Loss:0.32538383826613426 | R2:0.469402805244226 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[266] Loss:0.25199720859527586 | L1 Loss:0.3971008598804474 | R2:0.2674266884346034 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[267] Loss:0.17690428346395493 | L1 Loss:0.32231838442385197 | R2:0.49172127059407406 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[267] Loss:0.24697088003158568 | L1 Loss:0.39420449137687685 | R2:0.28112105744341986 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[268] Loss:0.17714219074696302 | L1 Loss:0.32771680876612663 | R2:0.48436147622218384 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[268] Loss:0.2531614497303963 | L1 Loss:0.39646835923194884 | R2:0.25931415317689355 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[269] Loss:0.17027135891839862 | L1 Loss:0.3197598271071911 | R2:0.5110886906595701 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[269] Loss:0.24906120002269744 | L1 Loss:0.3967710644006729 | R2:0.26872617329335846 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[270] Loss:0.17345830937847495 | L1 Loss:0.3213636577129364 | R2:0.5040029056303815 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[270] Loss:0.24417391568422317 | L1 Loss:0.39301808178424835 | R2:0.2874819740731258 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[271] Loss:0.178495435975492 | L1 Loss:0.323518855497241 | R2:0.4892020178129482 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[271] Loss:0.2447707936167717 | L1 Loss:0.38375319838523864 | R2:0.28254753497581175 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[272] Loss:0.1741583594121039 | L1 Loss:0.31913095340132713 | R2:0.5083144596856283 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[272] Loss:0.2365923583507538 | L1 Loss:0.3788671255111694 | R2:0.3062932506565728 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[273] Loss:0.18086373107507825 | L1 Loss:0.32574826665222645 | R2:0.4889645453015731 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[273] Loss:0.24998364448547364 | L1 Loss:0.3877000272274017 | R2:0.26537327043499903 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[274] Loss:0.18545277370139956 | L1 Loss:0.3364013396203518 | R2:0.47120439503568634 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[274] Loss:0.24692331701517106 | L1 Loss:0.390207839012146 | R2:0.2768359076423196 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[275] Loss:0.18826726078987122 | L1 Loss:0.33284967020154 | R2:0.4557998354731554 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[275] Loss:0.24166194945573807 | L1 Loss:0.3805667281150818 | R2:0.29643444964980853 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[276] Loss:0.1847629388794303 | L1 Loss:0.33185866940766573 | R2:0.4695927075831087 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[276] Loss:0.2470910519361496 | L1 Loss:0.38868158161640165 | R2:0.2747171413551513 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[277] Loss:0.1782738035544753 | L1 Loss:0.3231262154877186 | R2:0.4878522513724823 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[277] Loss:0.25653055012226106 | L1 Loss:0.3956157982349396 | R2:0.2512837800409818 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[278] Loss:0.17747477069497108 | L1 Loss:0.3259213715791702 | R2:0.4947550039028949 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[278] Loss:0.24740686863660813 | L1 Loss:0.39280364513397215 | R2:0.276377674998021 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[279] Loss:0.17775490740314126 | L1 Loss:0.3230572631582618 | R2:0.4918390948532914 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[279] Loss:0.25497441440820695 | L1 Loss:0.4021364599466324 | R2:0.2562759316796287 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[280] Loss:0.1742673390544951 | L1 Loss:0.3232678808271885 | R2:0.49926130684125003 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[280] Loss:0.2472006767988205 | L1 Loss:0.3876770496368408 | R2:0.2789547757660328 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[281] Loss:0.17468454036861658 | L1 Loss:0.3223115270957351 | R2:0.5014913461908503 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[281] Loss:0.24616504162549974 | L1 Loss:0.38687686026096346 | R2:0.28025501730416746 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[282] Loss:0.1763979084789753 | L1 Loss:0.32304652594029903 | R2:0.49694507301602486 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[282] Loss:0.2399148538708687 | L1 Loss:0.38571099042892454 | R2:0.29925506566906057 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[283] Loss:0.17695031315088272 | L1 Loss:0.32540177926421165 | R2:0.4877125314304154 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[283] Loss:0.24562080204486847 | L1 Loss:0.3844719171524048 | R2:0.2859682688839391 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[284] Loss:0.17709497129544616 | L1 Loss:0.32975335232913494 | R2:0.4909469213315834 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[284] Loss:0.24469908326864243 | L1 Loss:0.386341866850853 | R2:0.2872568320979832 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[285] Loss:0.17452134750783443 | L1 Loss:0.32570304349064827 | R2:0.4973305726994568 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[285] Loss:0.24349166452884674 | L1 Loss:0.3847329795360565 | R2:0.28572679423219427 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[286] Loss:0.1676343409344554 | L1 Loss:0.3166244886815548 | R2:0.5139865631340876 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[286] Loss:0.238871006667614 | L1 Loss:0.37801607251167296 | R2:0.30183639590509903 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[287] Loss:0.1769327037036419 | L1 Loss:0.32272572442889214 | R2:0.49018437979986745 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[287] Loss:0.2338612914085388 | L1 Loss:0.37826783359050753 | R2:0.3168774280100708 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[288] Loss:0.1707578538917005 | L1 Loss:0.32085382379591465 | R2:0.5100383137903091 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[288] Loss:0.24216812551021577 | L1 Loss:0.3796004235744476 | R2:0.2939987399778558 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[289] Loss:0.18016367545351386 | L1 Loss:0.32921052537858486 | R2:0.48158795226124995 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[289] Loss:0.22152405977249146 | L1 Loss:0.3619490385055542 | R2:0.35236566155413096 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[290] Loss:0.17393219098448753 | L1 Loss:0.3260882291942835 | R2:0.5036368483406002 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[290] Loss:0.2318067729473114 | L1 Loss:0.37725047767162323 | R2:0.32239011007290125 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[291] Loss:0.17481293715536594 | L1 Loss:0.3241494707763195 | R2:0.5035085635296853 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[291] Loss:0.2321784570813179 | L1 Loss:0.37575514912605285 | R2:0.3209683967432796 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[292] Loss:0.17953343968838453 | L1 Loss:0.3289708383381367 | R2:0.47866364632476355 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[292] Loss:0.23698378056287767 | L1 Loss:0.3776946783065796 | R2:0.3060377375689777 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[293] Loss:0.18249233346432447 | L1 Loss:0.33386277221143246 | R2:0.4742627490152873 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[293] Loss:0.23450153023004533 | L1 Loss:0.38170366585254667 | R2:0.3125894290577357 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[294] Loss:0.17181424191221595 | L1 Loss:0.3236199300736189 | R2:0.506118457969992 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[294] Loss:0.2315952494740486 | L1 Loss:0.37131722569465636 | R2:0.325011573290711 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[295] Loss:0.18239941028878093 | L1 Loss:0.33099639415740967 | R2:0.47272687306401706 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[295] Loss:0.2444606304168701 | L1 Loss:0.3837396681308746 | R2:0.2805234686040513 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[296] Loss:0.17128977738320827 | L1 Loss:0.31758397351950407 | R2:0.5053716285315013 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[296] Loss:0.2482605442404747 | L1 Loss:0.38833015859127046 | R2:0.27220857494364437 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[297] Loss:0.17252662358805537 | L1 Loss:0.3206779193133116 | R2:0.5049624913582837 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[297] Loss:0.24039801359176635 | L1 Loss:0.3836668461561203 | R2:0.2939474790076589 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[298] Loss:0.17631367780268192 | L1 Loss:0.32688472885638475 | R2:0.49377554201875956 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[298] Loss:0.24849986284971237 | L1 Loss:0.3865141779184341 | R2:0.27116139548975926 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[299] Loss:0.18203999288380146 | L1 Loss:0.3320085918530822 | R2:0.474769373159063 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[299] Loss:0.24437107592821122 | L1 Loss:0.3835406005382538 | R2:0.2871707238033042 | ACC: 72.3333%(217/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6594e664-f502-4747-8ab3-72cac57fdfb8",
        "id": "_2qJSGZxi6cT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.35236566155413096\n",
            "min test_loss_l1_record  0.3619490385055542\n",
            "min test_loss_record  0.22152405977249146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df9241c-ac4a-433b-9160-71ef5b64b6fb",
        "id": "FPRSk-kti6cU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.29113141404294013\n",
            "min test_loss_l1_record  0.3860948860645294\n",
            "min test_loss_record  0.2408668428659439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273e31e3-89a6-4222-a49e-2a69de3be404",
        "id": "bherU9R0i6cU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.24125977594833423\n",
            "min test_loss_l1_record  0.3945860624313354\n",
            "min test_loss_record  0.2588698729872704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "1499a3bf-8d4e-49b3-be2d-404aa81c9f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE2549LVi6cU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.05217188509984466\n",
            "min test_loss_l1_record  0.4276060789823532\n",
            "min test_loss_record  0.32687768489122393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6 mer Regression"
      ],
      "metadata": {
        "id": "iXaxqlLTi2r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=6, mer_dict=sixMer_dict)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHS94bbvjJf1",
        "outputId": "b3050631-3603-48e6-8152-20c712c21708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5N3uctljJf1",
        "outputId": "11242e4c-aef7-4b8d-8f60-4cdef1a177ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "-xjHmIAvjJf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oeoy-81jJf2",
        "outputId": "1c48d876-7600-4feb-fb12-d19b07c186cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "0r7m89t_jJf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "cKbp6TKFjJf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj5vfAF0jJf3",
        "outputId": "3bc79b65-6bbf-42f1-8ab1-c7c67ead6e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3848584108054638 | L1 Loss:0.459384985268116 | R2:-0.002205471947135898 | ACC: 59.6000%(298/500)\n",
            "Testing Epoch[0] Loss:0.3734422340989113 | L1 Loss:0.44270366728305816 | R2:-0.07860680700291577 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[1] Loss:0.3714039158076048 | L1 Loss:0.456578116863966 | R2:0.03114503068303251 | ACC: 60.2000%(301/500)\n",
            "Testing Epoch[1] Loss:0.3653530731797218 | L1 Loss:0.4404906392097473 | R2:-0.061118931956883726 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[2] Loss:0.3651737663894892 | L1 Loss:0.4568273760378361 | R2:0.05409500572654729 | ACC: 59.8000%(299/500)\n",
            "Testing Epoch[2] Loss:0.36211059540510177 | L1 Loss:0.4418748676776886 | R2:-0.05757172930762354 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[3] Loss:0.34695370588451624 | L1 Loss:0.453519893810153 | R2:0.09720247797611017 | ACC: 59.8000%(299/500)\n",
            "Testing Epoch[3] Loss:0.36452842503786087 | L1 Loss:0.4495199918746948 | R2:-0.06212233856877249 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[4] Loss:0.33750071842223406 | L1 Loss:0.44589380361139774 | R2:0.12415403590986426 | ACC: 60.8000%(304/500)\n",
            "Testing Epoch[4] Loss:0.3740881890058517 | L1 Loss:0.45509662926197053 | R2:-0.09783042443396398 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[5] Loss:0.3221779614686966 | L1 Loss:0.4399331156164408 | R2:0.1609760877417322 | ACC: 62.2000%(311/500)\n",
            "Testing Epoch[5] Loss:0.36176078766584396 | L1 Loss:0.45692963898181915 | R2:-0.06198976973788729 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[6] Loss:0.3231773478910327 | L1 Loss:0.43895833380520344 | R2:0.16001157948705014 | ACC: 61.6000%(308/500)\n",
            "Testing Epoch[6] Loss:0.3578046500682831 | L1 Loss:0.45655556917190554 | R2:-0.05886486887063289 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[7] Loss:0.317659106105566 | L1 Loss:0.4344349969178438 | R2:0.17351989895364733 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[7] Loss:0.35695419311523435 | L1 Loss:0.4554861098527908 | R2:-0.06625067191109145 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[8] Loss:0.3113578287884593 | L1 Loss:0.43346412293612957 | R2:0.18790735925307372 | ACC: 63.6000%(318/500)\n",
            "Testing Epoch[8] Loss:0.36046178340911866 | L1 Loss:0.4627479761838913 | R2:-0.07606922643840022 | ACC: 59.6667%(179/300)\n",
            "Training Epoch[9] Loss:0.29729858599603176 | L1 Loss:0.4230132047086954 | R2:0.22514069600186426 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[9] Loss:0.36255103945732114 | L1 Loss:0.4764285832643509 | R2:-0.07081668708576791 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[10] Loss:0.2828063704073429 | L1 Loss:0.4149999711662531 | R2:0.2624483007806 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[10] Loss:0.34688819050788877 | L1 Loss:0.4649802953004837 | R2:-0.03624845463217208 | ACC: 61.3333%(184/300)\n",
            "Training Epoch[11] Loss:0.29272760450839996 | L1 Loss:0.4215484019368887 | R2:0.23110074497746122 | ACC: 66.8000%(334/500)\n",
            "Testing Epoch[11] Loss:0.36927505731582644 | L1 Loss:0.4797642111778259 | R2:-0.09992426360649645 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[12] Loss:0.2863763077184558 | L1 Loss:0.4209106434136629 | R2:0.24580946021785274 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[12] Loss:0.3812852680683136 | L1 Loss:0.4807672441005707 | R2:-0.1278818227202416 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[13] Loss:0.2823552815243602 | L1 Loss:0.4203191418200731 | R2:0.2507531401775318 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[13] Loss:0.3713517487049103 | L1 Loss:0.4769441246986389 | R2:-0.10105899306385979 | ACC: 59.3333%(178/300)\n",
            "Training Epoch[14] Loss:0.28893726225942373 | L1 Loss:0.427327373996377 | R2:0.236639513790362 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[14] Loss:0.3821448415517807 | L1 Loss:0.48449358344078064 | R2:-0.1396750291219241 | ACC: 59.0000%(177/300)\n",
            "Training Epoch[15] Loss:0.2842083154246211 | L1 Loss:0.42018559761345387 | R2:0.25427160458418124 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[15] Loss:0.3872883766889572 | L1 Loss:0.4842787027359009 | R2:-0.14657299814263702 | ACC: 58.6667%(176/300)\n",
            "Training Epoch[16] Loss:0.2957837004214525 | L1 Loss:0.4266214966773987 | R2:0.2227403714324433 | ACC: 65.2000%(326/500)\n",
            "Testing Epoch[16] Loss:0.3941311240196228 | L1 Loss:0.48723317980766295 | R2:-0.17007965017583912 | ACC: 60.0000%(180/300)\n",
            "Training Epoch[17] Loss:0.29038168769329786 | L1 Loss:0.42502008751034737 | R2:0.23254831466925846 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[17] Loss:0.38959125280380247 | L1 Loss:0.48429250717163086 | R2:-0.16311414254850215 | ACC: 58.3333%(175/300)\n",
            "Training Epoch[18] Loss:0.29521681275218725 | L1 Loss:0.4275909084826708 | R2:0.21617726351934796 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[18] Loss:0.37526380121707914 | L1 Loss:0.47697809636592864 | R2:-0.10938240976242594 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[19] Loss:0.2796326596289873 | L1 Loss:0.4207781255245209 | R2:0.2553252550657744 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[19] Loss:0.3558784082531929 | L1 Loss:0.45952177941799166 | R2:-0.03694446567160402 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[20] Loss:0.26475788466632366 | L1 Loss:0.4051620587706566 | R2:0.2958824337877236 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[20] Loss:0.35219923555850985 | L1 Loss:0.4602185100317001 | R2:-0.05035246416948743 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[21] Loss:0.27039617020636797 | L1 Loss:0.4109142106026411 | R2:0.28391286348036093 | ACC: 66.8000%(334/500)\n",
            "Testing Epoch[21] Loss:0.34328231066465376 | L1 Loss:0.4537071496248245 | R2:-0.007775799627604873 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[22] Loss:0.2626063823699951 | L1 Loss:0.40182218700647354 | R2:0.29848397821597755 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[22] Loss:0.34557580649852754 | L1 Loss:0.4497859746217728 | R2:-0.0020269244546518685 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[23] Loss:0.2607339397072792 | L1 Loss:0.40322825871407986 | R2:0.3063998427021911 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[23] Loss:0.35252750664949417 | L1 Loss:0.4484807223081589 | R2:-0.01814404898774883 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[24] Loss:0.2695261752232909 | L1 Loss:0.41138955391943455 | R2:0.28809904264995095 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[24] Loss:0.36842980682849885 | L1 Loss:0.47035087645053864 | R2:-0.07651101336745879 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[25] Loss:0.2682905923575163 | L1 Loss:0.41202643886208534 | R2:0.28450810158717554 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[25] Loss:0.3607159495353699 | L1 Loss:0.4658280789852142 | R2:-0.05966274467500471 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[26] Loss:0.25959955528378487 | L1 Loss:0.40974138490855694 | R2:0.3104647083720373 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[26] Loss:0.3678495854139328 | L1 Loss:0.4754755973815918 | R2:-0.07710639164109825 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[27] Loss:0.26957663614302874 | L1 Loss:0.41112611815333366 | R2:0.2845280209196993 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[27] Loss:0.3613251566886902 | L1 Loss:0.474377253651619 | R2:-0.04883468938855638 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[28] Loss:0.25747913774102926 | L1 Loss:0.4042182583361864 | R2:0.31410353103517147 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[28] Loss:0.35836135149002074 | L1 Loss:0.4703434377908707 | R2:-0.057942926025856475 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[29] Loss:0.2679374273866415 | L1 Loss:0.41653518937528133 | R2:0.28800609276505446 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[29] Loss:0.33913831114768983 | L1 Loss:0.46348912715911866 | R2:0.010071706597178676 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[30] Loss:0.26509383600205183 | L1 Loss:0.40855731442570686 | R2:0.2943060240017843 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[30] Loss:0.33953297436237334 | L1 Loss:0.4507922440767288 | R2:0.009566882478143269 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[31] Loss:0.24571618065238 | L1 Loss:0.39816208370029926 | R2:0.34609038942640197 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[31] Loss:0.3380581334233284 | L1 Loss:0.45899311900138856 | R2:0.0026621606324782167 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[32] Loss:0.2606700360774994 | L1 Loss:0.405627079308033 | R2:0.30331114828636707 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[32] Loss:0.35047885179519656 | L1 Loss:0.4616713374853134 | R2:-0.047119117115300854 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[33] Loss:0.2575409756973386 | L1 Loss:0.4108789190649986 | R2:0.31151106665874184 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[33] Loss:0.34437279403209686 | L1 Loss:0.46002623438835144 | R2:-0.01006316141447452 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[34] Loss:0.2723554018884897 | L1 Loss:0.41788902319967747 | R2:0.2740512243816623 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[34] Loss:0.3357370063662529 | L1 Loss:0.4496530681848526 | R2:0.010608234806942141 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[35] Loss:0.2701364275999367 | L1 Loss:0.41802391968667507 | R2:0.28417086934715785 | ACC: 66.8000%(334/500)\n",
            "Testing Epoch[35] Loss:0.34897845685482026 | L1 Loss:0.45296662449836733 | R2:-0.032204831070005435 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[36] Loss:0.27033624053001404 | L1 Loss:0.4139183424413204 | R2:0.28560857194278855 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[36] Loss:0.34297250062227247 | L1 Loss:0.45320155620574953 | R2:-0.001621853638669213 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[37] Loss:0.2581543130800128 | L1 Loss:0.40745897591114044 | R2:0.32045119664082583 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[37] Loss:0.36368811726570127 | L1 Loss:0.4671176731586456 | R2:-0.0657343967434472 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[38] Loss:0.25910259410738945 | L1 Loss:0.4073155242949724 | R2:0.3160189597807637 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[38] Loss:0.3522462248802185 | L1 Loss:0.45701606571674347 | R2:-0.026035061937082337 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[39] Loss:0.2565520107746124 | L1 Loss:0.4018371868878603 | R2:0.3285413077117178 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[39] Loss:0.3550611466169357 | L1 Loss:0.45596486926078794 | R2:-0.03301077792344291 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[40] Loss:0.24998365063220263 | L1 Loss:0.40248721465468407 | R2:0.3439185807407097 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[40] Loss:0.3673159942030907 | L1 Loss:0.47171546816825866 | R2:-0.0795409494875444 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[41] Loss:0.24664947343990207 | L1 Loss:0.3954955730587244 | R2:0.34847716158214675 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[41] Loss:0.3414462611079216 | L1 Loss:0.4530428141355515 | R2:-0.0069354227863323795 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[42] Loss:0.24798849038779736 | L1 Loss:0.3961620256304741 | R2:0.3458703646736607 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[42] Loss:0.3327657341957092 | L1 Loss:0.4442364573478699 | R2:0.007830384987509797 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[43] Loss:0.24611036386340857 | L1 Loss:0.39521548710763454 | R2:0.3537947240275883 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[43] Loss:0.327268061041832 | L1 Loss:0.43836856484413145 | R2:0.04397604805336922 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[44] Loss:0.26077073626220226 | L1 Loss:0.40971083380281925 | R2:0.3151239689476126 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[44] Loss:0.3609234720468521 | L1 Loss:0.4634800612926483 | R2:-0.05749473263147046 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[45] Loss:0.24776484351605177 | L1 Loss:0.4017237201333046 | R2:0.34986982537649564 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[45] Loss:0.3414186596870422 | L1 Loss:0.4450277239084244 | R2:-0.0073875113428804595 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[46] Loss:0.24863710394129157 | L1 Loss:0.4002685435116291 | R2:0.34429707971369433 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[46] Loss:0.3416362538933754 | L1 Loss:0.44367110133171084 | R2:-0.0031185566214618765 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[47] Loss:0.25132260005921125 | L1 Loss:0.40108237601816654 | R2:0.3372705556186091 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[47] Loss:0.3670218467712402 | L1 Loss:0.4776941925287247 | R2:-0.08594509143313395 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[48] Loss:0.2383066238835454 | L1 Loss:0.38860259763896465 | R2:0.3707077546656334 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[48] Loss:0.3574829697608948 | L1 Loss:0.46410884261131286 | R2:-0.06524001701545563 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[49] Loss:0.2539300471544266 | L1 Loss:0.4002288058400154 | R2:0.3320804282686276 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[49] Loss:0.35678315311670306 | L1 Loss:0.4613308161497116 | R2:-0.05303023737352029 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[50] Loss:0.24615443032234907 | L1 Loss:0.3986142259091139 | R2:0.3508559343359741 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[50] Loss:0.3430399090051651 | L1 Loss:0.45060780048370364 | R2:-0.036462456454781925 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[51] Loss:0.2325020506978035 | L1 Loss:0.3849439471960068 | R2:0.3859000471705 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[51] Loss:0.3318409025669098 | L1 Loss:0.4477933615446091 | R2:0.01638940036107036 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[52] Loss:0.24792291037738323 | L1 Loss:0.3934051226824522 | R2:0.3470971361092827 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[52] Loss:0.34289552122354505 | L1 Loss:0.44368318617343905 | R2:-0.004128878256002888 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[53] Loss:0.2644868725910783 | L1 Loss:0.4038320053368807 | R2:0.29727381517765644 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[53] Loss:0.3628307804465294 | L1 Loss:0.46580820977687837 | R2:-0.0736457966781937 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[54] Loss:0.24544637463986874 | L1 Loss:0.39119129441678524 | R2:0.352073627854814 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[54] Loss:0.3486466258764267 | L1 Loss:0.4446621984243393 | R2:-0.03714171663406819 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[55] Loss:0.25152371916919947 | L1 Loss:0.3957827351987362 | R2:0.3337587389823285 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[55] Loss:0.3444841608405113 | L1 Loss:0.45452860593795774 | R2:-0.027591299802816403 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[56] Loss:0.229414370842278 | L1 Loss:0.3790263943374157 | R2:0.3900774071096604 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[56] Loss:0.3271795570850372 | L1 Loss:0.4441810339689255 | R2:0.04391643109409506 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[57] Loss:0.237297800835222 | L1 Loss:0.3875021506100893 | R2:0.3745828607010969 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[57] Loss:0.3301514148712158 | L1 Loss:0.44997954070568086 | R2:0.024446791414655777 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[58] Loss:0.23913921508938074 | L1 Loss:0.39349045045673847 | R2:0.36222967105088066 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[58] Loss:0.3093656271696091 | L1 Loss:0.428646320104599 | R2:0.10027784299947673 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[59] Loss:0.23706215899437666 | L1 Loss:0.38741108775138855 | R2:0.37349454789713094 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[59] Loss:0.3452253833413124 | L1 Loss:0.4518728494644165 | R2:-0.018962319580901943 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[60] Loss:0.25144815258681774 | L1 Loss:0.40210753306746483 | R2:0.32865073332947586 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[60] Loss:0.3394782289862633 | L1 Loss:0.4403995454311371 | R2:0.01828246471582732 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[61] Loss:0.23125563189387321 | L1 Loss:0.3848533686250448 | R2:0.386550967006723 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[61] Loss:0.32644501477479937 | L1 Loss:0.44516508281230927 | R2:0.042434076229652486 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[62] Loss:0.23485731286928058 | L1 Loss:0.38398871570825577 | R2:0.3755237806363425 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[62] Loss:0.30019928216934205 | L1 Loss:0.41731407046318053 | R2:0.13077836672918114 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[63] Loss:0.22646452393382788 | L1 Loss:0.3811621814966202 | R2:0.3967811898795116 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[63] Loss:0.30443632900714873 | L1 Loss:0.4252608329057693 | R2:0.10225613374325968 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[64] Loss:0.2176804095506668 | L1 Loss:0.3731597736477852 | R2:0.420749257967562 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[64] Loss:0.3137212246656418 | L1 Loss:0.43021986484527586 | R2:0.07008382094909335 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[65] Loss:0.21631670370697975 | L1 Loss:0.37075067311525345 | R2:0.4246858505430715 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[65] Loss:0.3078975141048431 | L1 Loss:0.4183133512735367 | R2:0.09527703355749775 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[66] Loss:0.2207280583679676 | L1 Loss:0.3716654144227505 | R2:0.4156505142796114 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[66] Loss:0.33176896125078204 | L1 Loss:0.44430517852306367 | R2:0.025859903673234385 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[67] Loss:0.2147214775905013 | L1 Loss:0.370913989841938 | R2:0.42944799933023053 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[67] Loss:0.3274103969335556 | L1 Loss:0.43693660497665404 | R2:0.05197132101542632 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[68] Loss:0.2208922985009849 | L1 Loss:0.37802866101264954 | R2:0.4144723727490694 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[68] Loss:0.33949793726205824 | L1 Loss:0.44375982582569123 | R2:0.014492510784621682 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[69] Loss:0.20220393501222134 | L1 Loss:0.358984787017107 | R2:0.4633983750810258 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[69] Loss:0.3373066410422325 | L1 Loss:0.4376379042863846 | R2:0.02366467342812344 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[70] Loss:0.19875679537653923 | L1 Loss:0.3581695258617401 | R2:0.47135832197592253 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[70] Loss:0.3192184090614319 | L1 Loss:0.43503614962100984 | R2:0.07079799733784772 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[71] Loss:0.21030250703915954 | L1 Loss:0.36695683747529984 | R2:0.44366474208173284 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[71] Loss:0.3286574512720108 | L1 Loss:0.43174087107181547 | R2:0.046697941365609695 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[72] Loss:0.21986338589340448 | L1 Loss:0.3782763220369816 | R2:0.4140461230543808 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[72] Loss:0.32745175063610077 | L1 Loss:0.43661092817783353 | R2:0.049301029122716056 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[73] Loss:0.21167389024049044 | L1 Loss:0.3665915075689554 | R2:0.4417420350372639 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[73] Loss:0.34242366850376127 | L1 Loss:0.44759578108787534 | R2:-0.007764668999321678 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[74] Loss:0.21663940604776144 | L1 Loss:0.36692097783088684 | R2:0.4243906016669947 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[74] Loss:0.3341680258512497 | L1 Loss:0.4296491056680679 | R2:0.030496153746393785 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[75] Loss:0.21015856321901083 | L1 Loss:0.36253990791738033 | R2:0.4430579601007545 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[75] Loss:0.33284387141466143 | L1 Loss:0.4383046060800552 | R2:0.02892981176356981 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[76] Loss:0.21354827657341957 | L1 Loss:0.3696074467152357 | R2:0.43578993291165563 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[76] Loss:0.32975265085697175 | L1 Loss:0.437798747420311 | R2:0.03300315576268995 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[77] Loss:0.2070779325440526 | L1 Loss:0.3551317900419235 | R2:0.449790524927379 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[77] Loss:0.3287877783179283 | L1 Loss:0.4353912442922592 | R2:0.031226779132696347 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[78] Loss:0.20135000348091125 | L1 Loss:0.3596984576433897 | R2:0.46365204597470583 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[78] Loss:0.31728906035423277 | L1 Loss:0.43346054553985597 | R2:0.07411506043301277 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[79] Loss:0.1992551307193935 | L1 Loss:0.3555769454687834 | R2:0.47267678212822595 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[79] Loss:0.3190370053052902 | L1 Loss:0.4320278614759445 | R2:0.06466478610007034 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[80] Loss:0.1988252718001604 | L1 Loss:0.3485952168703079 | R2:0.475129598550163 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[80] Loss:0.3181831181049347 | L1 Loss:0.43436565697193147 | R2:0.06720773533933651 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[81] Loss:0.19853088073432446 | L1 Loss:0.3542416915297508 | R2:0.47468822444557807 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[81] Loss:0.3154011949896812 | L1 Loss:0.4302248775959015 | R2:0.08021961746062437 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[82] Loss:0.20449627470225096 | L1 Loss:0.3531222492456436 | R2:0.46225580212239764 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[82] Loss:0.3133820056915283 | L1 Loss:0.4228772670030594 | R2:0.08302385405560168 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[83] Loss:0.19468019669875503 | L1 Loss:0.3463260028511286 | R2:0.4845124558312514 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[83] Loss:0.326070199906826 | L1 Loss:0.4380952179431915 | R2:0.04941658745030888 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[84] Loss:0.18818679358810186 | L1 Loss:0.3390913810580969 | R2:0.5015903578397679 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[84] Loss:0.32313667237758636 | L1 Loss:0.42869619131088255 | R2:0.06149405010718403 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[85] Loss:0.19662525737658143 | L1 Loss:0.347032243385911 | R2:0.47709125457975665 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[85] Loss:0.3167837306857109 | L1 Loss:0.42603158950805664 | R2:0.08100134347708625 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[86] Loss:0.18261728854849935 | L1 Loss:0.3349736826494336 | R2:0.5143523487506905 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[86] Loss:0.3014490455389023 | L1 Loss:0.42535639405250547 | R2:0.12889502176400708 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[87] Loss:0.1866660974919796 | L1 Loss:0.3379468712955713 | R2:0.5047529831042945 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[87] Loss:0.30628053545951844 | L1 Loss:0.4251887172460556 | R2:0.11065977307160998 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[88] Loss:0.185432817786932 | L1 Loss:0.3404131978750229 | R2:0.5085947615805538 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[88] Loss:0.2959141179919243 | L1 Loss:0.4103544861078262 | R2:0.15410858714246572 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[89] Loss:0.1923005529679358 | L1 Loss:0.33744993805885315 | R2:0.49287624413104536 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[89] Loss:0.3142822474241257 | L1 Loss:0.42019361555576323 | R2:0.09084960832074256 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[90] Loss:0.18536758795380592 | L1 Loss:0.34189084731042385 | R2:0.506724020808371 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[90] Loss:0.30758524388074876 | L1 Loss:0.4208116173744202 | R2:0.11327730231671382 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[91] Loss:0.19193941447883844 | L1 Loss:0.34375644475221634 | R2:0.48819947746840664 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[91] Loss:0.28923620134592054 | L1 Loss:0.4054848223924637 | R2:0.16715866673992183 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[92] Loss:0.18483570031821728 | L1 Loss:0.3378892205655575 | R2:0.511208582978601 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[92] Loss:0.3071070536971092 | L1 Loss:0.4260378032922745 | R2:0.116033191639396 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[93] Loss:0.1907770950347185 | L1 Loss:0.3425090843811631 | R2:0.4930497623090845 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[93] Loss:0.3032653532922268 | L1 Loss:0.4156274378299713 | R2:0.12498425520093015 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[94] Loss:0.19208898255601525 | L1 Loss:0.3541445303708315 | R2:0.4914056201969095 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[94] Loss:0.30593966990709304 | L1 Loss:0.41537210047245027 | R2:0.11381242456008882 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[95] Loss:0.1941880751401186 | L1 Loss:0.35120150819420815 | R2:0.49091408989453167 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[95] Loss:0.29625392258167266 | L1 Loss:0.4121768236160278 | R2:0.1387017821568405 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[96] Loss:0.18187222257256508 | L1 Loss:0.3372384672984481 | R2:0.5218043931979309 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[96] Loss:0.3052178114652634 | L1 Loss:0.41723879277706144 | R2:0.10953980536790857 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[97] Loss:0.18397178128361702 | L1 Loss:0.341323877684772 | R2:0.5169484737406222 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[97] Loss:0.30972007364034654 | L1 Loss:0.41551759243011477 | R2:0.10882964659948577 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[98] Loss:0.18698666896671057 | L1 Loss:0.3402251563966274 | R2:0.5034431653610669 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[98] Loss:0.3058614283800125 | L1 Loss:0.4090234816074371 | R2:0.11097408565934672 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[99] Loss:0.1774782077409327 | L1 Loss:0.3296166341751814 | R2:0.532360563001675 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[99] Loss:0.2960481882095337 | L1 Loss:0.40358836352825167 | R2:0.13413168563171574 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[100] Loss:0.1789616965688765 | L1 Loss:0.332391775213182 | R2:0.528064278066976 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[100] Loss:0.3100345194339752 | L1 Loss:0.41363265216350553 | R2:0.10049995977297292 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[101] Loss:0.18549680663272738 | L1 Loss:0.3390794722363353 | R2:0.511221766455723 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[101] Loss:0.2940472051501274 | L1 Loss:0.40772147476673126 | R2:0.1444525306455898 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[102] Loss:0.1785261081531644 | L1 Loss:0.3362426236271858 | R2:0.5297528379618012 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[102] Loss:0.2928113713860512 | L1 Loss:0.3995250046253204 | R2:0.15845766572321518 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[103] Loss:0.18860465520992875 | L1 Loss:0.33704506047070026 | R2:0.5052113983266233 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[103] Loss:0.2811589688062668 | L1 Loss:0.3965806245803833 | R2:0.1947515506758239 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[104] Loss:0.19522115215659142 | L1 Loss:0.3445756174623966 | R2:0.4879215557031141 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[104] Loss:0.3075495406985283 | L1 Loss:0.4100770980119705 | R2:0.11507836563198245 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[105] Loss:0.19091281155124307 | L1 Loss:0.34117249865084887 | R2:0.49749613407436033 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[105] Loss:0.3005970284342766 | L1 Loss:0.4067852348089218 | R2:0.12682469553862905 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[106] Loss:0.1949580041691661 | L1 Loss:0.3447300223633647 | R2:0.48597712189250425 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[106] Loss:0.30690452083945274 | L1 Loss:0.4168084800243378 | R2:0.11396617261633271 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[107] Loss:0.1912872069515288 | L1 Loss:0.3435963597148657 | R2:0.49735252952653974 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[107] Loss:0.32519630491733553 | L1 Loss:0.4266886204481125 | R2:0.05205486734268243 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[108] Loss:0.20010475162416697 | L1 Loss:0.350328141823411 | R2:0.47070261528914953 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[108] Loss:0.3177854835987091 | L1 Loss:0.42927336394786836 | R2:0.06723741023177206 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[109] Loss:0.19516706326976418 | L1 Loss:0.3448029290884733 | R2:0.48704017241974507 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[109] Loss:0.32126470655202866 | L1 Loss:0.42313310205936433 | R2:0.05878887551854624 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[110] Loss:0.19195757573470473 | L1 Loss:0.3382376432418823 | R2:0.4951853615444909 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[110] Loss:0.3244733765721321 | L1 Loss:0.42884002029895785 | R2:0.04879911387342047 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[111] Loss:0.1892641056329012 | L1 Loss:0.3388173785060644 | R2:0.4992590135637611 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[111] Loss:0.307009668648243 | L1 Loss:0.418779781460762 | R2:0.1058338713184928 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[112] Loss:0.18792051961645484 | L1 Loss:0.3354095881804824 | R2:0.503643956974074 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[112] Loss:0.31216374337673186 | L1 Loss:0.41939412653446195 | R2:0.08712988222795655 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[113] Loss:0.18677190272137523 | L1 Loss:0.3323114551603794 | R2:0.506946889408571 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[113] Loss:0.3071021899580956 | L1 Loss:0.4161719411611557 | R2:0.09961756336347713 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[114] Loss:0.19187086774036288 | L1 Loss:0.3395617287606001 | R2:0.4899092458636053 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[114] Loss:0.3262713611125946 | L1 Loss:0.42939800918102267 | R2:0.059330390131411684 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[115] Loss:0.19358701771125197 | L1 Loss:0.34110921807587147 | R2:0.4880102570649157 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[115] Loss:0.31972180902957914 | L1 Loss:0.4196478217840195 | R2:0.07707290807099439 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[116] Loss:0.19167948281392455 | L1 Loss:0.3417395297437906 | R2:0.4953902915027504 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[116] Loss:0.313937121629715 | L1 Loss:0.41128386855125426 | R2:0.08074368584694246 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[117] Loss:0.1954471105709672 | L1 Loss:0.34769669733941555 | R2:0.48199454546071213 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[117] Loss:0.3107692331075668 | L1 Loss:0.4160075455904007 | R2:0.09438740626078201 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[118] Loss:0.18872273713350296 | L1 Loss:0.3368958681821823 | R2:0.5006164790801763 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[118] Loss:0.3041921898722649 | L1 Loss:0.4118292242288589 | R2:0.11950798828423481 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[119] Loss:0.18282663263380527 | L1 Loss:0.33288940228521824 | R2:0.5122340686406456 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[119] Loss:0.30966211557388307 | L1 Loss:0.42138810753822326 | R2:0.09190867977482578 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[120] Loss:0.18795152939856052 | L1 Loss:0.33657927718013525 | R2:0.5015742294822662 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[120] Loss:0.3124029219150543 | L1 Loss:0.4159824848175049 | R2:0.0948537022170162 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[121] Loss:0.181130635086447 | L1 Loss:0.3327349880710244 | R2:0.521446197605807 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[121] Loss:0.3161986768245697 | L1 Loss:0.4173677504062653 | R2:0.08927528757198437 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[122] Loss:0.18215015809983015 | L1 Loss:0.3321396913379431 | R2:0.5209950778958278 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[122] Loss:0.31317917853593824 | L1 Loss:0.4179490119218826 | R2:0.0890972551182734 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[123] Loss:0.17939337622374296 | L1 Loss:0.3290986567735672 | R2:0.5304492660051263 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[123] Loss:0.29786608964204786 | L1 Loss:0.41085546612739565 | R2:0.1476438308738281 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[124] Loss:0.17651887889951468 | L1 Loss:0.33390454202890396 | R2:0.529504744662517 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[124] Loss:0.3116695284843445 | L1 Loss:0.4282976180315018 | R2:0.09487699928962694 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[125] Loss:0.1743843792937696 | L1 Loss:0.3333607967942953 | R2:0.5400712178933179 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[125] Loss:0.30985532253980635 | L1 Loss:0.41646668016910554 | R2:0.10030813399548193 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[126] Loss:0.17728736391291022 | L1 Loss:0.3293649898841977 | R2:0.534791366024634 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[126] Loss:0.28665911555290224 | L1 Loss:0.4077122062444687 | R2:0.1669794815232602 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[127] Loss:0.18491495260968804 | L1 Loss:0.3353852164000273 | R2:0.5171627428599898 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[127] Loss:0.2907160043716431 | L1 Loss:0.40578515231609347 | R2:0.14800011104733737 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[128] Loss:0.18153430614620447 | L1 Loss:0.33038771618157625 | R2:0.5254721002986545 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[128] Loss:0.28731904476881026 | L1 Loss:0.4047840565443039 | R2:0.16772496981407212 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[129] Loss:0.18276107730343938 | L1 Loss:0.32830581068992615 | R2:0.5160072703138033 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[129] Loss:0.29731050357222555 | L1 Loss:0.4121930032968521 | R2:0.14285071175831687 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[130] Loss:0.17667857883498073 | L1 Loss:0.326433127745986 | R2:0.5372504267472379 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[130] Loss:0.3014939472079277 | L1 Loss:0.4108096122741699 | R2:0.11961661810854429 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[131] Loss:0.18159850244410336 | L1 Loss:0.33112134598195553 | R2:0.5252964534077985 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[131] Loss:0.30371151566505433 | L1 Loss:0.4141502916812897 | R2:0.1177581190131006 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[132] Loss:0.18761873245239258 | L1 Loss:0.3387068286538124 | R2:0.5051170861815811 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[132] Loss:0.28303156793117523 | L1 Loss:0.4046886831521988 | R2:0.17687102110409925 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[133] Loss:0.1813981393352151 | L1 Loss:0.3347180113196373 | R2:0.5244179491741794 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[133] Loss:0.3012328416109085 | L1 Loss:0.4097298920154572 | R2:0.1288867813698997 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[134] Loss:0.16934923361986876 | L1 Loss:0.32333105336874723 | R2:0.5514923249720086 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[134] Loss:0.30922900438308715 | L1 Loss:0.418924418091774 | R2:0.10470913346242978 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[135] Loss:0.18037321139127016 | L1 Loss:0.33226908929646015 | R2:0.5271454287021827 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[135] Loss:0.31663032025098803 | L1 Loss:0.4171137481927872 | R2:0.07148489538468202 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[136] Loss:0.1713404357433319 | L1 Loss:0.32025536336004734 | R2:0.5485202764611685 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[136] Loss:0.31730200797319413 | L1 Loss:0.4202373087406158 | R2:0.08205004530080692 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[137] Loss:0.17647378472611308 | L1 Loss:0.3230691496282816 | R2:0.5368450175669287 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[137] Loss:0.32138953506946566 | L1 Loss:0.4308746039867401 | R2:0.07784314213489962 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[138] Loss:0.17641287948936224 | L1 Loss:0.32542724162340164 | R2:0.5328161536323679 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[138] Loss:0.3068025469779968 | L1 Loss:0.42172509133815766 | R2:0.1050064039222079 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[139] Loss:0.1700983066111803 | L1 Loss:0.3271963931620121 | R2:0.5535930248211446 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[139] Loss:0.3037453919649124 | L1 Loss:0.4138629585504532 | R2:0.11123021086117262 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[140] Loss:0.1756950644776225 | L1 Loss:0.32813101541250944 | R2:0.5400008341282044 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[140] Loss:0.3073879271745682 | L1 Loss:0.4165945291519165 | R2:0.10831578357577332 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[141] Loss:0.169647429138422 | L1 Loss:0.3214092068374157 | R2:0.5554557055714563 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[141] Loss:0.3289498522877693 | L1 Loss:0.42450629770755766 | R2:0.05013734786229267 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[142] Loss:0.17011852748692036 | L1 Loss:0.318886942230165 | R2:0.5537109211918569 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[142] Loss:0.3164451137185097 | L1 Loss:0.4243520528078079 | R2:0.08230035553962685 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[143] Loss:0.1713463980704546 | L1 Loss:0.31910766288638115 | R2:0.5509397446731075 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[143] Loss:0.2975289553403854 | L1 Loss:0.41789879500865934 | R2:0.132192859993686 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[144] Loss:0.1701416065916419 | L1 Loss:0.3204832999035716 | R2:0.5506197648366797 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[144] Loss:0.30706953182816504 | L1 Loss:0.41915216743946077 | R2:0.11516710789184995 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[145] Loss:0.1751462398096919 | L1 Loss:0.3237830866128206 | R2:0.5384053890076688 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[145] Loss:0.3043137863278389 | L1 Loss:0.41880127787590027 | R2:0.12183062615332366 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[146] Loss:0.1797276851721108 | L1 Loss:0.3342826347798109 | R2:0.5300284345539831 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[146] Loss:0.3150780528783798 | L1 Loss:0.42095869183540346 | R2:0.0932694215970628 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[147] Loss:0.17203210899606347 | L1 Loss:0.3240669807419181 | R2:0.5488947695123385 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[147] Loss:0.3104532308876514 | L1 Loss:0.41397453248500826 | R2:0.10680311178698983 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[148] Loss:0.17922785319387913 | L1 Loss:0.32972775027155876 | R2:0.5293642973286685 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[148] Loss:0.2957995757460594 | L1 Loss:0.4138925760984421 | R2:0.13985117861847543 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[149] Loss:0.17060395888984203 | L1 Loss:0.3237402392551303 | R2:0.5531879135439229 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[149] Loss:0.29692464619874953 | L1 Loss:0.41152905821800234 | R2:0.13856461650209975 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[150] Loss:0.17476559709757566 | L1 Loss:0.3256327901035547 | R2:0.5417859021384045 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[150] Loss:0.28203772455453874 | L1 Loss:0.3966141760349274 | R2:0.18619159727519358 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[151] Loss:0.17609008122235537 | L1 Loss:0.32841791678220034 | R2:0.5369733455973248 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[151] Loss:0.3140232041478157 | L1 Loss:0.42627676427364347 | R2:0.09270105621434274 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[152] Loss:0.17662062123417854 | L1 Loss:0.32884219102561474 | R2:0.5350930780292378 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[152] Loss:0.28928403928875923 | L1 Loss:0.40266052931547164 | R2:0.1736736071388731 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[153] Loss:0.18498405441641808 | L1 Loss:0.33819881081581116 | R2:0.5151506434711909 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[153] Loss:0.28389453440904616 | L1 Loss:0.40593649446964264 | R2:0.17763953281303815 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[154] Loss:0.1782182827591896 | L1 Loss:0.32294857408851385 | R2:0.5348934905305005 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[154] Loss:0.30805265679955485 | L1 Loss:0.41142303943634034 | R2:0.10903419135755643 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[155] Loss:0.17914693849161267 | L1 Loss:0.32659859023988247 | R2:0.5296429978623842 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[155] Loss:0.2942353397607803 | L1 Loss:0.4103007078170776 | R2:0.15180369514754 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[156] Loss:0.17742295935750008 | L1 Loss:0.32486962527036667 | R2:0.5329781270061577 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[156] Loss:0.2729316189885139 | L1 Loss:0.39320175647735595 | R2:0.2112250942738018 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[157] Loss:0.1828833152540028 | L1 Loss:0.32921919971704483 | R2:0.521299858364691 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[157] Loss:0.28854810148477555 | L1 Loss:0.4001791149377823 | R2:0.16514046994840337 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[158] Loss:0.1750791990198195 | L1 Loss:0.3232874032109976 | R2:0.5446322997692779 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[158] Loss:0.2970884546637535 | L1 Loss:0.4068300873041153 | R2:0.1372540924982242 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[159] Loss:0.16922683641314507 | L1 Loss:0.3177709709852934 | R2:0.5554867627289423 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[159] Loss:0.28206755220890045 | L1 Loss:0.3939345747232437 | R2:0.18324198009407042 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[160] Loss:0.1663570129312575 | L1 Loss:0.3172736931592226 | R2:0.5635447013500177 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[160] Loss:0.28185420483350754 | L1 Loss:0.399260476231575 | R2:0.17654311205851395 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[161] Loss:0.1696930609177798 | L1 Loss:0.3191519808024168 | R2:0.5564578987790079 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[161] Loss:0.2952557936310768 | L1 Loss:0.4075901582837105 | R2:0.14210123165883273 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[162] Loss:0.16728309588506818 | L1 Loss:0.3158519547432661 | R2:0.5615706604804271 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[162] Loss:0.29638250172138214 | L1 Loss:0.406877712905407 | R2:0.13865529745507424 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[163] Loss:0.17362759448587894 | L1 Loss:0.31796624045819044 | R2:0.5462987467091376 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[163] Loss:0.29868861138820646 | L1 Loss:0.4096164911985397 | R2:0.11515670728286949 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[164] Loss:0.17666221968829632 | L1 Loss:0.33238391019403934 | R2:0.5410303943475792 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[164] Loss:0.3008734241127968 | L1 Loss:0.4158265799283981 | R2:0.11818335822980966 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[165] Loss:0.1571992407552898 | L1 Loss:0.31159389205276966 | R2:0.586517524592977 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[165] Loss:0.30013630241155625 | L1 Loss:0.41564975380897523 | R2:0.12298733187807823 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[166] Loss:0.16781915444880724 | L1 Loss:0.31996289175003767 | R2:0.5570356600613584 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[166] Loss:0.3047472432255745 | L1 Loss:0.4121326982975006 | R2:0.10548744208239388 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[167] Loss:0.1716974633745849 | L1 Loss:0.32549789641052485 | R2:0.5454045777059225 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[167] Loss:0.3073373481631279 | L1 Loss:0.4177178680896759 | R2:0.10390345082847238 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[168] Loss:0.15995234437286854 | L1 Loss:0.31154010258615017 | R2:0.5754443461156932 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[168] Loss:0.32207089364528657 | L1 Loss:0.4318483918905258 | R2:0.05868029881985091 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[169] Loss:0.16534161567687988 | L1 Loss:0.3146210201084614 | R2:0.5622186136055705 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[169] Loss:0.2825651437044144 | L1 Loss:0.4067778825759888 | R2:0.1715645758596674 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[170] Loss:0.17242795787751675 | L1 Loss:0.32468339428305626 | R2:0.5445100164581238 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[170] Loss:0.27805632948875425 | L1 Loss:0.40727325081825255 | R2:0.18522229230824577 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[171] Loss:0.16909487126395106 | L1 Loss:0.32532730512320995 | R2:0.5538131617432731 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[171] Loss:0.2817246377468109 | L1 Loss:0.4012844741344452 | R2:0.18092502259661458 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[172] Loss:0.16475886525586247 | L1 Loss:0.3177905669435859 | R2:0.5677076045367214 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[172] Loss:0.2953999057412148 | L1 Loss:0.416201177239418 | R2:0.1474615147297034 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[173] Loss:0.1681618858128786 | L1 Loss:0.3209623768925667 | R2:0.5597143906917641 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[173] Loss:0.29212231934070587 | L1 Loss:0.41287633776664734 | R2:0.14706275750370712 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[174] Loss:0.17009084206074476 | L1 Loss:0.3207833059132099 | R2:0.5521690658284786 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[174] Loss:0.2846572116017342 | L1 Loss:0.405961412191391 | R2:0.17466887855241373 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[175] Loss:0.17191882757470012 | L1 Loss:0.3214799929410219 | R2:0.5500151857148232 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[175] Loss:0.29028831124305726 | L1 Loss:0.4063948690891266 | R2:0.16058396689734028 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[176] Loss:0.16973029007203877 | L1 Loss:0.31703051179647446 | R2:0.5554167772201231 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[176] Loss:0.28847585916519164 | L1 Loss:0.405149307847023 | R2:0.16639031725006578 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[177] Loss:0.1775931902229786 | L1 Loss:0.3235233351588249 | R2:0.5345880352421896 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[177] Loss:0.2796561121940613 | L1 Loss:0.3956975668668747 | R2:0.18720885561061057 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[178] Loss:0.18058083998039365 | L1 Loss:0.33096833527088165 | R2:0.5283868052018101 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[178] Loss:0.2652802184224129 | L1 Loss:0.3839146584272385 | R2:0.22459261777515968 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[179] Loss:0.1730692912824452 | L1 Loss:0.32630202174186707 | R2:0.5467422959952212 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[179] Loss:0.282090712338686 | L1 Loss:0.39593160450458526 | R2:0.18260336168312202 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[180] Loss:0.16716250916942954 | L1 Loss:0.3167792893946171 | R2:0.5618592413256929 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[180] Loss:0.2928478643298149 | L1 Loss:0.4066314876079559 | R2:0.14757810506412825 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[181] Loss:0.17675247183069587 | L1 Loss:0.3246785467490554 | R2:0.5397139379546473 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[181] Loss:0.2892135113477707 | L1 Loss:0.4126350700855255 | R2:0.15649447588312476 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[182] Loss:0.16057075466960669 | L1 Loss:0.31044019386172295 | R2:0.5774566791586697 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[182] Loss:0.2888779640197754 | L1 Loss:0.40338351130485534 | R2:0.15362056726500972 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[183] Loss:0.17286469135433435 | L1 Loss:0.32610975205898285 | R2:0.5466114409846606 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[183] Loss:0.2861935287714005 | L1 Loss:0.4041103333234787 | R2:0.16817388832149144 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[184] Loss:0.16211409028619528 | L1 Loss:0.31319290678948164 | R2:0.5699103216904355 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[184] Loss:0.2900484710931778 | L1 Loss:0.4056155920028687 | R2:0.1532635085185397 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[185] Loss:0.16223333543166518 | L1 Loss:0.31797447614371777 | R2:0.5701549102368317 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[185] Loss:0.2742673248052597 | L1 Loss:0.39884771406650543 | R2:0.2027564911388718 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[186] Loss:0.16599614406004548 | L1 Loss:0.31936668790876865 | R2:0.5634098306269328 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[186] Loss:0.2891311928629875 | L1 Loss:0.39922780394554136 | R2:0.16804065338208204 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[187] Loss:0.17348238872364163 | L1 Loss:0.3291774019598961 | R2:0.5409959813070467 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[187] Loss:0.28911182582378386 | L1 Loss:0.41187937259674073 | R2:0.1606519774325914 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[188] Loss:0.17312103742733598 | L1 Loss:0.3285419438034296 | R2:0.5404710050350908 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[188] Loss:0.28737423941493034 | L1 Loss:0.4004966452717781 | R2:0.17767456979520274 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[189] Loss:0.16985783446580172 | L1 Loss:0.32121984474360943 | R2:0.5513075768022797 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[189] Loss:0.2866365037858486 | L1 Loss:0.39945721328258516 | R2:0.1758197724903868 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[190] Loss:0.1680159973911941 | L1 Loss:0.31794002652168274 | R2:0.5564289011027266 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[190] Loss:0.28164889067411425 | L1 Loss:0.39738056510686875 | R2:0.18378883001571528 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[191] Loss:0.17019814439117908 | L1 Loss:0.3212940199300647 | R2:0.5522604216401252 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[191] Loss:0.2879067465662956 | L1 Loss:0.4117529630661011 | R2:0.16547762722832096 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[192] Loss:0.16799813229590654 | L1 Loss:0.31775963213294744 | R2:0.5595840669914859 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[192] Loss:0.2762532651424408 | L1 Loss:0.40243578255176543 | R2:0.20324864208517326 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[193] Loss:0.16699999058619142 | L1 Loss:0.31342433765530586 | R2:0.5623732526289141 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[193] Loss:0.2888591632246971 | L1 Loss:0.4060499846935272 | R2:0.15850057084217936 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[194] Loss:0.17093957029283047 | L1 Loss:0.32102382369339466 | R2:0.5569615182952464 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[194] Loss:0.2962063491344452 | L1 Loss:0.41710089445114135 | R2:0.13822393100626015 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[195] Loss:0.1656502545811236 | L1 Loss:0.3184039145708084 | R2:0.5671486688650317 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[195] Loss:0.30028399378061293 | L1 Loss:0.41504081189632414 | R2:0.12886749855324064 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[196] Loss:0.17206838028505445 | L1 Loss:0.3250002544373274 | R2:0.5467497050067712 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[196] Loss:0.2800988361239433 | L1 Loss:0.4044121354818344 | R2:0.18937551220607934 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[197] Loss:0.17097655730322003 | L1 Loss:0.31975365802645683 | R2:0.5478167715979341 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[197] Loss:0.2707865029573441 | L1 Loss:0.3974130690097809 | R2:0.21049875680724633 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[198] Loss:0.16446048161014915 | L1 Loss:0.3156816214323044 | R2:0.5680378151548573 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[198] Loss:0.2964694917201996 | L1 Loss:0.41596066355705263 | R2:0.13869038926430188 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[199] Loss:0.16764305578544736 | L1 Loss:0.32046092208474874 | R2:0.563370505165665 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[199] Loss:0.2907413214445114 | L1 Loss:0.4145543724298477 | R2:0.15811493546383043 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[200] Loss:0.16122879646718502 | L1 Loss:0.3136769514530897 | R2:0.5757148731331836 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[200] Loss:0.2782866284251213 | L1 Loss:0.3963778853416443 | R2:0.19023238821718857 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[201] Loss:0.17042234633117914 | L1 Loss:0.3232026118785143 | R2:0.5516804103659534 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[201] Loss:0.29655505120754244 | L1 Loss:0.4140266180038452 | R2:0.12879896615222136 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[202] Loss:0.16809717332944274 | L1 Loss:0.3221602067351341 | R2:0.5571384925992591 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[202] Loss:0.28519643694162367 | L1 Loss:0.40731211602687833 | R2:0.17445032603423968 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[203] Loss:0.16208869498223066 | L1 Loss:0.31307991221547127 | R2:0.5741754794565569 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[203] Loss:0.27860542237758634 | L1 Loss:0.40374000668525695 | R2:0.19285353762814958 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[204] Loss:0.16817250568419695 | L1 Loss:0.32248107716441154 | R2:0.5550489920565445 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[204] Loss:0.27779147773981094 | L1 Loss:0.40301158726215364 | R2:0.19397463068390305 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[205] Loss:0.16578706121072173 | L1 Loss:0.31915792915970087 | R2:0.5600400635522421 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[205] Loss:0.26870436370372774 | L1 Loss:0.3917133778333664 | R2:0.22283157518077715 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[206] Loss:0.16534979501739144 | L1 Loss:0.324071048758924 | R2:0.5658655566296773 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[206] Loss:0.27555042058229445 | L1 Loss:0.4058748006820679 | R2:0.20465827118051827 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[207] Loss:0.1663418747484684 | L1 Loss:0.3211378362029791 | R2:0.5612593266187853 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[207] Loss:0.2783651351928711 | L1 Loss:0.41314619183540346 | R2:0.18590633279287633 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[208] Loss:0.16104031587019563 | L1 Loss:0.31477004662156105 | R2:0.5734628476749755 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[208] Loss:0.2773978918790817 | L1 Loss:0.402202907204628 | R2:0.19144137762929908 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[209] Loss:0.1581979552283883 | L1 Loss:0.3093793746083975 | R2:0.5865326522076143 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[209] Loss:0.28102938532829286 | L1 Loss:0.39810225665569304 | R2:0.1867081906620048 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[210] Loss:0.15915697440505028 | L1 Loss:0.31608385872095823 | R2:0.5799887287265229 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[210] Loss:0.2721237525343895 | L1 Loss:0.39362533390522003 | R2:0.21455958623530097 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[211] Loss:0.15841369749978185 | L1 Loss:0.3144669886678457 | R2:0.5806070609647778 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[211] Loss:0.2650270164012909 | L1 Loss:0.39438684582710265 | R2:0.2400003698511998 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[212] Loss:0.1560391983948648 | L1 Loss:0.3107988405972719 | R2:0.5867316305100312 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[212] Loss:0.2777008876204491 | L1 Loss:0.40199655294418335 | R2:0.2007237909630592 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[213] Loss:0.16266359854489565 | L1 Loss:0.31711012963205576 | R2:0.5706494010840543 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[213] Loss:0.2834555201232433 | L1 Loss:0.4064921230077744 | R2:0.18436547928576402 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[214] Loss:0.16515974747017026 | L1 Loss:0.3142164461314678 | R2:0.5659022910840109 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[214] Loss:0.27528919726610185 | L1 Loss:0.40436487197875975 | R2:0.2026513172797597 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[215] Loss:0.157977438531816 | L1 Loss:0.31052793003618717 | R2:0.5831682489668196 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[215] Loss:0.27771675065159795 | L1 Loss:0.4031947135925293 | R2:0.2015681789349803 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[216] Loss:0.15589530486613512 | L1 Loss:0.30702725797891617 | R2:0.5927586847141162 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[216] Loss:0.2749819099903107 | L1 Loss:0.39887327551841734 | R2:0.20898007743790847 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[217] Loss:0.16077397810295224 | L1 Loss:0.3124778140336275 | R2:0.5797814377412632 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[217] Loss:0.2901295356452465 | L1 Loss:0.40533373057842254 | R2:0.1693138837340039 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[218] Loss:0.16396758193150163 | L1 Loss:0.3176069110631943 | R2:0.5739765757930048 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[218] Loss:0.28888157457113267 | L1 Loss:0.40053137838840486 | R2:0.16893515816396037 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[219] Loss:0.16065324377268553 | L1 Loss:0.3127199364826083 | R2:0.5803677794300145 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[219] Loss:0.2769660070538521 | L1 Loss:0.3961360305547714 | R2:0.20249891932594893 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[220] Loss:0.15631322655826807 | L1 Loss:0.30691090039908886 | R2:0.5907658524680595 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[220] Loss:0.2766903191804886 | L1 Loss:0.39125783145427706 | R2:0.2016070046679001 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[221] Loss:0.1610913686454296 | L1 Loss:0.3123382283374667 | R2:0.5786110544566625 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[221] Loss:0.276684533059597 | L1 Loss:0.3907830059528351 | R2:0.20123181466891632 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[222] Loss:0.16074349079281092 | L1 Loss:0.3072715885937214 | R2:0.580354869520282 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[222] Loss:0.2740205556154251 | L1 Loss:0.39933802783489225 | R2:0.20722000446760908 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[223] Loss:0.1605782900005579 | L1 Loss:0.3112159138545394 | R2:0.5742793815611467 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[223] Loss:0.27875181287527084 | L1 Loss:0.4056864470243454 | R2:0.19318679212186027 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[224] Loss:0.16609741980209947 | L1 Loss:0.3156943693757057 | R2:0.5637645958328574 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[224] Loss:0.2773637562990189 | L1 Loss:0.3945915699005127 | R2:0.2024529484450705 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[225] Loss:0.15792492032051086 | L1 Loss:0.31089860387146473 | R2:0.5856040175313822 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[225] Loss:0.27506274208426473 | L1 Loss:0.399369490146637 | R2:0.209421579762328 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[226] Loss:0.1546772918663919 | L1 Loss:0.30719932820647955 | R2:0.5923836435229147 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[226] Loss:0.28141722083091736 | L1 Loss:0.4024583876132965 | R2:0.19252047954468304 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[227] Loss:0.15824196953326464 | L1 Loss:0.31289710849523544 | R2:0.5847163987785923 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[227] Loss:0.27554875388741495 | L1 Loss:0.3921322703361511 | R2:0.21031168595493382 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[228] Loss:0.16087946901097894 | L1 Loss:0.31124053336679935 | R2:0.5756279461953971 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[228] Loss:0.28453110009431837 | L1 Loss:0.40191487669944764 | R2:0.1824879027725247 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[229] Loss:0.16432880191132426 | L1 Loss:0.3137375107035041 | R2:0.5695271500523408 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[229] Loss:0.2863202661275864 | L1 Loss:0.405631947517395 | R2:0.17248658702570413 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[230] Loss:0.1585085093975067 | L1 Loss:0.3097667880356312 | R2:0.5836153264807149 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[230] Loss:0.29345459714531896 | L1 Loss:0.4099688410758972 | R2:0.15629395958822617 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[231] Loss:0.16375951375812292 | L1 Loss:0.3176443222910166 | R2:0.571181601178369 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[231] Loss:0.288803531229496 | L1 Loss:0.4059854596853256 | R2:0.16596626548556268 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[232] Loss:0.15970012871548533 | L1 Loss:0.3093568542972207 | R2:0.5801013806212053 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[232] Loss:0.2839370109140873 | L1 Loss:0.40385451912879944 | R2:0.19233532199650566 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[233] Loss:0.15856538200750947 | L1 Loss:0.30907605681568384 | R2:0.5836519122470973 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[233] Loss:0.283162297308445 | L1 Loss:0.39960273802280427 | R2:0.18316179455453702 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[234] Loss:0.15531656239181757 | L1 Loss:0.30610690638422966 | R2:0.5922366663228983 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[234] Loss:0.2815040051937103 | L1 Loss:0.40671828091144563 | R2:0.18870698692113 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[235] Loss:0.1508581261150539 | L1 Loss:0.30373732652515173 | R2:0.6015700541612538 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[235] Loss:0.2882858902215958 | L1 Loss:0.4087153226137161 | R2:0.1695482591953133 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[236] Loss:0.1665044855326414 | L1 Loss:0.31853861175477505 | R2:0.5611734926852386 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[236] Loss:0.2767202243208885 | L1 Loss:0.4052496671676636 | R2:0.20101884344135074 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[237] Loss:0.15321689378470182 | L1 Loss:0.3081862712278962 | R2:0.5985186135392875 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[237] Loss:0.26029379218816756 | L1 Loss:0.3881200671195984 | R2:0.24724386954050917 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[238] Loss:0.15989098511636257 | L1 Loss:0.3138779541477561 | R2:0.5816186267558172 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[238] Loss:0.25458749383687973 | L1 Loss:0.3832522988319397 | R2:0.26091085552061644 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[239] Loss:0.1584638855420053 | L1 Loss:0.3136244844645262 | R2:0.5852017549510582 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[239] Loss:0.2702008433640003 | L1 Loss:0.3971839040517807 | R2:0.22168542867422972 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[240] Loss:0.15753609035164118 | L1 Loss:0.3119390243664384 | R2:0.5854465380646904 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[240] Loss:0.27509592697024343 | L1 Loss:0.39637728333473204 | R2:0.21092294869188427 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[241] Loss:0.15619253274053335 | L1 Loss:0.3164586089551449 | R2:0.5891845191186078 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[241] Loss:0.26909717321395876 | L1 Loss:0.3916799783706665 | R2:0.22575676328764346 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[242] Loss:0.15816830843687057 | L1 Loss:0.3159911949187517 | R2:0.5826493610144113 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[242] Loss:0.2625873304903507 | L1 Loss:0.39001756608486177 | R2:0.24706291241299977 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[243] Loss:0.1561970072798431 | L1 Loss:0.3128982502967119 | R2:0.5875556890879949 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[243] Loss:0.2664409905672073 | L1 Loss:0.3960864990949631 | R2:0.2322719023864234 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[244] Loss:0.16745423059910536 | L1 Loss:0.32076748833060265 | R2:0.5570189647294613 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[244] Loss:0.27321271151304244 | L1 Loss:0.39648987650871276 | R2:0.21906682573536407 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[245] Loss:0.15996307414025068 | L1 Loss:0.3137641018256545 | R2:0.5778534286693686 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[245] Loss:0.2909424856305122 | L1 Loss:0.412403067946434 | R2:0.16050578418167483 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[246] Loss:0.15704397251829505 | L1 Loss:0.3076359685510397 | R2:0.5875734813460565 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[246] Loss:0.2745451770722866 | L1 Loss:0.4014160633087158 | R2:0.207845376242462 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[247] Loss:0.15674934489652514 | L1 Loss:0.31040571443736553 | R2:0.5881210727296546 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[247] Loss:0.28357617408037183 | L1 Loss:0.40679035782814027 | R2:0.18099513662799618 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[248] Loss:0.1591170821338892 | L1 Loss:0.3088144790381193 | R2:0.5821148214105124 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[248] Loss:0.2798392713069916 | L1 Loss:0.3992776840925217 | R2:0.20399975726351177 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[249] Loss:0.15844039153307676 | L1 Loss:0.30831044539809227 | R2:0.5847174804146683 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[249] Loss:0.28279880434274673 | L1 Loss:0.4062122941017151 | R2:0.18180456569035866 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[250] Loss:0.15814095037057996 | L1 Loss:0.31112808641046286 | R2:0.5851209959915074 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[250] Loss:0.2770975723862648 | L1 Loss:0.39830693155527114 | R2:0.20593815413457048 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[251] Loss:0.16232789121568203 | L1 Loss:0.3127500116825104 | R2:0.573622092989392 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[251] Loss:0.27379344701766967 | L1 Loss:0.39556183815002444 | R2:0.2089126302397434 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[252] Loss:0.1517123021185398 | L1 Loss:0.3039327375590801 | R2:0.6011944483124824 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[252] Loss:0.2731809601187706 | L1 Loss:0.39665752053260805 | R2:0.20835023174196307 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[253] Loss:0.15791554655879736 | L1 Loss:0.3103353437036276 | R2:0.587883753957022 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[253] Loss:0.272860512137413 | L1 Loss:0.3913609623908997 | R2:0.21136063653659082 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[254] Loss:0.1581540727056563 | L1 Loss:0.31130709778517485 | R2:0.5866768962275156 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[254] Loss:0.264614237844944 | L1 Loss:0.3877044051885605 | R2:0.2410813108791317 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[255] Loss:0.1626816033385694 | L1 Loss:0.3146031741052866 | R2:0.576874928877179 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[255] Loss:0.27148203998804094 | L1 Loss:0.39223710596561434 | R2:0.21623802900452374 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[256] Loss:0.15496996510773897 | L1 Loss:0.3077993681654334 | R2:0.5934495504307048 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[256] Loss:0.2721711002290249 | L1 Loss:0.38902335464954374 | R2:0.21187942998172266 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[257] Loss:0.1548516913317144 | L1 Loss:0.309739145450294 | R2:0.5939890117567407 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[257] Loss:0.27579403668642044 | L1 Loss:0.3916242837905884 | R2:0.20054344307322633 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[258] Loss:0.15363741712644696 | L1 Loss:0.3071651356294751 | R2:0.5962714885619428 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[258] Loss:0.26364329606294634 | L1 Loss:0.3900283843278885 | R2:0.2389403444573815 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[259] Loss:0.1559774475172162 | L1 Loss:0.3108736900612712 | R2:0.5908257662929627 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[259] Loss:0.2704258173704147 | L1 Loss:0.40193470418453214 | R2:0.2256199054290752 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[260] Loss:0.1579721919260919 | L1 Loss:0.3076574746519327 | R2:0.5863178007666139 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[260] Loss:0.26633830070495607 | L1 Loss:0.3934885710477829 | R2:0.23344608617378998 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[261] Loss:0.159699949901551 | L1 Loss:0.3090287186205387 | R2:0.580689890416425 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[261] Loss:0.28133438527584076 | L1 Loss:0.3978362143039703 | R2:0.18907503619000945 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[262] Loss:0.15728903701528907 | L1 Loss:0.3061030376702547 | R2:0.5869096959991347 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[262] Loss:0.2661433979868889 | L1 Loss:0.38667996525764464 | R2:0.23475660427656148 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[263] Loss:0.15265834564343095 | L1 Loss:0.3022711407393217 | R2:0.5944141526663966 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[263] Loss:0.263724934309721 | L1 Loss:0.3863456234335899 | R2:0.2478970134142789 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[264] Loss:0.15833851415663958 | L1 Loss:0.3078375831246376 | R2:0.5805667579544126 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[264] Loss:0.2801141612231731 | L1 Loss:0.4009014040231705 | R2:0.1993127718691084 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[265] Loss:0.16264978563413024 | L1 Loss:0.3133470071479678 | R2:0.570507659161519 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[265] Loss:0.2886451959609985 | L1 Loss:0.41210460364818574 | R2:0.1756208408358402 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[266] Loss:0.16347264405339956 | L1 Loss:0.3132370747625828 | R2:0.5657670152243549 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[266] Loss:0.28957453072071077 | L1 Loss:0.4096419006586075 | R2:0.16787050267194262 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[267] Loss:0.1593073105905205 | L1 Loss:0.30818520206958055 | R2:0.5804599254062837 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[267] Loss:0.2848311707377434 | L1 Loss:0.40567609667778015 | R2:0.1738195739466508 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[268] Loss:0.15942308772355318 | L1 Loss:0.3069459991529584 | R2:0.5813665734393685 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[268] Loss:0.2802957817912102 | L1 Loss:0.40346927642822267 | R2:0.18474559178540648 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[269] Loss:0.16571033606305718 | L1 Loss:0.3153711315244436 | R2:0.5639761693275029 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[269] Loss:0.28318932726979257 | L1 Loss:0.4035995781421661 | R2:0.1883102355188939 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[270] Loss:0.15861803945153952 | L1 Loss:0.3075382625684142 | R2:0.5830774600661032 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[270] Loss:0.266126911342144 | L1 Loss:0.3859105631709099 | R2:0.2353485343242986 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[271] Loss:0.15901580872014165 | L1 Loss:0.3111078180372715 | R2:0.5825664775046119 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[271] Loss:0.2721625119447708 | L1 Loss:0.39248980581760406 | R2:0.2190607564443095 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[272] Loss:0.15602112002670765 | L1 Loss:0.3082098690792918 | R2:0.5912719740138989 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[272] Loss:0.26372242644429206 | L1 Loss:0.3942995101213455 | R2:0.24053096412029848 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[273] Loss:0.15068023465573788 | L1 Loss:0.3049310827627778 | R2:0.6036332800617565 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[273] Loss:0.2656280159950256 | L1 Loss:0.39436011016368866 | R2:0.23914117959877138 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[274] Loss:0.15004154527559876 | L1 Loss:0.30418024957180023 | R2:0.6039576244148165 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[274] Loss:0.2673171564936638 | L1 Loss:0.38878446221351626 | R2:0.2313327606934445 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[275] Loss:0.16186647024005651 | L1 Loss:0.31252860836684704 | R2:0.5713090621866095 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[275] Loss:0.26108427345752716 | L1 Loss:0.384681424498558 | R2:0.2540902029260814 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[276] Loss:0.15572850592434406 | L1 Loss:0.30838995426893234 | R2:0.5877729531399445 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[276] Loss:0.2558135315775871 | L1 Loss:0.3882644087076187 | R2:0.2640198339754594 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[277] Loss:0.15922534186393023 | L1 Loss:0.3092410806566477 | R2:0.5795833050781733 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[277] Loss:0.25645931959152224 | L1 Loss:0.38844681084156035 | R2:0.2571264114465144 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[278] Loss:0.16557717509567738 | L1 Loss:0.3109400924295187 | R2:0.5637238072561054 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[278] Loss:0.2515385806560516 | L1 Loss:0.3846341595053673 | R2:0.2747544098611395 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[279] Loss:0.15782418241724372 | L1 Loss:0.30491853505373 | R2:0.583330013156917 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[279] Loss:0.2594013974070549 | L1 Loss:0.38701375722885134 | R2:0.26050074225073594 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[280] Loss:0.1603225781582296 | L1 Loss:0.30842411518096924 | R2:0.5759081521424794 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[280] Loss:0.26672676354646685 | L1 Loss:0.4005353271961212 | R2:0.22839538486061225 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[281] Loss:0.1581847509369254 | L1 Loss:0.3076885314658284 | R2:0.5848507669926686 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[281] Loss:0.24967314451932907 | L1 Loss:0.3849534571170807 | R2:0.281215156418977 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[282] Loss:0.1596575160510838 | L1 Loss:0.3063912093639374 | R2:0.5797021370802963 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[282] Loss:0.2548693582415581 | L1 Loss:0.3845442056655884 | R2:0.26457976101283576 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[283] Loss:0.16038454370573163 | L1 Loss:0.30786820873618126 | R2:0.5815684085145869 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[283] Loss:0.254952509701252 | L1 Loss:0.38674196153879165 | R2:0.26388391482171913 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[284] Loss:0.15274488739669323 | L1 Loss:0.3044072361662984 | R2:0.5984408282534777 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[284] Loss:0.2712994933128357 | L1 Loss:0.3965428426861763 | R2:0.21934040772682756 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[285] Loss:0.15558088338002563 | L1 Loss:0.30540975742042065 | R2:0.5895095334710427 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[285] Loss:0.2634523317217827 | L1 Loss:0.3910184442996979 | R2:0.23727103300449412 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[286] Loss:0.15508580580353737 | L1 Loss:0.3063822565600276 | R2:0.5920888966361503 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[286] Loss:0.2757213994860649 | L1 Loss:0.4003913849592209 | R2:0.20126091620293854 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[287] Loss:0.15236637648195028 | L1 Loss:0.2995625436306 | R2:0.6014817050759192 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[287] Loss:0.2659481354057789 | L1 Loss:0.38798944354057313 | R2:0.23738270749834714 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[288] Loss:0.1497870311141014 | L1 Loss:0.3009151956066489 | R2:0.6079755292141126 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[288] Loss:0.26244612485170365 | L1 Loss:0.39330613017082217 | R2:0.25110074017118517 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[289] Loss:0.150507474783808 | L1 Loss:0.29687520023435354 | R2:0.6038335355786273 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[289] Loss:0.2639341473579407 | L1 Loss:0.39606453478336334 | R2:0.24272664387730644 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[290] Loss:0.1491535841487348 | L1 Loss:0.3007173119112849 | R2:0.6068725522807981 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[290] Loss:0.25433625727891923 | L1 Loss:0.3857217311859131 | R2:0.2635225544482972 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[291] Loss:0.15605828491970897 | L1 Loss:0.30804762709885836 | R2:0.5901932857184894 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[291] Loss:0.2513029597699642 | L1 Loss:0.38004550337791443 | R2:0.278501710206875 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[292] Loss:0.1570805567316711 | L1 Loss:0.30766004603356123 | R2:0.5858907894106551 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[292] Loss:0.254456739872694 | L1 Loss:0.38866135478019714 | R2:0.27626692438400235 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[293] Loss:0.15413154801353812 | L1 Loss:0.3042642120271921 | R2:0.5953361470876154 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[293] Loss:0.26339736506342887 | L1 Loss:0.39123475551605225 | R2:0.24082021950816035 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[294] Loss:0.15679220762103796 | L1 Loss:0.30603242199867964 | R2:0.5886229710412931 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[294] Loss:0.2757596082985401 | L1 Loss:0.3890407636761665 | R2:0.20747376078556917 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[295] Loss:0.1564927469007671 | L1 Loss:0.30858372151851654 | R2:0.5887082022085184 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[295] Loss:0.2686830498278141 | L1 Loss:0.3950871258974075 | R2:0.23098924445801677 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[296] Loss:0.15412207366898656 | L1 Loss:0.30315941013395786 | R2:0.5936662238129481 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[296] Loss:0.26200748383998873 | L1 Loss:0.38827112317085266 | R2:0.2447929037244762 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[297] Loss:0.15136902686208487 | L1 Loss:0.2990465583279729 | R2:0.5996112424774477 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[297] Loss:0.25109849125146866 | L1 Loss:0.37722773402929305 | R2:0.2761067253359173 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[298] Loss:0.1530198974069208 | L1 Loss:0.3025311678647995 | R2:0.5989747902204792 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[298] Loss:0.2638824924826622 | L1 Loss:0.38938477337360383 | R2:0.24252867306168727 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[299] Loss:0.1590418997220695 | L1 Loss:0.30428727250546217 | R2:0.5835090986809778 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[299] Loss:0.2667358838021755 | L1 Loss:0.3962119624018669 | R2:0.23635785768292505 | ACC: 70.3333%(211/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjry24MOjJf3",
        "outputId": "9e0c6835-56bd-4e55-86a0-14e7a20de0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.281215156418977\n",
            "min test_loss_l1_record  0.37722773402929305\n",
            "min test_loss_record  0.24967314451932907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8bK-dNojJf3",
        "outputId": "92c69de5-f650-462a-db98-3ca09f445215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.22459261777515968\n",
            "min test_loss_l1_record  0.3839146584272385\n",
            "min test_loss_record  0.2652802184224129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZziP03R2jJf3",
        "outputId": "7391e6a7-07e8-4c8e-c63c-bf6d53cb89e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.16715866673992183\n",
            "min test_loss_l1_record  0.40358836352825167\n",
            "min test_loss_record  0.28923620134592054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E9RUF0tjJf3",
        "outputId": "2eaba906-5706-4520-99a4-70a1b673f5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.04397604805336922\n",
            "min test_loss_l1_record  0.43836856484413145\n",
            "min test_loss_record  0.327268061041832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7 mer Regression"
      ],
      "metadata": {
        "id": "x1wwmv42jXPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zUuqP0fZjZ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False, num_mer=7, mer_dict=sevenMer_dict)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCaBZnYojaIS",
        "outputId": "970f0a32-a223-4966-b9c9-4cdaaf57244f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMhqFbK-jaIS",
        "outputId": "fea86209-be51-45e1-f2a9-3fa98fb5b472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "ldTG3R5WjaIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZyWGxRUjaIT",
        "outputId": "081a6f03-b7b2-446f-c38d-545fc42e0095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "pi23Q502jaIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "BQkxm0M3jaIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hNysVhijaIT",
        "outputId": "f0361144-df0f-4ff2-d785-9a43aef13e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3470283458009362 | L1 Loss:0.4254583679139614 | R2:-0.00016638241583216695 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[0] Loss:0.3401211053133011 | L1 Loss:0.4150086432695389 | R2:-0.024471422623915006 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[1] Loss:0.3338801860809326 | L1 Loss:0.4227796532213688 | R2:0.03818505709323636 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[1] Loss:0.34352617561817167 | L1 Loss:0.42289176881313323 | R2:-0.031590984793857434 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[2] Loss:0.3181202169507742 | L1 Loss:0.42119472101330757 | R2:0.07631488763766356 | ACC: 63.8000%(319/500)\n",
            "Testing Epoch[2] Loss:0.34654198586940765 | L1 Loss:0.4285539448261261 | R2:-0.040983289482691566 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[3] Loss:0.30562557000666857 | L1 Loss:0.41409782506525517 | R2:0.11430957555043059 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[3] Loss:0.35254013538360596 | L1 Loss:0.4312356173992157 | R2:-0.06232124580080076 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[4] Loss:0.2913000425323844 | L1 Loss:0.4114553462713957 | R2:0.15355768382741883 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[4] Loss:0.3479010060429573 | L1 Loss:0.4368602365255356 | R2:-0.05150912740628216 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[5] Loss:0.29689358174800873 | L1 Loss:0.4168567955493927 | R2:0.12955024624349504 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[5] Loss:0.3392610132694244 | L1 Loss:0.43512197136878966 | R2:-0.027619646357662953 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[6] Loss:0.28121819999068975 | L1 Loss:0.4050963632762432 | R2:0.17417972834678025 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[6] Loss:0.3555202350020409 | L1 Loss:0.44892269372940063 | R2:-0.07855047455356348 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[7] Loss:0.2632045364007354 | L1 Loss:0.395273944362998 | R2:0.222288531420977 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[7] Loss:0.3533699929714203 | L1 Loss:0.4502191424369812 | R2:-0.0670952820204269 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[8] Loss:0.2619893429800868 | L1 Loss:0.3891108352690935 | R2:0.22405325448968083 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[8] Loss:0.34490505158901213 | L1 Loss:0.45133491456508634 | R2:-0.0544605788909999 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[9] Loss:0.25283805280923843 | L1 Loss:0.38658605702221394 | R2:0.24915763954921036 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[9] Loss:0.3558462232351303 | L1 Loss:0.4569963335990906 | R2:-0.0865101467827987 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[10] Loss:0.25163497775793076 | L1 Loss:0.38253448717296124 | R2:0.25459301307232307 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[10] Loss:0.3363736867904663 | L1 Loss:0.4441661685705185 | R2:-0.039352316973690925 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[11] Loss:0.24570874590426683 | L1 Loss:0.38328779861330986 | R2:0.27310431698664317 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[11] Loss:0.3445126384496689 | L1 Loss:0.450597408413887 | R2:-0.05651379369681331 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[12] Loss:0.2596797626465559 | L1 Loss:0.3919287659227848 | R2:0.2306671609701846 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[12] Loss:0.33836570084095 | L1 Loss:0.44473805725574495 | R2:-0.05287112257387404 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[13] Loss:0.24340087547898293 | L1 Loss:0.3767822738736868 | R2:0.2817742050243244 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[13] Loss:0.33444520831108093 | L1 Loss:0.4551187723875046 | R2:-0.02378637073477684 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[14] Loss:0.2312948191538453 | L1 Loss:0.3678597379475832 | R2:0.30892841879980987 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[14] Loss:0.33054806739091874 | L1 Loss:0.4484699636697769 | R2:-0.021678952681144947 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[15] Loss:0.22686323896050453 | L1 Loss:0.35830544494092464 | R2:0.3220130724866969 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[15] Loss:0.3397547960281372 | L1 Loss:0.4591894716024399 | R2:-0.044358366847062555 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[16] Loss:0.22271812241524458 | L1 Loss:0.3588808514177799 | R2:0.3346433872602953 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[16] Loss:0.33387861102819444 | L1 Loss:0.4574917763471603 | R2:-0.03330886616923436 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[17] Loss:0.20670021791011095 | L1 Loss:0.3491509538143873 | R2:0.38500765723174996 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[17] Loss:0.31829034388065336 | L1 Loss:0.44391792118549345 | R2:0.012649490925859486 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[18] Loss:0.22030037734657526 | L1 Loss:0.36366065591573715 | R2:0.3464967859006827 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[18] Loss:0.333785280585289 | L1 Loss:0.4464875966310501 | R2:-0.030869758639064902 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[19] Loss:0.21173411048948765 | L1 Loss:0.35650598257780075 | R2:0.3671862772618963 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[19] Loss:0.34487256705760955 | L1 Loss:0.4529216349124908 | R2:-0.07649531417215236 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[20] Loss:0.20679424237459898 | L1 Loss:0.35392254777252674 | R2:0.38113199239684464 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[20] Loss:0.33620170056819915 | L1 Loss:0.4519203811883926 | R2:-0.042963108598511546 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[21] Loss:0.20434568729251623 | L1 Loss:0.34757256135344505 | R2:0.3861828963788762 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[21] Loss:0.32814049571752546 | L1 Loss:0.44758090674877166 | R2:-0.013571249837073363 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[22] Loss:0.21409767773002386 | L1 Loss:0.35970773734152317 | R2:0.35887088786738797 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[22] Loss:0.3298183187842369 | L1 Loss:0.4460832476615906 | R2:-0.01284646701918879 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[23] Loss:0.20856846449896693 | L1 Loss:0.3536520507186651 | R2:0.3763317783849927 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[23] Loss:0.3468108430504799 | L1 Loss:0.4659216344356537 | R2:-0.07736949957903068 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[24] Loss:0.22181133087724447 | L1 Loss:0.3605415839701891 | R2:0.33800118141853935 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[24] Loss:0.3440381184220314 | L1 Loss:0.46705895066261294 | R2:-0.058173561343540445 | ACC: 60.6667%(182/300)\n",
            "Training Epoch[25] Loss:0.21400704700499773 | L1 Loss:0.3547750227153301 | R2:0.3677182021985712 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[25] Loss:0.3221865132451057 | L1 Loss:0.4460964173078537 | R2:0.00944159150212497 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[26] Loss:0.2105287192389369 | L1 Loss:0.35324460081756115 | R2:0.3795359856747178 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[26] Loss:0.32213512510061265 | L1 Loss:0.45517131090164187 | R2:0.003442926028213433 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[27] Loss:0.20463181473314762 | L1 Loss:0.3485420495271683 | R2:0.3980148618205743 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[27] Loss:0.3466765612363815 | L1 Loss:0.4619702249765396 | R2:-0.05442267514578782 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[28] Loss:0.2021102560684085 | L1 Loss:0.3452729359269142 | R2:0.40000948916390444 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[28] Loss:0.3359300523996353 | L1 Loss:0.4531601518392563 | R2:-0.03541160814930035 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[29] Loss:0.20156268822029233 | L1 Loss:0.344789108261466 | R2:0.39912410521356223 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[29] Loss:0.33844291865825654 | L1 Loss:0.46394429802894593 | R2:-0.06025010966744606 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[30] Loss:0.19396210461854935 | L1 Loss:0.33894663117825985 | R2:0.4221781324421894 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[30] Loss:0.3257404059171677 | L1 Loss:0.447115820646286 | R2:-0.00708956163412483 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[31] Loss:0.20082334894686937 | L1 Loss:0.3491068631410599 | R2:0.40324239871267153 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[31] Loss:0.34109545946121217 | L1 Loss:0.4545632690191269 | R2:-0.05554197429162007 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[32] Loss:0.20244465256109834 | L1 Loss:0.35218141227960587 | R2:0.4009184841614445 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[32] Loss:0.3428154557943344 | L1 Loss:0.4514800488948822 | R2:-0.06654052091925561 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[33] Loss:0.21106404531747103 | L1 Loss:0.3553686384111643 | R2:0.37168122645821006 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[33] Loss:0.3451356589794159 | L1 Loss:0.4606414347887039 | R2:-0.06652871592351657 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[34] Loss:0.21789064025506377 | L1 Loss:0.3590714540332556 | R2:0.3485521008993202 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[34] Loss:0.3387659043073654 | L1 Loss:0.45493623316287995 | R2:-0.055375210394774235 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[35] Loss:0.20141676627099514 | L1 Loss:0.3468688577413559 | R2:0.4014559119581219 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[35] Loss:0.33109483420848845 | L1 Loss:0.44598476886749266 | R2:-0.014278363731426113 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[36] Loss:0.21142361406236887 | L1 Loss:0.3497534394264221 | R2:0.37559327043915 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[36] Loss:0.3655830413103104 | L1 Loss:0.4667163848876953 | R2:-0.1313093775503487 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[37] Loss:0.20871924422681332 | L1 Loss:0.3499817978590727 | R2:0.3790125227074603 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[37] Loss:0.3461803302168846 | L1 Loss:0.45192753672599795 | R2:-0.07893980681952134 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[38] Loss:0.21867283154278994 | L1 Loss:0.3634664807468653 | R2:0.3447604285993633 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[38] Loss:0.32778110802173616 | L1 Loss:0.4487139403820038 | R2:-0.021939318089782965 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[39] Loss:0.2096608099527657 | L1 Loss:0.3567149490118027 | R2:0.3703886723960007 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[39] Loss:0.32813408970832825 | L1 Loss:0.4506192594766617 | R2:-0.020962902329215792 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[40] Loss:0.2142690303735435 | L1 Loss:0.3645103685557842 | R2:0.35685673725107053 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[40] Loss:0.3401329666376114 | L1 Loss:0.45547161996364594 | R2:-0.05462174139804228 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[41] Loss:0.21600848762318492 | L1 Loss:0.3602022584527731 | R2:0.3482927522230107 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[41] Loss:0.34176653027534487 | L1 Loss:0.4516812652349472 | R2:-0.060870666044638644 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[42] Loss:0.2086049010977149 | L1 Loss:0.35693719051778316 | R2:0.37857861708827545 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[42] Loss:0.37666020095348357 | L1 Loss:0.46683751940727236 | R2:-0.18328017501773003 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[43] Loss:0.2079703053459525 | L1 Loss:0.3560949796810746 | R2:0.38046496715882666 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[43] Loss:0.3669294402003288 | L1 Loss:0.4724438667297363 | R2:-0.13331444988002553 | ACC: 62.3333%(187/300)\n",
            "Training Epoch[44] Loss:0.20763719826936722 | L1 Loss:0.3569689616560936 | R2:0.38486043079923704 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[44] Loss:0.36167989671230316 | L1 Loss:0.4711965411901474 | R2:-0.11262518319944977 | ACC: 62.6667%(188/300)\n",
            "Training Epoch[45] Loss:0.2215546453371644 | L1 Loss:0.372580923140049 | R2:0.34226343151032873 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[45] Loss:0.3403277322649956 | L1 Loss:0.44800838828086853 | R2:-0.043117655909844586 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[46] Loss:0.22893297672271729 | L1 Loss:0.3747835047543049 | R2:0.311584173734624 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[46] Loss:0.3257799744606018 | L1 Loss:0.4384921848773956 | R2:-0.009419603503084451 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[47] Loss:0.219619395211339 | L1 Loss:0.36327596567571163 | R2:0.3491858650456365 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[47] Loss:0.30521220266819 | L1 Loss:0.41992231607437136 | R2:0.06147317345937096 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[48] Loss:0.20791566744446754 | L1 Loss:0.3514799736440182 | R2:0.38422586692504956 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[48] Loss:0.32720450311899185 | L1 Loss:0.431687530875206 | R2:-0.012678190933879419 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[49] Loss:0.2192390263080597 | L1 Loss:0.366488054394722 | R2:0.34708217265630115 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[49] Loss:0.32370463460683824 | L1 Loss:0.43337635397911073 | R2:-0.004859201591085815 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[50] Loss:0.2213669205084443 | L1 Loss:0.3668221402913332 | R2:0.3471230732778282 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[50] Loss:0.3200651615858078 | L1 Loss:0.4347824305295944 | R2:0.004476024228321429 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[51] Loss:0.2217859886586666 | L1 Loss:0.36074095219373703 | R2:0.34515418111431845 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[51] Loss:0.3131235808134079 | L1 Loss:0.4322702705860138 | R2:0.035923416328919246 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[52] Loss:0.22512017656117678 | L1 Loss:0.3648616522550583 | R2:0.3453255788556567 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[52] Loss:0.31672871857881546 | L1 Loss:0.4254461407661438 | R2:0.005869605395356759 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[53] Loss:0.21872377954423428 | L1 Loss:0.36111023649573326 | R2:0.35400240407743305 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[53] Loss:0.3113490030169487 | L1 Loss:0.4336616784334183 | R2:0.03094796492753795 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[54] Loss:0.21956912148743868 | L1 Loss:0.3584756385535002 | R2:0.34730595241146434 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[54] Loss:0.3213861584663391 | L1 Loss:0.44090189039707184 | R2:-0.0011834879686726473 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[55] Loss:0.22969349194318056 | L1 Loss:0.3653296921402216 | R2:0.3266701140077634 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[55] Loss:0.34938876926898954 | L1 Loss:0.4521841436624527 | R2:-0.08760108197470953 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[56] Loss:0.20936495251953602 | L1 Loss:0.34737742133438587 | R2:0.38439074061468387 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[56] Loss:0.3129120603203773 | L1 Loss:0.4361377626657486 | R2:0.031139059193105223 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[57] Loss:0.213293194770813 | L1 Loss:0.3543352745473385 | R2:0.37124902870009285 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[57] Loss:0.3221596464514732 | L1 Loss:0.43814982771873473 | R2:-0.008060210499870334 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[58] Loss:0.22676750924438238 | L1 Loss:0.3654488380998373 | R2:0.332297645345026 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[58] Loss:0.32986686527729037 | L1 Loss:0.43528295755386354 | R2:-0.01672266937264536 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[59] Loss:0.2158226566389203 | L1 Loss:0.36429840698838234 | R2:0.35891407634423156 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[59] Loss:0.3094287350773811 | L1 Loss:0.4274806320667267 | R2:0.04075471560245915 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[60] Loss:0.2109082778915763 | L1 Loss:0.35916306637227535 | R2:0.3761519681814717 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[60] Loss:0.32509290128946305 | L1 Loss:0.4338041454553604 | R2:-0.02089440692513995 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[61] Loss:0.19901625532656908 | L1 Loss:0.3447265522554517 | R2:0.4068168565244523 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[61] Loss:0.3345778971910477 | L1 Loss:0.4469664841890335 | R2:-0.030074818330019494 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[62] Loss:0.21821738593280315 | L1 Loss:0.3613286968320608 | R2:0.348004228147068 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[62] Loss:0.34506488144397734 | L1 Loss:0.4556554913520813 | R2:-0.07750299591650098 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[63] Loss:0.21023059403523803 | L1 Loss:0.358565179631114 | R2:0.3721827656215614 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[63] Loss:0.32003291696310043 | L1 Loss:0.4354764848947525 | R2:0.010488998753302059 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[64] Loss:0.2086219461634755 | L1 Loss:0.35548380203545094 | R2:0.3896304260254361 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[64] Loss:0.31313526183366774 | L1 Loss:0.43143545389175414 | R2:0.04455582360442646 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[65] Loss:0.20815328229218721 | L1 Loss:0.3531006369739771 | R2:0.37515687177935675 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[65] Loss:0.3345519438385963 | L1 Loss:0.4428968161344528 | R2:-0.039901035662511686 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[66] Loss:0.21757089905440807 | L1 Loss:0.359665859490633 | R2:0.35027709907178656 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[66] Loss:0.3563018709421158 | L1 Loss:0.45686107575893403 | R2:-0.10959641733110567 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[67] Loss:0.19339637830853462 | L1 Loss:0.3447946812957525 | R2:0.4312455793459468 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[67] Loss:0.3425210639834404 | L1 Loss:0.4539422780275345 | R2:-0.05316398697055345 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[68] Loss:0.19562382996082306 | L1 Loss:0.34399778954684734 | R2:0.41781488448999515 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[68] Loss:0.3125346079468727 | L1 Loss:0.43891077041625975 | R2:0.041514494764713195 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[69] Loss:0.20051971776410937 | L1 Loss:0.3464713227003813 | R2:0.4078925766976371 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[69] Loss:0.30830345004796983 | L1 Loss:0.4325358599424362 | R2:0.039303101672259186 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[70] Loss:0.19948570802807808 | L1 Loss:0.3449940327554941 | R2:0.4097566585488027 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[70] Loss:0.31602098792791367 | L1 Loss:0.4393488436937332 | R2:0.020227484360794034 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[71] Loss:0.19569867430254817 | L1 Loss:0.34644608199596405 | R2:0.42055080682821727 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[71] Loss:0.3065673038363457 | L1 Loss:0.4315733641386032 | R2:0.05892801131186905 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[72] Loss:0.20123291015625 | L1 Loss:0.3449054919183254 | R2:0.40635363758589316 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[72] Loss:0.31095084100961684 | L1 Loss:0.44349718391895293 | R2:0.03947593727446658 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[73] Loss:0.19323702342808247 | L1 Loss:0.3392972182482481 | R2:0.4258840083391571 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[73] Loss:0.30441542714834213 | L1 Loss:0.42969420850276946 | R2:0.06021406250559804 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[74] Loss:0.19572995509952307 | L1 Loss:0.34042904898524284 | R2:0.41581900702419755 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[74] Loss:0.32526486814022065 | L1 Loss:0.44108766913414 | R2:-0.0061175133285404915 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[75] Loss:0.19414854235947132 | L1 Loss:0.34030459448695183 | R2:0.42246920385024106 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[75] Loss:0.3092805489897728 | L1 Loss:0.42970689535140993 | R2:0.029872665751911432 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[76] Loss:0.18706889310851693 | L1 Loss:0.33030973467975855 | R2:0.4393707015817039 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[76] Loss:0.3002433508634567 | L1 Loss:0.42699005305767057 | R2:0.06849040143281833 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[77] Loss:0.19648858392611146 | L1 Loss:0.3410097695887089 | R2:0.41478937212251027 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[77] Loss:0.3113696798682213 | L1 Loss:0.4346379518508911 | R2:0.02925624812618286 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[78] Loss:0.19759852904826403 | L1 Loss:0.34244239889085293 | R2:0.4138450443376403 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[78] Loss:0.321373938024044 | L1 Loss:0.4273227214813232 | R2:0.004067364704480292 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[79] Loss:0.21204232331365347 | L1 Loss:0.3541064281016588 | R2:0.3662574585713265 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[79] Loss:0.29639965295791626 | L1 Loss:0.4161956787109375 | R2:0.07919608155389948 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[80] Loss:0.19761989684775472 | L1 Loss:0.3450267445296049 | R2:0.40935421414473905 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[80] Loss:0.292678727209568 | L1 Loss:0.4133441597223282 | R2:0.10136737345739293 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[81] Loss:0.19288313016295433 | L1 Loss:0.34416105039417744 | R2:0.4231398254595077 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[81] Loss:0.28993197083473204 | L1 Loss:0.4249016731977463 | R2:0.10628528360815478 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[82] Loss:0.19057564064860344 | L1 Loss:0.33911001309752464 | R2:0.43031524993401454 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[82] Loss:0.30082953721284866 | L1 Loss:0.4261517286300659 | R2:0.07349458755467579 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[83] Loss:0.1971738375723362 | L1 Loss:0.3446942185983062 | R2:0.4135377890298623 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[83] Loss:0.2943901315331459 | L1 Loss:0.42361960262060167 | R2:0.10484920444238421 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[84] Loss:0.18987442180514336 | L1 Loss:0.3393179988488555 | R2:0.43535343913631486 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[84] Loss:0.30865010917186736 | L1 Loss:0.424997079372406 | R2:0.05374122499805054 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[85] Loss:0.2008453500457108 | L1 Loss:0.3467815797775984 | R2:0.40116239337618464 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[85] Loss:0.2932331904768944 | L1 Loss:0.4242773324251175 | R2:0.10625402057655584 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[86] Loss:0.19642250007018447 | L1 Loss:0.3416101709008217 | R2:0.40702753715706624 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[86] Loss:0.29880801439285276 | L1 Loss:0.4188850700855255 | R2:0.0767982191415721 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[87] Loss:0.1952513074502349 | L1 Loss:0.34359839372336864 | R2:0.41685400740056383 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[87] Loss:0.2959077477455139 | L1 Loss:0.4200477600097656 | R2:0.08911107318487943 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[88] Loss:0.1916923439130187 | L1 Loss:0.34122968278825283 | R2:0.429083829796731 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[88] Loss:0.2956640087068081 | L1 Loss:0.4142685323953629 | R2:0.09251298547663227 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[89] Loss:0.1840927368029952 | L1 Loss:0.3353669699281454 | R2:0.44989235512835946 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[89] Loss:0.28920420408248904 | L1 Loss:0.41475685834884646 | R2:0.11024459426556832 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[90] Loss:0.18930498510599136 | L1 Loss:0.33406291902065277 | R2:0.44002179572084515 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[90] Loss:0.2952514588832855 | L1 Loss:0.41541714817285535 | R2:0.08786240406967283 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[91] Loss:0.1872621257789433 | L1 Loss:0.333026971668005 | R2:0.44954627743176456 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[91] Loss:0.2856327950954437 | L1 Loss:0.4117805600166321 | R2:0.117083602206338 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[92] Loss:0.2003155737183988 | L1 Loss:0.34382470324635506 | R2:0.4009105534515387 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[92] Loss:0.3017419636249542 | L1 Loss:0.41924136877059937 | R2:0.07488431442311413 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[93] Loss:0.19440860394388437 | L1 Loss:0.3333547208458185 | R2:0.4245277691870026 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[93] Loss:0.27538013011217116 | L1 Loss:0.3990610182285309 | R2:0.1552716496707634 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[94] Loss:0.19428757671266794 | L1 Loss:0.33513688296079636 | R2:0.4232534210320044 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[94] Loss:0.28968561366200446 | L1 Loss:0.4114365965127945 | R2:0.11559339489121594 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[95] Loss:0.18905186746269464 | L1 Loss:0.3331177271902561 | R2:0.4395119674109673 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[95] Loss:0.2938332691788673 | L1 Loss:0.4110936179757118 | R2:0.09317390977025575 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[96] Loss:0.19909452134743333 | L1 Loss:0.34566996805369854 | R2:0.40589515790986214 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[96] Loss:0.2860532641410828 | L1 Loss:0.4130376040935516 | R2:0.12204098140299643 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[97] Loss:0.1952335862442851 | L1 Loss:0.341278619132936 | R2:0.418693860010964 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[97] Loss:0.2850881464779377 | L1 Loss:0.41543954610824585 | R2:0.12630111255153015 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[98] Loss:0.20266474597156048 | L1 Loss:0.34623571299016476 | R2:0.39549832181541317 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[98] Loss:0.29260830879211425 | L1 Loss:0.4203451097011566 | R2:0.09932327898100402 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[99] Loss:0.19085419597104192 | L1 Loss:0.337411742657423 | R2:0.43102361057611205 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[99] Loss:0.27485528737306597 | L1 Loss:0.40583004951477053 | R2:0.15450144183874828 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[100] Loss:0.19004093995317817 | L1 Loss:0.341274144127965 | R2:0.4324551247919012 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[100] Loss:0.29181565567851064 | L1 Loss:0.419013312458992 | R2:0.10820304044695779 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[101] Loss:0.1906920443288982 | L1 Loss:0.33864772133529186 | R2:0.4320454460323344 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[101] Loss:0.27528970688581467 | L1 Loss:0.40690077245235445 | R2:0.1520124936151141 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[102] Loss:0.1806603674776852 | L1 Loss:0.33111492544412613 | R2:0.4580218643113996 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[102] Loss:0.25383127182722093 | L1 Loss:0.3911820650100708 | R2:0.21524958640181807 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[103] Loss:0.18445105524733663 | L1 Loss:0.33320044819265604 | R2:0.4536700041267184 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[103] Loss:0.2608346126973629 | L1 Loss:0.39927818179130553 | R2:0.20213545205321326 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[104] Loss:0.19015692034736276 | L1 Loss:0.33931390568614006 | R2:0.43502143453467984 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[104] Loss:0.2627460442483425 | L1 Loss:0.40081084370613096 | R2:0.19685685794845847 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[105] Loss:0.19668129971250892 | L1 Loss:0.3418494947254658 | R2:0.42036139725713817 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[105] Loss:0.2569844454526901 | L1 Loss:0.4038174569606781 | R2:0.2136429480909979 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[106] Loss:0.19159646052867174 | L1 Loss:0.3363845981657505 | R2:0.4296049839976295 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[106] Loss:0.2560525044798851 | L1 Loss:0.3980363667011261 | R2:0.20914257592256646 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[107] Loss:0.18317951587960124 | L1 Loss:0.3304083179682493 | R2:0.45364321497694493 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[107] Loss:0.25730646550655367 | L1 Loss:0.39804679751396177 | R2:0.20718148224418184 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[108] Loss:0.1860742336139083 | L1 Loss:0.33206690568476915 | R2:0.45110121398808767 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[108] Loss:0.2819632366299629 | L1 Loss:0.4149795323610306 | R2:0.12889756120375512 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[109] Loss:0.18677923642098904 | L1 Loss:0.3309482280164957 | R2:0.4483465784661963 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[109] Loss:0.26423083543777465 | L1 Loss:0.40028471052646636 | R2:0.17936532912889322 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[110] Loss:0.18401533365249634 | L1 Loss:0.3355705011636019 | R2:0.45526191217049755 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[110] Loss:0.26706011295318605 | L1 Loss:0.399031800031662 | R2:0.17718055244679176 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[111] Loss:0.1919794911518693 | L1 Loss:0.33203062042593956 | R2:0.4327699879305656 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[111] Loss:0.28403708785772325 | L1 Loss:0.41409958600997926 | R2:0.11723638113818109 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[112] Loss:0.18859546724706888 | L1 Loss:0.335110941901803 | R2:0.44051517013301694 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[112] Loss:0.2822590708732605 | L1 Loss:0.4076262414455414 | R2:0.127218909772501 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[113] Loss:0.1767648309469223 | L1 Loss:0.32635085470974445 | R2:0.4751917818692552 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[113] Loss:0.26410881504416467 | L1 Loss:0.3970271497964859 | R2:0.18821889065563593 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[114] Loss:0.17555347736924887 | L1 Loss:0.3190895188599825 | R2:0.4737403798258327 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[114] Loss:0.27997601106762887 | L1 Loss:0.4103568777441978 | R2:0.1350935022347607 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[115] Loss:0.1768580856733024 | L1 Loss:0.3210110832005739 | R2:0.4707577941169522 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[115] Loss:0.26987976878881453 | L1 Loss:0.3994607239961624 | R2:0.16341552920992145 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[116] Loss:0.17523362673819065 | L1 Loss:0.3201313456520438 | R2:0.4757991454981803 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[116] Loss:0.27994161993265154 | L1 Loss:0.4081782281398773 | R2:0.1256732699308548 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[117] Loss:0.1796034835278988 | L1 Loss:0.3259842284023762 | R2:0.4617312328974207 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[117] Loss:0.2651896268129349 | L1 Loss:0.4039100676774979 | R2:0.17214810729291685 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[118] Loss:0.17850855458527803 | L1 Loss:0.3296098168939352 | R2:0.46939885412046206 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[118] Loss:0.2684090495109558 | L1 Loss:0.40433560609817504 | R2:0.16655492903410024 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[119] Loss:0.1809701407328248 | L1 Loss:0.3239310961216688 | R2:0.4557809871028967 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[119] Loss:0.265000607073307 | L1 Loss:0.40854823887348174 | R2:0.18790191325128036 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[120] Loss:0.18657544208690524 | L1 Loss:0.333712600171566 | R2:0.44255225054518926 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[120] Loss:0.2618111610412598 | L1 Loss:0.40155089199543 | R2:0.19980905422980605 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[121] Loss:0.17935340898111463 | L1 Loss:0.3236601706594229 | R2:0.45738429204554754 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[121] Loss:0.2609107822179794 | L1 Loss:0.4051882982254028 | R2:0.19074222806946523 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[122] Loss:0.17856588261201978 | L1 Loss:0.3287440948188305 | R2:0.4614650122704694 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[122] Loss:0.2561331525444984 | L1 Loss:0.402727872133255 | R2:0.21283132700833765 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[123] Loss:0.18436520593240857 | L1 Loss:0.3291755486279726 | R2:0.45007619841503776 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[123] Loss:0.2656642347574234 | L1 Loss:0.4020648509263992 | R2:0.18301821435388907 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[124] Loss:0.1775793917477131 | L1 Loss:0.32617018930613995 | R2:0.4727155828920138 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[124] Loss:0.27696390450000763 | L1 Loss:0.4108212351799011 | R2:0.1383068622748223 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[125] Loss:0.17961709341034293 | L1 Loss:0.32732152193784714 | R2:0.4634222219341043 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[125] Loss:0.25906348749995234 | L1 Loss:0.3946406751871109 | R2:0.21105088203729866 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[126] Loss:0.17776376800611615 | L1 Loss:0.32678476721048355 | R2:0.4703916043378456 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[126] Loss:0.2770491048693657 | L1 Loss:0.40490934550762175 | R2:0.14809571945265038 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[127] Loss:0.17073302529752254 | L1 Loss:0.32134208641946316 | R2:0.4899936282005315 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[127] Loss:0.2550911262631416 | L1 Loss:0.39417368173599243 | R2:0.20935188192482707 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[128] Loss:0.17729652393609285 | L1 Loss:0.3284219317138195 | R2:0.46901202626706784 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[128] Loss:0.25871694684028623 | L1 Loss:0.39133027791976926 | R2:0.1992128245679374 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[129] Loss:0.17742560943588614 | L1 Loss:0.32294816337525845 | R2:0.471769651481331 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[129] Loss:0.2552686408162117 | L1 Loss:0.3955536842346191 | R2:0.21123816217055866 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[130] Loss:0.17828834056854248 | L1 Loss:0.3271340327337384 | R2:0.4683017962167882 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[130] Loss:0.2565830245614052 | L1 Loss:0.3956242561340332 | R2:0.20708932641985217 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[131] Loss:0.17793410830199718 | L1 Loss:0.3248080871999264 | R2:0.4675928235708509 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[131] Loss:0.26708373352885245 | L1 Loss:0.3970906734466553 | R2:0.17602632838308918 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[132] Loss:0.17827597819268703 | L1 Loss:0.3273867592215538 | R2:0.4641520499672787 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[132] Loss:0.2659506879746914 | L1 Loss:0.4054256439208984 | R2:0.17183260378345014 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[133] Loss:0.18184191454201937 | L1 Loss:0.33167204819619656 | R2:0.45268980812616827 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[133] Loss:0.2835890598595142 | L1 Loss:0.4130080670118332 | R2:0.12507974943098438 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[134] Loss:0.17083403281867504 | L1 Loss:0.3164886645972729 | R2:0.486722743934608 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[134] Loss:0.2717427581548691 | L1 Loss:0.3982797980308533 | R2:0.15719578933391662 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[135] Loss:0.17890400253236294 | L1 Loss:0.3276225198060274 | R2:0.4635566882211469 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[135] Loss:0.2537981778383255 | L1 Loss:0.3936974048614502 | R2:0.2233413273100407 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[136] Loss:0.1850581420585513 | L1 Loss:0.33100175857543945 | R2:0.4441538634635904 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[136] Loss:0.2609472095966339 | L1 Loss:0.4034761726856232 | R2:0.18783106755848028 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[137] Loss:0.17491010669618845 | L1 Loss:0.3247801335528493 | R2:0.47419038156902404 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[137] Loss:0.2568741463124752 | L1 Loss:0.39609014987945557 | R2:0.20315261922418407 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[138] Loss:0.1794041432440281 | L1 Loss:0.33100429736077785 | R2:0.4624795772517756 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[138] Loss:0.25768320709466935 | L1 Loss:0.3982200711965561 | R2:0.2104987415053697 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[139] Loss:0.1831965888850391 | L1 Loss:0.3291563130915165 | R2:0.4548018419521779 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[139] Loss:0.2725451126694679 | L1 Loss:0.40684287846088407 | R2:0.15780738963716714 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[140] Loss:0.1838671718724072 | L1 Loss:0.3280847370624542 | R2:0.45335601560824323 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[140] Loss:0.2838191226124763 | L1 Loss:0.4140035569667816 | R2:0.12338678999977959 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[141] Loss:0.18043159041553736 | L1 Loss:0.3273137677460909 | R2:0.458936407881925 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[141] Loss:0.26316159069538114 | L1 Loss:0.4007903397083282 | R2:0.18688382846945978 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[142] Loss:0.17961037997156382 | L1 Loss:0.3284984938800335 | R2:0.4609683111263171 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[142] Loss:0.25890511870384214 | L1 Loss:0.3995227813720703 | R2:0.2059911777105882 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[143] Loss:0.18283407157287002 | L1 Loss:0.32945263385772705 | R2:0.45596667276824226 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[143] Loss:0.2390196993947029 | L1 Loss:0.39154167771339415 | R2:0.26219715549283074 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[144] Loss:0.1800385396927595 | L1 Loss:0.33010236732661724 | R2:0.4607890828836111 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[144] Loss:0.2471931681036949 | L1 Loss:0.39404442012310026 | R2:0.2434825724556895 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[145] Loss:0.17674220027402043 | L1 Loss:0.3240263815969229 | R2:0.4737593215868676 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[145] Loss:0.24878527894616126 | L1 Loss:0.3958511590957642 | R2:0.24519825810215248 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[146] Loss:0.17574663972482085 | L1 Loss:0.3234216384589672 | R2:0.4760376327431736 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[146] Loss:0.25658355504274366 | L1 Loss:0.395290806889534 | R2:0.20571904051749454 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[147] Loss:0.1715466994792223 | L1 Loss:0.3164387531578541 | R2:0.4866030931340431 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[147] Loss:0.24952869415283202 | L1 Loss:0.3962960183620453 | R2:0.23338830324564333 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[148] Loss:0.17639245837926865 | L1 Loss:0.3216996695846319 | R2:0.47318766417759595 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[148] Loss:0.2523127645254135 | L1 Loss:0.39400875866413115 | R2:0.220335498395539 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[149] Loss:0.1773361931554973 | L1 Loss:0.32254746090620756 | R2:0.469554114671967 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[149] Loss:0.2630559131503105 | L1 Loss:0.3965110629796982 | R2:0.18823796453281896 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[150] Loss:0.16944340895861387 | L1 Loss:0.3146829102188349 | R2:0.4899363320125729 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[150] Loss:0.24893195405602456 | L1 Loss:0.39369519203901293 | R2:0.23857788578065192 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[151] Loss:0.17156285652890801 | L1 Loss:0.32149848714470863 | R2:0.4840964881808302 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[151] Loss:0.2504028782248497 | L1 Loss:0.3991425961256027 | R2:0.23454372837065102 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[152] Loss:0.16693805530667305 | L1 Loss:0.31925398856401443 | R2:0.49745291083392984 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[152] Loss:0.2652374267578125 | L1 Loss:0.4090830832719803 | R2:0.1906805108252462 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[153] Loss:0.17155257426202297 | L1 Loss:0.32183957286179066 | R2:0.4911982518946953 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[153] Loss:0.23850102573633195 | L1 Loss:0.39102572202682495 | R2:0.2735108192788322 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[154] Loss:0.16939287493005395 | L1 Loss:0.32070302963256836 | R2:0.49862534299893435 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[154] Loss:0.2518418446183205 | L1 Loss:0.40483039915561675 | R2:0.229459846082886 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[155] Loss:0.16558718727901578 | L1 Loss:0.3172875624150038 | R2:0.5051755615455843 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[155] Loss:0.25090319812297823 | L1 Loss:0.3947245359420776 | R2:0.22974721489498515 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[156] Loss:0.1697842562571168 | L1 Loss:0.31738077104091644 | R2:0.4887376208476142 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[156] Loss:0.2606396183371544 | L1 Loss:0.40450659990310667 | R2:0.2036906262336613 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[157] Loss:0.17518292786553502 | L1 Loss:0.32276742346584797 | R2:0.4741501147144495 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[157] Loss:0.2549625888466835 | L1 Loss:0.39846059679985046 | R2:0.21445044641143948 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[158] Loss:0.1752258758060634 | L1 Loss:0.31950110755860806 | R2:0.47638956551885675 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[158] Loss:0.25612581223249437 | L1 Loss:0.3998937517404556 | R2:0.2126176981992695 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[159] Loss:0.16483433032408357 | L1 Loss:0.3129681972786784 | R2:0.5042235354181922 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[159] Loss:0.25734987258911135 | L1 Loss:0.3996471166610718 | R2:0.20421299923154504 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[160] Loss:0.1700029792264104 | L1 Loss:0.3155969399958849 | R2:0.48738209216647316 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[160] Loss:0.24682895988225936 | L1 Loss:0.3945943683385849 | R2:0.23752855199084993 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[161] Loss:0.1676473831757903 | L1 Loss:0.314913684502244 | R2:0.49265946225895724 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[161] Loss:0.2741103246808052 | L1 Loss:0.4050494521856308 | R2:0.1460150799047346 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[162] Loss:0.17199276620522141 | L1 Loss:0.3189568407833576 | R2:0.4843775736788225 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[162] Loss:0.25330598652362823 | L1 Loss:0.39174124896526336 | R2:0.22588633214016615 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[163] Loss:0.16951638739556074 | L1 Loss:0.31534540094435215 | R2:0.4890153471706423 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[163] Loss:0.2440571203827858 | L1 Loss:0.39322840571403506 | R2:0.25105312995148965 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[164] Loss:0.17478803917765617 | L1 Loss:0.32134968414902687 | R2:0.477692783355272 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[164] Loss:0.2454718753695488 | L1 Loss:0.3951491147279739 | R2:0.2440781692964263 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[165] Loss:0.16562833078205585 | L1 Loss:0.31201793625950813 | R2:0.5047212089600424 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[165] Loss:0.25859851241111753 | L1 Loss:0.3993947356939316 | R2:0.20686260864613343 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[166] Loss:0.17028997279703617 | L1 Loss:0.3171718940138817 | R2:0.49235280383900526 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[166] Loss:0.24097711741924285 | L1 Loss:0.37885503470897675 | R2:0.25868009843999606 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[167] Loss:0.16485245013609529 | L1 Loss:0.31495955027639866 | R2:0.5057820425103317 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[167] Loss:0.2459956392645836 | L1 Loss:0.3852644473314285 | R2:0.24083876586296524 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[168] Loss:0.16383869294077158 | L1 Loss:0.3099593408405781 | R2:0.5055298324909534 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[168] Loss:0.25142941921949385 | L1 Loss:0.392627477645874 | R2:0.22429869682746562 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[169] Loss:0.166197937913239 | L1 Loss:0.3116560596972704 | R2:0.5060768107919136 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[169] Loss:0.254315747320652 | L1 Loss:0.3894528031349182 | R2:0.22537760024313389 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[170] Loss:0.17478988599032164 | L1 Loss:0.32084702886641026 | R2:0.47792937613131675 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[170] Loss:0.25486380606889725 | L1 Loss:0.38987101018428805 | R2:0.22015189762191154 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[171] Loss:0.17665894608944654 | L1 Loss:0.32391274347901344 | R2:0.46799368261200824 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[171] Loss:0.2487894967198372 | L1 Loss:0.38336061537265775 | R2:0.23768483878361107 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[172] Loss:0.17743662372231483 | L1 Loss:0.3252097759395838 | R2:0.46924439802056434 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[172] Loss:0.2505506306886673 | L1 Loss:0.3869918525218964 | R2:0.23768006635930422 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[173] Loss:0.17681381898000836 | L1 Loss:0.3262312561273575 | R2:0.474456853724039 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[173] Loss:0.25646859407424927 | L1 Loss:0.39679485857486724 | R2:0.21351586232933287 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[174] Loss:0.16987202130258083 | L1 Loss:0.31757848896086216 | R2:0.48914602219392905 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[174] Loss:0.25722303688526155 | L1 Loss:0.40427653193473817 | R2:0.20606885216502163 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[175] Loss:0.1651486922055483 | L1 Loss:0.3104630699381232 | R2:0.5043625415611604 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[175] Loss:0.26402501612901685 | L1 Loss:0.4053916037082672 | R2:0.19018210748218334 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[176] Loss:0.1673294696956873 | L1 Loss:0.31307963840663433 | R2:0.5029308266782523 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[176] Loss:0.2491615891456604 | L1 Loss:0.39659899175167085 | R2:0.24235296012929952 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[177] Loss:0.17013373039662838 | L1 Loss:0.3165684714913368 | R2:0.49046903221962007 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[177] Loss:0.24408273920416831 | L1 Loss:0.38346291780471803 | R2:0.2566308268159164 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[178] Loss:0.16538014821708202 | L1 Loss:0.31738749332726 | R2:0.5066601317685075 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[178] Loss:0.24836955070495606 | L1 Loss:0.3883885473012924 | R2:0.235435619271506 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[179] Loss:0.17308482201769948 | L1 Loss:0.3220368530601263 | R2:0.4811933686746894 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[179] Loss:0.2571871176362038 | L1 Loss:0.39102143943309786 | R2:0.21357807783032062 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[180] Loss:0.16924315225332975 | L1 Loss:0.31883983686566353 | R2:0.4934899049937938 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[180] Loss:0.24763977527618408 | L1 Loss:0.3829857498407364 | R2:0.24566782589640984 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[181] Loss:0.17732761381193995 | L1 Loss:0.32529179006814957 | R2:0.4728488219224225 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[181] Loss:0.25681463479995725 | L1 Loss:0.3868961066007614 | R2:0.2033552820569632 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[182] Loss:0.17216326016932726 | L1 Loss:0.3153344541788101 | R2:0.48774559875268225 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[182] Loss:0.2546776458621025 | L1 Loss:0.3889742612838745 | R2:0.22089609437593025 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[183] Loss:0.16732388688251376 | L1 Loss:0.31726999394595623 | R2:0.5043884529780891 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[183] Loss:0.24224086999893188 | L1 Loss:0.3911884158849716 | R2:0.256430913775172 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[184] Loss:0.1690657241269946 | L1 Loss:0.3161310665309429 | R2:0.49900196519566314 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[184] Loss:0.24322314262390138 | L1 Loss:0.3913637399673462 | R2:0.25559680304666915 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[185] Loss:0.17367760790511966 | L1 Loss:0.3208517301827669 | R2:0.48346738259216554 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[185] Loss:0.2474010929465294 | L1 Loss:0.3865633964538574 | R2:0.23923011613581294 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[186] Loss:0.1731203505769372 | L1 Loss:0.32255333475768566 | R2:0.4819776577537657 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[186] Loss:0.2431335359811783 | L1 Loss:0.3935990959405899 | R2:0.2506515986988057 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[187] Loss:0.16325525753200054 | L1 Loss:0.311674278229475 | R2:0.5128187407412232 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[187] Loss:0.24937969893217088 | L1 Loss:0.3912054270505905 | R2:0.2283947349847649 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[188] Loss:0.16853315709158778 | L1 Loss:0.3225089814513922 | R2:0.4976722458834039 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[188] Loss:0.24179607927799224 | L1 Loss:0.3809495002031326 | R2:0.2593345141248421 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[189] Loss:0.16617812775075436 | L1 Loss:0.3128743302077055 | R2:0.5035339285161159 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[189] Loss:0.25097660198807714 | L1 Loss:0.3899594575166702 | R2:0.2294379194366039 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[190] Loss:0.17180008115246892 | L1 Loss:0.31891835294663906 | R2:0.4924181151970861 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[190] Loss:0.2472631812095642 | L1 Loss:0.3835774928331375 | R2:0.24164400302997197 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[191] Loss:0.16762878047302365 | L1 Loss:0.3200227804481983 | R2:0.4991973993994904 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[191] Loss:0.2399469792842865 | L1 Loss:0.38256704807281494 | R2:0.26291897423826793 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[192] Loss:0.17212692135944963 | L1 Loss:0.3216960672289133 | R2:0.4844703424982924 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[192] Loss:0.2572007060050964 | L1 Loss:0.3952698528766632 | R2:0.2078393222267727 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[193] Loss:0.17349305097013712 | L1 Loss:0.32095799408853054 | R2:0.48301017431225796 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[193] Loss:0.25100196003913877 | L1 Loss:0.391716405749321 | R2:0.22371063396986735 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[194] Loss:0.17106386041268706 | L1 Loss:0.3188537023961544 | R2:0.4942075400058955 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[194] Loss:0.24403143674135208 | L1 Loss:0.3879442662000656 | R2:0.24149513531170178 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[195] Loss:0.17149851471185684 | L1 Loss:0.3190899156033993 | R2:0.49321815338634667 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[195] Loss:0.25128780528903005 | L1 Loss:0.3932235360145569 | R2:0.23244229366652025 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[196] Loss:0.16943908389657736 | L1 Loss:0.3189826160669327 | R2:0.5005977762384973 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[196] Loss:0.23663916438817978 | L1 Loss:0.3835969835519791 | R2:0.26826779117809185 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[197] Loss:0.16370192356407642 | L1 Loss:0.31174826994538307 | R2:0.5152321673986142 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[197] Loss:0.24796314239501954 | L1 Loss:0.3948217809200287 | R2:0.22577661573575902 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[198] Loss:0.17345014680176973 | L1 Loss:0.32177288830280304 | R2:0.48011233247491303 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[198] Loss:0.2527996599674225 | L1 Loss:0.394880086183548 | R2:0.22722312898818858 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[199] Loss:0.16179761569947004 | L1 Loss:0.31007055565714836 | R2:0.517757745059851 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[199] Loss:0.25516808927059176 | L1 Loss:0.389782977104187 | R2:0.21022632579499517 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[200] Loss:0.17173245269805193 | L1 Loss:0.31732265930622816 | R2:0.48802383271149297 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[200] Loss:0.26133943349123 | L1 Loss:0.39488984644412994 | R2:0.19962527789323709 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[201] Loss:0.16759558860212564 | L1 Loss:0.31612745858728886 | R2:0.4983801305932238 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[201] Loss:0.24208618998527526 | L1 Loss:0.38540284633636473 | R2:0.26066874589648753 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[202] Loss:0.1720419111661613 | L1 Loss:0.3213820941746235 | R2:0.48575980875437147 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[202] Loss:0.23690303340554236 | L1 Loss:0.37256950736045835 | R2:0.27734067739628304 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[203] Loss:0.1763540357351303 | L1 Loss:0.3292951602488756 | R2:0.4680421114330596 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[203] Loss:0.24462112933397293 | L1 Loss:0.3846390455961227 | R2:0.24150271688962946 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[204] Loss:0.17097142105922103 | L1 Loss:0.32023140601813793 | R2:0.48789762882836285 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[204] Loss:0.24762869626283646 | L1 Loss:0.3911210924386978 | R2:0.2336793901250754 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[205] Loss:0.16877247905358672 | L1 Loss:0.3205451313406229 | R2:0.49567705826457126 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[205] Loss:0.25206426084041594 | L1 Loss:0.3884492337703705 | R2:0.22246087355593894 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[206] Loss:0.1686751628294587 | L1 Loss:0.3173942035064101 | R2:0.4931988191087152 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[206] Loss:0.252540335804224 | L1 Loss:0.3987936288118362 | R2:0.22210108059443315 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[207] Loss:0.17280363338068128 | L1 Loss:0.3224313613027334 | R2:0.482488349150026 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[207] Loss:0.25106628611683846 | L1 Loss:0.3954986810684204 | R2:0.2328910966875095 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[208] Loss:0.1781761166639626 | L1 Loss:0.32863050885498524 | R2:0.4662777194331761 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[208] Loss:0.24594339579343796 | L1 Loss:0.3856928050518036 | R2:0.24414340448136795 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[209] Loss:0.16639077523723245 | L1 Loss:0.31718952022492886 | R2:0.501794426758412 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[209] Loss:0.23371002003550528 | L1 Loss:0.37549851536750795 | R2:0.28284946589180715 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[210] Loss:0.16477984469383955 | L1 Loss:0.31876853946596384 | R2:0.5066149977273612 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[210] Loss:0.24315325617790223 | L1 Loss:0.3842855304479599 | R2:0.25454555408009455 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[211] Loss:0.16615779884159565 | L1 Loss:0.3142252368852496 | R2:0.50424304181487 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[211] Loss:0.2491031788289547 | L1 Loss:0.38969542980194094 | R2:0.23566760354469266 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[212] Loss:0.1709462683647871 | L1 Loss:0.3201865963637829 | R2:0.4932988785025134 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[212] Loss:0.24797440320253372 | L1 Loss:0.3851871877908707 | R2:0.24078475140504288 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[213] Loss:0.16911592287942767 | L1 Loss:0.3177431542426348 | R2:0.49783911005024517 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[213] Loss:0.2512050479650497 | L1 Loss:0.3859469145536423 | R2:0.22827901329638825 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[214] Loss:0.17132426612079144 | L1 Loss:0.32366777397692204 | R2:0.4884637350151097 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[214] Loss:0.26769490391016004 | L1 Loss:0.39872425198554995 | R2:0.17161406333188328 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[215] Loss:0.16370548447594047 | L1 Loss:0.3133332971483469 | R2:0.5133282588934946 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[215] Loss:0.2363150328397751 | L1 Loss:0.3763198137283325 | R2:0.2728270375977798 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[216] Loss:0.17391654197126627 | L1 Loss:0.3259394224733114 | R2:0.48136864470174046 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[216] Loss:0.24691013991832733 | L1 Loss:0.38681632578372954 | R2:0.23685266195234228 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[217] Loss:0.17105534207075834 | L1 Loss:0.32391437888145447 | R2:0.4895379198481091 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[217] Loss:0.22806437090039253 | L1 Loss:0.3706564366817474 | R2:0.2981714250218364 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[218] Loss:0.1744004669599235 | L1 Loss:0.3233661223202944 | R2:0.47836818057186475 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[218] Loss:0.2339665934443474 | L1 Loss:0.3771986156702042 | R2:0.2840436233356983 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[219] Loss:0.16832520393654704 | L1 Loss:0.320828715339303 | R2:0.5020113371327244 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[219] Loss:0.24616824984550476 | L1 Loss:0.38451763689517976 | R2:0.24394228848333968 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[220] Loss:0.16603792365640402 | L1 Loss:0.3186885751783848 | R2:0.5105092924887056 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[220] Loss:0.23193595111370086 | L1 Loss:0.37669559717178347 | R2:0.2800737405159265 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[221] Loss:0.16823665471747518 | L1 Loss:0.3213016763329506 | R2:0.5000950068892966 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[221] Loss:0.22346920967102052 | L1 Loss:0.37288968563079833 | R2:0.31141098243075643 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[222] Loss:0.16258269036188722 | L1 Loss:0.3132211174815893 | R2:0.5166612291516173 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[222] Loss:0.22193200141191483 | L1 Loss:0.36949033141136167 | R2:0.31351763327594273 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[223] Loss:0.16681401804089546 | L1 Loss:0.3192366994917393 | R2:0.5021833062209905 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[223] Loss:0.22975206151604652 | L1 Loss:0.3782676011323929 | R2:0.3003917954767036 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[224] Loss:0.16665351670235395 | L1 Loss:0.31896810233592987 | R2:0.5035871940556562 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[224] Loss:0.23644824624061583 | L1 Loss:0.3808500856161118 | R2:0.2704908393927222 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[225] Loss:0.16912477184087038 | L1 Loss:0.31899647414684296 | R2:0.5004668514416526 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[225] Loss:0.22775111794471742 | L1 Loss:0.3760207504034042 | R2:0.3015786405929678 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[226] Loss:0.17068818444386125 | L1 Loss:0.3201015330851078 | R2:0.4948309036965632 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[226] Loss:0.24197618514299393 | L1 Loss:0.388763889670372 | R2:0.25581848153805453 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[227] Loss:0.1708279070444405 | L1 Loss:0.3187065338715911 | R2:0.4920972702436985 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[227] Loss:0.2310577303171158 | L1 Loss:0.37547310888767244 | R2:0.2938390224634239 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[228] Loss:0.16308571631088853 | L1 Loss:0.3152645640075207 | R2:0.5102479547211604 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[228] Loss:0.24536223858594894 | L1 Loss:0.3861976206302643 | R2:0.2499015991684111 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[229] Loss:0.17387637635692954 | L1 Loss:0.32632241025567055 | R2:0.47504802904786253 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[229] Loss:0.24235024750232698 | L1 Loss:0.38203705847263336 | R2:0.2532366057262827 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[230] Loss:0.1675531887449324 | L1 Loss:0.31616463232785463 | R2:0.4967814467784223 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[230] Loss:0.23605942055583 | L1 Loss:0.38007603883743285 | R2:0.27299186181800483 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[231] Loss:0.1691137459129095 | L1 Loss:0.3220374099910259 | R2:0.495623181825367 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[231] Loss:0.243967005610466 | L1 Loss:0.38341828286647794 | R2:0.24679813087468389 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[232] Loss:0.16142016276717186 | L1 Loss:0.31272144243121147 | R2:0.5203273194004981 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[232] Loss:0.24149980396032333 | L1 Loss:0.3783329755067825 | R2:0.2603107470636149 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[233] Loss:0.16204308299347758 | L1 Loss:0.3135131411254406 | R2:0.5169985807103361 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[233] Loss:0.2531754165887833 | L1 Loss:0.38232868909835815 | R2:0.22114805300519264 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[234] Loss:0.1647407002747059 | L1 Loss:0.311474135145545 | R2:0.5110223703231412 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[234] Loss:0.23560546785593034 | L1 Loss:0.3785939395427704 | R2:0.2794450798647472 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[235] Loss:0.16994283068925142 | L1 Loss:0.3237057067453861 | R2:0.4931518374002415 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[235] Loss:0.24127247035503388 | L1 Loss:0.3794246733188629 | R2:0.257758948732189 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[236] Loss:0.16299537336453795 | L1 Loss:0.31445080414414406 | R2:0.5174122468629855 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[236] Loss:0.23513038754463195 | L1 Loss:0.3762500762939453 | R2:0.2840773229914263 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[237] Loss:0.1697183521464467 | L1 Loss:0.32406563870608807 | R2:0.495097140180162 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[237] Loss:0.23506129384040833 | L1 Loss:0.38371812701225283 | R2:0.2834815563365597 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[238] Loss:0.1682148203253746 | L1 Loss:0.31805631052702665 | R2:0.499468393865124 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[238] Loss:0.2383431002497673 | L1 Loss:0.3813549667596817 | R2:0.2659502983043948 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[239] Loss:0.17176987044513226 | L1 Loss:0.3226131796836853 | R2:0.49091385908564217 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[239] Loss:0.2473737195134163 | L1 Loss:0.3838690102100372 | R2:0.23427706830622727 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[240] Loss:0.17119882442057133 | L1 Loss:0.32405654713511467 | R2:0.48845884789394434 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[240] Loss:0.24166781604290008 | L1 Loss:0.3826695650815964 | R2:0.25767071248484114 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[241] Loss:0.1695315670222044 | L1 Loss:0.3218070138245821 | R2:0.4956195695428592 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[241] Loss:0.2497275397181511 | L1 Loss:0.3874559819698334 | R2:0.2321616822074281 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[242] Loss:0.16948040574789047 | L1 Loss:0.32396650593727827 | R2:0.4919069267993046 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[242] Loss:0.23562815934419631 | L1 Loss:0.37729108333587646 | R2:0.27467910880112134 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[243] Loss:0.16806952422484756 | L1 Loss:0.3224548604339361 | R2:0.5022269660254504 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[243] Loss:0.24953714907169341 | L1 Loss:0.38550244867801664 | R2:0.23373141799452393 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[244] Loss:0.16878707986325026 | L1 Loss:0.32322427816689014 | R2:0.5011519558460645 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[244] Loss:0.24552505537867547 | L1 Loss:0.3826178640127182 | R2:0.25475420354924433 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[245] Loss:0.1750268004834652 | L1 Loss:0.3283886406570673 | R2:0.47760687931788876 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[245] Loss:0.23040494024753572 | L1 Loss:0.3790788233280182 | R2:0.29151783430090805 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[246] Loss:0.16793954372406006 | L1 Loss:0.32312525250017643 | R2:0.4982551808111815 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[246] Loss:0.23742377758026123 | L1 Loss:0.38217217922210694 | R2:0.2734441463632037 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[247] Loss:0.16594742890447378 | L1 Loss:0.3217762280255556 | R2:0.5033955054946669 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[247] Loss:0.23510526940226556 | L1 Loss:0.3746137470006943 | R2:0.2880482707980435 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[248] Loss:0.1657438613474369 | L1 Loss:0.32010129280388355 | R2:0.5075709264736683 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[248] Loss:0.2208339676260948 | L1 Loss:0.3701006680727005 | R2:0.3214610142989919 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[249] Loss:0.1629919335246086 | L1 Loss:0.3185483403503895 | R2:0.512433201217598 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[249] Loss:0.22758933454751967 | L1 Loss:0.37564879059791567 | R2:0.30049895928294673 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[250] Loss:0.16675769723951817 | L1 Loss:0.31868151389062405 | R2:0.5036123098316112 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[250] Loss:0.2322579577565193 | L1 Loss:0.38049153089523313 | R2:0.2920665115158168 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[251] Loss:0.16063396632671356 | L1 Loss:0.31335063744336367 | R2:0.5215374584079051 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[251] Loss:0.24781128019094467 | L1 Loss:0.3929907321929932 | R2:0.2332038967134265 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[252] Loss:0.1652368805371225 | L1 Loss:0.31336445920169353 | R2:0.5040718015080184 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[252] Loss:0.22916629016399384 | L1 Loss:0.3756965219974518 | R2:0.2849904555256299 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[253] Loss:0.16138440743088722 | L1 Loss:0.31271402537822723 | R2:0.519076248718696 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[253] Loss:0.2282939687371254 | L1 Loss:0.380169153213501 | R2:0.29480230902117127 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[254] Loss:0.16357429046183825 | L1 Loss:0.31470245122909546 | R2:0.5138350585840099 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[254] Loss:0.2365832135081291 | L1 Loss:0.3895574420690536 | R2:0.27239615351870505 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[255] Loss:0.1641069441102445 | L1 Loss:0.3193388916552067 | R2:0.5111124439044251 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[255] Loss:0.23414120078086853 | L1 Loss:0.3798886388540268 | R2:0.27329019358439544 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[256] Loss:0.16061142506077886 | L1 Loss:0.31351449619978666 | R2:0.5186591575386059 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[256] Loss:0.24029189497232437 | L1 Loss:0.38604812026023866 | R2:0.26149850665493374 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[257] Loss:0.16456534713506699 | L1 Loss:0.31638474483042955 | R2:0.5066212488817674 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[257] Loss:0.23975294679403306 | L1 Loss:0.37890956103801726 | R2:0.2659425506530012 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[258] Loss:0.16358562232926488 | L1 Loss:0.31393940933048725 | R2:0.5112320321295941 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[258] Loss:0.23557154536247255 | L1 Loss:0.3746756315231323 | R2:0.27231232982443593 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[259] Loss:0.16972196055576205 | L1 Loss:0.3178897649049759 | R2:0.49094038669342766 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[259] Loss:0.24368878751993178 | L1 Loss:0.3885971367359161 | R2:0.24951808432161954 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[260] Loss:0.16723418608307838 | L1 Loss:0.3175303917378187 | R2:0.49835553987478287 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[260] Loss:0.24301606863737107 | L1 Loss:0.3873774766921997 | R2:0.2527447830740286 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[261] Loss:0.1621291576884687 | L1 Loss:0.31657753325998783 | R2:0.5119194464020291 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[261] Loss:0.24419541507959366 | L1 Loss:0.3878237158060074 | R2:0.24707908954657407 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[262] Loss:0.16664418578147888 | L1 Loss:0.31758658215403557 | R2:0.4988061137095813 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[262] Loss:0.22940755486488343 | L1 Loss:0.37334394454956055 | R2:0.29546217302757444 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[263] Loss:0.16354750003665686 | L1 Loss:0.3167571146041155 | R2:0.5120677611811247 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[263] Loss:0.23332393318414688 | L1 Loss:0.3749097228050232 | R2:0.2807244045587146 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[264] Loss:0.16043041180819273 | L1 Loss:0.3115327525883913 | R2:0.5205617999533839 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[264] Loss:0.2341676563024521 | L1 Loss:0.38233756721019746 | R2:0.2773489530784841 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[265] Loss:0.1644121496938169 | L1 Loss:0.3170893155038357 | R2:0.5115730927110956 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[265] Loss:0.23250114619731904 | L1 Loss:0.38232189416885376 | R2:0.2832675493223421 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[266] Loss:0.16422250540927052 | L1 Loss:0.31906684674322605 | R2:0.5122017487473935 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[266] Loss:0.23122759759426117 | L1 Loss:0.37077526152133944 | R2:0.2839038993248966 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[267] Loss:0.16661501536145806 | L1 Loss:0.3197425864636898 | R2:0.5022754826961964 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[267] Loss:0.2325422927737236 | L1 Loss:0.37687736451625825 | R2:0.279332406902849 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[268] Loss:0.16180156031623483 | L1 Loss:0.3186691403388977 | R2:0.5191785132778793 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[268] Loss:0.22576472163200378 | L1 Loss:0.37046492993831637 | R2:0.3043803820489946 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[269] Loss:0.15980877401307225 | L1 Loss:0.31192855164408684 | R2:0.5224135580220053 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[269] Loss:0.23219321221113204 | L1 Loss:0.38002372682094576 | R2:0.28923964891162196 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[270] Loss:0.1624987544491887 | L1 Loss:0.3138571809977293 | R2:0.5163536717436273 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[270] Loss:0.23920824974775315 | L1 Loss:0.3813361793756485 | R2:0.2674470724105924 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[271] Loss:0.1636037565767765 | L1 Loss:0.31440671533346176 | R2:0.5109376809424518 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[271] Loss:0.22769338190555571 | L1 Loss:0.37703858911991117 | R2:0.29437316663641905 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[272] Loss:0.1669597439467907 | L1 Loss:0.318206125870347 | R2:0.4992952648968614 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[272] Loss:0.24324168264865875 | L1 Loss:0.3876011610031128 | R2:0.25454078436613636 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[273] Loss:0.159081416670233 | L1 Loss:0.31302674021571875 | R2:0.522616065392605 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[273] Loss:0.24378790259361266 | L1 Loss:0.391287037730217 | R2:0.2571775757861426 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[274] Loss:0.16561156744137406 | L1 Loss:0.3206569775938988 | R2:0.5057643032848763 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[274] Loss:0.2409873053431511 | L1 Loss:0.3795366853475571 | R2:0.26043094024631513 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[275] Loss:0.15750053944066167 | L1 Loss:0.30720248725265265 | R2:0.5298849759586717 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[275] Loss:0.2397106632590294 | L1 Loss:0.3840598911046982 | R2:0.26730723932213674 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[276] Loss:0.16118523012846708 | L1 Loss:0.3137007635086775 | R2:0.5213919179833502 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[276] Loss:0.2289440080523491 | L1 Loss:0.3747342139482498 | R2:0.2925266601073982 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[277] Loss:0.15620286529883742 | L1 Loss:0.3086199425160885 | R2:0.5316206338949369 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[277] Loss:0.24971414506435394 | L1 Loss:0.38919913172721865 | R2:0.23185915865217535 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[278] Loss:0.16473539639264345 | L1 Loss:0.31466616317629814 | R2:0.508786065383031 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[278] Loss:0.2357594594359398 | L1 Loss:0.38046352565288544 | R2:0.2696183268955033 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[279] Loss:0.17131063900887966 | L1 Loss:0.32165232487022877 | R2:0.4922181824006402 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[279] Loss:0.23271409273147584 | L1 Loss:0.3808250218629837 | R2:0.2794045326336543 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[280] Loss:0.17147383466362953 | L1 Loss:0.3225304652005434 | R2:0.48915019664711284 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[280] Loss:0.24201446026563644 | L1 Loss:0.38284812271595003 | R2:0.25295883376172695 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[281] Loss:0.16840209998190403 | L1 Loss:0.321196012198925 | R2:0.5014497865517166 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[281] Loss:0.2378774106502533 | L1 Loss:0.38783190548419955 | R2:0.2676524060290587 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[282] Loss:0.170924860984087 | L1 Loss:0.31878664903342724 | R2:0.4940291132516485 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[282] Loss:0.24629125744104385 | L1 Loss:0.3847046852111816 | R2:0.24299170118499722 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[283] Loss:0.16501439595595002 | L1 Loss:0.31746453791856766 | R2:0.5128873730959941 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[283] Loss:0.24002295285463332 | L1 Loss:0.3851566880941391 | R2:0.25863031278187776 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[284] Loss:0.16512776585295796 | L1 Loss:0.31448973529040813 | R2:0.50914207514048 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[284] Loss:0.2352296829223633 | L1 Loss:0.3812737762928009 | R2:0.2738243854390051 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[285] Loss:0.16735087148845196 | L1 Loss:0.3166526611894369 | R2:0.49975545529062665 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[285] Loss:0.23363415896892548 | L1 Loss:0.3749675273895264 | R2:0.2782126827526704 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[286] Loss:0.1675882302224636 | L1 Loss:0.31374049559235573 | R2:0.5024614588963349 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[286] Loss:0.24193771928548813 | L1 Loss:0.3848934233188629 | R2:0.25406585871918347 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[287] Loss:0.15987610165029764 | L1 Loss:0.31148621905595064 | R2:0.5238827013230469 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[287] Loss:0.21986811384558677 | L1 Loss:0.3650461345911026 | R2:0.32084379016032394 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[288] Loss:0.16419165581464767 | L1 Loss:0.31250633113086224 | R2:0.5118206249211918 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[288] Loss:0.22927042841911316 | L1 Loss:0.3720083266496658 | R2:0.2928956811361475 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[289] Loss:0.1665535792708397 | L1 Loss:0.31572368927299976 | R2:0.5044099059606345 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[289] Loss:0.22793975248932838 | L1 Loss:0.37807300984859465 | R2:0.2955085900229487 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[290] Loss:0.16722674667835236 | L1 Loss:0.31706579960882664 | R2:0.5045851882774905 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[290] Loss:0.2258969008922577 | L1 Loss:0.3733926028013229 | R2:0.3016977542963413 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[291] Loss:0.1601791293360293 | L1 Loss:0.31228999234735966 | R2:0.5240136581359799 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[291] Loss:0.2260196939110756 | L1 Loss:0.3713024765253067 | R2:0.3023731464549604 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[292] Loss:0.1629690807312727 | L1 Loss:0.31137295812368393 | R2:0.517865993984162 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[292] Loss:0.2296040654182434 | L1 Loss:0.3800525516271591 | R2:0.3015791992550695 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[293] Loss:0.16705912165343761 | L1 Loss:0.3133375868201256 | R2:0.5048410047917626 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[293] Loss:0.2235760435461998 | L1 Loss:0.3667754292488098 | R2:0.3159189209891809 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[294] Loss:0.1615547677502036 | L1 Loss:0.3086677985265851 | R2:0.5202459695231435 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[294] Loss:0.23718150556087494 | L1 Loss:0.3742420583963394 | R2:0.27134320876405493 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[295] Loss:0.16942518390715122 | L1 Loss:0.31501713674515486 | R2:0.49723184438437845 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[295] Loss:0.23523204028606415 | L1 Loss:0.37990758419036863 | R2:0.2767171756101039 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[296] Loss:0.1644868878647685 | L1 Loss:0.31049342453479767 | R2:0.5115627188300199 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[296] Loss:0.22526584416627884 | L1 Loss:0.3694633662700653 | R2:0.3051616367212474 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[297] Loss:0.1627858867868781 | L1 Loss:0.31176962703466415 | R2:0.5175673770660509 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[297] Loss:0.22968732714653015 | L1 Loss:0.37319383919239046 | R2:0.2961142267026414 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[298] Loss:0.15996249159798026 | L1 Loss:0.30903974547982216 | R2:0.5262904920232581 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[298] Loss:0.23911551237106324 | L1 Loss:0.3822433888912201 | R2:0.2564978270633446 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[299] Loss:0.1651270012371242 | L1 Loss:0.31664000637829304 | R2:0.5118895481337707 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[299] Loss:0.22741320133209228 | L1 Loss:0.3668971866369247 | R2:0.29552686252972105 | ACC: 72.3333%(217/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJEX57ZmjaIT",
        "outputId": "8453706d-4418-43d3-b86e-7ad88c296323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.3214610142989919\n",
            "min test_loss_l1_record  0.3650461345911026\n",
            "min test_loss_record  0.21986811384558677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE6T19C9jaIU",
        "outputId": "a798b5d5-b1f2-4b7a-c10c-97eb27c4b9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.2735108192788322\n",
            "min test_loss_l1_record  0.37885503470897675\n",
            "min test_loss_record  0.23663916438817978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R-t8GiUjaIU",
        "outputId": "e079dbdd-7cd7-477b-e1df-a5ebb166bc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.1552716496707634\n",
            "min test_loss_l1_record  0.3990610182285309\n",
            "min test_loss_record  0.27485528737306597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loiaXZ9hjaIU",
        "outputId": "afd290f4-fdb5-4277-bc1a-72c20822bee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.06147317345937096\n",
            "min test_loss_l1_record  0.4150086432695389\n",
            "min test_loss_record  0.30521220266819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "iwAWRmhkUmGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9-YGhICXe_nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "# dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=True)\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-320, 320])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36adb4f-00e6-45c1-da1a-e25a1f44918d",
        "id": "bFrRlI8NciSW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88781702-8cf9-44a9-b69e-c020e562400f",
        "id": "Eides7FaciSX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001) # , weight_decay=0.0025\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "# criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "cJFSHkocciSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6599f7a8-4186-4e92-8f27-c432af3ff322",
        "id": "HKdMWzcTciSX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    \n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "SsilHuQWciSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "EZnEOuk6ciSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "\n",
        "train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in enumerate(dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    digits = model_class(data)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long)\n",
        "    # print(type(label))\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "\n",
        "    # L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  train_loss_record.append(loss.item())\n",
        "\n",
        "  val_loss, val_acc = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  val_acc_record.append(val_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  # val_r2_record.append(val_r2)\n",
        "  # val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc = test_acc_output(epoch)\n",
        "  test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  # test_r2_record.append(test_r2)\n",
        "  # test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cf0a42-f97b-4673-c06c-e7ad591b4ba2",
        "id": "RjqIYdjRciSY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.902546089142561 | ACC: 60.6000%(303/500)\n",
            "Testing Epoch[0] Loss:0.9227523386478425 | ACC: 58.6667%(176/300)\n",
            "Training Epoch[1] Loss:0.868553601205349 | ACC: 60.2000%(301/500)\n",
            "Testing Epoch[1] Loss:0.8758161664009094 | ACC: 61.0000%(183/300)\n",
            "Training Epoch[2] Loss:0.8249377980828285 | ACC: 63.8000%(319/500)\n",
            "Testing Epoch[2] Loss:0.8655176281929016 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[3] Loss:0.7928692139685154 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[3] Loss:0.8420146763324737 | ACC: 59.3333%(178/300)\n",
            "Training Epoch[4] Loss:0.7883928902447224 | ACC: 63.6000%(318/500)\n",
            "Testing Epoch[4] Loss:0.8388717472553253 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[5] Loss:0.7710687927901745 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[5] Loss:0.8406473696231842 | ACC: 60.3333%(181/300)\n",
            "Training Epoch[6] Loss:0.7631013467907906 | ACC: 64.4000%(322/500)\n",
            "Testing Epoch[6] Loss:0.8221246898174286 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[7] Loss:0.7368157207965851 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[7] Loss:0.8367516279220581 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[8] Loss:0.7300846427679062 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[8] Loss:0.8523246049880981 | ACC: 61.6667%(185/300)\n",
            "Training Epoch[9] Loss:0.7090313173830509 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[9] Loss:0.8589411020278931 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[10] Loss:0.6974406186491251 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[10] Loss:0.8404109954833985 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[11] Loss:0.6944637969136238 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[11] Loss:0.8981752753257751 | ACC: 61.3333%(184/300)\n",
            "Training Epoch[12] Loss:0.6872346717864275 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[12] Loss:0.8912065476179123 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[13] Loss:0.721827594563365 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[13] Loss:0.9278078079223633 | ACC: 65.3333%(196/300)\n",
            "Training Epoch[14] Loss:0.7098196856677532 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[14] Loss:0.9340231537818908 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[15] Loss:0.70833745226264 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[15] Loss:0.940319937467575 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[16] Loss:0.694536454975605 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[16] Loss:1.009890615940094 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[17] Loss:0.6947026159614325 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[17] Loss:0.9987747132778168 | ACC: 65.0000%(195/300)\n",
            "Training Epoch[18] Loss:0.6778302527964115 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[18] Loss:1.0237051963806152 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[19] Loss:0.6742471475154161 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[19] Loss:1.0156708478927612 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[20] Loss:0.6956639941781759 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[20] Loss:1.0940152287483216 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[21] Loss:0.7371343318372965 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[21] Loss:1.045998114347458 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[22] Loss:0.7070445287972689 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[22] Loss:1.0804085791110993 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[23] Loss:0.723817490041256 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[23] Loss:1.093221753835678 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[24] Loss:0.7233235407620668 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[24] Loss:1.1267411828041076 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[25] Loss:0.7111672312021255 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[25] Loss:1.1532385349273682 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[26] Loss:0.6853161882609129 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[26] Loss:1.2439386665821075 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[27] Loss:0.6715621612966061 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[27] Loss:1.1681805849075317 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[28] Loss:0.7139304410666227 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[28] Loss:1.2158204674720765 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[29] Loss:0.6968221683055162 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[29] Loss:1.1066513180732727 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[30] Loss:0.6925219520926476 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[30] Loss:1.1361029416322708 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[31] Loss:0.7006508838385344 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[31] Loss:1.2584555745124817 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[32] Loss:0.6525853611528873 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[32] Loss:1.2695704460144044 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[33] Loss:0.6735459677875042 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[33] Loss:1.2531592011451722 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[34] Loss:0.599128570407629 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[34] Loss:1.1910180151462555 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[35] Loss:0.6463909409940243 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[35] Loss:1.2584408462047576 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[36] Loss:0.5857454370707273 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[36] Loss:1.1949520349502563 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[37] Loss:0.6135969553142786 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[37] Loss:1.1288189232349395 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[38] Loss:0.6200532149523497 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[38] Loss:1.1949571311473846 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[39] Loss:0.6140473075211048 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[39] Loss:1.2850773513317109 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[40] Loss:0.7188628651201725 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[40] Loss:1.4250437557697295 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[41] Loss:0.6757220774888992 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[41] Loss:1.3316029548645019 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[42] Loss:0.6842149514704943 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[42] Loss:1.4252337872982026 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[43] Loss:0.6032787878066301 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[43] Loss:1.377293437719345 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[44] Loss:0.6368542518466711 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[44] Loss:1.3464989960193634 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[45] Loss:0.5765412421897054 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[45] Loss:1.226105409860611 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[46] Loss:0.6443782979622483 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[46] Loss:1.2286971271038056 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[47] Loss:0.6263919044286013 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[47] Loss:1.1698195159435272 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[48] Loss:0.631476866081357 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[48] Loss:1.2920965075492858 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[49] Loss:0.663371991366148 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[49] Loss:1.2937924146652222 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[50] Loss:0.7339077275246382 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[50] Loss:1.416317844390869 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[51] Loss:0.5948035679757595 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[51] Loss:1.2737693578004836 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[52] Loss:0.6565163135528564 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[52] Loss:1.5017412185668946 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[53] Loss:0.674897313117981 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[53] Loss:1.3520722001791001 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[54] Loss:0.6742440164089203 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[54] Loss:1.2742081522941588 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[55] Loss:0.7903608717024326 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[55] Loss:1.1683521807193755 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[56] Loss:0.6629411894828081 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[56] Loss:1.452101480960846 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[57] Loss:0.608324196189642 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[57] Loss:1.427475380897522 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[58] Loss:0.6169302891939878 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[58] Loss:1.2924346268177032 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[59] Loss:0.5603573452681303 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[59] Loss:1.2970121264457704 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[60] Loss:0.5138640804216266 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[60] Loss:1.1974733531475068 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[61] Loss:0.5631356295198202 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[61] Loss:1.241752916574478 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[62] Loss:0.6378733413293958 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[62] Loss:1.374313199520111 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[63] Loss:0.6093303598463535 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[63] Loss:1.364737719297409 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[64] Loss:0.6687149703502655 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[64] Loss:1.3483932316303253 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[65] Loss:0.645524151623249 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[65] Loss:1.4182615101337432 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[66] Loss:0.6090258592739701 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[66] Loss:1.4510427296161652 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[67] Loss:0.7800528816878796 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[67] Loss:1.3042425274848939 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[68] Loss:0.6406727246940136 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[68] Loss:1.301969164609909 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[69] Loss:0.5512919826433063 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[69] Loss:1.3221863031387329 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[70] Loss:0.5303990636020899 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[70] Loss:1.261090376973152 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[71] Loss:0.5436251200735569 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[71] Loss:1.2714483559131622 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[72] Loss:0.513061729259789 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[72] Loss:1.2324361056089401 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[73] Loss:0.5621246322989464 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[73] Loss:1.2955916732549668 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[74] Loss:0.5175074199214578 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[74] Loss:1.3543968141078948 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[75] Loss:0.48165248706936836 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[75] Loss:1.315074411034584 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[76] Loss:0.5248049041256309 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[76] Loss:1.3555159091949462 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[77] Loss:1.1556239984929562 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[77] Loss:1.7407273948192596 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[78] Loss:0.6324858851730824 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[78] Loss:1.2897371113300324 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[79] Loss:0.5168628422543406 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[79] Loss:1.2157337933778762 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[80] Loss:0.4751118868589401 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[80] Loss:1.2692186415195466 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[81] Loss:0.4615118280053139 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[81] Loss:1.2214205890893937 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[82] Loss:0.4511256320402026 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[82] Loss:1.236645570397377 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[83] Loss:0.4395084660500288 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[83] Loss:1.232044479250908 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[84] Loss:0.4492194624617696 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[84] Loss:1.2339997112751007 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[85] Loss:0.44694571988657117 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[85] Loss:1.2388000339269638 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[86] Loss:0.45473504066467285 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[86] Loss:1.2506064921617508 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[87] Loss:0.4503930136561394 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[87] Loss:1.2630857437849046 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[88] Loss:0.4558882317505777 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[88] Loss:1.287615042924881 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[89] Loss:0.4584509748965502 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[89] Loss:1.2733225136995316 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[90] Loss:0.6773368362337351 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[90] Loss:1.377468889951706 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[91] Loss:0.7118378239683807 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[91] Loss:1.0528361350297928 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[92] Loss:0.6002553254365921 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[92] Loss:1.11220680475235 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[93] Loss:0.5348450066521764 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[93] Loss:1.1607449322938919 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[94] Loss:0.5118637895211577 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[94] Loss:1.1078991919755936 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[95] Loss:0.48129504080861807 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[95] Loss:1.1152480751276017 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[96] Loss:0.4761640792712569 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[96] Loss:1.1324718981981277 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[97] Loss:0.46910752914845943 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[97] Loss:1.1415043830871583 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[98] Loss:0.46828179247677326 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[98] Loss:1.1588707178831101 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[99] Loss:0.4726432799361646 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[99] Loss:1.173321118950844 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[100] Loss:0.46887677535414696 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[100] Loss:1.1917175948619843 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[101] Loss:0.4896770673803985 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[101] Loss:1.1798070579767228 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[102] Loss:0.539035583846271 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[102] Loss:1.2255584627389908 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[103] Loss:1.442165894433856 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[103] Loss:1.677906334400177 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[104] Loss:0.6156849293038249 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[104] Loss:1.1529943704605103 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[105] Loss:0.5322502255439758 | ACC: 82.8000%(414/500)\n",
            "Testing Epoch[105] Loss:1.12291561961174 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[106] Loss:0.4969174019061029 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[106] Loss:1.0650577336549758 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[107] Loss:0.49546171398833394 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[107] Loss:1.0573619842529296 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[108] Loss:0.4747661193832755 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[108] Loss:1.0542461454868317 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[109] Loss:0.46036738622933626 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[109] Loss:1.0391678258776664 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[110] Loss:0.46131986984983087 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[110] Loss:1.0693639427423478 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[111] Loss:0.45786763448268175 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[111] Loss:1.086011491715908 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[112] Loss:0.4559952733106911 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[112] Loss:1.0929620251059533 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[113] Loss:0.4717134456150234 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[113] Loss:1.1049067378044128 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[114] Loss:0.4614255828782916 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[114] Loss:1.111378012597561 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[115] Loss:1.077607151120901 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[115] Loss:1.4244734406471253 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[116] Loss:0.651403971016407 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[116] Loss:1.2146955609321595 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[117] Loss:0.5121367750689387 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[117] Loss:1.0111187100410461 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[118] Loss:0.48142856638878584 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[118] Loss:1.0418180108070374 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[119] Loss:0.4608099451288581 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[119] Loss:0.9951705485582352 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[120] Loss:0.4527549045160413 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[120] Loss:1.0007598504424096 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[121] Loss:0.45391542837023735 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[121] Loss:1.001349550485611 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[122] Loss:0.4606102081015706 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[122] Loss:1.0135907083749771 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[123] Loss:0.457715418189764 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[123] Loss:1.0332818374037742 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[124] Loss:0.4675657721236348 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[124] Loss:1.0547656103968621 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[125] Loss:0.4675722047686577 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[125] Loss:1.0575837343931198 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[126] Loss:0.48440461698919535 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[126] Loss:1.0723609805107117 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[127] Loss:0.4791829641908407 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[127] Loss:1.0851955175399781 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[128] Loss:0.4864323018118739 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[128] Loss:1.1026402175426484 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[129] Loss:0.4917321763932705 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[129] Loss:1.1053328305482863 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[130] Loss:1.230655800551176 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[130] Loss:1.689467090368271 | ACC: 62.0000%(186/300)\n",
            "Training Epoch[131] Loss:0.6722505837678909 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[131] Loss:1.167280560731888 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[132] Loss:0.5771918641403317 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[132] Loss:1.0431418657302856 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[133] Loss:0.46698020212352276 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[133] Loss:0.9901058971881866 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[134] Loss:0.4597181510180235 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[134] Loss:1.0291574776172638 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[135] Loss:0.4510370474308729 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[135] Loss:1.0255353182554245 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[136] Loss:0.4548889147117734 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[136] Loss:1.0357495814561843 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[137] Loss:0.4554460747167468 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[137] Loss:1.056516519188881 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[138] Loss:0.46908096224069595 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[138] Loss:1.0657567650079727 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[139] Loss:0.46223704889416695 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[139] Loss:1.0758324682712554 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[140] Loss:0.47309632040560246 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[140] Loss:1.098606038093567 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[141] Loss:0.47488241270184517 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[141] Loss:1.1052192389965056 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[142] Loss:0.485435769893229 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[142] Loss:1.1283104419708252 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[143] Loss:0.6703068818897009 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[143] Loss:1.1008351683616637 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[144] Loss:0.7582905450835824 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[144] Loss:1.1314160227775574 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[145] Loss:0.546796603128314 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[145] Loss:0.9998884290456772 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[146] Loss:0.49548645270988345 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[146] Loss:0.9309192925691605 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[147] Loss:0.43067168444395065 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[147] Loss:0.9243436813354492 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[148] Loss:0.42140380572527647 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[148] Loss:0.9317984521389008 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[149] Loss:0.4235526127740741 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[149] Loss:0.9508145332336426 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[150] Loss:0.430387906730175 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[150] Loss:0.9789626985788346 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[151] Loss:0.4255981398746371 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[151] Loss:0.9874159395694733 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[152] Loss:0.43932204600423574 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[152] Loss:1.0073762089014053 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[153] Loss:0.43466359190642834 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[153] Loss:1.0137149214744567 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[154] Loss:0.47213630098849535 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[154] Loss:1.0253530859947204 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[155] Loss:0.447489645332098 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[155] Loss:1.0304742187261582 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[156] Loss:0.5367700923234224 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[156] Loss:1.0884946703910827 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[157] Loss:0.8250609040260315 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[157] Loss:1.3808638215065003 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[158] Loss:0.5316334869712591 | ACC: 81.6000%(408/500)\n",
            "Testing Epoch[158] Loss:0.9408054262399673 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[159] Loss:0.44100749772042036 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[159] Loss:0.8815638288855553 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[160] Loss:0.41454212460666895 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[160] Loss:0.9278203681111336 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[161] Loss:0.41532926773652434 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[161] Loss:0.9365355178713799 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[162] Loss:0.41436822758987546 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[162] Loss:0.940903976559639 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[163] Loss:0.4130434668622911 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[163] Loss:0.9467928037047386 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[164] Loss:0.4204613994807005 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[164] Loss:0.9710440456867218 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[165] Loss:0.41796459164470434 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[165] Loss:0.9656937375664711 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[166] Loss:0.4245470995083451 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[166] Loss:0.9868947610259056 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[167] Loss:0.419727629981935 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[167] Loss:1.0046693071722985 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[168] Loss:0.4302576328627765 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[168] Loss:1.0380377769470215 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[169] Loss:0.42516673542559147 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[169] Loss:1.0147838771343232 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[170] Loss:0.42874858528375626 | ACC: 87.2000%(436/500)\n",
            "Testing Epoch[170] Loss:1.0657314896583556 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[171] Loss:0.4243007143959403 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[171] Loss:1.033299058675766 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[172] Loss:0.4480970958247781 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[172] Loss:1.1010311365127563 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[173] Loss:0.5393063826486468 | ACC: 83.0000%(415/500)\n",
            "Testing Epoch[173] Loss:1.230929733812809 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[174] Loss:0.6509058838710189 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[174] Loss:1.1629797577857972 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[175] Loss:0.5624887645244598 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[175] Loss:1.0118200957775116 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[176] Loss:0.49999980721622705 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[176] Loss:0.9601920083165169 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[177] Loss:0.4954734714701772 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[177] Loss:0.9898638099431991 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[178] Loss:0.496648583561182 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[178] Loss:0.9932423785328865 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[179] Loss:0.5040577920153737 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[179] Loss:1.0125274181365966 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[180] Loss:0.4968866966664791 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[180] Loss:1.029556478559971 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[181] Loss:0.5110596297308803 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[181] Loss:1.0448635399341584 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[182] Loss:0.5047683278098702 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[182] Loss:1.0602524057030678 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[183] Loss:0.5141804628074169 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[183] Loss:1.0647556692361833 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[184] Loss:0.5117359068244696 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[184] Loss:1.0877038195729256 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[185] Loss:0.5406001014634967 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[185] Loss:1.1140779227018356 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[186] Loss:0.86961898393929 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[186] Loss:1.3732387244701385 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[187] Loss:0.7183147324249148 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[187] Loss:1.253555464744568 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[188] Loss:0.5229720305651426 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[188] Loss:1.012881988286972 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[189] Loss:0.47012349031865597 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[189] Loss:1.0757483780384063 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[190] Loss:0.4429563023149967 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[190] Loss:1.0360828459262847 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[191] Loss:0.4370252788066864 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[191] Loss:1.0496538817882537 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[192] Loss:0.43162265233695507 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[192] Loss:1.0474640995264053 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[193] Loss:0.43696872144937515 | ACC: 85.8000%(429/500)\n",
            "Testing Epoch[193] Loss:1.0585862785577773 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[194] Loss:0.437944152392447 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[194] Loss:1.0762629449367522 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[195] Loss:0.4420188255608082 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[195] Loss:1.078226974606514 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[196] Loss:0.4440229842439294 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[196] Loss:1.0947998285293579 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[197] Loss:0.45336570497602224 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[197] Loss:1.1064020544290543 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[198] Loss:0.4707162892445922 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[198] Loss:1.138077288866043 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[199] Loss:0.4637081231921911 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[199] Loss:1.1326586812734605 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[200] Loss:0.4640645096078515 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[200] Loss:1.131867414712906 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[201] Loss:0.46643984504044056 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[201] Loss:1.138414865732193 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[202] Loss:0.8934926353394985 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[202] Loss:1.7173338055610656 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[203] Loss:0.6850655302405357 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[203] Loss:1.0887751191854478 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[204] Loss:0.5965067390352488 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[204] Loss:0.9414445847272873 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[205] Loss:0.5177682135254145 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[205] Loss:0.9752207189798355 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[206] Loss:0.5068596908822656 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[206] Loss:0.9303774416446686 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[207] Loss:0.49542131973430514 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[207] Loss:0.949360328912735 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[208] Loss:0.496477706823498 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[208] Loss:0.9615005284547806 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[209] Loss:0.49085869500413537 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[209] Loss:0.9762471437454223 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[210] Loss:0.505548904184252 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[210] Loss:0.9884378105401993 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[211] Loss:0.4993099868297577 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[211] Loss:0.9931403905153274 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[212] Loss:0.4994751736521721 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[212] Loss:1.0074778825044632 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[213] Loss:0.49802342522889376 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[213] Loss:1.01089568734169 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[214] Loss:0.5046479422599077 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[214] Loss:1.0364563882350921 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[215] Loss:0.5060763722285628 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[215] Loss:1.0442274749279021 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[216] Loss:0.5189654547721148 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[216] Loss:1.0755771011114121 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[217] Loss:0.5104241659864783 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[217] Loss:1.0743988692760467 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[218] Loss:0.5547968139871955 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[218] Loss:1.0652537554502488 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[219] Loss:0.5476430151611567 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[219] Loss:1.109112697839737 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[220] Loss:1.4942920617759228 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[220] Loss:1.605542826652527 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[221] Loss:0.5886902790516615 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[221] Loss:1.0961369425058365 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[222] Loss:0.5163784907199442 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[222] Loss:0.9423856720328331 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[223] Loss:0.4596543009392917 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[223] Loss:0.9731919184327126 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[224] Loss:0.4534871852956712 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[224] Loss:0.994090610742569 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[225] Loss:0.45038169203326106 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[225] Loss:1.0024824023246766 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[226] Loss:0.45563064655289054 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[226] Loss:1.0159243613481521 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[227] Loss:0.4576306086964905 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[227] Loss:1.0258608341217041 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[228] Loss:0.45623924722895026 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[228] Loss:1.0374192520976067 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[229] Loss:0.4622278045862913 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[229] Loss:1.0477932006120683 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[230] Loss:0.46397144068032503 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[230] Loss:1.0632673054933548 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[231] Loss:0.472413940820843 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[231] Loss:1.0737578019499778 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[232] Loss:0.46939308708533645 | ACC: 87.0000%(435/500)\n",
            "Testing Epoch[232] Loss:1.0828555211424828 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[233] Loss:0.47706959024071693 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[233] Loss:1.1044149413704871 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[234] Loss:0.5642950148321688 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[234] Loss:1.131076180934906 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[235] Loss:0.7690385803580284 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[235] Loss:1.237484449148178 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[236] Loss:0.5658581526950002 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[236] Loss:1.0347866415977478 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[237] Loss:0.5107048586942255 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[237] Loss:1.0456028044223786 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[238] Loss:0.5010383133776486 | ACC: 83.2000%(416/500)\n",
            "Testing Epoch[238] Loss:1.036188018321991 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[239] Loss:0.49229264771565795 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[239] Loss:1.0412890017032623 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[240] Loss:0.4920166162773967 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[240] Loss:1.0498342096805573 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[241] Loss:0.49484220193699 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[241] Loss:1.0576916635036469 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[242] Loss:0.49900538567453623 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[242] Loss:1.0652998596429826 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[243] Loss:0.5001888857223094 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[243] Loss:1.075696411728859 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[244] Loss:0.5058225761167705 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[244] Loss:1.079949566721916 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[245] Loss:0.5041301702149212 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[245] Loss:1.0979999333620072 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[246] Loss:0.5172326867468655 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[246] Loss:1.1017359018325805 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[247] Loss:0.5095310281030834 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[247] Loss:1.1206729620695115 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[248] Loss:0.5448573417961597 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[248] Loss:1.1437666326761247 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[249] Loss:1.1398929506540298 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[249] Loss:1.7260652899742126 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[250] Loss:0.6258414285257459 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[250] Loss:1.1452966690063477 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[251] Loss:0.537862550932914 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[251] Loss:1.1151017159223557 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[252] Loss:0.47604964999482036 | ACC: 84.0000%(420/500)\n",
            "Testing Epoch[252] Loss:1.0481070160865784 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[253] Loss:0.45835503563284874 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[253] Loss:1.0884424954652787 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[254] Loss:0.4679904277436435 | ACC: 86.0000%(430/500)\n",
            "Testing Epoch[254] Loss:1.0840495318174361 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[255] Loss:0.4641722743399441 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[255] Loss:1.1032785266637801 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[256] Loss:0.4665152197703719 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[256] Loss:1.117844370007515 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[257] Loss:0.4645217298530042 | ACC: 86.4000%(432/500)\n",
            "Testing Epoch[257] Loss:1.133294293284416 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[258] Loss:0.469615385401994 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[258] Loss:1.1442441582679748 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[259] Loss:0.46668184641748667 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[259] Loss:1.1586072146892548 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[260] Loss:0.48303964314982295 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[260] Loss:1.186594820022583 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[261] Loss:0.4722567154094577 | ACC: 86.8000%(434/500)\n",
            "Testing Epoch[261] Loss:1.1809951812028885 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[262] Loss:0.48014880949631333 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[262] Loss:1.211218562722206 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[263] Loss:0.48148646811023355 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[263] Loss:1.2280608981847763 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[264] Loss:0.47756050946190953 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[264] Loss:1.224130964279175 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[265] Loss:0.4797941045835614 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[265] Loss:1.2249293148517608 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[266] Loss:0.9401462599635124 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[266] Loss:1.4658905506134032 | ACC: 64.0000%(192/300)\n",
            "Training Epoch[267] Loss:0.5215357816778123 | ACC: 81.8000%(409/500)\n",
            "Testing Epoch[267] Loss:0.9748283475637436 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[268] Loss:0.5122692782897502 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[268] Loss:0.9827168762683869 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[269] Loss:0.4786846060305834 | ACC: 82.6000%(413/500)\n",
            "Testing Epoch[269] Loss:1.0139720857143402 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[270] Loss:0.47696091793477535 | ACC: 83.6000%(418/500)\n",
            "Testing Epoch[270] Loss:1.0037973999977112 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[271] Loss:0.477821983397007 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[271] Loss:0.9955751121044158 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[272] Loss:0.47956943372264504 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[272] Loss:1.0083108812570571 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[273] Loss:0.48193370876833797 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[273] Loss:1.0130617529153825 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[274] Loss:0.48208743892610073 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[274] Loss:1.0249471336603164 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[275] Loss:0.48694104235619307 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[275] Loss:1.0297261774539948 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[276] Loss:0.4856124226935208 | ACC: 84.2000%(421/500)\n",
            "Testing Epoch[276] Loss:1.0371420800685882 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[277] Loss:0.49223624309524894 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[277] Loss:1.0539220601320267 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[278] Loss:0.4913243497721851 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[278] Loss:1.0616385608911514 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[279] Loss:0.49786710599437356 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[279] Loss:1.0638987481594087 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[280] Loss:0.5056647695600986 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[280] Loss:1.058740386366844 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[281] Loss:0.5077859899029136 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[281] Loss:1.0782931447029114 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[282] Loss:1.107789235189557 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[282] Loss:1.6124082803726196 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[283] Loss:0.633928625844419 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[283] Loss:1.1504662811756134 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[284] Loss:0.5164176719263196 | ACC: 83.4000%(417/500)\n",
            "Testing Epoch[284] Loss:1.0277883917093278 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[285] Loss:0.47614196268841624 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[285] Loss:1.0182808637619019 | ACC: 78.3333%(235/300)\n",
            "Training Epoch[286] Loss:0.47519826889038086 | ACC: 83.8000%(419/500)\n",
            "Testing Epoch[286] Loss:1.034399837255478 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[287] Loss:0.49172926787286997 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[287] Loss:1.051620289683342 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[288] Loss:0.4778036782518029 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[288] Loss:1.055457490682602 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[289] Loss:0.4714892329648137 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[289] Loss:1.0576363801956177 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[290] Loss:0.47344853449612856 | ACC: 84.8000%(424/500)\n",
            "Testing Epoch[290] Loss:1.0805294275283814 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[291] Loss:0.48441243916749954 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[291] Loss:1.1062124818563461 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[292] Loss:0.5168565528001636 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[292] Loss:1.1471149504184723 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[293] Loss:0.4913909137248993 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[293] Loss:1.1412650763988494 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[294] Loss:0.4887812868691981 | ACC: 84.4000%(422/500)\n",
            "Testing Epoch[294] Loss:1.163300985097885 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[295] Loss:0.4961539567448199 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[295] Loss:1.1633049815893173 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[296] Loss:0.4863725765608251 | ACC: 84.6000%(423/500)\n",
            "Testing Epoch[296] Loss:1.1947799324989319 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[297] Loss:0.4806482386775315 | ACC: 85.4000%(427/500)\n",
            "Testing Epoch[297] Loss:1.2035651415586472 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[298] Loss:1.285463910549879 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[298] Loss:1.836233788728714 | ACC: 65.6667%(197/300)\n",
            "Training Epoch[299] Loss:0.6540141925215721 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[299] Loss:1.2465913534164428 | ACC: 70.0000%(210/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 300 epoch second+mature mirna\n",
        "print('max val_acc_record ', max(val_acc_record))\n",
        "print('min val_loss_record ', min(val_loss_record))\n",
        "\n",
        "print('min test_loss_record ', min(test_loss_record))\n",
        "print('max test_acc_record ', max(test_acc_record))\n",
        "# max val_acc_record  84.6\n",
        "# min val_loss_record  0.5722110476344824\n",
        "# min test_loss_record  0.7427837997674942\n",
        "# max test_acc_record  75.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eede607-3b00-4624-8441-6eb224a9c5cc",
        "id": "7Ba9M8TCciSZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max val_acc_record  87.2\n",
            "min val_loss_record  0.4130434668622911\n",
            "min test_loss_record  0.8221246898174286\n",
            "max test_acc_record  78.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XDg21xErdGqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_class, '/content/drive/MyDrive/miRNA/last_data/classification_result/gcn_gru_300.pt')\n",
        "torch.save(model_class.state_dict(), '/content/drive/MyDrive/miRNA/last_data/classification_result/gcn_gru_300_state.pt')"
      ],
      "metadata": {
        "id": "InuqY7_y-ZLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_acc_list_gcn_gru_300.npy', val_acc_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_loss_list_gcn_gru_300.npy', val_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_loss_list_gcn_gru_300.npy', test_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_acc_list_gcn_gru_300.npy', test_acc_record, allow_pickle=True)\n"
      ],
      "metadata": {
        "id": "iN9Vk4VW-ZC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader"
      ],
      "metadata": {
        "id": "VFEB1_qac9g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "model_class.load_state_dict(torch.load('/content/drive/MyDrive/miRNA/last_data/classification_result/gcn_gru_300_state.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxcFjBkOq7cQ",
        "outputId": "0cb92940-9680-480b-89ac-81d16291b861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_roc():\n",
        "  \n",
        "  score_list=[]\n",
        "  score_one_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    score_temp=digits\n",
        "    score_list.extend(score_temp.detach().cpu().numpy())\n",
        "    score_temp=predicted\n",
        "    score_one_list.extend(score_temp.detach().cpu().numpy())\n",
        "    label_list.extend(label)\n",
        "  # correct = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   if label_list[i] == score_list[i]:\n",
        "  #     correct += 1\n",
        "  # print(correct/320)\n",
        "\n",
        "  score_array = np.array(score_list)\n",
        "  score_tensor = torch.tensor(score_one_list)\n",
        "  score_tensor = score_tensor.reshape(score_tensor.shape[0], 1)\n",
        "  score_one_hot = torch.zeros(score_tensor.shape[0], 3)\n",
        "  score_one_hot.scatter_(dim=1, index=score_tensor, value=1)\n",
        "  score_one_hot = np.array(score_one_hot)\n",
        "\n",
        "  label_tensor = torch.tensor(label_list)\n",
        "  label_tensor = label_tensor.reshape(label_tensor.shape[0], 1)\n",
        "  label_one_hot = torch.zeros(label_tensor.shape[0], 3)\n",
        "  label_one_hot.scatter_(dim=1, index=label_tensor, value=1)\n",
        "  label_one_hot = np.array(label_one_hot)\n",
        "  ns_c = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   score_list\n",
        "  print('score_array shape', score_array.shape)\n",
        "  print('label_one_hot shape', label_one_hot.shape)\n",
        "  # roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "  print('roc_auc_score', roc_auc_score(label_one_hot, score_array, average=None))\n",
        "  print('recall_score', recall_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('precision_score', precision_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('f1_score', f1_score(label_one_hot, score_one_hot, average=None))\n",
        "  # return 0\n",
        "\n",
        "  fpr_dict = dict()\n",
        "  tpr_dict = dict()\n",
        "  roc_auc_dict = dict()\n",
        "  print(score_array.shape)\n",
        "  for i in range(3):\n",
        "    fpr_dict[i], tpr_dict[i], _ = roc_curve(label_one_hot[:, i], score_array[:, i])\n",
        "    roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
        "  \n",
        "  fpr_dict['micro'], tpr_dict['micro'], _ = roc_curve(label_one_hot.ravel(), score_array.ravel())\n",
        "  roc_auc_dict['micro'] = auc(fpr_dict['micro'], tpr_dict['micro'])\n",
        "\n",
        "  all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(3)]))\n",
        "  mean_tpr = np.zeros_like(all_fpr)\n",
        "  for i in range(3):\n",
        "    mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
        "\n",
        "  mean_tpr /= 3\n",
        "  fpr_dict['macro'] = all_fpr\n",
        "  tpr_dict['macro'] = mean_tpr\n",
        "  roc_auc_dict['macro'] = auc(fpr_dict['macro'], tpr_dict['macro'])\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  lw = 1.5\n",
        "  plt.plot(fpr_dict['micro'], tpr_dict['micro'], label='micro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['micro']), color='deeppink', linestyle=':', linewidth=4)\n",
        "  plt.plot(fpr_dict['macro'], tpr_dict['macro'], label='macro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['macro']), color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "  colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "  labels = cycle(['down', 'ns', 'up'])\n",
        "  # print(fpr_dict[i])\n",
        "  print(roc_auc_dict[i])\n",
        "  for i, color, label in zip(range(3), colors, labels):\n",
        "    plt.plot(fpr_dict[i], tpr_dict[i], lw=lw, color=color, label='ROC curve of class {0} (area={1:0.4f})'.format(label, roc_auc_dict[i]))\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--', lw=lw)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Circulation ROC')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.savefig('GRU_GCN_classification_ROC_curve.jpg')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "2q_utdL2exp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "2O_6bZFya1s_",
        "outputId": "95c61b35-d8d6-4e53-b43a-9d249581ecf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score_array shape (320, 3)\n",
            "label_one_hot shape (320, 3)\n",
            "roc_auc_score [0.93310335 0.91029476 0.93532724]\n",
            "recall_score [0.69642857 0.86885246 0.79012346]\n",
            "precision_score [0.79591837 0.84126984 0.7804878 ]\n",
            "f1_score [0.74285714 0.85483871 0.78527607]\n",
            "(320, 3)\n",
            "0.9353272379771682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po7iM2JLk8Dc",
        "outputId": "4c4181e0-48f6-4a0a-cddf-7fbf8afbef4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.85625\n",
            "score_array shape (320, 3)\n",
            "label_one_hot shape (320, 3)\n",
            "[0.84551282 0.88985026 0.8680545 ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_metrix():\n",
        "  \n",
        "  score_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    score_temp=predicted\n",
        "    score_list.extend(score_temp.detach().cpu().numpy())\n",
        "    label_list.extend(label)\n",
        "  correct = 0\n",
        "  for i in range(len(score_list)):\n",
        "    if label_list[i] == score_list[i]:\n",
        "      correct += 1\n",
        "  print(correct/320)\n",
        "\n",
        "  # score_array = np.array(score_list)\n",
        "  score_tensor = torch.tensor(score_list)\n",
        "  score_tensor = score_tensor.reshape(score_tensor.shape[0], 1)\n",
        "  score_one_hot = torch.zeros(score_tensor.shape[0], 3)\n",
        "  score_one_hot.scatter_(dim=1, index=score_tensor, value=1)\n",
        "  score_one_hot = np.array(score_one_hot)\n",
        "\n",
        "  label_tensor = torch.tensor(label_list)\n",
        "  label_tensor = label_tensor.reshape(label_tensor.shape[0], 1)\n",
        "  label_one_hot = torch.zeros(label_tensor.shape[0], 3)\n",
        "  label_one_hot.scatter_(dim=1, index=label_tensor, value=1)\n",
        "  label_one_hot = np.array(label_one_hot)\n",
        "  ns_c = 0\n",
        "  for i in range(len(score_list)):\n",
        "    score_list\n",
        "  print('score_array shape', score_one_hot.shape)\n",
        "  print('label_one_hot shape', label_one_hot.shape)\n",
        "  # roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "  print('roc_auc_score', roc_auc_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('recall_score', recall_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('precision_score', precision_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('f1_score', f1_score(label_one_hot, score_one_hot, average=None))\n",
        "  return 0"
      ],
      "metadata": {
        "id": "k1G-saXcbprK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaj0mlEF99WF",
        "outputId": "983e3c0c-d4be-4afb-f376-13eeb432f7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.80625\n",
            "score_array shape (320, 3)\n",
            "label_one_hot shape (320, 3)\n",
            "roc_auc_score [0.79411401 0.83676359 0.81476599]\n",
            "recall_score [0.63492063 0.90710383 0.7027027 ]\n",
            "precision_score [0.76923077 0.83838384 0.74285714]\n",
            "f1_score [0.69565217 0.87139108 0.72222222]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All"
      ],
      "metadata": {
        "id": "OQ88VhKV99e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression 未做"
      ],
      "metadata": {
        "id": "mv3SyHYf99e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wBhim8vp99e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_soft, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-300, 300])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afee768-f113-4fa2-f389-c62e243b7b91",
        "id": "NQ2PK8MK99e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534f77da-2710-4785-9b75-cf5f73aabfd2",
        "id": "6Q95tUwV99e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "rcsGuTRA99e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c04368-b045-4e84-d78b-551e4547427e",
        "id": "hYG65aSf99e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "\n",
        "  # state = default_evaluator.run([[y_pred, y_true]])\n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "QNmfDM0Q99e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  losses = 0\n",
        "  losses_l1 = 0\n",
        "  r2s = 0\n",
        "  c = 0\n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.LongTensor).cuda()\n",
        "    loss = criterion(digits, label)\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "\n",
        "    r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (torch.round(digits) == label).sum()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    losses_l1 += L1_loss.item()\n",
        "    r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  avg_loss_l1 = losses_l1/c\n",
        "  avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | L1 Loss:{} | R2:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, avg_loss_l1, avg_r2, acc, corrects, the_batch_size))\n",
        "\n",
        "  return avg_loss, acc, avg_r2, avg_loss_l1"
      ],
      "metadata": {
        "id": "eWBsyJtr99e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "train_loss_record = []\n",
        "train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "val_r2_record = []\n",
        "val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "test_r2_record = []\n",
        "test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(200)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad55804-400b-41da-9a95-96e55b255928",
        "id": "9grP57Ix99e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.3296254351735115 | L1 Loss:0.4196109399199486 | R2:0.004306853342364324 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[0] Loss:0.3399969957768917 | L1 Loss:0.4142574042081833 | R2:-0.005120984381598881 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[1] Loss:0.3143345592543483 | L1 Loss:0.4138961136341095 | R2:0.0516866560065313 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[1] Loss:0.3359823912382126 | L1 Loss:0.415238182246685 | R2:0.028397673702505177 | ACC: 63.0000%(189/300)\n",
            "Training Epoch[2] Loss:0.2969454349949956 | L1 Loss:0.40616719983518124 | R2:0.10757993295167004 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[2] Loss:0.33121492378413675 | L1 Loss:0.41696353554725646 | R2:0.041810402047088656 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[3] Loss:0.28797789569944143 | L1 Loss:0.39978105388581753 | R2:0.1313539077847189 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[3] Loss:0.31714696139097215 | L1 Loss:0.40736897587776183 | R2:0.08653116915736966 | ACC: 63.6667%(191/300)\n",
            "Training Epoch[4] Loss:0.2937034321948886 | L1 Loss:0.40461462922394276 | R2:0.113826035565008 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[4] Loss:0.3242771830409765 | L1 Loss:0.41933524906635283 | R2:0.04744910593617909 | ACC: 63.3333%(190/300)\n",
            "Training Epoch[5] Loss:0.27269609179347754 | L1 Loss:0.3959722276777029 | R2:0.1786145008893203 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[5] Loss:0.3054047837853432 | L1 Loss:0.4115792691707611 | R2:0.09612486841188633 | ACC: 64.6667%(194/300)\n",
            "Training Epoch[6] Loss:0.2629864355549216 | L1 Loss:0.3821997959166765 | R2:0.20345157847539547 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[6] Loss:0.3083438850939274 | L1 Loss:0.41495600044727327 | R2:0.07976274955566673 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[7] Loss:0.2622698312625289 | L1 Loss:0.3837854042649269 | R2:0.20870071678327284 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[7] Loss:0.2978216990828514 | L1 Loss:0.40575005412101744 | R2:0.12398512256158152 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[8] Loss:0.25785098411142826 | L1 Loss:0.38414562307298183 | R2:0.21577799775997064 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[8] Loss:0.300031990557909 | L1 Loss:0.40817771255970003 | R2:0.1055713022667798 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[9] Loss:0.25377621315419674 | L1 Loss:0.38065260648727417 | R2:0.22640803218554112 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[9] Loss:0.28417536839842794 | L1 Loss:0.40035504996776583 | R2:0.13522632031163423 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[10] Loss:0.24295682180672884 | L1 Loss:0.38055297918617725 | R2:0.2591536399172148 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[10] Loss:0.29019022211432455 | L1 Loss:0.40589720010757446 | R2:0.09739797992916994 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[11] Loss:0.24426393117755651 | L1 Loss:0.38592609390616417 | R2:0.25491830884262795 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[11] Loss:0.2799809895455837 | L1 Loss:0.40241540521383284 | R2:0.15542278667731962 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[12] Loss:0.23835653997957706 | L1 Loss:0.37922370806336403 | R2:0.26928882872058146 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[12] Loss:0.2792442053556442 | L1 Loss:0.39435062408447263 | R2:0.10728962079104962 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[13] Loss:0.2424359880387783 | L1 Loss:0.38143178820610046 | R2:0.24806449577857342 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[13] Loss:0.27201291769742963 | L1 Loss:0.39950857758522035 | R2:0.14556016846978292 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[14] Loss:0.23185630980879068 | L1 Loss:0.37471180595457554 | R2:0.28077000185373385 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[14] Loss:0.280552351474762 | L1 Loss:0.4065214693546295 | R2:0.001068354337432198 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[15] Loss:0.2315006796270609 | L1 Loss:0.3757195584475994 | R2:0.2857899475555966 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[15] Loss:0.27349670976400375 | L1 Loss:0.4017858624458313 | R2:0.0910774761933673 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[16] Loss:0.23442580178380013 | L1 Loss:0.37330151349306107 | R2:0.2756775900172537 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[16] Loss:0.271575428545475 | L1 Loss:0.3973183065652847 | R2:0.10213952155980517 | ACC: 67.3333%(202/300)\n",
            "Training Epoch[17] Loss:0.23832957539707422 | L1 Loss:0.37962526082992554 | R2:0.26644706043292454 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[17] Loss:0.2581130817532539 | L1 Loss:0.3892859399318695 | R2:0.1366913506702897 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[18] Loss:0.2252620654180646 | L1 Loss:0.36971089988946915 | R2:0.29782009840603435 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[18] Loss:0.25816304683685304 | L1 Loss:0.38664981424808503 | R2:0.13117403123650523 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[19] Loss:0.22248970065265894 | L1 Loss:0.3725962471216917 | R2:0.31365009357179846 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[19] Loss:0.2748323306441307 | L1 Loss:0.39930634796619413 | R2:0.1041482920841507 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[20] Loss:0.2294869478791952 | L1 Loss:0.37842169031500816 | R2:0.28856061295789803 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[20] Loss:0.281589449942112 | L1 Loss:0.40861374139785767 | R2:0.023142854489275666 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[21] Loss:0.23103732708841562 | L1 Loss:0.3829188942909241 | R2:0.2856165818180657 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[21] Loss:0.2690607443451881 | L1 Loss:0.39305061995983126 | R2:0.08667728732899094 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[22] Loss:0.24547186587005854 | L1 Loss:0.3909059502184391 | R2:0.23924756745583653 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[22] Loss:0.2624188005924225 | L1 Loss:0.39211089313030245 | R2:0.10458993343551137 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[23] Loss:0.23790911585092545 | L1 Loss:0.38295865431427956 | R2:0.2617881200893775 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[23] Loss:0.2647839620709419 | L1 Loss:0.39463455975055695 | R2:0.15198788727625023 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[24] Loss:0.24132018350064754 | L1 Loss:0.39316723495721817 | R2:0.24776358732056522 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[24] Loss:0.26334845572710036 | L1 Loss:0.3941794693470001 | R2:0.09772666703984267 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[25] Loss:0.22987259645015 | L1 Loss:0.3790876865386963 | R2:0.28652839107631456 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[25] Loss:0.26227190643548964 | L1 Loss:0.40021399557590487 | R2:0.08008073602707766 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[26] Loss:0.24183739628642797 | L1 Loss:0.3920405451208353 | R2:0.23765859873878595 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[26] Loss:0.268300823867321 | L1 Loss:0.41182430684566496 | R2:0.010512351905261008 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[27] Loss:0.2251952588558197 | L1 Loss:0.3748347070068121 | R2:0.29423226600542474 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[27] Loss:0.26570150405168536 | L1 Loss:0.4065246254205704 | R2:0.028971547628056628 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[28] Loss:0.23244967777282 | L1 Loss:0.3765881359577179 | R2:0.2660640227456178 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[28] Loss:0.271737776696682 | L1 Loss:0.398888373374939 | R2:0.09963462926091057 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[29] Loss:0.2331772744655609 | L1 Loss:0.3747181501239538 | R2:0.27426905609038815 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[29] Loss:0.2651686668395996 | L1 Loss:0.405582982301712 | R2:0.08302151768155897 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[30] Loss:0.24139620643109083 | L1 Loss:0.3819538988173008 | R2:0.24108155477045826 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[30] Loss:0.2734318867325783 | L1 Loss:0.40986343324184416 | R2:-0.004964306572344634 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[31] Loss:0.22780936723574996 | L1 Loss:0.3753809332847595 | R2:0.2854785165042214 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[31] Loss:0.274230495095253 | L1 Loss:0.41357086002826693 | R2:0.041123485363399895 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[32] Loss:0.23221099749207497 | L1 Loss:0.3794390391558409 | R2:0.2706887874344653 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[32] Loss:0.2897728428244591 | L1 Loss:0.4258828967809677 | R2:0.00649740149563256 | ACC: 64.3333%(193/300)\n",
            "Training Epoch[33] Loss:0.22870124829933047 | L1 Loss:0.368757514283061 | R2:0.282621146889236 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[33] Loss:0.28204554319381714 | L1 Loss:0.42664546370506284 | R2:-0.06803385151243116 | ACC: 66.6667%(200/300)\n",
            "Training Epoch[34] Loss:0.22271250188350677 | L1 Loss:0.3727548122406006 | R2:0.2961497383729045 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[34] Loss:0.26404029577970506 | L1 Loss:0.4068407893180847 | R2:0.04906420115152148 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[35] Loss:0.22022556513547897 | L1 Loss:0.3634347654879093 | R2:0.304470478624362 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[35] Loss:0.28075208216905595 | L1 Loss:0.4158394157886505 | R2:0.006670166058237825 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[36] Loss:0.21331898029893637 | L1 Loss:0.3586149029433727 | R2:0.32613995675526414 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[36] Loss:0.27628555446863173 | L1 Loss:0.4116987258195877 | R2:-0.07713206332205838 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[37] Loss:0.22395383194088936 | L1 Loss:0.3718669358640909 | R2:0.2849060231712543 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[37] Loss:0.27426848858594893 | L1 Loss:0.40535837709903716 | R2:-0.07466467306935928 | ACC: 66.0000%(198/300)\n",
            "Training Epoch[38] Loss:0.22197094559669495 | L1 Loss:0.37064291909337044 | R2:0.29377532149024455 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[38] Loss:0.2826380431652069 | L1 Loss:0.4136990547180176 | R2:-0.06753780633289175 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[39] Loss:0.227225411683321 | L1 Loss:0.37717052549123764 | R2:0.28823232302888524 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[39] Loss:0.2721856489777565 | L1 Loss:0.40044246017932894 | R2:-0.09161919819238165 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[40] Loss:0.22847493272274733 | L1 Loss:0.3744868282228708 | R2:0.27771521249068365 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[40] Loss:0.2670182138681412 | L1 Loss:0.39979431331157683 | R2:-0.009362157344484113 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[41] Loss:0.22439967561513186 | L1 Loss:0.3682191763073206 | R2:0.28049293428880706 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[41] Loss:0.2697265326976776 | L1 Loss:0.38873541355133057 | R2:-0.05638511088399918 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[42] Loss:0.21095296368002892 | L1 Loss:0.36105410009622574 | R2:0.33259998815702657 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[42] Loss:0.2672200739383698 | L1 Loss:0.3987687200307846 | R2:-0.11377551712659417 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[43] Loss:0.20819106232374907 | L1 Loss:0.3583377208560705 | R2:0.34083894128581615 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[43] Loss:0.2709537610411644 | L1 Loss:0.40547938644886017 | R2:-0.07370187349738293 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[44] Loss:0.21471489779651165 | L1 Loss:0.3637805189937353 | R2:0.31859644567551665 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[44] Loss:0.2700744688510895 | L1 Loss:0.3983111321926117 | R2:-0.12568254012517383 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[45] Loss:0.21828235127031803 | L1 Loss:0.3702671341598034 | R2:0.31035552402715977 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[45] Loss:0.26401531249284743 | L1 Loss:0.3970141768455505 | R2:-0.10906655257899303 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[46] Loss:0.23570883041247725 | L1 Loss:0.3774340096861124 | R2:0.26202607803096 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[46] Loss:0.27074802219867705 | L1 Loss:0.4105977416038513 | R2:-0.07055387245564 | ACC: 69.0000%(207/300)\n",
            "Training Epoch[47] Loss:0.21979546546936035 | L1 Loss:0.3666223995387554 | R2:0.31504673026160124 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[47] Loss:0.2702622011303902 | L1 Loss:0.4073637545108795 | R2:-0.02564580855016123 | ACC: 67.0000%(201/300)\n",
            "Training Epoch[48] Loss:0.23268951289355755 | L1 Loss:0.3780253864824772 | R2:0.26695375759840523 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[48] Loss:0.258462493121624 | L1 Loss:0.40360223650932314 | R2:0.005987690275444724 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[49] Loss:0.22431340720504522 | L1 Loss:0.37152416445314884 | R2:0.28599920196187856 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[49] Loss:0.2512748628854752 | L1 Loss:0.39842215180397034 | R2:-0.021987711257619936 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[50] Loss:0.23467267956584692 | L1 Loss:0.3783695660531521 | R2:0.25453843184973896 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[50] Loss:0.2653921008110046 | L1 Loss:0.4089005708694458 | R2:-0.14555884741167216 | ACC: 66.3333%(199/300)\n",
            "Training Epoch[51] Loss:0.23279014602303505 | L1 Loss:0.38057032600045204 | R2:0.272414827144478 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[51] Loss:0.2615174427628517 | L1 Loss:0.40635415017604826 | R2:-0.13936179922914177 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[52] Loss:0.23098673205822706 | L1 Loss:0.3801701832562685 | R2:0.2656996522094219 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[52] Loss:0.2533126771450043 | L1 Loss:0.39890651404857635 | R2:0.0077312580889282055 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[53] Loss:0.23562518134713173 | L1 Loss:0.3808254189789295 | R2:0.25015929707130374 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[53] Loss:0.2559247761964798 | L1 Loss:0.4035760223865509 | R2:-0.011587994770466592 | ACC: 68.0000%(204/300)\n",
            "Training Epoch[54] Loss:0.21239279676228762 | L1 Loss:0.3617365211248398 | R2:0.3337208734252466 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[54] Loss:0.2564786016941071 | L1 Loss:0.3983342111110687 | R2:-0.06896619376278981 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[55] Loss:0.21691265376284719 | L1 Loss:0.36822385154664516 | R2:0.3202619087834465 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[55] Loss:0.2541067719459534 | L1 Loss:0.40342149436473845 | R2:-0.015397077056755127 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[56] Loss:0.2278960170224309 | L1 Loss:0.3735077492892742 | R2:0.2907006123934304 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[56] Loss:0.26805727928876877 | L1 Loss:0.4154013395309448 | R2:-0.2274305781620028 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[57] Loss:0.212089697830379 | L1 Loss:0.3629046902060509 | R2:0.33499237677262117 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[57] Loss:0.28426468223333357 | L1 Loss:0.42025556564331057 | R2:-0.34292053793629845 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[58] Loss:0.21166426688432693 | L1 Loss:0.3605423625558615 | R2:0.3307137243150681 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[58] Loss:0.261237707734108 | L1 Loss:0.4003205537796021 | R2:-0.1845250247321605 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[59] Loss:0.21599331684410572 | L1 Loss:0.3668721802532673 | R2:0.3200838034101315 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[59] Loss:0.2576099023222923 | L1 Loss:0.4057181984186172 | R2:-0.05147472471323864 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[60] Loss:0.205868701916188 | L1 Loss:0.35170053504407406 | R2:0.35518557391041705 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[60] Loss:0.25136862099170687 | L1 Loss:0.3936184287071228 | R2:-0.11312533326154846 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[61] Loss:0.20340612484142184 | L1 Loss:0.35342616960406303 | R2:0.35662638867750773 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[61] Loss:0.2657588556408882 | L1 Loss:0.4063467293977737 | R2:-0.10981203689485883 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[62] Loss:0.20570515422150493 | L1 Loss:0.35104035399854183 | R2:0.3392602648872997 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[62] Loss:0.25053214430809023 | L1 Loss:0.39411253333091734 | R2:-0.06003244132654313 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[63] Loss:0.1995795746333897 | L1 Loss:0.3496712725609541 | R2:0.3685625932952281 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[63] Loss:0.25038124322891236 | L1 Loss:0.39395267367362974 | R2:-0.036676883487197554 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[64] Loss:0.20868712291121483 | L1 Loss:0.3539031073451042 | R2:0.3395649493891166 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[64] Loss:0.2577328622341156 | L1 Loss:0.3975537300109863 | R2:-0.14937111836409317 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[65] Loss:0.2136736772954464 | L1 Loss:0.36257123574614525 | R2:0.32361682877904296 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[65] Loss:0.25377798825502396 | L1 Loss:0.382728374004364 | R2:-0.07785515042393829 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[66] Loss:0.21051744557917118 | L1 Loss:0.3581484016031027 | R2:0.32879318626742576 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[66] Loss:0.2505942076444626 | L1 Loss:0.39073833227157595 | R2:-0.0861768933951681 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[67] Loss:0.1943911616690457 | L1 Loss:0.3413666561245918 | R2:0.38894697106745846 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[67] Loss:0.2570167511701584 | L1 Loss:0.39376962184906006 | R2:-0.1356044149917036 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[68] Loss:0.20128227770328522 | L1 Loss:0.3469811361283064 | R2:0.3671721369758901 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[68] Loss:0.25984683334827424 | L1 Loss:0.39620618522167206 | R2:-0.09241658076598211 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[69] Loss:0.1894498597830534 | L1 Loss:0.33645425364375114 | R2:0.4050616633702305 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[69] Loss:0.2488752484321594 | L1 Loss:0.39263927936553955 | R2:-0.09111123512520111 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[70] Loss:0.2017202554270625 | L1 Loss:0.3481245916336775 | R2:0.3509953086191981 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[70] Loss:0.2517008751630783 | L1 Loss:0.39777455627918246 | R2:-0.11226274830039315 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[71] Loss:0.18262996058911085 | L1 Loss:0.3323684874922037 | R2:0.4257844286537448 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[71] Loss:0.2683027505874634 | L1 Loss:0.4097653716802597 | R2:-0.1558729754653579 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[72] Loss:0.19318900164216757 | L1 Loss:0.3369259536266327 | R2:0.39260802239742304 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[72] Loss:0.265778674185276 | L1 Loss:0.4040511131286621 | R2:-0.16685866831238885 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[73] Loss:0.19559209886938334 | L1 Loss:0.34410815965384245 | R2:0.3846119808122382 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[73] Loss:0.26013476103544236 | L1 Loss:0.39638465642929077 | R2:-0.17931568911510673 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[74] Loss:0.1945292390882969 | L1 Loss:0.3368349075317383 | R2:0.3862529909429629 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[74] Loss:0.26986427903175353 | L1 Loss:0.40348453223705294 | R2:-0.21809415680553315 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[75] Loss:0.21415710635483265 | L1 Loss:0.35693821124732494 | R2:0.3161651031759636 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[75] Loss:0.2437156081199646 | L1 Loss:0.38310587108135224 | R2:0.003726530049651 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[76] Loss:0.196671427693218 | L1 Loss:0.34415117278695107 | R2:0.3753090787941018 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[76] Loss:0.24763444662094117 | L1 Loss:0.38684681355953215 | R2:-0.06223439099339072 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[77] Loss:0.1975696124136448 | L1 Loss:0.346567215397954 | R2:0.36940125524873646 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[77] Loss:0.23443384021520614 | L1 Loss:0.38022182881832123 | R2:0.00813537164130813 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[78] Loss:0.1974705383181572 | L1 Loss:0.34409172274172306 | R2:0.3693030230505549 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[78] Loss:0.2693298816680908 | L1 Loss:0.40393536984920503 | R2:-0.14019022129454792 | ACC: 67.6667%(203/300)\n",
            "Training Epoch[79] Loss:0.20165390986949205 | L1 Loss:0.350618664175272 | R2:0.3625076542802345 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[79] Loss:0.2635445982217789 | L1 Loss:0.4032857120037079 | R2:-0.16941591961066038 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[80] Loss:0.20601312536746264 | L1 Loss:0.35261095501482487 | R2:0.34753056447998953 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[80] Loss:0.2619158074259758 | L1 Loss:0.40158946216106417 | R2:-0.18754863848339326 | ACC: 68.6667%(206/300)\n",
            "Training Epoch[81] Loss:0.21830840315669775 | L1 Loss:0.36211929097771645 | R2:0.30580061324687935 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[81] Loss:0.26258105635643003 | L1 Loss:0.3958979308605194 | R2:-0.17994746985828197 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[82] Loss:0.2127960417419672 | L1 Loss:0.35531700029969215 | R2:0.3279051330010263 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[82] Loss:0.25107661485671995 | L1 Loss:0.38423742949962614 | R2:-0.06437815875554742 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[83] Loss:0.2111806431785226 | L1 Loss:0.35864052549004555 | R2:0.3268654139059052 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[83] Loss:0.2535670563578606 | L1 Loss:0.3881229192018509 | R2:-0.09084909213454688 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[84] Loss:0.2155171036720276 | L1 Loss:0.3569618947803974 | R2:0.31598209011135975 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[84] Loss:0.254886668920517 | L1 Loss:0.3952104240655899 | R2:-0.09690358423805216 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[85] Loss:0.20077538769692183 | L1 Loss:0.34786084294319153 | R2:0.3620312555184897 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[85] Loss:0.24448743909597398 | L1 Loss:0.38449235558509826 | R2:-0.013188205676146047 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[86] Loss:0.2088598357513547 | L1 Loss:0.35828652046620846 | R2:0.33022336196962765 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[86] Loss:0.24035694003105162 | L1 Loss:0.37806341648101804 | R2:-0.020476607523511083 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[87] Loss:0.19139389786869287 | L1 Loss:0.3408808037638664 | R2:0.3927362281349456 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[87] Loss:0.24533158540725708 | L1 Loss:0.386537104845047 | R2:-0.04492937823891587 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[88] Loss:0.20398839749395847 | L1 Loss:0.35482815839350224 | R2:0.34970052034285387 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[88] Loss:0.2582665517926216 | L1 Loss:0.404383584856987 | R2:-0.1846661308046175 | ACC: 68.3333%(205/300)\n",
            "Training Epoch[89] Loss:0.213503603823483 | L1 Loss:0.35998948477208614 | R2:0.31187729668575465 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[89] Loss:0.24587473273277283 | L1 Loss:0.3882334679365158 | R2:-0.06265782428036223 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[90] Loss:0.20700987288728356 | L1 Loss:0.3534387703984976 | R2:0.33997199362509567 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[90] Loss:0.243800613284111 | L1 Loss:0.39091330766677856 | R2:-0.10877391077261889 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[91] Loss:0.2042462876997888 | L1 Loss:0.3537631183862686 | R2:0.34193952023923946 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[91] Loss:0.23420182168483733 | L1 Loss:0.3895130425691605 | R2:-0.019269746347735907 | ACC: 71.0000%(213/300)\n",
            "Training Epoch[92] Loss:0.1984350960701704 | L1 Loss:0.34375216625630856 | R2:0.3616865842666307 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[92] Loss:0.23587813079357148 | L1 Loss:0.383453431725502 | R2:0.036931613282132146 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[93] Loss:0.20795916393399239 | L1 Loss:0.34815334901213646 | R2:0.3358503093853673 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[93] Loss:0.22628291547298432 | L1 Loss:0.37377047836780547 | R2:0.0958298972810395 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[94] Loss:0.2034721989184618 | L1 Loss:0.3491386380046606 | R2:0.3564579777487163 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[94] Loss:0.2453520879149437 | L1 Loss:0.3861427128314972 | R2:-0.029003150580230487 | ACC: 69.6667%(209/300)\n",
            "Training Epoch[95] Loss:0.20897466596215963 | L1 Loss:0.35696629248559475 | R2:0.33754819991091534 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[95] Loss:0.22419714778661728 | L1 Loss:0.3739503473043442 | R2:0.08412093932389389 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[96] Loss:0.19860492832958698 | L1 Loss:0.3430492617189884 | R2:0.36869745262881226 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[96] Loss:0.2203592985868454 | L1 Loss:0.36857505440711974 | R2:0.0519221594710523 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[97] Loss:0.19473350420594215 | L1 Loss:0.33861696906387806 | R2:0.38025076499921756 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[97] Loss:0.2336642012000084 | L1 Loss:0.37998571395874026 | R2:-0.049116207253799796 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[98] Loss:0.1931802136823535 | L1 Loss:0.3407818730920553 | R2:0.38584319189833954 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[98] Loss:0.22590822726488113 | L1 Loss:0.37769546210765836 | R2:0.017969774833465735 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[99] Loss:0.20335367880761623 | L1 Loss:0.3451413344591856 | R2:0.34329006921586713 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[99] Loss:0.23410993367433547 | L1 Loss:0.3809837013483047 | R2:0.0039027324963213593 | ACC: 69.3333%(208/300)\n",
            "Training Epoch[100] Loss:0.19997838232666254 | L1 Loss:0.3470175825059414 | R2:0.3568162304317195 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[100] Loss:0.23551167994737626 | L1 Loss:0.3847134649753571 | R2:0.006022513524105965 | ACC: 70.3333%(211/300)\n",
            "Training Epoch[101] Loss:0.20859671104699373 | L1 Loss:0.35756104439496994 | R2:0.3344288909986627 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[101] Loss:0.21633267253637314 | L1 Loss:0.36574083268642427 | R2:0.07684114381755429 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[102] Loss:0.19151919335126877 | L1 Loss:0.3379427623003721 | R2:0.38389230565404453 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[102] Loss:0.23616336062550544 | L1 Loss:0.3798920810222626 | R2:0.08143824646042001 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[103] Loss:0.1985283773392439 | L1 Loss:0.3386306017637253 | R2:0.36091069973345435 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[103] Loss:0.23001716881990433 | L1 Loss:0.3821467012166977 | R2:0.055718100687387785 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[104] Loss:0.1996227614581585 | L1 Loss:0.3420384917408228 | R2:0.3558549557818321 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[104] Loss:0.22829024121165276 | L1 Loss:0.3756955459713936 | R2:0.07786801979046051 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[105] Loss:0.19678079709410667 | L1 Loss:0.34562043845653534 | R2:0.37363181906719567 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[105] Loss:0.2280684605240822 | L1 Loss:0.3825815230607986 | R2:0.04119041797058927 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[106] Loss:0.1952175348997116 | L1 Loss:0.34161724895238876 | R2:0.3715319786541941 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[106] Loss:0.23322077989578247 | L1 Loss:0.3875553965568542 | R2:0.002568950187461905 | ACC: 70.6667%(212/300)\n",
            "Training Epoch[107] Loss:0.1861212132498622 | L1 Loss:0.3393206521868706 | R2:0.4005975440689154 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[107] Loss:0.22569470703601838 | L1 Loss:0.3731609761714935 | R2:0.06957503084341465 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[108] Loss:0.1943103726953268 | L1 Loss:0.34589115530252457 | R2:0.37378917555582275 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[108] Loss:0.21942102313041686 | L1 Loss:0.37012421786785127 | R2:0.10664841376227327 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[109] Loss:0.19378329813480377 | L1 Loss:0.3371041268110275 | R2:0.37928719093353785 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[109] Loss:0.23090270310640335 | L1 Loss:0.3793043941259384 | R2:-0.013879065849950667 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[110] Loss:0.18840699549764395 | L1 Loss:0.3336411714553833 | R2:0.3920688604798911 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[110] Loss:0.22893106192350388 | L1 Loss:0.381673538684845 | R2:-0.0018053934828994756 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[111] Loss:0.1970996968448162 | L1 Loss:0.34783639945089817 | R2:0.3726652568162353 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[111] Loss:0.2368098847568035 | L1 Loss:0.39199924767017363 | R2:-0.03782692312321543 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[112] Loss:0.18471235781908035 | L1 Loss:0.33122288808226585 | R2:0.4162587926769461 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[112] Loss:0.21994187980890273 | L1 Loss:0.3751146733760834 | R2:0.10767615798575769 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[113] Loss:0.18646399024873972 | L1 Loss:0.3259563725441694 | R2:0.4042111876390423 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[113] Loss:0.21186127364635468 | L1 Loss:0.367697548866272 | R2:0.16303259114678456 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[114] Loss:0.20005754474550486 | L1 Loss:0.34442429803311825 | R2:0.3621566933504592 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[114] Loss:0.23009276986122132 | L1 Loss:0.38087025880813596 | R2:-0.018835777519424425 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[115] Loss:0.19168833084404469 | L1 Loss:0.3412946090102196 | R2:0.39101540582453265 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[115] Loss:0.22625109702348709 | L1 Loss:0.37757526636123656 | R2:0.04732472843259163 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[116] Loss:0.1947271851822734 | L1 Loss:0.3386636935174465 | R2:0.38783411002103263 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[116] Loss:0.22525091767311095 | L1 Loss:0.3822179913520813 | R2:0.035997491339270216 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[117] Loss:0.19692614022642374 | L1 Loss:0.3382932413369417 | R2:0.37787500001780433 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[117] Loss:0.23948462903499604 | L1 Loss:0.38696602582931516 | R2:-0.021349425818744595 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[118] Loss:0.19224785640835762 | L1 Loss:0.33546600677073 | R2:0.3894215311012318 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[118] Loss:0.2276524730026722 | L1 Loss:0.37790735363960265 | R2:0.018598620507714214 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[119] Loss:0.19793555233627558 | L1 Loss:0.3433750905096531 | R2:0.37206734453056045 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[119] Loss:0.23132158815860748 | L1 Loss:0.3768292307853699 | R2:0.04853685246290738 | ACC: 72.0000%(216/300)\n",
            "Training Epoch[120] Loss:0.1981717413291335 | L1 Loss:0.3417010623961687 | R2:0.3641549200110938 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[120] Loss:0.22598531544208528 | L1 Loss:0.3725235790014267 | R2:0.091565692667074 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[121] Loss:0.20282546430826187 | L1 Loss:0.3499372135847807 | R2:0.355117852855969 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[121] Loss:0.23119453340768814 | L1 Loss:0.37741456627845765 | R2:-0.008635193421057341 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[122] Loss:0.19842910300940275 | L1 Loss:0.3430321402847767 | R2:0.3721195092960222 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[122] Loss:0.2298460692167282 | L1 Loss:0.37132092416286466 | R2:-0.009237534803722403 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[123] Loss:0.19274466391652822 | L1 Loss:0.33854122646152973 | R2:0.3875435489973296 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[123] Loss:0.22485439628362655 | L1 Loss:0.37357168793678286 | R2:0.10608725726078902 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[124] Loss:0.20474448800086975 | L1 Loss:0.347599521279335 | R2:0.3435821080618364 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[124] Loss:0.23702210262417794 | L1 Loss:0.3821491912007332 | R2:0.031620667625892815 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[125] Loss:0.18921236135065556 | L1 Loss:0.33569007739424706 | R2:0.39463216242339527 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[125] Loss:0.22740356996655464 | L1 Loss:0.3726381778717041 | R2:0.11115763987386398 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[126] Loss:0.1950305625796318 | L1 Loss:0.340190589427948 | R2:0.3810478750572015 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[126] Loss:0.23481981158256532 | L1 Loss:0.3829813450574875 | R2:-0.03470803212414388 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[127] Loss:0.19137737713754177 | L1 Loss:0.33423832803964615 | R2:0.3908656755990181 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[127] Loss:0.21895873323082923 | L1 Loss:0.36273048222064974 | R2:0.1358479347609456 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[128] Loss:0.18327741138637066 | L1 Loss:0.3226562365889549 | R2:0.41885035913971086 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[128] Loss:0.21254067420959472 | L1 Loss:0.3563309073448181 | R2:0.1625444561657073 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[129] Loss:0.18373161274939775 | L1 Loss:0.3286452814936638 | R2:0.41611060957105345 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[129] Loss:0.2255066603422165 | L1 Loss:0.3752483606338501 | R2:0.07049257878143797 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[130] Loss:0.19543044548481703 | L1 Loss:0.3372851237654686 | R2:0.38340180509607574 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[130] Loss:0.21693962067365646 | L1 Loss:0.37174178957939147 | R2:0.09646047557298698 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[131] Loss:0.18372905626893044 | L1 Loss:0.33077922835946083 | R2:0.41565830334697806 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[131] Loss:0.22562415301799774 | L1 Loss:0.37564446330070494 | R2:0.06236663004401586 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[132] Loss:0.18417619168758392 | L1 Loss:0.331869650632143 | R2:0.41631361375191894 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[132] Loss:0.2311454251408577 | L1 Loss:0.373234298825264 | R2:-0.02666844813831557 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[133] Loss:0.1845894455909729 | L1 Loss:0.32744927145540714 | R2:0.4176530350539305 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[133] Loss:0.22062019258737564 | L1 Loss:0.3691971778869629 | R2:0.07998789479114868 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[134] Loss:0.19198098313063383 | L1 Loss:0.33815633319318295 | R2:0.3869598049339224 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[134] Loss:0.22831363528966903 | L1 Loss:0.37233637273311615 | R2:0.057759307919250255 | ACC: 71.6667%(215/300)\n",
            "Training Epoch[135] Loss:0.18784878263249993 | L1 Loss:0.3274166341871023 | R2:0.399712825418289 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[135] Loss:0.22752808928489685 | L1 Loss:0.3753012716770172 | R2:0.05369439849720705 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[136] Loss:0.19079369492828846 | L1 Loss:0.33819805830717087 | R2:0.39738740148450225 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[136] Loss:0.2300588980317116 | L1 Loss:0.37848607301712034 | R2:0.02292747870852572 | ACC: 70.0000%(210/300)\n",
            "Training Epoch[137] Loss:0.18962874775752425 | L1 Loss:0.3372882381081581 | R2:0.40032293979927275 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[137] Loss:0.2359587885439396 | L1 Loss:0.38218180537223817 | R2:0.0959324065513063 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[138] Loss:0.186689720954746 | L1 Loss:0.32947084680199623 | R2:0.40895032945326937 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[138] Loss:0.22860134169459342 | L1 Loss:0.3719244062900543 | R2:0.06848758388208402 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[139] Loss:0.1923976168036461 | L1 Loss:0.334061935544014 | R2:0.3782239015839277 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[139] Loss:0.21293383464217186 | L1 Loss:0.3644735336303711 | R2:0.19649911075023888 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[140] Loss:0.193254463840276 | L1 Loss:0.33341892063617706 | R2:0.37967311898706335 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[140] Loss:0.22190464437007903 | L1 Loss:0.37044805884361265 | R2:0.13539568717108558 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[141] Loss:0.18776019755750895 | L1 Loss:0.3351184483617544 | R2:0.40583628971034696 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[141] Loss:0.21279566287994384 | L1 Loss:0.35993788540363314 | R2:0.09954830924388762 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[142] Loss:0.1874199598096311 | L1 Loss:0.32861958630383015 | R2:0.4033240114857506 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[142] Loss:0.20303244218230249 | L1 Loss:0.35075069665908815 | R2:0.16308228894210827 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[143] Loss:0.20272202184423804 | L1 Loss:0.3428029790520668 | R2:0.35726093465702635 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[143] Loss:0.22336710691452027 | L1 Loss:0.37304768413305284 | R2:0.06976524059365205 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[144] Loss:0.18757406901568174 | L1 Loss:0.33542306534945965 | R2:0.4015868688282777 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[144] Loss:0.2253894180059433 | L1 Loss:0.3731694847345352 | R2:0.044475212083524604 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[145] Loss:0.18346030730754137 | L1 Loss:0.33255874924361706 | R2:0.42014506178754485 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[145] Loss:0.213186876475811 | L1 Loss:0.36350185573101046 | R2:0.14365034322311146 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[146] Loss:0.19062191247940063 | L1 Loss:0.33814193680882454 | R2:0.39139158752844116 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[146] Loss:0.21026307791471482 | L1 Loss:0.3604844182729721 | R2:0.1304352859236447 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[147] Loss:0.19089407054707408 | L1 Loss:0.34105752035975456 | R2:0.3974646001671842 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[147] Loss:0.22730296701192856 | L1 Loss:0.3670147180557251 | R2:0.0376269254204864 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[148] Loss:0.19226012006402016 | L1 Loss:0.3437026161700487 | R2:0.3824750852686296 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[148] Loss:0.21355398148298263 | L1 Loss:0.3631781965494156 | R2:0.1150930854901949 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[149] Loss:0.1907387189567089 | L1 Loss:0.3360300660133362 | R2:0.39786706877412326 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[149] Loss:0.21916665583848954 | L1 Loss:0.3653746247291565 | R2:0.10495697120701974 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[150] Loss:0.19221737701445818 | L1 Loss:0.3316049613058567 | R2:0.3858329516293657 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[150] Loss:0.21754493564367294 | L1 Loss:0.36446482837200167 | R2:0.08919348775790077 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[151] Loss:0.18675271747633815 | L1 Loss:0.33461362682282925 | R2:0.4047843527236208 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[151] Loss:0.21233559399843216 | L1 Loss:0.369433668255806 | R2:0.14201542796118688 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[152] Loss:0.18307252321392298 | L1 Loss:0.3313378430902958 | R2:0.4155744708238577 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[152] Loss:0.21902773082256316 | L1 Loss:0.36963153779506686 | R2:0.17305597200515405 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[153] Loss:0.18687562132254243 | L1 Loss:0.33769870921969414 | R2:0.4079662302314321 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[153] Loss:0.22244053706526756 | L1 Loss:0.37052049934864045 | R2:0.050241380773021135 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[154] Loss:0.18658420536667109 | L1 Loss:0.33064040914177895 | R2:0.40576767647230555 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[154] Loss:0.2200247436761856 | L1 Loss:0.3699145019054413 | R2:0.13126165306272247 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[155] Loss:0.18738224636763334 | L1 Loss:0.3345951046794653 | R2:0.40848460750958726 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[155] Loss:0.2210286259651184 | L1 Loss:0.3679726004600525 | R2:0.11847417602135857 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[156] Loss:0.19002343714237213 | L1 Loss:0.33476847037672997 | R2:0.39598838587403085 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[156] Loss:0.22158498615026473 | L1 Loss:0.36862511932849884 | R2:0.07202382739873787 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[157] Loss:0.19114490551874042 | L1 Loss:0.3329754192382097 | R2:0.395323591524833 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[157] Loss:0.21913149505853652 | L1 Loss:0.37149629294872283 | R2:0.09227603699662149 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[158] Loss:0.19394862418994308 | L1 Loss:0.34199741296470165 | R2:0.3825668941561816 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[158] Loss:0.20870010256767274 | L1 Loss:0.36106460094451903 | R2:0.20665463651278512 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[159] Loss:0.19453924428671598 | L1 Loss:0.34027754329144955 | R2:0.3755767320700601 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[159] Loss:0.22263813018798828 | L1 Loss:0.36897504329681396 | R2:0.14199851075668796 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[160] Loss:0.18930571153759956 | L1 Loss:0.3331070505082607 | R2:0.3923113671081905 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[160] Loss:0.21337335407733918 | L1 Loss:0.3598228573799133 | R2:0.14708441247391862 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[161] Loss:0.19407886173576117 | L1 Loss:0.3359682969748974 | R2:0.3769460144677124 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[161] Loss:0.21402304768562316 | L1 Loss:0.36425513625144956 | R2:0.18412128424542584 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[162] Loss:0.18474552175030112 | L1 Loss:0.33136823400855064 | R2:0.4103845617100675 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[162] Loss:0.21879338547587396 | L1 Loss:0.366450771689415 | R2:0.12074756057585141 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[163] Loss:0.18617603462189436 | L1 Loss:0.3315091449767351 | R2:0.40512052324151715 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[163] Loss:0.2119999498128891 | L1 Loss:0.3663319796323776 | R2:0.15582060343597132 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[164] Loss:0.19218685571104288 | L1 Loss:0.33625063486397266 | R2:0.38642443577689134 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[164] Loss:0.21109917461872102 | L1 Loss:0.3617849797010422 | R2:0.15182024661054108 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[165] Loss:0.19321262696757913 | L1 Loss:0.33810629695653915 | R2:0.38452358626582284 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[165] Loss:0.20747668594121932 | L1 Loss:0.35543409585952757 | R2:0.16455826127748452 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[166] Loss:0.1817279290407896 | L1 Loss:0.3270806074142456 | R2:0.4213476722467173 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[166] Loss:0.22476999163627626 | L1 Loss:0.3748910754919052 | R2:0.07533828205785631 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[167] Loss:0.1884494535624981 | L1 Loss:0.3342558601871133 | R2:0.40072794396962697 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[167] Loss:0.22028395533561707 | L1 Loss:0.3707153260707855 | R2:0.09489142531875458 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[168] Loss:0.1782925925217569 | L1 Loss:0.32780327275395393 | R2:0.43693934167644766 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[168] Loss:0.20808248072862626 | L1 Loss:0.3600267797708511 | R2:0.18901707337288376 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[169] Loss:0.1812161742709577 | L1 Loss:0.3266574013978243 | R2:0.4274682434498479 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[169] Loss:0.21525256037712098 | L1 Loss:0.3631667077541351 | R2:0.1811059242060348 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[170] Loss:0.18612039182335138 | L1 Loss:0.33492714539170265 | R2:0.41241713507349304 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[170] Loss:0.2189696177840233 | L1 Loss:0.3732584238052368 | R2:0.16067879920793976 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[171] Loss:0.18338393094018102 | L1 Loss:0.3328200411051512 | R2:0.4096295199667962 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[171] Loss:0.21001278832554818 | L1 Loss:0.36295250058174133 | R2:0.2145722210029589 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[172] Loss:0.18653934728354216 | L1 Loss:0.3326473478227854 | R2:0.4031800838238798 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[172] Loss:0.20618061423301698 | L1 Loss:0.35478740334510805 | R2:0.2141284356599666 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[173] Loss:0.1814790042117238 | L1 Loss:0.32954167015850544 | R2:0.42093333667323873 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[173] Loss:0.22227704375982285 | L1 Loss:0.37103596031665803 | R2:0.11984134607387142 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[174] Loss:0.18369599198922515 | L1 Loss:0.33076145127415657 | R2:0.41829012083633293 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[174] Loss:0.20522329807281495 | L1 Loss:0.346773961186409 | R2:0.25662598601705594 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[175] Loss:0.17496788688004017 | L1 Loss:0.3254005163908005 | R2:0.44646829611404326 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[175] Loss:0.2046014204621315 | L1 Loss:0.3537438154220581 | R2:0.1898494957846142 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[176] Loss:0.18233685102313757 | L1 Loss:0.3293995186686516 | R2:0.4153992075669038 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[176] Loss:0.2099607393145561 | L1 Loss:0.3553656667470932 | R2:0.19483988319276105 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[177] Loss:0.18579118559136987 | L1 Loss:0.33109762892127037 | R2:0.4090373089029269 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[177] Loss:0.2126417115330696 | L1 Loss:0.3624517098069191 | R2:0.14602966291063332 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[178] Loss:0.17675346555188298 | L1 Loss:0.32816330529749393 | R2:0.44145039887271814 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[178] Loss:0.20341480076313018 | L1 Loss:0.3540895700454712 | R2:0.18960991057689713 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[179] Loss:0.18607442500069737 | L1 Loss:0.33564145117998123 | R2:0.4090190813499485 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[179] Loss:0.20826613456010817 | L1 Loss:0.3617632299661636 | R2:0.16825421746575206 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[180] Loss:0.17700938694179058 | L1 Loss:0.3269849009811878 | R2:0.43910581717666486 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[180] Loss:0.20579032599925995 | L1 Loss:0.35707288682460786 | R2:0.2078690706662482 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[181] Loss:0.18729256745427847 | L1 Loss:0.33234808407723904 | R2:0.40025936452380706 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[181] Loss:0.2036367490887642 | L1 Loss:0.35255122780799864 | R2:0.2085483766225747 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[182] Loss:0.183368728030473 | L1 Loss:0.3285383749753237 | R2:0.4166945874484417 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[182] Loss:0.19964195787906647 | L1 Loss:0.35375154614448545 | R2:0.25553085613620485 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[183] Loss:0.18258208828046918 | L1 Loss:0.328390846028924 | R2:0.42176766347385664 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[183] Loss:0.20379514694213868 | L1 Loss:0.3541657507419586 | R2:0.21587892454510565 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[184] Loss:0.18134581111371517 | L1 Loss:0.3309138659387827 | R2:0.4265964584111469 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[184] Loss:0.20516592189669608 | L1 Loss:0.3557006925344467 | R2:0.2005712160386087 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[185] Loss:0.18306965846568346 | L1 Loss:0.3275331426411867 | R2:0.4228023021693738 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[185] Loss:0.21541527584195136 | L1 Loss:0.37118864357471465 | R2:0.12490597155659663 | ACC: 72.3333%(217/300)\n",
            "Training Epoch[186] Loss:0.1821247790940106 | L1 Loss:0.3290718551725149 | R2:0.42230960854476374 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[186] Loss:0.20722756087779998 | L1 Loss:0.36314005553722384 | R2:0.1721427163402931 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[187] Loss:0.1758320932276547 | L1 Loss:0.32290216349065304 | R2:0.4421936697190945 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[187] Loss:0.21043796390295028 | L1 Loss:0.36659455895423887 | R2:0.1756048249560999 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[188] Loss:0.18342440063133836 | L1 Loss:0.3297082222998142 | R2:0.41677620715248037 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[188] Loss:0.20967302173376084 | L1 Loss:0.36570564806461336 | R2:0.18990985088967177 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[189] Loss:0.18584318924695253 | L1 Loss:0.33170194178819656 | R2:0.41144733090816377 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[189] Loss:0.19840391278266906 | L1 Loss:0.34775558710098264 | R2:0.23099858916138763 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[190] Loss:0.18692354625090957 | L1 Loss:0.3325460199266672 | R2:0.4054826155335315 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[190] Loss:0.20754344016313553 | L1 Loss:0.361069992184639 | R2:0.13002271368864043 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[191] Loss:0.18612125795334578 | L1 Loss:0.32419850304722786 | R2:0.40919380872920047 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[191] Loss:0.2010717511177063 | L1 Loss:0.3537627667188644 | R2:0.22694388395667375 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[192] Loss:0.17816891567781568 | L1 Loss:0.32217102125287056 | R2:0.43425379881905646 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[192] Loss:0.19679106324911116 | L1 Loss:0.3466374784708023 | R2:0.2529261356249368 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[193] Loss:0.18012942327186465 | L1 Loss:0.3256035465747118 | R2:0.4302346806336814 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[193] Loss:0.21063371300697326 | L1 Loss:0.36181644201278684 | R2:0.20320335121673888 | ACC: 73.0000%(219/300)\n",
            "Training Epoch[194] Loss:0.1803723550401628 | L1 Loss:0.33082581125199795 | R2:0.43170750608376884 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[194] Loss:0.2029554843902588 | L1 Loss:0.35831978619098664 | R2:0.19043254912978522 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[195] Loss:0.17602221900597215 | L1 Loss:0.32932309061288834 | R2:0.44522627675376913 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[195] Loss:0.20126714259386064 | L1 Loss:0.3555022329092026 | R2:0.16726062799734892 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[196] Loss:0.18095772666856647 | L1 Loss:0.328933609649539 | R2:0.4249267902675653 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[196] Loss:0.20269514471292496 | L1 Loss:0.34920883774757383 | R2:0.2059382453809857 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[197] Loss:0.17957892687991261 | L1 Loss:0.3290159907191992 | R2:0.4290867611413406 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[197] Loss:0.20503466576337814 | L1 Loss:0.35741124153137205 | R2:0.18892322343200596 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[198] Loss:0.1850218614563346 | L1 Loss:0.3286630157381296 | R2:0.41200940458803353 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[198] Loss:0.21269240975379944 | L1 Loss:0.36066338419914246 | R2:0.1605981951899212 | ACC: 72.6667%(218/300)\n",
            "Training Epoch[199] Loss:0.18422106187790632 | L1 Loss:0.3279115743935108 | R2:0.41744674786828145 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[199] Loss:0.20528821647167206 | L1 Loss:0.35545015037059785 | R2:0.16170699863503457 | ACC: 77.6667%(233/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "# train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "# val_acc_record = []\n",
        "# val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "# test_acc_record = []\n",
        "# test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(100)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in (enumerate(dataloader)):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    digits = model_class(data)\n",
        "    \n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    digits = digits.to(torch.float32).squeeze()\n",
        "    label = label.type(torch.float32).cuda()\n",
        "    L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "    \n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # if epoch < 10:\n",
        "    #   loss.backward()\n",
        "    # else:\n",
        "    #   L1_loss.backward()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  \n",
        "  train_loss_record.append(loss.item())\n",
        "  train_loss_l1_record.append(L1_loss.item())\n",
        "\n",
        "  val_loss, val_acc, val_r2, val_l1 = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  # train_acc_record.append(train_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  val_r2_record.append(val_r2)\n",
        "  val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc, test_r2, test_l1 = test_acc_output(epoch)\n",
        "  # test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  test_r2_record.append(test_r2)\n",
        "  test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ebede9-8852-4341-979e-9eb2d8649db2",
        "id": "MGbzMrlr99e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.1792503520846367 | L1 Loss:0.318916454911232 | R2:0.42235851608198155 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[0] Loss:0.21110317707061768 | L1 Loss:0.3559573829174042 | R2:0.18391239896399875 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[1] Loss:0.1840020762756467 | L1 Loss:0.32465980388224125 | R2:0.41161924329369515 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[1] Loss:0.2065364971756935 | L1 Loss:0.35401942431926725 | R2:0.15988851982220903 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[2] Loss:0.17866325238719583 | L1 Loss:0.32482163794338703 | R2:0.43223496792760685 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[2] Loss:0.2095300853252411 | L1 Loss:0.359379968047142 | R2:0.2299105260451027 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[3] Loss:0.17630019085481763 | L1 Loss:0.31945849768817425 | R2:0.43553953278292723 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[3] Loss:0.20743122547864914 | L1 Loss:0.35229367911815646 | R2:0.11278857833492931 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[4] Loss:0.18008623318746686 | L1 Loss:0.323214091360569 | R2:0.42624346042780575 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[4] Loss:0.21846072301268576 | L1 Loss:0.36248535215854644 | R2:0.07804853616694803 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[5] Loss:0.17974242335185409 | L1 Loss:0.32750021666288376 | R2:0.42557991821321983 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[5] Loss:0.22195070162415503 | L1 Loss:0.3649343058466911 | R2:0.09605945955017078 | ACC: 71.3333%(214/300)\n",
            "Training Epoch[6] Loss:0.18273593485355377 | L1 Loss:0.3272496536374092 | R2:0.4222010523941729 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[6] Loss:0.20576176941394805 | L1 Loss:0.35026370286941527 | R2:0.1551067959393352 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[7] Loss:0.1784199788235128 | L1 Loss:0.3243572488427162 | R2:0.43554980922336234 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[7] Loss:0.20897321924567222 | L1 Loss:0.3564242720603943 | R2:0.14067471799416822 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[8] Loss:0.18410741072148085 | L1 Loss:0.323081842623651 | R2:0.4157203550255115 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[8] Loss:0.21542673334479331 | L1 Loss:0.3628082424402237 | R2:0.14898286318799775 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[9] Loss:0.18615863006561995 | L1 Loss:0.326682073995471 | R2:0.4098010578351009 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[9] Loss:0.20952247381210326 | L1 Loss:0.3554060488939285 | R2:0.1507902009019523 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[10] Loss:0.1895186686888337 | L1 Loss:0.3320170268416405 | R2:0.3965252314631127 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[10] Loss:0.20827405378222466 | L1 Loss:0.35358245521783827 | R2:0.1647979997176608 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[11] Loss:0.17695728037506342 | L1 Loss:0.32363252714276314 | R2:0.4386407742113987 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[11] Loss:0.19596533328294755 | L1 Loss:0.3492030829191208 | R2:0.21612519249628587 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[12] Loss:0.176099656149745 | L1 Loss:0.32350850105285645 | R2:0.440533562186496 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[12] Loss:0.20965696200728418 | L1 Loss:0.3614507704973221 | R2:0.10914760149384387 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[13] Loss:0.17777728009968996 | L1 Loss:0.3247948419302702 | R2:0.4323909385749336 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[13] Loss:0.20599443167448045 | L1 Loss:0.3481693476438522 | R2:0.17519756445486187 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[14] Loss:0.17783154733479023 | L1 Loss:0.3250787351280451 | R2:0.43435099201794036 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[14] Loss:0.2070453368127346 | L1 Loss:0.35766723603010175 | R2:0.06911781518857163 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[15] Loss:0.178551958873868 | L1 Loss:0.3213099855929613 | R2:0.43235247902438245 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[15] Loss:0.21285016238689422 | L1 Loss:0.35633803009986875 | R2:0.10888925443913564 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[16] Loss:0.1809648834168911 | L1 Loss:0.3282248508185148 | R2:0.4258157538909007 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[16] Loss:0.20077619925141335 | L1 Loss:0.35251061618328094 | R2:0.19919109703786247 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[17] Loss:0.18468352686613798 | L1 Loss:0.32958369702100754 | R2:0.41475763080251743 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[17] Loss:0.20432633012533188 | L1 Loss:0.3546117275953293 | R2:0.14778372119386737 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[18] Loss:0.17828907119110227 | L1 Loss:0.32793210074305534 | R2:0.43143220163459367 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[18] Loss:0.20851651579141617 | L1 Loss:0.3518746465444565 | R2:0.14043266426066198 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[19] Loss:0.1848644227720797 | L1 Loss:0.32837252877652645 | R2:0.4066012611580352 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[19] Loss:0.20208663642406463 | L1 Loss:0.3436281353235245 | R2:0.15226512639074968 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[20] Loss:0.18773270724341273 | L1 Loss:0.3337366674095392 | R2:0.40126082142825076 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[20] Loss:0.2095115527510643 | L1 Loss:0.3558940201997757 | R2:0.1226699754022325 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[21] Loss:0.17670774087309837 | L1 Loss:0.32301178108900785 | R2:0.44057688213063223 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[21] Loss:0.20183204412460326 | L1 Loss:0.3499909371137619 | R2:0.17318146419453498 | ACC: 78.3333%(235/300)\n",
            "Training Epoch[22] Loss:0.18502218509092927 | L1 Loss:0.33364298194646835 | R2:0.4128286142341515 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[22] Loss:0.20018587708473207 | L1 Loss:0.34746154844760896 | R2:0.1401446199224845 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[23] Loss:0.18525605322793126 | L1 Loss:0.3330986350774765 | R2:0.4070659070973019 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[23] Loss:0.21135660111904145 | L1 Loss:0.35934823751449585 | R2:0.11933094538780513 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[24] Loss:0.17554004024714231 | L1 Loss:0.3197202943265438 | R2:0.44528622313343613 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[24] Loss:0.2009635090827942 | L1 Loss:0.35149946212768557 | R2:0.18452063020183487 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[25] Loss:0.1782108903862536 | L1 Loss:0.3177125118672848 | R2:0.4303607956258005 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[25] Loss:0.20367996841669084 | L1 Loss:0.35322930216789244 | R2:0.19537572872859416 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[26] Loss:0.17451817030087113 | L1 Loss:0.32327608950436115 | R2:0.44033920960930384 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[26] Loss:0.20284675508737565 | L1 Loss:0.35205980837345124 | R2:0.14616432082952063 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[27] Loss:0.18048398150131106 | L1 Loss:0.32734891399741173 | R2:0.4283510509624742 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[27] Loss:0.20876239687204362 | L1 Loss:0.3576627016067505 | R2:0.1329060678214363 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[28] Loss:0.18352648429572582 | L1 Loss:0.3277815841138363 | R2:0.41957073427646757 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[28] Loss:0.20607564598321915 | L1 Loss:0.3524254381656647 | R2:0.17903731313732077 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[29] Loss:0.18280667625367641 | L1 Loss:0.3289712369441986 | R2:0.41847665323404487 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[29] Loss:0.21622442156076432 | L1 Loss:0.36262232065200806 | R2:0.07461733090991504 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[30] Loss:0.1754690152592957 | L1 Loss:0.3179233381524682 | R2:0.43773848749860933 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[30] Loss:0.2033611461520195 | L1 Loss:0.3521135985851288 | R2:0.16653529750946974 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[31] Loss:0.1723999590612948 | L1 Loss:0.3167451322078705 | R2:0.45106284055684054 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[31] Loss:0.2132283106446266 | L1 Loss:0.36213861107826234 | R2:0.11478616369377179 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[32] Loss:0.18008282640948892 | L1 Loss:0.3232492208480835 | R2:0.4251427107583384 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[32] Loss:0.20886130183935164 | L1 Loss:0.35951625406742094 | R2:0.13959012524368383 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[33] Loss:0.18500963784754276 | L1 Loss:0.3261072598397732 | R2:0.40993594585588394 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[33] Loss:0.1975571945309639 | L1 Loss:0.34264416694641114 | R2:0.17461879554579585 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[34] Loss:0.18587292591109872 | L1 Loss:0.3324496429413557 | R2:0.4043770851532639 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[34] Loss:0.20503010973334312 | L1 Loss:0.35009179413318636 | R2:0.16957537281673624 | ACC: 78.0000%(234/300)\n",
            "Training Epoch[35] Loss:0.18099709693342447 | L1 Loss:0.32528912648558617 | R2:0.42540767677903235 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[35] Loss:0.21700752079486846 | L1 Loss:0.3551895976066589 | R2:0.09475931922631275 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[36] Loss:0.18470871169120073 | L1 Loss:0.33009472116827965 | R2:0.41757105589123195 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[36] Loss:0.21213798373937606 | L1 Loss:0.35957007110118866 | R2:0.11922514012162826 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[37] Loss:0.18226983211934566 | L1 Loss:0.32995800487697124 | R2:0.41951669885207754 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[37] Loss:0.20510198175907135 | L1 Loss:0.35071728229522703 | R2:0.11500398741507283 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[38] Loss:0.1822764789685607 | L1 Loss:0.3278390225023031 | R2:0.4196625053470753 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[38] Loss:0.20665836334228516 | L1 Loss:0.3552266061306 | R2:0.145863834394039 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[39] Loss:0.17391219548881054 | L1 Loss:0.3207577746361494 | R2:0.4545609796768617 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[39] Loss:0.20560189709067345 | L1 Loss:0.3461966246366501 | R2:0.15009940647076242 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[40] Loss:0.17884299252182245 | L1 Loss:0.3257527593523264 | R2:0.42970712507529474 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[40] Loss:0.20686361491680144 | L1 Loss:0.34811781346797943 | R2:0.1254526836839903 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[41] Loss:0.17724998947232962 | L1 Loss:0.324887590482831 | R2:0.43478116845829434 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[41] Loss:0.19353825077414513 | L1 Loss:0.3398220747709274 | R2:0.21331601810711356 | ACC: 79.6667%(239/300)\n",
            "Training Epoch[42] Loss:0.18505253922194242 | L1 Loss:0.3279789201915264 | R2:0.4140527366681491 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[42] Loss:0.21375333815813063 | L1 Loss:0.36244540214538573 | R2:0.13526628575664904 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[43] Loss:0.18834547325968742 | L1 Loss:0.3304793331772089 | R2:0.4027734144964321 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[43] Loss:0.20471108183264733 | L1 Loss:0.3502654224634171 | R2:0.18701352884640485 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[44] Loss:0.17634939355775714 | L1 Loss:0.321502136066556 | R2:0.44437061965947416 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[44] Loss:0.1982812002301216 | L1 Loss:0.3454268991947174 | R2:0.20710504656244916 | ACC: 74.0000%(222/300)\n",
            "Training Epoch[45] Loss:0.1702778385952115 | L1 Loss:0.3190623577684164 | R2:0.46095459539254696 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[45] Loss:0.1900847226381302 | L1 Loss:0.34214303493499754 | R2:0.26476217812429415 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[46] Loss:0.17985394783318043 | L1 Loss:0.328423535451293 | R2:0.4265188721330769 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[46] Loss:0.1964576631784439 | L1 Loss:0.35207530558109285 | R2:0.22118168587762504 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[47] Loss:0.1768862484022975 | L1 Loss:0.3257296923547983 | R2:0.4413474989515658 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[47] Loss:0.19261846467852592 | L1 Loss:0.34639483094215395 | R2:0.2538642481993701 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[48] Loss:0.17500933399423957 | L1 Loss:0.3293854631483555 | R2:0.44572891102548257 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[48] Loss:0.20316227823495864 | L1 Loss:0.3579286694526672 | R2:0.1510934076227338 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[49] Loss:0.18343781447038054 | L1 Loss:0.32873720675706863 | R2:0.4165878305073733 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[49] Loss:0.19190754145383834 | L1 Loss:0.3393350958824158 | R2:0.29365482372071067 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[50] Loss:0.1768465177156031 | L1 Loss:0.321490490809083 | R2:0.43887683579347236 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[50] Loss:0.20592711120843887 | L1 Loss:0.35891175270080566 | R2:0.18095932043208313 | ACC: 73.3333%(220/300)\n",
            "Training Epoch[51] Loss:0.17611486930400133 | L1 Loss:0.3215442821383476 | R2:0.44281463275706723 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[51] Loss:0.20112770348787307 | L1 Loss:0.35072739124298097 | R2:0.20405987330710657 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[52] Loss:0.17609345074743032 | L1 Loss:0.31952260807156563 | R2:0.4406331878362737 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[52] Loss:0.19519261717796327 | L1 Loss:0.3464746832847595 | R2:0.2521102686614045 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[53] Loss:0.17582480469718575 | L1 Loss:0.3241795878857374 | R2:0.43769618610236904 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[53] Loss:0.20440751165151597 | L1 Loss:0.35565756261348724 | R2:0.1925306085507214 | ACC: 76.6667%(230/300)\n",
            "Training Epoch[54] Loss:0.17232108674943447 | L1 Loss:0.3209190424531698 | R2:0.45505143732566067 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[54] Loss:0.19506292045116425 | L1 Loss:0.34566419422626493 | R2:0.23675253218746173 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[55] Loss:0.17859783116728067 | L1 Loss:0.3225435186177492 | R2:0.43844960491167506 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[55] Loss:0.20172756612300874 | L1 Loss:0.3532158821821213 | R2:0.1765064633349957 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[56] Loss:0.18072075443342328 | L1 Loss:0.3270661346614361 | R2:0.42956214332027653 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[56] Loss:0.19959060177206994 | L1 Loss:0.34960522651672366 | R2:0.18803878459544218 | ACC: 79.0000%(237/300)\n",
            "Training Epoch[57] Loss:0.17732193693518639 | L1 Loss:0.3263854794204235 | R2:0.44090299984838033 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[57] Loss:0.1904282584786415 | L1 Loss:0.34173092246055603 | R2:0.24240284471085843 | ACC: 78.6667%(236/300)\n",
            "Training Epoch[58] Loss:0.1743134050630033 | L1 Loss:0.3217397481203079 | R2:0.44934648080557776 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[58] Loss:0.20557164251804352 | L1 Loss:0.3528894007205963 | R2:0.1665777314923536 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[59] Loss:0.18023984460160136 | L1 Loss:0.3211241690441966 | R2:0.42539466663074493 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[59] Loss:0.19249816834926606 | L1 Loss:0.34386648833751676 | R2:0.2534698455228471 | ACC: 79.3333%(238/300)\n",
            "Training Epoch[60] Loss:0.17371304286643863 | L1 Loss:0.32056113332509995 | R2:0.4494904911848139 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[60] Loss:0.19640713110566138 | L1 Loss:0.3510263502597809 | R2:0.23570012674129076 | ACC: 79.3333%(238/300)\n",
            "Training Epoch[61] Loss:0.17999977059662342 | L1 Loss:0.3303717616945505 | R2:0.4288033342211836 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[61] Loss:0.19771444499492646 | L1 Loss:0.3448794215917587 | R2:0.21187492646957956 | ACC: 79.3333%(238/300)\n",
            "Training Epoch[62] Loss:0.17830810835584998 | L1 Loss:0.329617103561759 | R2:0.43745713465820074 | ACC: 76.8000%(384/500)\n",
            "Testing Epoch[62] Loss:0.19380818158388138 | L1 Loss:0.342907577753067 | R2:0.2458246383776745 | ACC: 79.0000%(237/300)\n",
            "Training Epoch[63] Loss:0.17406408349052072 | L1 Loss:0.32259461283683777 | R2:0.4444860757410975 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[63] Loss:0.20286732986569406 | L1 Loss:0.34890347719192505 | R2:0.20818484509718913 | ACC: 77.6667%(233/300)\n",
            "Training Epoch[64] Loss:0.17496386962011456 | L1 Loss:0.32219001837074757 | R2:0.4404108454513088 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[64] Loss:0.20028179436922072 | L1 Loss:0.3444271057844162 | R2:0.2317402931592511 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[65] Loss:0.1757985968142748 | L1 Loss:0.32175373286008835 | R2:0.4467621549573842 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[65] Loss:0.20342337787151338 | L1 Loss:0.35154229700565337 | R2:0.19488463994910102 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[66] Loss:0.17110839067026973 | L1 Loss:0.3173175659030676 | R2:0.4600728989090472 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[66] Loss:0.2007280983030796 | L1 Loss:0.350221061706543 | R2:0.20514242393755286 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[67] Loss:0.17430580826476216 | L1 Loss:0.3208610527217388 | R2:0.4467523077211939 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[67] Loss:0.20538988783955575 | L1 Loss:0.3497692734003067 | R2:0.1889971825765382 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[68] Loss:0.17407739209011197 | L1 Loss:0.3198283277451992 | R2:0.44393334362648323 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[68] Loss:0.2047651544213295 | L1 Loss:0.3461913108825684 | R2:0.15832830108945833 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[69] Loss:0.1784143685363233 | L1 Loss:0.3234889106824994 | R2:0.4334865039333596 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[69] Loss:0.20743943974375725 | L1 Loss:0.3514179319143295 | R2:0.19508069800689892 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[70] Loss:0.17546380683779716 | L1 Loss:0.3200639896094799 | R2:0.4401113243960698 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[70] Loss:0.20529073476791382 | L1 Loss:0.3526106297969818 | R2:0.17940453710136986 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[71] Loss:0.16881833132356405 | L1 Loss:0.31437971629202366 | R2:0.46698934033461886 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[71] Loss:0.19553587585687637 | L1 Loss:0.34711475372314454 | R2:0.2378300035099396 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[72] Loss:0.16936180694028735 | L1 Loss:0.3171624168753624 | R2:0.46287271333160457 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[72] Loss:0.21291636824607849 | L1 Loss:0.35918877422809603 | R2:0.13865795045813445 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[73] Loss:0.1687407810240984 | L1 Loss:0.31213827710598707 | R2:0.4633470868795918 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[73] Loss:0.19857027381658554 | L1 Loss:0.34973208904266356 | R2:0.21982887784629573 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[74] Loss:0.17316779820248485 | L1 Loss:0.3140950333327055 | R2:0.44876564326921564 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[74] Loss:0.19915825873613358 | L1 Loss:0.3471752434968948 | R2:0.18326946829522486 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[75] Loss:0.18437478598207235 | L1 Loss:0.3291874788701534 | R2:0.4130792260076123 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[75] Loss:0.19902992695569993 | L1 Loss:0.345320463180542 | R2:0.19944537982078536 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[76] Loss:0.17833457002416253 | L1 Loss:0.32420592103153467 | R2:0.43372034133070175 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[76] Loss:0.20482003688812256 | L1 Loss:0.35347605049610137 | R2:0.17259766064589788 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[77] Loss:0.1851993454620242 | L1 Loss:0.3330973759293556 | R2:0.4125112108656147 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[77] Loss:0.19710323065519333 | L1 Loss:0.3471158087253571 | R2:0.24636687659198403 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[78] Loss:0.17454830929636955 | L1 Loss:0.3216309333220124 | R2:0.45000363411423666 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[78] Loss:0.1961127206683159 | L1 Loss:0.3454925864934921 | R2:0.22595117993035627 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[79] Loss:0.17685619881376624 | L1 Loss:0.3234048970043659 | R2:0.44192680253362643 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[79] Loss:0.20504395812749862 | L1 Loss:0.3539068102836609 | R2:0.158867630065943 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[80] Loss:0.17783618858084083 | L1 Loss:0.32364383712410927 | R2:0.43640569725684647 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[80] Loss:0.20692707300186158 | L1 Loss:0.35209537446498873 | R2:0.1370818201562186 | ACC: 76.3333%(229/300)\n",
            "Training Epoch[81] Loss:0.1801477763801813 | L1 Loss:0.33033883944153786 | R2:0.4297374776498367 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[81] Loss:0.20422862619161605 | L1 Loss:0.35064623355865476 | R2:0.1873737669639397 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[82] Loss:0.17525000916793942 | L1 Loss:0.3287323899567127 | R2:0.45033268019734984 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[82] Loss:0.20494810342788697 | L1 Loss:0.3517706662416458 | R2:0.1634053653051306 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[83] Loss:0.17204551678150892 | L1 Loss:0.3200224135071039 | R2:0.4558782656442961 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[83] Loss:0.2056331530213356 | L1 Loss:0.3498408406972885 | R2:0.1500242861438948 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[84] Loss:0.17731552943587303 | L1 Loss:0.3204976096749306 | R2:0.4391947755187475 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[84] Loss:0.20564890056848525 | L1 Loss:0.3544841468334198 | R2:0.17194795240506963 | ACC: 75.3333%(226/300)\n",
            "Training Epoch[85] Loss:0.17268561339005828 | L1 Loss:0.31807006150484085 | R2:0.4514216714357727 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[85] Loss:0.19885349422693252 | L1 Loss:0.3495825231075287 | R2:0.14561428667829768 | ACC: 76.0000%(228/300)\n",
            "Training Epoch[86] Loss:0.18181499326601624 | L1 Loss:0.3258863799273968 | R2:0.4194991714241099 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[86] Loss:0.20749763995409012 | L1 Loss:0.35813920497894286 | R2:0.10917769295377515 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[87] Loss:0.18203379726037383 | L1 Loss:0.3268481455743313 | R2:0.42564727420885984 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[87] Loss:0.19991749674081802 | L1 Loss:0.3498108834028244 | R2:0.20551314777156676 | ACC: 77.3333%(232/300)\n",
            "Training Epoch[88] Loss:0.1807365445420146 | L1 Loss:0.3284481819719076 | R2:0.4265929957345178 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[88] Loss:0.20584897547960282 | L1 Loss:0.34922908544540404 | R2:0.13462379096568683 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[89] Loss:0.17875975230708718 | L1 Loss:0.3265370260924101 | R2:0.43162701944289905 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[89] Loss:0.2163235917687416 | L1 Loss:0.35977144837379454 | R2:0.09930619840120265 | ACC: 73.6667%(221/300)\n",
            "Training Epoch[90] Loss:0.16845851996913552 | L1 Loss:0.31249707844108343 | R2:0.46334840992855564 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[90] Loss:0.2046004965901375 | L1 Loss:0.3501009911298752 | R2:0.14297562412401663 | ACC: 74.6667%(224/300)\n",
            "Training Epoch[91] Loss:0.17278189677745104 | L1 Loss:0.3196899779140949 | R2:0.4511613027494872 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[91] Loss:0.19890255481004715 | L1 Loss:0.346119812130928 | R2:0.1830740329412972 | ACC: 75.0000%(225/300)\n",
            "Training Epoch[92] Loss:0.17160502588376403 | L1 Loss:0.3160167317837477 | R2:0.45148389197205474 | ACC: 82.2000%(411/500)\n",
            "Testing Epoch[92] Loss:0.19944948256015776 | L1 Loss:0.3448230504989624 | R2:0.19902904668581112 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[93] Loss:0.17985289683565497 | L1 Loss:0.3233858682215214 | R2:0.4260743704024921 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[93] Loss:0.20377357229590415 | L1 Loss:0.3515474259853363 | R2:0.1426937822373214 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[94] Loss:0.1763316337019205 | L1 Loss:0.32168499659746885 | R2:0.440632960569167 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[94] Loss:0.20449405163526535 | L1 Loss:0.3530675619840622 | R2:0.19989609281224138 | ACC: 74.3333%(223/300)\n",
            "Training Epoch[95] Loss:0.17334074480459094 | L1 Loss:0.319707777351141 | R2:0.4472474284527723 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[95] Loss:0.19859573021531104 | L1 Loss:0.3415185004472733 | R2:0.2156136534374571 | ACC: 75.6667%(227/300)\n",
            "Training Epoch[96] Loss:0.17583625530824065 | L1 Loss:0.32149574533104897 | R2:0.44097731746342483 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[96] Loss:0.19198675453662872 | L1 Loss:0.3385679930448532 | R2:0.2705436558891498 | ACC: 79.6667%(239/300)\n",
            "Training Epoch[97] Loss:0.16840631002560258 | L1 Loss:0.31184957176446915 | R2:0.4669201092737051 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[97] Loss:0.20019753724336625 | L1 Loss:0.3413186401128769 | R2:0.22286971396652108 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[98] Loss:0.1833047354593873 | L1 Loss:0.32605506386607885 | R2:0.4129710512601282 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[98] Loss:0.20372905880212783 | L1 Loss:0.3455280691385269 | R2:0.18344219440071505 | ACC: 77.0000%(231/300)\n",
            "Training Epoch[99] Loss:0.17177737643942237 | L1 Loss:0.3207170767709613 | R2:0.4532812161296565 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[99] Loss:0.20526617839932443 | L1 Loss:0.34973983764648436 | R2:0.218282427824472 | ACC: 76.3333%(229/300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 300 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00793dd8-d48e-443a-9af8-dffe150f7a40",
        "id": "zByDwkbS99e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 300 epochs-----\n",
            "max test_r2_record  0.29365482372071067\n",
            "min test_loss_l1_record  0.3385679930448532\n",
            "min test_loss_record  0.1900847226381302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 200 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:200]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:200]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:200]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f686e15b-97a5-4f14-8723-f2f97e90569e",
        "id": "gTiGH3R599e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 200 epochs-----\n",
            "max test_r2_record  0.25662598601705594\n",
            "min test_loss_l1_record  0.3466374784708023\n",
            "min test_loss_record  0.19679106324911116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 100 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:100]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:100]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15251ed1-ec14-460b-c6e6-73ddc62158b7",
        "id": "WWygfP9k99e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 100 epochs-----\n",
            "max test_r2_record  0.15542278667731962\n",
            "min test_loss_l1_record  0.36857505440711974\n",
            "min test_loss_record  0.2203592985868454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----GRU+ GCN 50 epochs-----')\n",
        "# print('max val_r2_record ', max(val_r2_record))\n",
        "print('max test_r2_record ', max(test_r2_record[:50]))\n",
        "\n",
        "# print('min val_loss_l1_record ', min(val_loss_l1_record))\n",
        "print('min test_loss_l1_record ', min(test_loss_l1_record[:50]))\n",
        "\n",
        "# print('min val_loss_record ', min(val_loss_record))\n",
        "print('min test_loss_record ', min(test_loss_record[:50]))"
      ],
      "metadata": {
        "outputId": "4675d646-2ebb-4383-aac8-96aea857f27b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clx8HhtR99e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----GRU+ GCN 50 epochs-----\n",
            "max test_r2_record  0.15542278667731962\n",
            "min test_loss_l1_record  0.38664981424808503\n",
            "min test_loss_record  0.2512748628854752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "WorAbz9w99e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Iou3AeNF99e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "# dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=True)\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-640, 640])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb9cafb-7d27-41da-c25c-bc4ce983b855",
        "id": "T6zH4JJd99e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b114f13a-c19b-4f62-ffa8-1d25dcbf7020",
        "id": "FW60wyeJ99e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001) # , weight_decay=0.0025\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "# criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "jSm0BOF899e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e7a04b-0597-46f1-b4fd-48fdec5c643a",
        "id": "onyRgpev99e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    \n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "lkCxzT8b99e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "6DzSZY2g99e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "\n",
        "train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in enumerate(dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    digits = model_class(data)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long)\n",
        "    # print(type(label))\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "\n",
        "    # L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  train_loss_record.append(loss.item())\n",
        "\n",
        "  val_loss, val_acc = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  val_acc_record.append(val_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  # val_r2_record.append(val_r2)\n",
        "  # val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc = test_acc_output(epoch)\n",
        "  test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  # test_r2_record.append(test_r2)\n",
        "  # test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186f5292-a12a-4ece-ca65-591ad9581a03",
        "id": "aDsyvDCj99e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:1.0235863961279392 | ACC: 47.8000%(239/500)\n",
            "Testing Epoch[0] Loss:1.0666760087013245 | ACC: 43.7500%(280/640)\n",
            "Training Epoch[1] Loss:1.0037893243134022 | ACC: 49.8000%(249/500)\n",
            "Testing Epoch[1] Loss:1.0544891893863677 | ACC: 45.4688%(291/640)\n",
            "Training Epoch[2] Loss:0.9794542640447617 | ACC: 52.6000%(263/500)\n",
            "Testing Epoch[2] Loss:1.0361753404140472 | ACC: 47.1875%(302/640)\n",
            "Training Epoch[3] Loss:0.9777071997523308 | ACC: 51.8000%(259/500)\n",
            "Testing Epoch[3] Loss:1.0372651785612106 | ACC: 45.6250%(292/640)\n",
            "Training Epoch[4] Loss:0.9553368203341961 | ACC: 53.4000%(267/500)\n",
            "Testing Epoch[4] Loss:1.0296450227499008 | ACC: 47.3438%(303/640)\n",
            "Training Epoch[5] Loss:0.9417544230818748 | ACC: 54.2000%(271/500)\n",
            "Testing Epoch[5] Loss:1.0286341041326523 | ACC: 45.9375%(294/640)\n",
            "Training Epoch[6] Loss:0.9305671751499176 | ACC: 54.6000%(273/500)\n",
            "Testing Epoch[6] Loss:1.017645600438118 | ACC: 47.1875%(302/640)\n",
            "Training Epoch[7] Loss:0.9148711375892162 | ACC: 55.8000%(279/500)\n",
            "Testing Epoch[7] Loss:0.9984156966209412 | ACC: 50.1562%(321/640)\n",
            "Training Epoch[8] Loss:0.9176501482725143 | ACC: 56.0000%(280/500)\n",
            "Testing Epoch[8] Loss:0.990182289481163 | ACC: 50.9375%(326/640)\n",
            "Training Epoch[9] Loss:0.9053237028419971 | ACC: 55.0000%(275/500)\n",
            "Testing Epoch[9] Loss:0.97803835272789 | ACC: 53.2812%(341/640)\n",
            "Training Epoch[10] Loss:0.8901126570999622 | ACC: 56.8000%(284/500)\n",
            "Testing Epoch[10] Loss:0.9753970265388489 | ACC: 52.8125%(338/640)\n",
            "Training Epoch[11] Loss:0.8538199365139008 | ACC: 58.2000%(291/500)\n",
            "Testing Epoch[11] Loss:0.9654525101184845 | ACC: 53.1250%(340/640)\n",
            "Training Epoch[12] Loss:0.8617191575467587 | ACC: 60.2000%(301/500)\n",
            "Testing Epoch[12] Loss:0.9807900488376617 | ACC: 53.7500%(344/640)\n",
            "Training Epoch[13] Loss:0.8711952269077301 | ACC: 60.2000%(301/500)\n",
            "Testing Epoch[13] Loss:0.9638089329004288 | ACC: 53.7500%(344/640)\n",
            "Training Epoch[14] Loss:0.8659655526280403 | ACC: 58.4000%(292/500)\n",
            "Testing Epoch[14] Loss:0.9527886122465133 | ACC: 55.0000%(352/640)\n",
            "Training Epoch[15] Loss:0.8624415993690491 | ACC: 58.6000%(293/500)\n",
            "Testing Epoch[15] Loss:0.9721358925104141 | ACC: 55.4688%(355/640)\n",
            "Training Epoch[16] Loss:0.8538879156112671 | ACC: 61.0000%(305/500)\n",
            "Testing Epoch[16] Loss:0.9683378279209137 | ACC: 57.3438%(367/640)\n",
            "Training Epoch[17] Loss:0.8458374217152596 | ACC: 61.8000%(309/500)\n",
            "Testing Epoch[17] Loss:0.9759487390518189 | ACC: 55.7812%(357/640)\n",
            "Training Epoch[18] Loss:0.8494395986199379 | ACC: 62.0000%(310/500)\n",
            "Testing Epoch[18] Loss:0.9739565372467041 | ACC: 57.6562%(369/640)\n",
            "Training Epoch[19] Loss:0.8300258405506611 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[19] Loss:0.9704002887010574 | ACC: 58.4375%(374/640)\n",
            "Training Epoch[20] Loss:0.8190148286521435 | ACC: 63.0000%(315/500)\n",
            "Testing Epoch[20] Loss:0.9854735910892487 | ACC: 58.2812%(373/640)\n",
            "Training Epoch[21] Loss:0.8322343900799751 | ACC: 63.8000%(319/500)\n",
            "Testing Epoch[21] Loss:1.0045726209878922 | ACC: 59.0625%(378/640)\n",
            "Training Epoch[22] Loss:0.8588841147720814 | ACC: 64.2000%(321/500)\n",
            "Testing Epoch[22] Loss:1.0198752135038376 | ACC: 56.5625%(362/640)\n",
            "Training Epoch[23] Loss:0.8280470483005047 | ACC: 64.2000%(321/500)\n",
            "Testing Epoch[23] Loss:1.024195957183838 | ACC: 57.6562%(369/640)\n",
            "Training Epoch[24] Loss:0.8314528353512287 | ACC: 65.2000%(326/500)\n",
            "Testing Epoch[24] Loss:1.0245992332696914 | ACC: 58.5938%(375/640)\n",
            "Training Epoch[25] Loss:0.8282464742660522 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[25] Loss:1.054325070977211 | ACC: 58.1250%(372/640)\n",
            "Training Epoch[26] Loss:0.850039929151535 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[26] Loss:1.0732851594686508 | ACC: 56.5625%(362/640)\n",
            "Training Epoch[27] Loss:0.8487791232764721 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[27] Loss:1.070276039838791 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[28] Loss:0.8321737311780453 | ACC: 64.4000%(322/500)\n",
            "Testing Epoch[28] Loss:1.0457605510950088 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[29] Loss:0.8434645161032677 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[29] Loss:1.103590902686119 | ACC: 57.9688%(371/640)\n",
            "Training Epoch[30] Loss:0.8872761838138103 | ACC: 63.6000%(318/500)\n",
            "Testing Epoch[30] Loss:1.1219183117151261 | ACC: 57.9688%(371/640)\n",
            "Training Epoch[31] Loss:0.897188663482666 | ACC: 64.2000%(321/500)\n",
            "Testing Epoch[31] Loss:1.1113697797060014 | ACC: 58.4375%(374/640)\n",
            "Training Epoch[32] Loss:0.8735640645027161 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[32] Loss:1.12216454744339 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[33] Loss:0.8626328036189079 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[33] Loss:1.1704943269491195 | ACC: 57.5000%(368/640)\n",
            "Training Epoch[34] Loss:0.9173002280294895 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[34] Loss:1.1851325258612633 | ACC: 59.3750%(380/640)\n",
            "Training Epoch[35] Loss:0.9769935123622417 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[35] Loss:1.163197335600853 | ACC: 58.7500%(376/640)\n",
            "Training Epoch[36] Loss:0.9297888204455376 | ACC: 64.2000%(321/500)\n",
            "Testing Epoch[36] Loss:1.2013531565666198 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[37] Loss:0.9410398676991463 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[37] Loss:1.2099199831485747 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[38] Loss:0.9145145900547504 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[38] Loss:1.1998615443706513 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[39] Loss:0.9133742041885853 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[39] Loss:1.2546878635883332 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[40] Loss:0.9526317454874516 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[40] Loss:1.2559241980314255 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[41] Loss:0.9188329428434372 | ACC: 66.0000%(330/500)\n",
            "Testing Epoch[41] Loss:1.2482576191425323 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[42] Loss:0.9608006291091442 | ACC: 63.8000%(319/500)\n",
            "Testing Epoch[42] Loss:1.3052463740110398 | ACC: 58.1250%(372/640)\n",
            "Training Epoch[43] Loss:0.9326319433748722 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[43] Loss:1.3530187606811523 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[44] Loss:0.9560593366622925 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[44] Loss:1.3130504786968231 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[45] Loss:0.9980842061340809 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[45] Loss:1.3389466255903244 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[46] Loss:0.9137747138738632 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[46] Loss:1.3388047158718108 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[47] Loss:0.9291085116565228 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[47] Loss:1.3990763813257217 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[48] Loss:0.9519768953323364 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[48] Loss:1.3715195894241332 | ACC: 58.7500%(376/640)\n",
            "Training Epoch[49] Loss:1.0147075168788433 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[49] Loss:1.3805163115262986 | ACC: 57.6562%(369/640)\n",
            "Training Epoch[50] Loss:0.9986941628158092 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[50] Loss:1.4082660228013992 | ACC: 58.2812%(373/640)\n",
            "Training Epoch[51] Loss:1.0293566808104515 | ACC: 64.0000%(320/500)\n",
            "Testing Epoch[51] Loss:1.4121399015188216 | ACC: 57.3438%(367/640)\n",
            "Training Epoch[52] Loss:1.0293685235083103 | ACC: 66.8000%(334/500)\n",
            "Testing Epoch[52] Loss:1.3994711607694625 | ACC: 59.3750%(380/640)\n",
            "Training Epoch[53] Loss:0.984194178134203 | ACC: 66.0000%(330/500)\n",
            "Testing Epoch[53] Loss:1.4645614266395568 | ACC: 57.9688%(371/640)\n",
            "Training Epoch[54] Loss:1.031462885439396 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[54] Loss:1.499126374721527 | ACC: 57.8125%(370/640)\n",
            "Training Epoch[55] Loss:1.1682656854391098 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[55] Loss:1.5221194505691529 | ACC: 57.1875%(366/640)\n",
            "Training Epoch[56] Loss:1.0674705058336258 | ACC: 66.0000%(330/500)\n",
            "Testing Epoch[56] Loss:1.5629034519195557 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[57] Loss:1.113606408238411 | ACC: 63.4000%(317/500)\n",
            "Testing Epoch[57] Loss:1.5154599010944367 | ACC: 58.2812%(373/640)\n",
            "Training Epoch[58] Loss:1.064785197377205 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[58] Loss:1.6324909597635269 | ACC: 56.5625%(362/640)\n",
            "Training Epoch[59] Loss:1.0400471799075603 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[59] Loss:1.532897710800171 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[60] Loss:1.09153500944376 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[60] Loss:1.6063142895698548 | ACC: 57.8125%(370/640)\n",
            "Training Epoch[61] Loss:1.0980454348027706 | ACC: 65.2000%(326/500)\n",
            "Testing Epoch[61] Loss:1.509479969739914 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[62] Loss:1.1490739397704601 | ACC: 64.4000%(322/500)\n",
            "Testing Epoch[62] Loss:1.6436683118343354 | ACC: 58.4375%(374/640)\n",
            "Training Epoch[63] Loss:1.1850264109671116 | ACC: 63.6000%(318/500)\n",
            "Testing Epoch[63] Loss:1.6087666302919388 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[64] Loss:1.169491060078144 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[64] Loss:1.6328648805618287 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[65] Loss:1.1470539644360542 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[65] Loss:1.6479092568159104 | ACC: 59.0625%(378/640)\n",
            "Training Epoch[66] Loss:1.0643985196948051 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[66] Loss:1.5108676433563233 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[67] Loss:1.1663817577064037 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[67] Loss:1.614473906159401 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[68] Loss:1.02829023078084 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[68] Loss:1.641490525007248 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[69] Loss:1.1161593981087208 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[69] Loss:1.5939505651593209 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[70] Loss:1.1731201224029064 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[70] Loss:1.5797566622495651 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[71] Loss:1.135675422847271 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[71] Loss:1.711752313375473 | ACC: 59.0625%(378/640)\n",
            "Training Epoch[72] Loss:1.199816633015871 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[72] Loss:1.662312212586403 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[73] Loss:1.119700126349926 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[73] Loss:1.6197363972663879 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[74] Loss:1.2362400107085705 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[74] Loss:1.6265071600675582 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[75] Loss:1.1216207928955555 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[75] Loss:1.7238637298345565 | ACC: 59.0625%(378/640)\n",
            "Training Epoch[76] Loss:1.12848911434412 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[76] Loss:1.7008249551057815 | ACC: 59.2188%(379/640)\n",
            "Training Epoch[77] Loss:1.1253804340958595 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[77] Loss:1.7594131827354431 | ACC: 58.2812%(373/640)\n",
            "Training Epoch[78] Loss:1.212986033409834 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[78] Loss:1.691904529929161 | ACC: 58.4375%(374/640)\n",
            "Training Epoch[79] Loss:1.1619359329342842 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[79] Loss:1.6069230765104294 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[80] Loss:1.1867145039141178 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[80] Loss:1.7904105603694915 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[81] Loss:1.1106781847774982 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[81] Loss:1.7170624494552613 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[82] Loss:1.216786939650774 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[82] Loss:1.6652615934610366 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[83] Loss:1.137840036302805 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[83] Loss:1.7580566883087159 | ACC: 57.9688%(371/640)\n",
            "Training Epoch[84] Loss:1.207721747457981 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[84] Loss:1.6537705153226852 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[85] Loss:1.1191481612622738 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[85] Loss:1.7469921231269836 | ACC: 59.3750%(380/640)\n",
            "Training Epoch[86] Loss:1.1341769956052303 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[86] Loss:1.7141452878713608 | ACC: 59.5312%(381/640)\n",
            "Training Epoch[87] Loss:1.118781417608261 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[87] Loss:1.7827561318874359 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[88] Loss:1.0305797718465328 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[88] Loss:1.7957646906375886 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[89] Loss:1.1927812919020653 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[89] Loss:1.8232174217700958 | ACC: 59.6875%(382/640)\n",
            "Training Epoch[90] Loss:1.0902199856936932 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[90] Loss:1.7710624277591704 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[91] Loss:1.1147688701748848 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[91] Loss:1.6887449711561202 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[92] Loss:1.084277430549264 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[92] Loss:1.6666929692029953 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[93] Loss:1.1253570541739464 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[93] Loss:1.7796660304069518 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[94] Loss:1.1894754096865654 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[94] Loss:1.802730929851532 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[95] Loss:1.1859512105584145 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[95] Loss:1.7237713098526002 | ACC: 58.4375%(374/640)\n",
            "Training Epoch[96] Loss:1.1522343400865793 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[96] Loss:1.8203699469566346 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[97] Loss:1.251138098537922 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[97] Loss:1.7414877861738205 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[98] Loss:1.1508077755570412 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[98] Loss:1.6277627736330031 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[99] Loss:1.299304310232401 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[99] Loss:1.9378984868526459 | ACC: 58.5938%(375/640)\n",
            "Training Epoch[100] Loss:1.2320030368864536 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[100] Loss:1.8386727958917617 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[101] Loss:1.2572381757199764 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[101] Loss:1.812004354596138 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[102] Loss:1.1242901682853699 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[102] Loss:1.779285165667534 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[103] Loss:1.0733280777931213 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[103] Loss:1.78744275867939 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[104] Loss:1.1574082858860493 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[104] Loss:1.7542192101478578 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[105] Loss:1.218972995877266 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[105] Loss:1.8930279850959777 | ACC: 58.5938%(375/640)\n",
            "Training Epoch[106] Loss:1.1526418924331665 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[106] Loss:1.8138951539993287 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[107] Loss:1.1689649987965822 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[107] Loss:1.8604825288057327 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[108] Loss:1.0962599888443947 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[108] Loss:1.7594355374574662 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[109] Loss:1.1104419846087694 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[109] Loss:1.7308110356330872 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[110] Loss:1.2193329874426126 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[110] Loss:1.7798420399427415 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[111] Loss:1.1796890571713448 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[111] Loss:1.8554972112178802 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[112] Loss:1.1889604516327381 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[112] Loss:1.8630761563777924 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[113] Loss:1.1935777440667152 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[113] Loss:1.89834663271904 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[114] Loss:1.2253535501658916 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[114] Loss:1.8320759028196334 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[115] Loss:1.2368419207632542 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[115] Loss:1.8432741910219193 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[116] Loss:1.157271757721901 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[116] Loss:1.8281915605068206 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[117] Loss:1.18691186606884 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[117] Loss:1.766009908914566 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[118] Loss:1.1347293201833963 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[118] Loss:1.8448706835508346 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[119] Loss:1.237084724009037 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[119] Loss:1.9516908705234528 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[120] Loss:1.2331509329378605 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[120] Loss:1.6978764533996582 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[121] Loss:1.2212099097669125 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[121] Loss:1.7353969484567642 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[122] Loss:1.1907661706209183 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[122] Loss:1.725541028380394 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[123] Loss:1.3624100238084793 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[123] Loss:1.8296884387731551 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[124] Loss:1.2250442616641521 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[124] Loss:1.875667405128479 | ACC: 59.5312%(381/640)\n",
            "Training Epoch[125] Loss:1.2976067028939724 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[125] Loss:1.811043918132782 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[126] Loss:1.1034419015049934 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[126] Loss:1.8252213686704635 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[127] Loss:1.1963459886610508 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[127] Loss:1.7676567018032074 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[128] Loss:1.1851801127195358 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[128] Loss:1.8187221199274064 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[129] Loss:1.2095428444445133 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[129] Loss:1.8243251025676728 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[130] Loss:1.266543596982956 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[130] Loss:1.7568413496017456 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[131] Loss:1.1443780846893787 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[131] Loss:1.8335051834583282 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[132] Loss:1.2582986615598202 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[132] Loss:1.952178230881691 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[133] Loss:1.2322656400501728 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[133] Loss:1.7293688893318175 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[134] Loss:1.2435420267283916 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[134] Loss:1.9417322486639024 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[135] Loss:1.2838043384253979 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[135] Loss:1.968888247013092 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[136] Loss:1.1880905888974667 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[136] Loss:1.9399155348539352 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[137] Loss:1.1823350824415684 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[137] Loss:1.8399044215679168 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[138] Loss:1.2730082422494888 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[138] Loss:1.903556126356125 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[139] Loss:1.3351688347756863 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[139] Loss:1.8273028433322906 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[140] Loss:1.1754940785467625 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[140] Loss:1.8374971359968186 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[141] Loss:1.2309654019773006 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[141] Loss:1.6874276518821716 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[142] Loss:1.2007469981908798 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[142] Loss:1.7571164101362229 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[143] Loss:1.3324810303747654 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[143] Loss:1.8016731232404708 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[144] Loss:1.2887085117399693 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[144] Loss:1.786536556482315 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[145] Loss:1.217914093285799 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[145] Loss:1.752488973736763 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[146] Loss:1.1273288503289223 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[146] Loss:1.8300716519355773 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[147] Loss:1.201851086691022 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[147] Loss:1.8994045197963714 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[148] Loss:1.2370067164301872 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[148] Loss:1.916554605960846 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[149] Loss:1.321186389774084 | ACC: 64.6000%(323/500)\n",
            "Testing Epoch[149] Loss:1.735808128118515 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[150] Loss:1.2992485612630844 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[150] Loss:1.7698951482772827 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[151] Loss:1.3649995401501656 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[151] Loss:1.8154518127441406 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[152] Loss:1.167631782591343 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[152] Loss:1.7664611518383027 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[153] Loss:1.2595814876258373 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[153] Loss:1.7592435300350189 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[154] Loss:1.1952257603406906 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[154] Loss:1.8235943883657455 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[155] Loss:1.2226479947566986 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[155] Loss:1.8324675023555757 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[156] Loss:1.1987476535141468 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[156] Loss:1.780101117491722 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[157] Loss:1.216797860339284 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[157] Loss:1.7469836100935936 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[158] Loss:1.1497307531535625 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[158] Loss:1.6764545828104018 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[159] Loss:1.2640575915575027 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[159] Loss:1.9073113173246383 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[160] Loss:1.3228368759155273 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[160] Loss:1.7752480953931808 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[161] Loss:1.264650296419859 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[161] Loss:1.858573752641678 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[162] Loss:1.1962827742099762 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[162] Loss:1.875969997048378 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[163] Loss:1.2229875028133392 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[163] Loss:1.8274756968021393 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[164] Loss:1.2227676585316658 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[164] Loss:1.8494940727949143 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[165] Loss:1.275006216019392 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[165] Loss:1.779788112640381 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[166] Loss:1.1824611984193325 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[166] Loss:1.738395830988884 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[167] Loss:1.3038363233208656 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[167] Loss:1.8576254695653915 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[168] Loss:1.2059767059981823 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[168] Loss:1.7588926136493683 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[169] Loss:1.2525003887712955 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[169] Loss:1.8502434849739076 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[170] Loss:1.237823724746704 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[170] Loss:1.8714194655418397 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[171] Loss:1.2037082239985466 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[171] Loss:1.8062223598361016 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[172] Loss:1.2871103733778 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[172] Loss:1.825922781229019 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[173] Loss:1.3652178160846233 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[173] Loss:1.7420045197010041 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[174] Loss:1.2705365717411041 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[174] Loss:1.8303252160549164 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[175] Loss:1.2537139728665352 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[175] Loss:1.8024419039487838 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[176] Loss:1.239050816744566 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[176] Loss:1.86285260617733 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[177] Loss:1.383125863969326 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[177] Loss:1.7204874217510224 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[178] Loss:1.170311663299799 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[178] Loss:1.8611103624105454 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[179] Loss:1.224946316331625 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[179] Loss:1.8495928257703782 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[180] Loss:1.2061080634593964 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[180] Loss:1.8135010242462157 | ACC: 58.9062%(377/640)\n",
            "Training Epoch[181] Loss:1.331555787473917 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[181] Loss:1.819501206278801 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[182] Loss:1.3418543003499508 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[182] Loss:1.7902173787355422 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[183] Loss:1.3067113608121872 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[183] Loss:1.8456545293331146 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[184] Loss:1.370882548391819 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[184] Loss:1.7734871044754983 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[185] Loss:1.2390753962099552 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[185] Loss:1.70133475959301 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[186] Loss:1.2873579189181328 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[186] Loss:1.7789553165435792 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[187] Loss:1.2010497860610485 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[187] Loss:1.797697240114212 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[188] Loss:1.1975461170077324 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[188] Loss:1.7706564366817474 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[189] Loss:1.423160683363676 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[189] Loss:1.7712062031030655 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[190] Loss:1.3703925125300884 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[190] Loss:1.891745325922966 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[191] Loss:1.187670312821865 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[191] Loss:1.8402853026986121 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[192] Loss:1.277940347790718 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[192] Loss:1.821351197361946 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[193] Loss:1.4100626893341541 | ACC: 64.8000%(324/500)\n",
            "Testing Epoch[193] Loss:1.8904591768980026 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[194] Loss:1.3377703912556171 | ACC: 67.2000%(336/500)\n",
            "Testing Epoch[194] Loss:1.8861462980508805 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[195] Loss:1.3049087524414062 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[195] Loss:1.8306560128927232 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[196] Loss:1.2898466996848583 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[196] Loss:1.738354554772377 | ACC: 65.0000%(416/640)\n",
            "Training Epoch[197] Loss:1.208966240286827 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[197] Loss:1.7813100904226302 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[198] Loss:1.307354662567377 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[198] Loss:1.8923379600048065 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[199] Loss:1.2406745217740536 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[199] Loss:1.814264515042305 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[200] Loss:1.2125148735940456 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[200] Loss:1.7278907150030136 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[201] Loss:1.297682486474514 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[201] Loss:1.9146587044000625 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[202] Loss:1.3126699030399323 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[202] Loss:1.8835238486528396 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[203] Loss:1.2856911681592464 | ACC: 67.0000%(335/500)\n",
            "Testing Epoch[203] Loss:1.826594093441963 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[204] Loss:1.2101221904158592 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[204] Loss:1.742022266983986 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[205] Loss:1.2545605637133121 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[205] Loss:1.8129588305950164 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[206] Loss:1.2578896097838879 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[206] Loss:1.7342210382223129 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[207] Loss:1.3597053922712803 | ACC: 66.8000%(334/500)\n",
            "Testing Epoch[207] Loss:1.8964283972978593 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[208] Loss:1.3372457884252071 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[208] Loss:1.8978137493133544 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[209] Loss:1.2313112765550613 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[209] Loss:1.7921790167689324 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[210] Loss:1.2552627697587013 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[210] Loss:1.7551891475915908 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[211] Loss:1.3061440140008926 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[211] Loss:1.7287528723478318 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[212] Loss:1.3146382793784142 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[212] Loss:1.784695130586624 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[213] Loss:1.1984281167387962 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[213] Loss:1.8020796447992324 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[214] Loss:1.1152618192136288 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[214] Loss:1.8779852747917176 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[215] Loss:1.172558356076479 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[215] Loss:1.7423205897212029 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[216] Loss:1.3537658229470253 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[216] Loss:2.038684982061386 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[217] Loss:1.2187544666230679 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[217] Loss:1.8271125465631486 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[218] Loss:1.3570869322866201 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[218] Loss:1.8405700266361236 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[219] Loss:1.29563694819808 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[219] Loss:1.752802661061287 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[220] Loss:1.3000732194632292 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[220] Loss:1.844021674990654 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[221] Loss:1.320411367341876 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[221] Loss:1.787784069776535 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[222] Loss:1.2535380758345127 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[222] Loss:1.8010944932699204 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[223] Loss:1.4178198166191578 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[223] Loss:1.8841828167438508 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[224] Loss:1.2556628994643688 | ACC: 68.4000%(342/500)\n",
            "Testing Epoch[224] Loss:1.841214519739151 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[225] Loss:1.2196716032922268 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[225] Loss:1.9066714465618133 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[226] Loss:1.206422969698906 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[226] Loss:1.8237432271242142 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[227] Loss:1.196329440921545 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[227] Loss:1.8676472842693328 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[228] Loss:1.1808759309351444 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[228] Loss:1.790457409620285 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[229] Loss:1.153546653687954 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[229] Loss:1.8785545974969864 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[230] Loss:1.3714645616710186 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[230] Loss:1.9889335602521896 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[231] Loss:1.2031441815197468 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[231] Loss:1.7829419106245041 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[232] Loss:1.1358282156288624 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[232] Loss:1.839715188741684 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[233] Loss:1.2539532333612442 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[233] Loss:1.9188704431056975 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[234] Loss:1.193380393087864 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[234] Loss:1.8058218389749527 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[235] Loss:1.3581486865878105 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[235] Loss:1.9179281413555145 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[236] Loss:1.2743347100913525 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[236] Loss:1.9190289437770844 | ACC: 58.7500%(376/640)\n",
            "Training Epoch[237] Loss:1.2127120085060596 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[237] Loss:1.790664267539978 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[238] Loss:1.2720466312021017 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[238] Loss:1.7219371229410172 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[239] Loss:1.3527744300663471 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[239] Loss:1.7903918653726578 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[240] Loss:1.3835694715380669 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[240] Loss:1.90251484811306 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[241] Loss:1.2229646407067776 | ACC: 67.6000%(338/500)\n",
            "Testing Epoch[241] Loss:1.82497518658638 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[242] Loss:1.1928264647722244 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[242] Loss:1.7539432540535926 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[243] Loss:1.1893787533044815 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[243] Loss:1.7883764505386353 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[244] Loss:1.191437765955925 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[244] Loss:1.8115688115358353 | ACC: 59.2188%(379/640)\n",
            "Training Epoch[245] Loss:1.333615131676197 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[245] Loss:2.022983306646347 | ACC: 58.1250%(372/640)\n",
            "Training Epoch[246] Loss:1.2342672012746334 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[246] Loss:1.750845766067505 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[247] Loss:1.1883176192641258 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[247] Loss:1.818431669473648 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[248] Loss:1.224446639418602 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[248] Loss:1.7645127683877946 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[249] Loss:1.181832479313016 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[249] Loss:1.804565206170082 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[250] Loss:1.6940908320248127 | ACC: 63.2000%(316/500)\n",
            "Testing Epoch[250] Loss:2.2411704421043397 | ACC: 55.1562%(353/640)\n",
            "Training Epoch[251] Loss:1.1975887827575207 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[251] Loss:1.6852339595556258 | ACC: 59.6875%(382/640)\n",
            "Training Epoch[252] Loss:1.1118279658257961 | ACC: 68.6000%(343/500)\n",
            "Testing Epoch[252] Loss:1.7182747453451157 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[253] Loss:1.0820493139326572 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[253] Loss:1.6948036581277848 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[254] Loss:1.1450024973601103 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[254] Loss:1.7139031082391738 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[255] Loss:1.1070933621376753 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[255] Loss:1.8043296307325363 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[256] Loss:1.1335636898875237 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[256] Loss:1.9085517227649689 | ACC: 59.3750%(380/640)\n",
            "Training Epoch[257] Loss:1.0505403578281403 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[257] Loss:1.6674637854099275 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[258] Loss:1.0330717414617538 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[258] Loss:1.7076397597789765 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[259] Loss:1.0266380049288273 | ACC: 69.6000%(348/500)\n",
            "Testing Epoch[259] Loss:1.7091441482305527 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[260] Loss:1.0414569303393364 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[260] Loss:1.7227616429328918 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[261] Loss:1.1074302550405264 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[261] Loss:1.7685489118099214 | ACC: 64.0625%(410/640)\n",
            "Training Epoch[262] Loss:1.5405561551451683 | ACC: 62.6000%(313/500)\n",
            "Testing Epoch[262] Loss:1.928409829735756 | ACC: 55.3125%(354/640)\n",
            "Training Epoch[263] Loss:0.9543748758733273 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[263] Loss:1.510715365409851 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[264] Loss:1.0400538090616465 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[264] Loss:1.541375321149826 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[265] Loss:1.011863736435771 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[265] Loss:1.5447532653808593 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[266] Loss:1.0523729100823402 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[266] Loss:1.5818713575601577 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[267] Loss:1.0576041284948587 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[267] Loss:1.6191606491804122 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[268] Loss:1.0897210035473108 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[268] Loss:1.6550208255648613 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[269] Loss:1.3766632601618767 | ACC: 62.0000%(310/500)\n",
            "Testing Epoch[269] Loss:1.6147615015506744 | ACC: 58.4375%(374/640)\n",
            "Training Epoch[270] Loss:0.9609849508851767 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[270] Loss:1.4940323233604431 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[271] Loss:1.0393828805536032 | ACC: 69.0000%(345/500)\n",
            "Testing Epoch[271] Loss:1.5197984769940376 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[272] Loss:1.0505207814276218 | ACC: 68.8000%(344/500)\n",
            "Testing Epoch[272] Loss:1.5151876211166382 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[273] Loss:1.0491076838225126 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[273] Loss:1.5280023857951164 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[274] Loss:1.076505171135068 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[274] Loss:1.5695690542459488 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[275] Loss:1.3163061030209064 | ACC: 63.6000%(318/500)\n",
            "Testing Epoch[275] Loss:1.849348396062851 | ACC: 56.8750%(364/640)\n",
            "Training Epoch[276] Loss:1.1335377153009176 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[276] Loss:1.5220907002687454 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[277] Loss:1.0209023114293814 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[277] Loss:1.4888435274362564 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[278] Loss:1.057766454294324 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[278] Loss:1.5221277520060539 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[279] Loss:1.0811333302408457 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[279] Loss:1.5441278502345086 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[280] Loss:1.082867480814457 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[280] Loss:1.6012841165065765 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[281] Loss:1.1492166183888912 | ACC: 70.2000%(351/500)\n",
            "Testing Epoch[281] Loss:1.637219527363777 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[282] Loss:1.8104002997279167 | ACC: 59.4000%(297/500)\n",
            "Testing Epoch[282] Loss:1.998456734418869 | ACC: 53.7500%(344/640)\n",
            "Training Epoch[283] Loss:0.9319623000919819 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[283] Loss:1.457792615890503 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[284] Loss:0.9369849823415279 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[284] Loss:1.4781626135110855 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[285] Loss:1.0062648188322783 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[285] Loss:1.5061218708753585 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[286] Loss:1.009567480534315 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[286] Loss:1.5880010217428207 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[287] Loss:1.0241904612630606 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[287] Loss:1.6002842009067535 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[288] Loss:1.0536593701690435 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[288] Loss:1.6430481016635894 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[289] Loss:1.150933399796486 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[289] Loss:1.5945996940135956 | ACC: 59.2188%(379/640)\n",
            "Training Epoch[290] Loss:1.0122829861938953 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[290] Loss:1.3710670322179794 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[291] Loss:0.9442845340818167 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[291] Loss:1.4388804644346238 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[292] Loss:0.9774968158453703 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[292] Loss:1.4180620282888412 | ACC: 63.7500%(408/640)\n",
            "Training Epoch[293] Loss:0.9963934170082211 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[293] Loss:1.4660560846328736 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[294] Loss:1.0136477686464787 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[294] Loss:1.5199200510978699 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[295] Loss:1.0819862075150013 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[295] Loss:1.5858335614204406 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[296] Loss:1.4158413261175156 | ACC: 63.0000%(315/500)\n",
            "Testing Epoch[296] Loss:1.837280735373497 | ACC: 55.7812%(357/640)\n",
            "Training Epoch[297] Loss:0.9893222190439701 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[297] Loss:1.3533474117517472 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[298] Loss:0.991631431505084 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[298] Loss:1.3732228457927704 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[299] Loss:1.0122206080704927 | ACC: 71.0000%(355/500)\n",
            "Testing Epoch[299] Loss:1.4238450050354003 | ACC: 63.2812%(405/640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 300 epoch second+mature mirna\n",
        "print('max val_acc_record ', max(val_acc_record))\n",
        "print('min val_loss_record ', min(val_loss_record))\n",
        "\n",
        "print('min test_loss_record ', min(test_loss_record))\n",
        "print('max test_acc_record ', max(test_acc_record))\n",
        "# max val_acc_record  84.6\n",
        "# min val_loss_record  0.5722110476344824\n",
        "# min test_loss_record  0.7427837997674942\n",
        "# max test_acc_record  75.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91af7ea2-1348-4fb0-81b6-a86b7d357cb3",
        "id": "2aMVT1eg99e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max val_acc_record  73.6\n",
            "min val_loss_record  0.8190148286521435\n",
            "min test_loss_record  0.9527886122465133\n",
            "max test_acc_record  65.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9vuKw5pL99e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_class, '/content/drive/MyDrive/miRNA/last_data/classification_result/all_gcn_gru_300.pt')\n",
        "torch.save(model_class.state_dict(), '/content/drive/MyDrive/miRNA/last_data/classification_result/all_gcn_gru_300_state.pt')"
      ],
      "metadata": {
        "id": "42mnFbWg99e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_acc_list_all_gcn_gru_300.npy', val_acc_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_loss_list_all_gcn_gru_300.npy', val_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_loss_list_all_gcn_gru_300.npy', test_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_acc_list_all_gcn_gru_300.npy', test_acc_record, allow_pickle=True)\n"
      ],
      "metadata": {
        "id": "VuhoDiup99e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader"
      ],
      "metadata": {
        "id": "O_0QzeS-99e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "model_class.load_state_dict(torch.load('/content/drive/MyDrive/miRNA/last_data/classification_result/all_gcn_gru_300_state.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6e5503-7903-454b-d857-6603a59c316d",
        "id": "5mzLz2Q799e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_roc():\n",
        "  \n",
        "  score_list=[]\n",
        "  score_one_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    score_temp=digits\n",
        "    score_list.extend(score_temp.detach().cpu().numpy())\n",
        "    score_temp=predicted\n",
        "    score_one_list.extend(score_temp.detach().cpu().numpy())\n",
        "    label_list.extend(label)\n",
        "  # correct = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   if label_list[i] == score_list[i]:\n",
        "  #     correct += 1\n",
        "  # print(correct/320)\n",
        "\n",
        "  score_array = np.array(score_list)\n",
        "  score_tensor = torch.tensor(score_one_list)\n",
        "  score_tensor = score_tensor.reshape(score_tensor.shape[0], 1)\n",
        "  score_one_hot = torch.zeros(score_tensor.shape[0], 3)\n",
        "  score_one_hot.scatter_(dim=1, index=score_tensor, value=1)\n",
        "  score_one_hot = np.array(score_one_hot)\n",
        "\n",
        "  label_tensor = torch.tensor(label_list)\n",
        "  label_tensor = label_tensor.reshape(label_tensor.shape[0], 1)\n",
        "  label_one_hot = torch.zeros(label_tensor.shape[0], 3)\n",
        "  label_one_hot.scatter_(dim=1, index=label_tensor, value=1)\n",
        "  label_one_hot = np.array(label_one_hot)\n",
        "  ns_c = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   score_list\n",
        "  print('score_array shape', score_array.shape)\n",
        "  print('label_one_hot shape', label_one_hot.shape)\n",
        "  # roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "  print('roc_auc_score', roc_auc_score(label_one_hot, score_array, average=None))\n",
        "  print('recall_score', recall_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('precision_score', precision_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('f1_score', f1_score(label_one_hot, score_one_hot, average=None))\n",
        "  # return 0\n",
        "\n",
        "  fpr_dict = dict()\n",
        "  tpr_dict = dict()\n",
        "  roc_auc_dict = dict()\n",
        "  print(score_array.shape)\n",
        "  for i in range(3):\n",
        "    fpr_dict[i], tpr_dict[i], _ = roc_curve(label_one_hot[:, i], score_array[:, i])\n",
        "    roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
        "  \n",
        "  fpr_dict['micro'], tpr_dict['micro'], _ = roc_curve(label_one_hot.ravel(), score_array.ravel())\n",
        "  roc_auc_dict['micro'] = auc(fpr_dict['micro'], tpr_dict['micro'])\n",
        "\n",
        "  all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(3)]))\n",
        "  mean_tpr = np.zeros_like(all_fpr)\n",
        "  for i in range(3):\n",
        "    mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
        "\n",
        "  mean_tpr /= 3\n",
        "  fpr_dict['macro'] = all_fpr\n",
        "  tpr_dict['macro'] = mean_tpr\n",
        "  roc_auc_dict['macro'] = auc(fpr_dict['macro'], tpr_dict['macro'])\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  lw = 1.5\n",
        "  plt.plot(fpr_dict['micro'], tpr_dict['micro'], label='micro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['micro']), color='deeppink', linestyle=':', linewidth=4)\n",
        "  plt.plot(fpr_dict['macro'], tpr_dict['macro'], label='macro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['macro']), color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "  colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "  labels = cycle(['down', 'ns', 'up'])\n",
        "  # print(fpr_dict[i])\n",
        "  print(roc_auc_dict[i])\n",
        "  for i, color, label in zip(range(3), colors, labels):\n",
        "    plt.plot(fpr_dict[i], tpr_dict[i], lw=lw, color=color, label='ROC curve of class {0} (area={1:0.4f})'.format(label, roc_auc_dict[i]))\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--', lw=lw)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('All ROC')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.savefig('ALL_GRU_GCN_classification_ROC_curve.jpg')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "Q_OOyjF999e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "rWfRJwpYkT8N",
        "outputId": "7d48a730-c24d-4303-ed2e-337d0ca9ee59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score_array shape (320, 3)\n",
            "label_one_hot shape (320, 3)\n",
            "roc_auc_score [0.80708874 0.8137689  0.83341082]\n",
            "recall_score [0.625      0.56830601 0.72839506]\n",
            "precision_score [0.40697674 0.85950413 0.52212389]\n",
            "f1_score [0.49295775 0.68421053 0.60824742]\n",
            "(320, 3)\n",
            "0.833410816674415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gOTxFCS5kL",
        "outputId": "444eb2cb-f0b9-45e2-80c6-71eb010b7134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6328125\n",
            "score_array shape (640, 3)\n",
            "label_one_hot shape (640, 3)\n",
            "[0.71672591 0.72202107 0.72918804]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1bb3a0-b161-416b-b4d1-9e7f58c58371",
        "id": "tvn7bV-Y99e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6328125\n",
            "score_array shape (640, 3)\n",
            "label_one_hot shape (640, 3)\n",
            "[0.71672591 0.72202107 0.72918804]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_metrix():\n",
        "  \n",
        "  score_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    score_temp=predicted\n",
        "    score_list.extend(score_temp.detach().cpu().numpy())\n",
        "    label_list.extend(label)\n",
        "  correct = 0\n",
        "  for i in range(len(score_list)):\n",
        "    if label_list[i] == score_list[i]:\n",
        "      correct += 1\n",
        "  print(correct/640)\n",
        "\n",
        "  # score_array = np.array(score_list)\n",
        "  score_tensor = torch.tensor(score_list)\n",
        "  score_tensor = score_tensor.reshape(score_tensor.shape[0], 1)\n",
        "  score_one_hot = torch.zeros(score_tensor.shape[0], 3)\n",
        "  score_one_hot.scatter_(dim=1, index=score_tensor, value=1)\n",
        "  score_one_hot = np.array(score_one_hot)\n",
        "\n",
        "  label_tensor = torch.tensor(label_list)\n",
        "  label_tensor = label_tensor.reshape(label_tensor.shape[0], 1)\n",
        "  label_one_hot = torch.zeros(label_tensor.shape[0], 3)\n",
        "  label_one_hot.scatter_(dim=1, index=label_tensor, value=1)\n",
        "  label_one_hot = np.array(label_one_hot)\n",
        "  ns_c = 0\n",
        "  for i in range(len(score_list)):\n",
        "    score_list\n",
        "  print('score_array shape', score_one_hot.shape)\n",
        "  print('label_one_hot shape', label_one_hot.shape)\n",
        "  # roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "  print('roc_auc_score', roc_auc_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('recall_score', recall_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('precision_score', precision_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('f1_score', f1_score(label_one_hot, score_one_hot, average=None))\n",
        "  return 0"
      ],
      "metadata": {
        "id": "pdVMkkYdk9mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrU-qZWicr1D",
        "outputId": "4699b1c8-27d2-4b41-e0a8-b27ee3406196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6203125\n",
            "score_array shape (640, 3)\n",
            "label_one_hot shape (640, 3)\n",
            "roc_auc_score [0.75847956 0.72840219 0.71149633]\n",
            "recall_score [0.72093023 0.59580052 0.62427746]\n",
            "precision_score [0.35428571 0.86311787 0.53465347]\n",
            "f1_score [0.47509579 0.70496894 0.576     ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tissue"
      ],
      "metadata": {
        "id": "t__KAiGVqs-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "8NK4Hd2Kqu6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6Ia6GBh7c1Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "# dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=True)\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-640, 640])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee61323-2fa4-4f55-f23d-c53ea79828a9",
        "id": "pLG92vu8q0Cv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbf3d4d-269e-4432-aff8-1be585a20c90",
        "id": "31GexGKfq0Cv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001) # , weight_decay=0.0025\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "# criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "t2uBSUjkq0Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ad7579-4b8a-4e96-f14e-c6a7efc10855",
        "id": "irZMR1LXq0Cw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    \n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "qqJS4uQbq0Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "ji71cxYMq0Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "\n",
        "train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in enumerate(dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    digits = model_class(data)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long)\n",
        "    # print(type(label))\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "\n",
        "    # L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  train_loss_record.append(loss.item())\n",
        "\n",
        "  val_loss, val_acc = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  val_acc_record.append(val_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  # val_r2_record.append(val_r2)\n",
        "  # val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc = test_acc_output(epoch)\n",
        "  test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  # test_r2_record.append(test_r2)\n",
        "  # test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f2655c-f13e-438f-f411-e31ce13f91a7",
        "id": "xyQfTLVTq0Cx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:148: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:1.0336371883749962 | ACC: 45.4000%(227/500)\n",
            "Testing Epoch[0] Loss:1.0182917982339859 | ACC: 47.9688%(307/640)\n",
            "Training Epoch[1] Loss:0.9899523630738258 | ACC: 51.0000%(255/500)\n",
            "Testing Epoch[1] Loss:1.0081612586975097 | ACC: 46.8750%(300/640)\n",
            "Training Epoch[2] Loss:0.9416170977056026 | ACC: 53.4000%(267/500)\n",
            "Testing Epoch[2] Loss:0.9968519151210785 | ACC: 49.3750%(316/640)\n",
            "Training Epoch[3] Loss:0.9301662184298038 | ACC: 55.0000%(275/500)\n",
            "Testing Epoch[3] Loss:0.9730575382709503 | ACC: 51.7188%(331/640)\n",
            "Training Epoch[4] Loss:0.8958520404994488 | ACC: 57.6000%(288/500)\n",
            "Testing Epoch[4] Loss:0.9804816305637359 | ACC: 50.6250%(324/640)\n",
            "Training Epoch[5] Loss:0.8898449540138245 | ACC: 58.6000%(293/500)\n",
            "Testing Epoch[5] Loss:0.9715069234371185 | ACC: 52.1875%(334/640)\n",
            "Training Epoch[6] Loss:0.8714295104146004 | ACC: 61.6000%(308/500)\n",
            "Testing Epoch[6] Loss:0.9776925861835479 | ACC: 51.4062%(329/640)\n",
            "Training Epoch[7] Loss:0.8604600131511688 | ACC: 62.0000%(310/500)\n",
            "Testing Epoch[7] Loss:0.9726901203393936 | ACC: 53.2812%(341/640)\n",
            "Training Epoch[8] Loss:0.8449266701936722 | ACC: 62.8000%(314/500)\n",
            "Testing Epoch[8] Loss:0.9727698296308518 | ACC: 54.0625%(346/640)\n",
            "Training Epoch[9] Loss:0.8090957477688789 | ACC: 62.2000%(311/500)\n",
            "Testing Epoch[9] Loss:0.9858051776885987 | ACC: 55.7812%(357/640)\n",
            "Training Epoch[10] Loss:0.8211468644440174 | ACC: 63.0000%(315/500)\n",
            "Testing Epoch[10] Loss:0.9788308650255203 | ACC: 55.1562%(353/640)\n",
            "Training Epoch[11] Loss:0.7958022467792034 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[11] Loss:0.9988011747598649 | ACC: 55.1562%(353/640)\n",
            "Training Epoch[12] Loss:0.8189117945730686 | ACC: 65.6000%(328/500)\n",
            "Testing Epoch[12] Loss:1.0159212440252303 | ACC: 55.3125%(354/640)\n",
            "Training Epoch[13] Loss:0.8078184574842453 | ACC: 65.0000%(325/500)\n",
            "Testing Epoch[13] Loss:1.0067325115203858 | ACC: 57.0312%(365/640)\n",
            "Training Epoch[14] Loss:0.8047905117273331 | ACC: 65.8000%(329/500)\n",
            "Testing Epoch[14] Loss:1.0074866473674775 | ACC: 56.4062%(361/640)\n",
            "Training Epoch[15] Loss:0.8194949962198734 | ACC: 66.2000%(331/500)\n",
            "Testing Epoch[15] Loss:1.0560739517211915 | ACC: 55.3125%(354/640)\n",
            "Training Epoch[16] Loss:0.7883241716772318 | ACC: 67.8000%(339/500)\n",
            "Testing Epoch[16] Loss:1.0494862914085388 | ACC: 56.4062%(361/640)\n",
            "Training Epoch[17] Loss:0.8186713680624962 | ACC: 68.0000%(340/500)\n",
            "Testing Epoch[17] Loss:1.0432783126831056 | ACC: 57.8125%(370/640)\n",
            "Training Epoch[18] Loss:0.8001457341015339 | ACC: 69.2000%(346/500)\n",
            "Testing Epoch[18] Loss:1.0790179431438447 | ACC: 55.9375%(358/640)\n",
            "Training Epoch[19] Loss:0.8039506822824478 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[19] Loss:1.0680933266878128 | ACC: 59.2188%(379/640)\n",
            "Training Epoch[20] Loss:0.8283265195786953 | ACC: 68.2000%(341/500)\n",
            "Testing Epoch[20] Loss:1.14678952395916 | ACC: 55.6250%(356/640)\n",
            "Training Epoch[21] Loss:0.819781880825758 | ACC: 69.4000%(347/500)\n",
            "Testing Epoch[21] Loss:1.16796672642231 | ACC: 59.0625%(378/640)\n",
            "Training Epoch[22] Loss:0.7924517802894115 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[22] Loss:1.1042373180389404 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[23] Loss:0.772844959050417 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[23] Loss:1.122927290201187 | ACC: 60.1562%(385/640)\n",
            "Training Epoch[24] Loss:0.8324197754263878 | ACC: 70.0000%(350/500)\n",
            "Testing Epoch[24] Loss:1.1359052211046219 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[25] Loss:0.8226652964949608 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[25] Loss:1.184922033548355 | ACC: 59.6875%(382/640)\n",
            "Training Epoch[26] Loss:0.8271550796926022 | ACC: 70.4000%(352/500)\n",
            "Testing Epoch[26] Loss:1.1511862933635713 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[27] Loss:0.805730864405632 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[27] Loss:1.2097899109125136 | ACC: 59.5312%(381/640)\n",
            "Training Epoch[28] Loss:0.7961003389209509 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[28] Loss:1.2301241010427475 | ACC: 58.7500%(376/640)\n",
            "Training Epoch[29] Loss:0.8620981443673372 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[29] Loss:1.2792869746685027 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[30] Loss:0.8202025387436152 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[30] Loss:1.2906829863786697 | ACC: 59.3750%(380/640)\n",
            "Training Epoch[31] Loss:0.8181306105107069 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[31] Loss:1.268247091770172 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[32] Loss:0.8377862963825464 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[32] Loss:1.2817848324775696 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[33] Loss:0.8633521813899279 | ACC: 72.2000%(361/500)\n",
            "Testing Epoch[33] Loss:1.2611035317182542 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[34] Loss:0.8581606540828943 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[34] Loss:1.3227223306894302 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[35] Loss:0.8979475758969784 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[35] Loss:1.4031814068555832 | ACC: 59.6875%(382/640)\n",
            "Training Epoch[36] Loss:0.8799100760370493 | ACC: 72.0000%(360/500)\n",
            "Testing Epoch[36] Loss:1.4605914056301117 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[37] Loss:0.892873065546155 | ACC: 71.8000%(359/500)\n",
            "Testing Epoch[37] Loss:1.4702895343303681 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[38] Loss:0.8934843391180038 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[38] Loss:1.3747669547796249 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[39] Loss:0.9223220273852348 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[39] Loss:1.3962471961975098 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[40] Loss:0.9272825345396996 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[40] Loss:1.4638837277889252 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[41] Loss:0.9818495512008667 | ACC: 70.8000%(354/500)\n",
            "Testing Epoch[41] Loss:1.4968578785657882 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[42] Loss:0.8667930327355862 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[42] Loss:1.421508339047432 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[43] Loss:0.9260744750499725 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[43] Loss:1.4824404299259186 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[44] Loss:0.9454157706350088 | ACC: 71.6000%(358/500)\n",
            "Testing Epoch[44] Loss:1.448783853650093 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[45] Loss:0.9129766281694174 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[45] Loss:1.4619974792003632 | ACC: 64.3750%(412/640)\n",
            "Training Epoch[46] Loss:0.9609217215329409 | ACC: 73.2000%(366/500)\n",
            "Testing Epoch[46] Loss:1.472017315030098 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[47] Loss:0.8536869622766972 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[47] Loss:1.4636624336242676 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[48] Loss:0.8842117004096508 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[48] Loss:1.513741797208786 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[49] Loss:0.8670902643352747 | ACC: 71.4000%(357/500)\n",
            "Testing Epoch[49] Loss:1.5574945569038392 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[50] Loss:0.9396987687796354 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[50] Loss:1.4704113721847534 | ACC: 63.7500%(408/640)\n",
            "Training Epoch[51] Loss:0.980330765247345 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[51] Loss:1.5458712458610535 | ACC: 60.6250%(388/640)\n",
            "Training Epoch[52] Loss:0.8659010529518127 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[52] Loss:1.5884176522493363 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[53] Loss:0.8676794320344925 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[53] Loss:1.5222253650426865 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[54] Loss:0.8890463374555111 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[54] Loss:1.5468587398529052 | ACC: 65.0000%(416/640)\n",
            "Training Epoch[55] Loss:0.9777142386883497 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[55] Loss:1.6150816798210144 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[56] Loss:0.9735681675374508 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[56] Loss:1.6178361386060716 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[57] Loss:0.8361087199300528 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[57] Loss:1.7432985663414002 | ACC: 60.3125%(386/640)\n",
            "Training Epoch[58] Loss:0.9620023351162672 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[58] Loss:1.6808999627828598 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[59] Loss:0.9490934889763594 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[59] Loss:1.7093422949314117 | ACC: 61.2500%(392/640)\n",
            "Training Epoch[60] Loss:0.9602416008710861 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[60] Loss:1.7035131752490997 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[61] Loss:0.902108944952488 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[61] Loss:1.671757709980011 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[62] Loss:0.9309284947812557 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[62] Loss:1.8157704293727874 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[63] Loss:0.872830368578434 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[63] Loss:1.6807095766067506 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[64] Loss:1.0358299110084772 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[64] Loss:1.6690136551856996 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[65] Loss:0.9192557279020548 | ACC: 74.2000%(371/500)\n",
            "Testing Epoch[65] Loss:1.693832904100418 | ACC: 59.5312%(381/640)\n",
            "Training Epoch[66] Loss:0.9166763387620449 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[66] Loss:1.7887767136096955 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[67] Loss:0.8123355908319354 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[67] Loss:1.6462768256664275 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[68] Loss:0.8827775958925486 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[68] Loss:1.7264274656772614 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[69] Loss:0.9525991622358561 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[69] Loss:1.7540705025196075 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[70] Loss:0.8609410747885704 | ACC: 75.2000%(376/500)\n",
            "Testing Epoch[70] Loss:1.6843818366527556 | ACC: 65.0000%(416/640)\n",
            "Training Epoch[71] Loss:1.060219869017601 | ACC: 69.8000%(349/500)\n",
            "Testing Epoch[71] Loss:1.9111930400133132 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[72] Loss:0.9550894685089588 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[72] Loss:1.6812912970781326 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[73] Loss:0.9028101488947868 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[73] Loss:1.7591216087341308 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[74] Loss:0.9149471577256918 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[74] Loss:1.8501185297966003 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[75] Loss:0.8377532735466957 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[75] Loss:1.732403814792633 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[76] Loss:0.933865437284112 | ACC: 74.0000%(370/500)\n",
            "Testing Epoch[76] Loss:1.8036290287971497 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[77] Loss:0.9515164624899626 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[77] Loss:1.7103554844856261 | ACC: 64.3750%(412/640)\n",
            "Training Epoch[78] Loss:0.88795530423522 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[78] Loss:1.619754785299301 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[79] Loss:0.9306147918105125 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[79] Loss:1.8093043267726898 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[80] Loss:0.8476188201457262 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[80] Loss:1.8759280383586883 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[81] Loss:0.9572635218501091 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[81] Loss:1.8079143345355988 | ACC: 62.0312%(397/640)\n",
            "Training Epoch[82] Loss:0.8750506937503815 | ACC: 75.0000%(375/500)\n",
            "Testing Epoch[82] Loss:1.797317323088646 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[83] Loss:0.8575858566910028 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[83] Loss:1.795223468542099 | ACC: 60.4688%(387/640)\n",
            "Training Epoch[84] Loss:0.8568966621533036 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[84] Loss:1.752775102853775 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[85] Loss:0.7806811444461346 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[85] Loss:1.6435693681240082 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[86] Loss:0.7640990801155567 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[86] Loss:1.7229536592960357 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[87] Loss:0.8464438542723656 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[87] Loss:1.766654634475708 | ACC: 64.8438%(415/640)\n",
            "Training Epoch[88] Loss:1.057247125543654 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[88] Loss:1.9308353245258332 | ACC: 63.9062%(409/640)\n",
            "Training Epoch[89] Loss:1.0545303374528885 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[89] Loss:1.7215966403484344 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[90] Loss:0.9186209980398417 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[90] Loss:1.855295616388321 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[91] Loss:0.8253982607275248 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[91] Loss:1.8567400395870208 | ACC: 63.7500%(408/640)\n",
            "Training Epoch[92] Loss:0.9262886755168438 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[92] Loss:1.6523903906345367 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[93] Loss:0.8759961426258087 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[93] Loss:1.8051822781562805 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[94] Loss:1.0211974028497934 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[94] Loss:1.9153227120637895 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[95] Loss:1.0805579461157322 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[95] Loss:1.7970789343118667 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[96] Loss:0.8408850207924843 | ACC: 76.0000%(380/500)\n",
            "Testing Epoch[96] Loss:1.6983225971460343 | ACC: 63.7500%(408/640)\n",
            "Training Epoch[97] Loss:0.7274479176849127 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[97] Loss:1.7306967318058013 | ACC: 64.0625%(410/640)\n",
            "Training Epoch[98] Loss:0.783541870303452 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[98] Loss:1.8246185779571533 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[99] Loss:0.7780725955963135 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[99] Loss:1.7777868807315826 | ACC: 64.3750%(412/640)\n",
            "Training Epoch[100] Loss:1.0466138739138842 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[100] Loss:1.9054813504219055 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[101] Loss:0.9862917270511389 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[101] Loss:1.7316168785095214 | ACC: 62.1875%(398/640)\n",
            "Training Epoch[102] Loss:0.8221430201083422 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[102] Loss:1.8118979454040527 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[103] Loss:0.8102054744958878 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[103] Loss:1.901062273979187 | ACC: 60.9375%(390/640)\n",
            "Training Epoch[104] Loss:0.7285723835229874 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[104] Loss:1.801095151901245 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[105] Loss:0.6923350254073739 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[105] Loss:1.783234441280365 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[106] Loss:0.7967159952968359 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[106] Loss:1.8029270231723786 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[107] Loss:1.1496995575726032 | ACC: 71.2000%(356/500)\n",
            "Testing Epoch[107] Loss:2.09386984705925 | ACC: 60.0000%(384/640)\n",
            "Training Epoch[108] Loss:0.9291362147778273 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[108] Loss:1.7160998791456223 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[109] Loss:0.7944420203566551 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[109] Loss:1.6989100873470306 | ACC: 64.0625%(410/640)\n",
            "Training Epoch[110] Loss:0.7832596367225051 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[110] Loss:1.7825723558664321 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[111] Loss:0.8077251967042685 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[111] Loss:1.7768561959266662 | ACC: 64.8438%(415/640)\n",
            "Training Epoch[112] Loss:0.7695204485207796 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[112] Loss:1.7083218693733215 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[113] Loss:0.7522357702255249 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[113] Loss:1.7237940788269044 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[114] Loss:0.758166566491127 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[114] Loss:1.7474934577941894 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[115] Loss:0.7407570499926805 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[115] Loss:1.786330556869507 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[116] Loss:1.5232213735580444 | ACC: 65.4000%(327/500)\n",
            "Testing Epoch[116] Loss:2.271964579820633 | ACC: 58.1250%(372/640)\n",
            "Training Epoch[117] Loss:0.9381420519202948 | ACC: 73.6000%(368/500)\n",
            "Testing Epoch[117] Loss:1.6398616850376129 | ACC: 59.3750%(380/640)\n",
            "Training Epoch[118] Loss:0.7375687602907419 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[118] Loss:1.5985276699066162 | ACC: 64.0625%(410/640)\n",
            "Training Epoch[119] Loss:0.7043646406382322 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[119] Loss:1.6150022625923157 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[120] Loss:0.693347075022757 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[120] Loss:1.658797550201416 | ACC: 65.6250%(420/640)\n",
            "Training Epoch[121] Loss:0.7090270910412073 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[121] Loss:1.6743758380413056 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[122] Loss:0.7050071023404598 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[122] Loss:1.7059382796287537 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[123] Loss:0.7194583732634783 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[123] Loss:1.7249543249607087 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[124] Loss:0.7120625916868448 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[124] Loss:1.7698580205440522 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[125] Loss:0.7299292180687189 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[125] Loss:1.7684579730033874 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[126] Loss:0.7488938383758068 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[126] Loss:1.813736128807068 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[127] Loss:1.803819254040718 | ACC: 59.6000%(298/500)\n",
            "Testing Epoch[127] Loss:2.5551962554454803 | ACC: 53.1250%(340/640)\n",
            "Training Epoch[128] Loss:0.8556990418583155 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[128] Loss:1.5060865342617036 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[129] Loss:0.7115692719817162 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[129] Loss:1.50131958425045 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[130] Loss:0.7459748927503824 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[130] Loss:1.5335388660430909 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[131] Loss:0.7046937420964241 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[131] Loss:1.5491957157850265 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[132] Loss:0.7472935393452644 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[132] Loss:1.5839551270008088 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[133] Loss:0.754038579761982 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[133] Loss:1.5947405070066452 | ACC: 67.1875%(430/640)\n",
            "Training Epoch[134] Loss:0.7410406693816185 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[134] Loss:1.634529709815979 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[135] Loss:0.7728408966213465 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[135] Loss:1.661444717645645 | ACC: 67.1875%(430/640)\n",
            "Training Epoch[136] Loss:0.7750572953373194 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[136] Loss:1.698660534620285 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[137] Loss:0.8210089690983295 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[137] Loss:1.7207618653774261 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[138] Loss:0.8459640108048916 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[138] Loss:1.7898387253284453 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[139] Loss:0.9556636027991772 | ACC: 70.6000%(353/500)\n",
            "Testing Epoch[139] Loss:1.4663922399282456 | ACC: 59.8438%(383/640)\n",
            "Training Epoch[140] Loss:0.6796265169978142 | ACC: 76.4000%(382/500)\n",
            "Testing Epoch[140] Loss:1.4698874592781066 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[141] Loss:0.682727325707674 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[141] Loss:1.4438384532928468 | ACC: 64.0625%(410/640)\n",
            "Training Epoch[142] Loss:0.6431401632726192 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[142] Loss:1.4872832775115967 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[143] Loss:0.6704364959150553 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[143] Loss:1.5099541068077087 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[144] Loss:0.6774126961827278 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[144] Loss:1.5416040241718292 | ACC: 64.2188%(411/640)\n",
            "Training Epoch[145] Loss:0.68890873901546 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[145] Loss:1.5777541935443877 | ACC: 64.6875%(414/640)\n",
            "Training Epoch[146] Loss:0.6959672849625349 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[146] Loss:1.5965162217617035 | ACC: 65.0000%(416/640)\n",
            "Training Epoch[147] Loss:0.7087082769721746 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[147] Loss:1.6263655960559844 | ACC: 64.8438%(415/640)\n",
            "Training Epoch[148] Loss:0.7320991680026054 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[148] Loss:1.6461908042430877 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[149] Loss:1.153205567970872 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[149] Loss:2.1009376406669618 | ACC: 60.7812%(389/640)\n",
            "Training Epoch[150] Loss:0.845412090420723 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[150] Loss:1.5527983546257018 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[151] Loss:0.6948504308238626 | ACC: 77.2000%(386/500)\n",
            "Testing Epoch[151] Loss:1.4647413223981858 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[152] Loss:0.630675183609128 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[152] Loss:1.5115308910608292 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[153] Loss:0.6426649764180183 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[153] Loss:1.4974977403879166 | ACC: 64.8438%(415/640)\n",
            "Training Epoch[154] Loss:0.6256038462743163 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[154] Loss:1.5377625852823258 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[155] Loss:0.6287969835102558 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[155] Loss:1.5737188547849654 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[156] Loss:0.6420071925967932 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[156] Loss:1.6011373043060302 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[157] Loss:0.644370824098587 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[157] Loss:1.655665084719658 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[158] Loss:0.6797833237797022 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[158] Loss:1.6807314783334732 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[159] Loss:0.6697517689317465 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[159] Loss:1.7212388932704925 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[160] Loss:1.602217011153698 | ACC: 63.8000%(319/500)\n",
            "Testing Epoch[160] Loss:2.283927267789841 | ACC: 54.3750%(348/640)\n",
            "Training Epoch[161] Loss:0.7612367328256369 | ACC: 74.8000%(374/500)\n",
            "Testing Epoch[161] Loss:1.4108752965927125 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[162] Loss:0.6667693760246038 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[162] Loss:1.3976219862699508 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[163] Loss:0.6253979336470366 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[163] Loss:1.3933565825223924 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[164] Loss:0.6259347032755613 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[164] Loss:1.390234687924385 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[165] Loss:0.6317875683307648 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[165] Loss:1.4362955063581466 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[166] Loss:0.6361241731792688 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[166] Loss:1.4683124661445617 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[167] Loss:0.6460603643208742 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[167] Loss:1.5066609054803848 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[168] Loss:0.6583098694682121 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[168] Loss:1.5418109595775604 | ACC: 67.0312%(429/640)\n",
            "Training Epoch[169] Loss:0.673134557902813 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[169] Loss:1.58163480758667 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[170] Loss:0.6913587097078562 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[170] Loss:1.590074709057808 | ACC: 67.0312%(429/640)\n",
            "Training Epoch[171] Loss:1.793219268321991 | ACC: 61.6000%(308/500)\n",
            "Testing Epoch[171] Loss:2.3604006469249725 | ACC: 52.6562%(337/640)\n",
            "Training Epoch[172] Loss:0.7643114645034075 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[172] Loss:1.3873868852853775 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[173] Loss:0.6498779505491257 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[173] Loss:1.4090597093105317 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[174] Loss:0.6553663425147533 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[174] Loss:1.4107329666614532 | ACC: 64.3750%(412/640)\n",
            "Training Epoch[175] Loss:0.6414642632007599 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[175] Loss:1.4282342582941054 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[176] Loss:0.6451618708670139 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[176] Loss:1.4588174909353255 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[177] Loss:0.6600928548723459 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[177] Loss:1.493657723069191 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[178] Loss:0.6661094482988119 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[178] Loss:1.5268070548772812 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[179] Loss:0.6784354615956545 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[179] Loss:1.5570342361927032 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[180] Loss:0.6738157197833061 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[180] Loss:1.6281932890415192 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[181] Loss:0.7293248660862446 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[181] Loss:1.6433246672153472 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[182] Loss:1.7215881869196892 | ACC: 66.6000%(333/500)\n",
            "Testing Epoch[182] Loss:2.3935092210769655 | ACC: 56.5625%(362/640)\n",
            "Training Epoch[183] Loss:0.8979766257107258 | ACC: 72.8000%(364/500)\n",
            "Testing Epoch[183] Loss:1.4353621512651444 | ACC: 61.8750%(396/640)\n",
            "Training Epoch[184] Loss:0.6904814392328262 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[184] Loss:1.3768407255411148 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[185] Loss:0.6830148734152317 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[185] Loss:1.4816538512706756 | ACC: 62.5000%(400/640)\n",
            "Training Epoch[186] Loss:0.6799660343676805 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[186] Loss:1.519555503129959 | ACC: 62.9688%(403/640)\n",
            "Training Epoch[187] Loss:0.6745179425925016 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[187] Loss:1.5567965030670166 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[188] Loss:0.6841784389689565 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[188] Loss:1.5901241540908813 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[189] Loss:0.6888638371601701 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[189] Loss:1.6241792023181916 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[190] Loss:0.7070311056450009 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[190] Loss:1.64467031955719 | ACC: 62.3438%(399/640)\n",
            "Training Epoch[191] Loss:0.7031241962686181 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[191] Loss:1.6913291931152343 | ACC: 62.8125%(402/640)\n",
            "Training Epoch[192] Loss:0.7461205469444394 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[192] Loss:1.7155264437198638 | ACC: 63.1250%(404/640)\n",
            "Training Epoch[193] Loss:1.8118111416697502 | ACC: 61.6000%(308/500)\n",
            "Testing Epoch[193] Loss:2.362953335046768 | ACC: 54.3750%(348/640)\n",
            "Training Epoch[194] Loss:0.8245670702308416 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[194] Loss:1.4437552720308304 | ACC: 62.6562%(401/640)\n",
            "Training Epoch[195] Loss:0.7436335273087025 | ACC: 75.6000%(378/500)\n",
            "Testing Epoch[195] Loss:1.3846950024366378 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[196] Loss:0.6822359822690487 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[196] Loss:1.435351049900055 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[197] Loss:0.6723111197352409 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[197] Loss:1.4400494575500489 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[198] Loss:0.6802344750612974 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[198] Loss:1.4797811329364776 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[199] Loss:0.6906761452555656 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[199] Loss:1.5186314105987548 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[200] Loss:0.6999775469303131 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[200] Loss:1.5537049174308777 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[201] Loss:0.7060234751552343 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[201] Loss:1.5912816047668457 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[202] Loss:0.741058612242341 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[202] Loss:1.618411910533905 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[203] Loss:1.5400054566562176 | ACC: 67.4000%(337/500)\n",
            "Testing Epoch[203] Loss:2.235555389523506 | ACC: 58.7500%(376/640)\n",
            "Training Epoch[204] Loss:0.8512484487146139 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[204] Loss:1.4451603531837462 | ACC: 64.0625%(410/640)\n",
            "Training Epoch[205] Loss:0.7019277140498161 | ACC: 77.4000%(387/500)\n",
            "Testing Epoch[205] Loss:1.4083883821964265 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[206] Loss:0.6931339614093304 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[206] Loss:1.4849291175603867 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[207] Loss:0.6749682500958443 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[207] Loss:1.504588606953621 | ACC: 65.0000%(416/640)\n",
            "Training Epoch[208] Loss:0.6823597811162472 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[208] Loss:1.5204392969608307 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[209] Loss:0.6843422520905733 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[209] Loss:1.5397670686244964 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[210] Loss:0.6921866983175278 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[210] Loss:1.567026749253273 | ACC: 65.6250%(420/640)\n",
            "Training Epoch[211] Loss:0.7022395730018616 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[211] Loss:1.601084616780281 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[212] Loss:0.7169908508658409 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[212] Loss:1.6219411581754684 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[213] Loss:0.7324173990637064 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[213] Loss:1.6923308461904525 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[214] Loss:0.8290685750544071 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[214] Loss:1.7531103312969207 | ACC: 65.6250%(420/640)\n",
            "Training Epoch[215] Loss:0.872054435312748 | ACC: 72.6000%(363/500)\n",
            "Testing Epoch[215] Loss:1.460966593027115 | ACC: 61.0938%(391/640)\n",
            "Training Epoch[216] Loss:0.7561204712837934 | ACC: 75.8000%(379/500)\n",
            "Testing Epoch[216] Loss:1.4446798264980316 | ACC: 64.3750%(412/640)\n",
            "Training Epoch[217] Loss:0.6841808166354895 | ACC: 78.2000%(391/500)\n",
            "Testing Epoch[217] Loss:1.4415597587823867 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[218] Loss:0.6940747890621424 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[218] Loss:1.479346540570259 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[219] Loss:0.6899860166013241 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[219] Loss:1.5013029485940934 | ACC: 65.0000%(416/640)\n",
            "Training Epoch[220] Loss:0.7008864572271705 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[220] Loss:1.5326383620500565 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[221] Loss:0.7164529245346785 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[221] Loss:1.5671952545642853 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[222] Loss:0.7220247387886047 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[222] Loss:1.58572578728199 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[223] Loss:0.7400003504008055 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[223] Loss:1.617123305797577 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[224] Loss:0.7651889361441135 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[224] Loss:1.7049086958169937 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[225] Loss:0.9671947527676821 | ACC: 73.0000%(365/500)\n",
            "Testing Epoch[225] Loss:1.7121153473854065 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[226] Loss:0.7132632844150066 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[226] Loss:1.5120847344398498 | ACC: 63.2812%(405/640)\n",
            "Training Epoch[227] Loss:0.6155570782721043 | ACC: 80.6000%(403/500)\n",
            "Testing Epoch[227] Loss:1.4533962309360504 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[228] Loss:0.6371973920613527 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[228] Loss:1.484233444929123 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[229] Loss:0.6423003245145082 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[229] Loss:1.4995307147502899 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[230] Loss:0.6551017127931118 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[230] Loss:1.534844708442688 | ACC: 67.0312%(429/640)\n",
            "Training Epoch[231] Loss:0.6670612562447786 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[231] Loss:1.5591441720724106 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[232] Loss:0.6734273489564657 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[232] Loss:1.5878544390201568 | ACC: 67.1875%(430/640)\n",
            "Training Epoch[233] Loss:0.6815703045576811 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[233] Loss:1.6129910558462144 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[234] Loss:0.7339535132050514 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[234] Loss:1.722603690624237 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[235] Loss:0.8450522515922785 | ACC: 73.4000%(367/500)\n",
            "Testing Epoch[235] Loss:1.5314562767744064 | ACC: 61.4062%(393/640)\n",
            "Training Epoch[236] Loss:0.7166419215500355 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[236] Loss:1.457305473089218 | ACC: 65.3125%(418/640)\n",
            "Training Epoch[237] Loss:0.63287035189569 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[237] Loss:1.5069314122200013 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[238] Loss:0.6196720916777849 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[238] Loss:1.5035985946655273 | ACC: 67.0312%(429/640)\n",
            "Training Epoch[239] Loss:0.6439372599124908 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[239] Loss:1.5403385609388351 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[240] Loss:0.6448276285082102 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[240] Loss:1.5690555065870284 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[241] Loss:0.6521759424358606 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[241] Loss:1.5948005080223084 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[242] Loss:0.6785985995084047 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[242] Loss:1.6327245861291886 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[243] Loss:0.6672284454107285 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[243] Loss:1.6482176035642624 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[244] Loss:0.6994103156030178 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[244] Loss:1.697627004981041 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[245] Loss:0.8148394580930471 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[245] Loss:1.7910827159881593 | ACC: 64.5312%(413/640)\n",
            "Training Epoch[246] Loss:0.8260435461997986 | ACC: 74.6000%(373/500)\n",
            "Testing Epoch[246] Loss:1.5432233154773711 | ACC: 59.2188%(379/640)\n",
            "Training Epoch[247] Loss:0.708567202091217 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[247] Loss:1.4947392791509628 | ACC: 64.3750%(412/640)\n",
            "Training Epoch[248] Loss:0.6479558367282152 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[248] Loss:1.4331863939762115 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[249] Loss:0.6368320807814598 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[249] Loss:1.4765027195215226 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[250] Loss:0.646004781126976 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[250] Loss:1.5251250505447387 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[251] Loss:0.6549607794731855 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[251] Loss:1.5566801309585572 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[252] Loss:0.6640783585608006 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[252] Loss:1.586503240466118 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[253] Loss:0.6707541402429342 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[253] Loss:1.6108651757240295 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[254] Loss:0.6870721522718668 | ACC: 81.2000%(406/500)\n",
            "Testing Epoch[254] Loss:1.6407784461975097 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[255] Loss:0.7012729495763779 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[255] Loss:1.6741759777069092 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[256] Loss:1.7980774603784084 | ACC: 62.4000%(312/500)\n",
            "Testing Epoch[256] Loss:2.4494725942611693 | ACC: 55.9375%(358/640)\n",
            "Training Epoch[257] Loss:0.8310781363397837 | ACC: 73.8000%(369/500)\n",
            "Testing Epoch[257] Loss:1.5312694132328033 | ACC: 61.7188%(395/640)\n",
            "Training Epoch[258] Loss:0.6477225441485643 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[258] Loss:1.431599485874176 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[259] Loss:0.619161419570446 | ACC: 80.4000%(402/500)\n",
            "Testing Epoch[259] Loss:1.450234904885292 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[260] Loss:0.6257122308015823 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[260] Loss:1.4923862159252166 | ACC: 65.7812%(421/640)\n",
            "Training Epoch[261] Loss:0.6270885448902845 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[261] Loss:1.5079131931066514 | ACC: 65.6250%(420/640)\n",
            "Training Epoch[262] Loss:0.6323510613292456 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[262] Loss:1.5413323044776917 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[263] Loss:0.6486066170036793 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[263] Loss:1.554598221182823 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[264] Loss:0.6539115980267525 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[264] Loss:1.5971423178911208 | ACC: 65.6250%(420/640)\n",
            "Training Epoch[265] Loss:0.6719318591058254 | ACC: 80.8000%(404/500)\n",
            "Testing Epoch[265] Loss:1.6089870184659958 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[266] Loss:0.684068851172924 | ACC: 81.4000%(407/500)\n",
            "Testing Epoch[266] Loss:1.6748979806900024 | ACC: 65.1562%(417/640)\n",
            "Training Epoch[267] Loss:0.7098831497132778 | ACC: 81.0000%(405/500)\n",
            "Testing Epoch[267] Loss:1.6890952050685883 | ACC: 67.3438%(431/640)\n",
            "Training Epoch[268] Loss:1.3132815286517143 | ACC: 66.4000%(332/500)\n",
            "Testing Epoch[268] Loss:2.0116992205381394 | ACC: 53.9062%(345/640)\n",
            "Training Epoch[269] Loss:0.7760832086205482 | ACC: 76.2000%(381/500)\n",
            "Testing Epoch[269] Loss:1.4085267633199692 | ACC: 63.4375%(406/640)\n",
            "Training Epoch[270] Loss:0.6662641633301973 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[270] Loss:1.4297315657138825 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[271] Loss:0.6718113776296377 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[271] Loss:1.4525771290063858 | ACC: 63.9062%(409/640)\n",
            "Training Epoch[272] Loss:0.6726517379283905 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[272] Loss:1.4766855627298354 | ACC: 65.6250%(420/640)\n",
            "Training Epoch[273] Loss:0.688512409105897 | ACC: 80.0000%(400/500)\n",
            "Testing Epoch[273] Loss:1.504188221693039 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[274] Loss:0.6998775042593479 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[274] Loss:1.5315481632947923 | ACC: 66.2500%(424/640)\n",
            "Training Epoch[275] Loss:0.7088041957467794 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[275] Loss:1.560763481259346 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[276] Loss:0.7227070853114128 | ACC: 79.6000%(398/500)\n",
            "Testing Epoch[276] Loss:1.5881485849618913 | ACC: 65.9375%(422/640)\n",
            "Training Epoch[277] Loss:0.7219707760959864 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[277] Loss:1.6110030204057693 | ACC: 66.5625%(426/640)\n",
            "Training Epoch[278] Loss:0.7991742063313723 | ACC: 79.8000%(399/500)\n",
            "Testing Epoch[278] Loss:1.677363756299019 | ACC: 66.0938%(423/640)\n",
            "Training Epoch[279] Loss:0.9544860869646072 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[279] Loss:1.3534223467111588 | ACC: 61.5625%(394/640)\n",
            "Training Epoch[280] Loss:0.7073684744536877 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[280] Loss:1.3614227205514908 | ACC: 67.1875%(430/640)\n",
            "Training Epoch[281] Loss:0.634345443919301 | ACC: 80.2000%(401/500)\n",
            "Testing Epoch[281] Loss:1.406409004330635 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[282] Loss:0.639414744451642 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[282] Loss:1.4478175938129425 | ACC: 67.3438%(431/640)\n",
            "Training Epoch[283] Loss:0.654975289478898 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[283] Loss:1.4870464503765106 | ACC: 67.6562%(433/640)\n",
            "Training Epoch[284] Loss:0.6648783534765244 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[284] Loss:1.511528280377388 | ACC: 67.8125%(434/640)\n",
            "Training Epoch[285] Loss:0.6814494002610445 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[285] Loss:1.542643478512764 | ACC: 67.9688%(435/640)\n",
            "Training Epoch[286] Loss:0.6884159334003925 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[286] Loss:1.5756895631551742 | ACC: 68.4375%(438/640)\n",
            "Training Epoch[287] Loss:0.7012651152908802 | ACC: 79.2000%(396/500)\n",
            "Testing Epoch[287] Loss:1.593442088365555 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[288] Loss:0.7067517414689064 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[288] Loss:1.6288800597190858 | ACC: 67.8125%(434/640)\n",
            "Training Epoch[289] Loss:1.0227292589843273 | ACC: 74.4000%(372/500)\n",
            "Testing Epoch[289] Loss:1.7629867434501647 | ACC: 66.4062%(425/640)\n",
            "Training Epoch[290] Loss:0.9026841782033443 | ACC: 72.4000%(362/500)\n",
            "Testing Epoch[290] Loss:1.5066556692123414 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[291] Loss:0.6766621023416519 | ACC: 76.6000%(383/500)\n",
            "Testing Epoch[291] Loss:1.4314524590969087 | ACC: 63.5938%(407/640)\n",
            "Training Epoch[292] Loss:0.6396421799436212 | ACC: 78.8000%(394/500)\n",
            "Testing Epoch[292] Loss:1.4514070391654967 | ACC: 65.4688%(419/640)\n",
            "Training Epoch[293] Loss:0.6475268490612507 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[293] Loss:1.4587947130203247 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[294] Loss:0.6457396969199181 | ACC: 78.6000%(393/500)\n",
            "Testing Epoch[294] Loss:1.4691447556018828 | ACC: 67.1875%(430/640)\n",
            "Training Epoch[295] Loss:0.6563115790486336 | ACC: 78.4000%(392/500)\n",
            "Testing Epoch[295] Loss:1.497642570734024 | ACC: 67.1875%(430/640)\n",
            "Training Epoch[296] Loss:0.6588693875819445 | ACC: 77.8000%(389/500)\n",
            "Testing Epoch[296] Loss:1.5226540446281434 | ACC: 66.7188%(427/640)\n",
            "Training Epoch[297] Loss:0.6776430979371071 | ACC: 77.6000%(388/500)\n",
            "Testing Epoch[297] Loss:1.5574988842010498 | ACC: 67.5000%(432/640)\n",
            "Training Epoch[298] Loss:0.6771675441414118 | ACC: 79.0000%(395/500)\n",
            "Testing Epoch[298] Loss:1.574113005399704 | ACC: 66.8750%(428/640)\n",
            "Training Epoch[299] Loss:0.6922068931162357 | ACC: 79.4000%(397/500)\n",
            "Testing Epoch[299] Loss:1.634006267786026 | ACC: 66.5625%(426/640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 300 epoch second+mature mirna\n",
        "print('max val_acc_record ', max(val_acc_record))\n",
        "print('min val_loss_record ', min(val_loss_record))\n",
        "\n",
        "print('min test_loss_record ', min(test_loss_record))\n",
        "print('max test_acc_record ', max(test_acc_record))\n",
        "# Tissue Classification\n",
        "# max val_acc_record  81.4\n",
        "# min val_loss_record  0.6155570782721043\n",
        "# min test_loss_record  0.9715069234371185\n",
        "# max test_acc_record  68.4375"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eda22fb-92a9-4d97-c4b9-cad6f72e9097",
        "id": "hYMJ_zv_q0Cy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max val_acc_record  81.4\n",
            "min val_loss_record  0.6155570782721043\n",
            "min test_loss_record  0.9715069234371185\n",
            "max test_acc_record  68.4375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lsf2462dq0Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_class, '/content/drive/MyDrive/miRNA/last_data/classification_result/tis_gcn_gru_300.pt')\n",
        "torch.save(model_class.state_dict(), '/content/drive/MyDrive/miRNA/last_data/classification_result/tis_gcn_gru_300_state.pt')"
      ],
      "metadata": {
        "id": "qU8IN0kCq0Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_acc_list_tis_gcn_gru_300.npy', val_acc_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_loss_list_tis_gcn_gru_300.npy', val_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_loss_list_tis_gcn_gru_300.npy', test_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_acc_list_tis_gcn_gru_300.npy', test_acc_record, allow_pickle=True)\n"
      ],
      "metadata": {
        "id": "L6a0KOI0q0Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RVmaOFJq0Cz",
        "outputId": "1d1cc2de-946c-4874-8a94-937abc0c8d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch_geometric.loader.dataloader.DataLoader at 0x7f445224c110>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "model_class.load_state_dict(torch.load('/content/drive/MyDrive/miRNA/last_data/classification_result/tis_gcn_gru_300_state.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245c0589-209a-413f-daeb-eb96a2066f92",
        "id": "vMPE_1RQq0C0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_roc():\n",
        "  \n",
        "  score_list=[]\n",
        "  score_one_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    score_temp=digits\n",
        "    score_list.extend(score_temp.detach().cpu().numpy())\n",
        "    score_temp=predicted\n",
        "    score_one_list.extend(score_temp.detach().cpu().numpy())\n",
        "    label_list.extend(label)\n",
        "  # correct = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   if label_list[i] == score_list[i]:\n",
        "  #     correct += 1\n",
        "  # print(correct/320)\n",
        "\n",
        "  score_array = np.array(score_list)\n",
        "  score_tensor = torch.tensor(score_one_list)\n",
        "  score_tensor = score_tensor.reshape(score_tensor.shape[0], 1)\n",
        "  score_one_hot = torch.zeros(score_tensor.shape[0], 3)\n",
        "  score_one_hot.scatter_(dim=1, index=score_tensor, value=1)\n",
        "  score_one_hot = np.array(score_one_hot)\n",
        "\n",
        "  label_tensor = torch.tensor(label_list)\n",
        "  label_tensor = label_tensor.reshape(label_tensor.shape[0], 1)\n",
        "  label_one_hot = torch.zeros(label_tensor.shape[0], 3)\n",
        "  label_one_hot.scatter_(dim=1, index=label_tensor, value=1)\n",
        "  label_one_hot = np.array(label_one_hot)\n",
        "  ns_c = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   score_list\n",
        "  # print('score_array shape', score_array.shape)\n",
        "  # print('label_one_hot shape', label_one_hot.shape)\n",
        "  # roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "  print('roc_auc_score', roc_auc_score(label_one_hot, score_array, average=None))\n",
        "  print('recall_score', recall_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('precision_score', precision_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('f1_score', f1_score(label_one_hot, score_one_hot, average=None))\n",
        "  # return 0\n",
        "\n",
        "  fpr_dict = dict()\n",
        "  tpr_dict = dict()\n",
        "  roc_auc_dict = dict()\n",
        "  # print(score_array.shape)\n",
        "  for i in range(3):\n",
        "    fpr_dict[i], tpr_dict[i], _ = roc_curve(label_one_hot[:, i], score_array[:, i])\n",
        "    roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
        "  \n",
        "  fpr_dict['micro'], tpr_dict['micro'], _ = roc_curve(label_one_hot.ravel(), score_array.ravel())\n",
        "  roc_auc_dict['micro'] = auc(fpr_dict['micro'], tpr_dict['micro'])\n",
        "\n",
        "  all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(3)]))\n",
        "  mean_tpr = np.zeros_like(all_fpr)\n",
        "  for i in range(3):\n",
        "    mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
        "\n",
        "  mean_tpr /= 3\n",
        "  fpr_dict['macro'] = all_fpr\n",
        "  tpr_dict['macro'] = mean_tpr\n",
        "  roc_auc_dict['macro'] = auc(fpr_dict['macro'], tpr_dict['macro'])\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  lw = 1.5\n",
        "  plt.plot(fpr_dict['micro'], tpr_dict['micro'], label='micro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['micro']), color='deeppink', linestyle=':', linewidth=4)\n",
        "  plt.plot(fpr_dict['macro'], tpr_dict['macro'], label='macro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['macro']), color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "  colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "  labels = cycle(['down', 'ns', 'up'])\n",
        "  # print(fpr_dict[i])\n",
        "  print(roc_auc_dict[i])\n",
        "  for i, color, label in zip(range(3), colors, labels):\n",
        "    plt.plot(fpr_dict[i], tpr_dict[i], lw=lw, color=color, label='ROC curve of class {0} (area={1:0.4f})'.format(label, roc_auc_dict[i]))\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--', lw=lw)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Tissue ROC')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.savefig('Tissue_GRU_GCN_classification_ROC_curve.jpg')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "6-540GDtq0C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()\n",
        "# Tissue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "754b08b3-206a-4b91-ba73-b4c3873b0f7e",
        "id": "j3ngbMMYq0C3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roc_auc_score [0.8144042  0.83414711 0.80213437]\n",
            "recall_score [0.64550265 0.67821782 0.67871486]\n",
            "precision_score [0.62886598 0.66504854 0.70416667]\n",
            "f1_score [0.63707572 0.67156863 0.69120654]\n",
            "0.8021343686767529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1HJHVo1d4Otc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nGBSKTFq53AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gen"
      ],
      "metadata": {
        "id": "-bpBBd-i54Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "t7StZzSK54Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KF25mPA-54Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "# dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=True)\n",
        "dataset = MeshRNADataset(x_disease, x_rna, y_hard, edge_d, edge_attr_d, node_d, hair_x, is_conv1d=False)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-640, 640])\n",
        "val_dataset, _ = torch.utils.data.random_split(train_set, [500,len(train_set)-500])\n",
        "dataloader = PYG_DataLoader(train_set, batch_size=32)\n",
        "test_dataloader = PYG_DataLoader(test_set, batch_size=32)\n",
        "val_dataloader = PYG_DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4baac5d-efe8-4f66-e4fc-f581c68c2a1e",
        "id": "A_pt5tc_54Yq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ac6fc9-c816-48d8-efd7-3db5913b3c8e",
        "id": "zcThgpMk54Yq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model_class.parameters(), lr=0.001) # , weight_decay=0.0025\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.MSELoss()\n",
        "# criterion_L1 = nn.L1Loss()"
      ],
      "metadata": {
        "id": "jRBkZyXS54Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_class.train()\n",
        "model_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd0553f-0025-4bd4-f47d-ed6972d535ab",
        "id": "dB3IWaAj54Yr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classificaion_module(\n",
              "  (gru_module): GRU_module(\n",
              "    (gru): GRU(30, 64, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "    (FC): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gcn): GCN(\n",
              "    (initial_conv): GCNConv(300, 256)\n",
              "    (bn_1): BatchNorm(256)\n",
              "    (conv1): GCNConv(256, 128)\n",
              "    (bn_2): BatchNorm(128)\n",
              "    (conv2): GCNConv(128, 128)\n",
              "    (bn_3): BatchNorm(128)\n",
              "    (conv3): GCNConv(128, 64)\n",
              "    (bn_4): BatchNorm(64)\n",
              "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (mlp_sec): Sequential(\n",
              "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
              "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    \n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTesting Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "k01nPYjd54Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_acc_output(epoch):\n",
        "  corrects = 0\n",
        "  the_batch_size = 0\n",
        "  zeros = 0\n",
        "  ones = 0\n",
        "  m_ones = 0\n",
        "  losses = 0\n",
        "  # losses_l1 = 0\n",
        "  # r2s = 0\n",
        "  c = 0\n",
        "  \n",
        "  for idx, data in enumerate(val_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    # disease = disease.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    # digits = F.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long).squeeze()\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "    loss = criterion(digits, label)\n",
        "    # # loss_l1 = criterion_L1(digits, label)\n",
        "    # # r2 = r2_score(label.cpu().detach().numpy(), digits.cpu().detach().numpy())\n",
        "    corrects += (predicted == label).sum().item()\n",
        "    the_batch_size += label.shape[0]\n",
        "    c += 1\n",
        "    losses += loss.item()\n",
        "    # # losses_l1 += loss_l1.item()\n",
        "    # # r2s += r2\n",
        "  acc = 100.0 * corrects / the_batch_size\n",
        "  avg_loss = losses/c\n",
        "  # print('label ', label)\n",
        "  # print('predicted ', predicted)\n",
        "  # # avg_loss_l1 = losses_l1/c\n",
        "  # # avg_r2 = r2s/c\n",
        "  print('\\rTraining Epoch[{}] Loss:{} | ACC: {:.4f}%({}/{})'.format(epoch, avg_loss, acc, corrects, the_batch_size))\n",
        "  # loss = loss.item()\n",
        "  return avg_loss, acc\n",
        "  # corrects = (torch.max(logits, 1)[1] == label).sum()"
      ],
      "metadata": {
        "id": "F20W8zmr54Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "best_acc = 0\n",
        "best_performance = 0\n",
        "step_log_interval = []\n",
        "\n",
        "train_loss_record = []\n",
        "# train_loss_l1_record = []\n",
        "\n",
        "val_acc_record = []\n",
        "val_loss_record = []\n",
        "# val_r2_record = []\n",
        "# val_loss_l1_record = []\n",
        "\n",
        "test_acc_record = []\n",
        "test_loss_record = []\n",
        "# test_r2_record = []\n",
        "# test_loss_l1_record = []\n",
        "# for epoch in tqdm(range(1, config.epoch + 1)):\n",
        "for epoch in (range(300)):\n",
        "  repres_list = []\n",
        "  label_list = []\n",
        "\n",
        "  for idx, data in enumerate(dataloader):\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    digits = model_class(data)\n",
        "    # digits = torch.tanh(digits)\n",
        "    # print('output: ', output.shape)\n",
        "    # digits = digits.to(torch.long)\n",
        "    # print(type(label))\n",
        "    label = torch.Tensor(np.array(label)).cuda()\n",
        "    label = label.to(torch.long)\n",
        "\n",
        "    # L1_loss = criterion_L1(digits, label)\n",
        "    loss = criterion(digits, label)\n",
        "\n",
        "    # print('torch.round(output): ', torch.round(output))\n",
        "    \n",
        "    \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "  train_loss_record.append(loss.item())\n",
        "\n",
        "  val_loss, val_acc = val_acc_output(epoch)\n",
        "  # step_log_interval.append(steps)\n",
        "  val_acc_record.append(val_acc)\n",
        "  val_loss_record.append(val_loss)\n",
        "  # val_r2_record.append(val_r2)\n",
        "  # val_loss_l1_record.append(val_l1)\n",
        "  # train_loss_record.append(loss)\n",
        "  test_loss, test_acc = test_acc_output(epoch)\n",
        "  test_acc_record.append(test_acc)\n",
        "  test_loss_record.append(test_loss)\n",
        "  # test_r2_record.append(test_r2)\n",
        "  # test_loss_l1_record.append(test_l1)\n",
        "  # break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a606afa-0588-4a05-ac1c-516449da1576",
        "id": "AoR06I7854Ys"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch[0] Loss:0.5448716897517443 | ACC: 75.4000%(377/500)\n",
            "Testing Epoch[0] Loss:0.5978936195373535 | ACC: 70.9375%(454/640)\n",
            "Training Epoch[1] Loss:0.48672997392714024 | ACC: 77.0000%(385/500)\n",
            "Testing Epoch[1] Loss:0.563891963660717 | ACC: 72.8125%(466/640)\n",
            "Training Epoch[2] Loss:0.4613758083432913 | ACC: 78.0000%(390/500)\n",
            "Testing Epoch[2] Loss:0.5485683917999268 | ACC: 72.3438%(463/640)\n",
            "Training Epoch[3] Loss:0.41282276250422 | ACC: 82.0000%(410/500)\n",
            "Testing Epoch[3] Loss:0.5162147551774978 | ACC: 76.2500%(488/640)\n",
            "Training Epoch[4] Loss:0.38254330679774284 | ACC: 82.4000%(412/500)\n",
            "Testing Epoch[4] Loss:0.5065957188606263 | ACC: 75.6250%(484/640)\n",
            "Training Epoch[5] Loss:0.3497834876179695 | ACC: 85.0000%(425/500)\n",
            "Testing Epoch[5] Loss:0.4982570856809616 | ACC: 77.1875%(494/640)\n",
            "Training Epoch[6] Loss:0.34126588329672813 | ACC: 85.6000%(428/500)\n",
            "Testing Epoch[6] Loss:0.4974013239145279 | ACC: 75.7812%(485/640)\n",
            "Training Epoch[7] Loss:0.31507469713687897 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[7] Loss:0.5124722138047219 | ACC: 77.1875%(494/640)\n",
            "Training Epoch[8] Loss:0.30726712848991156 | ACC: 86.6000%(433/500)\n",
            "Testing Epoch[8] Loss:0.5500798687338829 | ACC: 75.4688%(483/640)\n",
            "Training Epoch[9] Loss:0.3140101316384971 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[9] Loss:0.5518391191959381 | ACC: 77.3438%(495/640)\n",
            "Training Epoch[10] Loss:0.29720698250457644 | ACC: 86.2000%(431/500)\n",
            "Testing Epoch[10] Loss:0.5690952360630035 | ACC: 77.6562%(497/640)\n",
            "Training Epoch[11] Loss:0.3295213533565402 | ACC: 85.2000%(426/500)\n",
            "Testing Epoch[11] Loss:0.5955358847975731 | ACC: 77.8125%(498/640)\n",
            "Training Epoch[12] Loss:0.2957477932795882 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[12] Loss:0.6052767753601074 | ACC: 77.5000%(496/640)\n",
            "Training Epoch[13] Loss:0.28233602875843644 | ACC: 88.8000%(444/500)\n",
            "Testing Epoch[13] Loss:0.6113639518618583 | ACC: 80.1562%(513/640)\n",
            "Training Epoch[14] Loss:0.2630114033818245 | ACC: 89.0000%(445/500)\n",
            "Testing Epoch[14] Loss:0.5953891754150391 | ACC: 77.5000%(496/640)\n",
            "Training Epoch[15] Loss:0.27831773250363767 | ACC: 90.2000%(451/500)\n",
            "Testing Epoch[15] Loss:0.5672280445694924 | ACC: 82.9688%(531/640)\n",
            "Training Epoch[16] Loss:0.2713506002910435 | ACC: 89.4000%(447/500)\n",
            "Testing Epoch[16] Loss:0.5946644566953182 | ACC: 79.2188%(507/640)\n",
            "Training Epoch[17] Loss:0.2640230506658554 | ACC: 88.4000%(442/500)\n",
            "Testing Epoch[17] Loss:0.6285858228802681 | ACC: 80.4688%(515/640)\n",
            "Training Epoch[18] Loss:0.2854247069917619 | ACC: 88.4000%(442/500)\n",
            "Testing Epoch[18] Loss:0.6295710720121861 | ACC: 82.1875%(526/640)\n",
            "Training Epoch[19] Loss:0.2680235090665519 | ACC: 91.0000%(455/500)\n",
            "Testing Epoch[19] Loss:0.6740570470690728 | ACC: 82.1875%(526/640)\n",
            "Training Epoch[20] Loss:0.23104586452245712 | ACC: 90.8000%(454/500)\n",
            "Testing Epoch[20] Loss:0.6061878561973572 | ACC: 84.3750%(540/640)\n",
            "Training Epoch[21] Loss:0.2552935783751309 | ACC: 91.8000%(459/500)\n",
            "Testing Epoch[21] Loss:0.6835428476333618 | ACC: 82.8125%(530/640)\n",
            "Training Epoch[22] Loss:0.2844912593718618 | ACC: 90.6000%(453/500)\n",
            "Testing Epoch[22] Loss:0.7283884875476361 | ACC: 80.7812%(517/640)\n",
            "Training Epoch[23] Loss:0.23653731937520206 | ACC: 92.0000%(460/500)\n",
            "Testing Epoch[23] Loss:0.6586777783930302 | ACC: 82.9688%(531/640)\n",
            "Training Epoch[24] Loss:0.24516382068395615 | ACC: 91.0000%(455/500)\n",
            "Testing Epoch[24] Loss:0.6757641613483429 | ACC: 82.6562%(529/640)\n",
            "Training Epoch[25] Loss:0.27759654982946813 | ACC: 90.6000%(453/500)\n",
            "Testing Epoch[25] Loss:0.759337131679058 | ACC: 80.9375%(518/640)\n",
            "Training Epoch[26] Loss:0.32516740541905165 | ACC: 87.4000%(437/500)\n",
            "Testing Epoch[26] Loss:0.74045779556036 | ACC: 80.6250%(516/640)\n",
            "Training Epoch[27] Loss:0.2906027100980282 | ACC: 89.4000%(447/500)\n",
            "Testing Epoch[27] Loss:0.7138774111866951 | ACC: 81.7188%(523/640)\n",
            "Training Epoch[28] Loss:0.235723742749542 | ACC: 90.6000%(453/500)\n",
            "Testing Epoch[28] Loss:0.7011237069964409 | ACC: 80.9375%(518/640)\n",
            "Training Epoch[29] Loss:0.2454586559906602 | ACC: 91.8000%(459/500)\n",
            "Testing Epoch[29] Loss:0.7246493034064769 | ACC: 82.9688%(531/640)\n",
            "Training Epoch[30] Loss:0.2827691053971648 | ACC: 90.4000%(452/500)\n",
            "Testing Epoch[30] Loss:0.7728545919060708 | ACC: 82.0312%(525/640)\n",
            "Training Epoch[31] Loss:0.2935428931377828 | ACC: 91.0000%(455/500)\n",
            "Testing Epoch[31] Loss:0.7019685879349709 | ACC: 83.7500%(536/640)\n",
            "Training Epoch[32] Loss:0.26283961348235607 | ACC: 91.4000%(457/500)\n",
            "Testing Epoch[32] Loss:0.7222234211862087 | ACC: 82.6562%(529/640)\n",
            "Training Epoch[33] Loss:0.25262960710097104 | ACC: 91.6000%(458/500)\n",
            "Testing Epoch[33] Loss:0.6835863746702671 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[34] Loss:0.22708422038704157 | ACC: 92.4000%(462/500)\n",
            "Testing Epoch[34] Loss:0.6827363103628159 | ACC: 83.1250%(532/640)\n",
            "Training Epoch[35] Loss:0.20450456300750375 | ACC: 93.4000%(467/500)\n",
            "Testing Epoch[35] Loss:0.6822601035237312 | ACC: 82.1875%(526/640)\n",
            "Training Epoch[36] Loss:0.1966510098427534 | ACC: 92.6000%(463/500)\n",
            "Testing Epoch[36] Loss:0.6538785874843598 | ACC: 83.9062%(537/640)\n",
            "Training Epoch[37] Loss:0.19975121319293976 | ACC: 92.6000%(463/500)\n",
            "Testing Epoch[37] Loss:0.7852498315274715 | ACC: 82.3438%(527/640)\n",
            "Training Epoch[38] Loss:0.19553412054665387 | ACC: 92.6000%(463/500)\n",
            "Testing Epoch[38] Loss:0.6621133953332901 | ACC: 84.5312%(541/640)\n",
            "Training Epoch[39] Loss:0.21388367004692554 | ACC: 92.6000%(463/500)\n",
            "Testing Epoch[39] Loss:0.7179678909480571 | ACC: 82.3438%(527/640)\n",
            "Training Epoch[40] Loss:0.21231724601238966 | ACC: 92.6000%(463/500)\n",
            "Testing Epoch[40] Loss:0.7023282319307327 | ACC: 82.6562%(529/640)\n",
            "Training Epoch[41] Loss:0.19050586526282132 | ACC: 93.2000%(466/500)\n",
            "Testing Epoch[41] Loss:0.6645711734890938 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[42] Loss:0.18308749236166477 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[42] Loss:0.6763295896351338 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[43] Loss:0.17848225962370634 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[43] Loss:0.670796125382185 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[44] Loss:0.17767900857143104 | ACC: 93.8000%(469/500)\n",
            "Testing Epoch[44] Loss:0.6796739242970944 | ACC: 84.5312%(541/640)\n",
            "Training Epoch[45] Loss:0.1842386773787439 | ACC: 93.8000%(469/500)\n",
            "Testing Epoch[45] Loss:0.6897058613598347 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[46] Loss:0.1794495047070086 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[46] Loss:0.680475752428174 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[47] Loss:0.17139574291650206 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[47] Loss:0.6989357903599739 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[48] Loss:0.17562600527890027 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[48] Loss:0.6769648015499115 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[49] Loss:0.17482363269664347 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[49] Loss:0.7026381649076938 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[50] Loss:0.17984750517643988 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[50] Loss:0.6989305224269629 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[51] Loss:0.17021168884821236 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[51] Loss:0.7145880110561847 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[52] Loss:0.16377647849731147 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[52] Loss:0.699495842307806 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[53] Loss:0.17693854274693877 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[53] Loss:0.7241259589791298 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[54] Loss:0.18466188851743937 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[54] Loss:0.7091295834630728 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[55] Loss:0.17841524607501924 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[55] Loss:0.7249036088585854 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[56] Loss:0.17491699557285756 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[56] Loss:0.7158617205917835 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[57] Loss:0.16763356572482735 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[57] Loss:0.730300322920084 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[58] Loss:0.16808973881416023 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[58] Loss:0.7328553702682257 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[59] Loss:0.1648551132529974 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[59] Loss:0.7433266483247281 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[60] Loss:0.16770207206718624 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[60] Loss:0.7366701431572438 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[61] Loss:0.1680695639224723 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[61] Loss:0.730031055957079 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[62] Loss:0.1735669084591791 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[62] Loss:0.7526268675923348 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[63] Loss:0.17021588154602796 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[63] Loss:0.7475168127566576 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[64] Loss:0.1728913930710405 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[64] Loss:0.752339594811201 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[65] Loss:0.18721467512659729 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[65] Loss:0.7545610189437866 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[66] Loss:0.1729598514502868 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[66] Loss:0.7563356157392264 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[67] Loss:0.17068256484344602 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[67] Loss:0.7588432759046555 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[68] Loss:0.17165398423094302 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[68] Loss:0.7489600270986557 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[69] Loss:0.17208912083879113 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[69] Loss:0.7600756607949734 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[70] Loss:0.17960272612981498 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[70] Loss:0.7757338032126426 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[71] Loss:0.17892352014314383 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[71] Loss:0.7732381582260132 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[72] Loss:0.18196349195204675 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[72] Loss:0.7705041866749525 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[73] Loss:0.17872111964970827 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[73] Loss:0.7878260191529989 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[74] Loss:0.17542048101313412 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[74] Loss:0.7878928631544113 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[75] Loss:0.18694165244232863 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[75] Loss:0.7928707804530859 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[76] Loss:0.17473002872429788 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[76] Loss:0.777379010617733 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[77] Loss:0.178954629576765 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[77] Loss:0.7875681057572365 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[78] Loss:0.17144757765345275 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[78] Loss:0.786065623909235 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[79] Loss:0.18572370905894786 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[79] Loss:0.7984259411692619 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[80] Loss:0.18055365944746882 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[80] Loss:0.8086806908249855 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[81] Loss:0.17185660696122795 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[81] Loss:0.7979464545845986 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[82] Loss:0.16900532669387758 | ACC: 95.8000%(479/500)\n",
            "Testing Epoch[82] Loss:0.7899416271597147 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[83] Loss:0.17297238152241334 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[83] Loss:0.7963437967002391 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[84] Loss:0.18165677541401237 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[84] Loss:0.808152374625206 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[85] Loss:0.18252282007597387 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[85] Loss:0.8050863701850176 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[86] Loss:0.17744630077504553 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[86] Loss:0.8074312023818493 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[87] Loss:0.17966608697315678 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[87] Loss:0.8167312011122704 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[88] Loss:0.18301775032887235 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[88] Loss:0.8191163197159768 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[89] Loss:0.17390538996551186 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[89] Loss:0.8049326993525028 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[90] Loss:0.1834350303397514 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[90] Loss:0.8237607620656491 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[91] Loss:0.18432141951052472 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[91] Loss:0.8130221508443356 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[92] Loss:0.17005404352676123 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[92] Loss:0.8190907046198845 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[93] Loss:0.1788113176007755 | ACC: 95.6000%(478/500)\n",
            "Testing Epoch[93] Loss:0.8275270540267229 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[94] Loss:0.18637846922501922 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[94] Loss:0.8191212244331837 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[95] Loss:0.18369155927211978 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[95] Loss:0.8303235586732626 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[96] Loss:0.18133407778805122 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[96] Loss:0.8316921666264534 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[97] Loss:0.17776274034986272 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[97] Loss:0.8252535816282034 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[98] Loss:0.18360434647183865 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[98] Loss:0.8385317534208298 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[99] Loss:0.1799080844502896 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[99] Loss:0.831949857994914 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[100] Loss:0.1754378318437375 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[100] Loss:0.8539950724691152 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[101] Loss:0.18709738628240302 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[101] Loss:0.8501702290028333 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[102] Loss:0.17782062638434581 | ACC: 95.6000%(478/500)\n",
            "Testing Epoch[102] Loss:0.8534905884414912 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[103] Loss:0.1785683512862306 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[103] Loss:0.8533059392124415 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[104] Loss:0.18157126771984622 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[104] Loss:0.8549740552902222 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[105] Loss:0.17459285847144201 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[105] Loss:0.8512126237154007 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[106] Loss:0.18216154220863245 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[106] Loss:0.847552164644003 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[107] Loss:0.18266898900037631 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[107] Loss:0.8362449459731579 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[108] Loss:0.17761513422010466 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[108] Loss:0.8569706905633211 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[109] Loss:0.19238073599990457 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[109] Loss:0.8569264855235815 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[110] Loss:0.18381566874450073 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[110] Loss:0.864570764452219 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[111] Loss:0.1849899100488983 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[111] Loss:0.8673481475561857 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[112] Loss:0.19224281393690035 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[112] Loss:0.8647279933094978 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[113] Loss:0.19010747142601758 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[113] Loss:0.8792620565742254 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[114] Loss:0.1897173888864927 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[114] Loss:0.8793162357062101 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[115] Loss:0.1797431955637876 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[115] Loss:0.8783131364732981 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[116] Loss:0.18344111274927855 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[116] Loss:0.8667037673294544 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[117] Loss:0.19203145726351067 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[117] Loss:0.8839824937283993 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[118] Loss:0.19222716568037868 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[118] Loss:0.8798891633749009 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[119] Loss:0.18750403058947995 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[119] Loss:0.8772955946624279 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[120] Loss:0.18581665417877957 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[120] Loss:0.8931806534528732 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[121] Loss:0.18560384985175915 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[121] Loss:0.898578192293644 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[122] Loss:0.19603437732439488 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[122] Loss:0.899317218735814 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[123] Loss:0.1951309558644425 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[123] Loss:0.9026133965700864 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[124] Loss:0.19126750872237608 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[124] Loss:0.8933420050889254 | ACC: 84.3750%(540/640)\n",
            "Training Epoch[125] Loss:0.19666818328551017 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[125] Loss:0.9096084140241146 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[126] Loss:0.19985252560582012 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[126] Loss:0.9093046110123396 | ACC: 84.5312%(541/640)\n",
            "Training Epoch[127] Loss:0.19371018785750493 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[127] Loss:0.8997689507901668 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[128] Loss:0.1956432184088044 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[128] Loss:0.9132657662034035 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[129] Loss:0.20089618363999762 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[129] Loss:0.894581251218915 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[130] Loss:0.194001640425995 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[130] Loss:0.9182386744767428 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[131] Loss:0.1911971532390453 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[131] Loss:0.9027652833610773 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[132] Loss:0.201223372889217 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[132] Loss:0.9085222769528627 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[133] Loss:0.18866762457764708 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[133] Loss:0.9158456850796938 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[134] Loss:0.19938988250214607 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[134] Loss:0.9187426060438156 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[135] Loss:0.20016824163030833 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[135] Loss:0.924331758543849 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[136] Loss:0.19525449903449044 | ACC: 95.4000%(477/500)\n",
            "Testing Epoch[136] Loss:0.9184679578989744 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[137] Loss:0.19564387240097858 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[137] Loss:0.9282682262361049 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[138] Loss:0.20026035921182483 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[138] Loss:0.9230074271559715 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[139] Loss:0.19509336963528767 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[139] Loss:0.9269448772072792 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[140] Loss:0.20107141049811617 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[140] Loss:0.9356997471302748 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[141] Loss:0.19885058520594612 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[141] Loss:0.9377914279699325 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[142] Loss:0.19203563244082034 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[142] Loss:0.9223107531666755 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[143] Loss:0.19788202980998904 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[143] Loss:0.9401969593018293 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[144] Loss:0.18967614394205157 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[144] Loss:0.9443909391760826 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[145] Loss:0.2044391008093953 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[145] Loss:0.9411103058606386 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[146] Loss:0.20151605404680595 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[146] Loss:0.9327966425567865 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[147] Loss:0.20519289484946057 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[147] Loss:0.944975882396102 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[148] Loss:0.19993587920907885 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[148] Loss:0.9549639891833067 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[149] Loss:0.20815195600152947 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[149] Loss:0.9460213851183653 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[150] Loss:0.2166929080558475 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[150] Loss:0.9421797100454569 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[151] Loss:0.2107374737970531 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[151] Loss:0.9659034721553326 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[152] Loss:0.21751376608153805 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[152] Loss:0.9663662008941174 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[153] Loss:0.20361008985491935 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[153] Loss:0.9717265371233225 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[154] Loss:0.20588383730500937 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[154] Loss:0.9629593309015035 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[155] Loss:0.21324008953524753 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[155] Loss:0.9562535308301449 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[156] Loss:0.20569464488653466 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[156] Loss:0.9888836566358805 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[157] Loss:0.20916550286347046 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[157] Loss:0.975320290029049 | ACC: 84.6875%(542/640)\n",
            "Training Epoch[158] Loss:0.21643155510537326 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[158] Loss:0.9682346414774656 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[159] Loss:0.20469562598736957 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[159] Loss:0.9683963175863027 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[160] Loss:0.20429265568964183 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[160] Loss:0.9758806534111499 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[161] Loss:0.20352679306233767 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[161] Loss:0.9763610757887363 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[162] Loss:0.21722599980421364 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[162] Loss:0.9863283541053534 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[163] Loss:0.19848008043481968 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[163] Loss:0.9676494289189577 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[164] Loss:0.20234574578353204 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[164] Loss:0.9801644049584866 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[165] Loss:0.20309526879282203 | ACC: 95.8000%(479/500)\n",
            "Testing Epoch[165] Loss:1.0000306632369758 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[166] Loss:0.218474304798292 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[166] Loss:0.9941874403506518 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[167] Loss:0.19606754116830416 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[167] Loss:0.9812067240476608 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[168] Loss:0.20980981667526066 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[168] Loss:0.9803470842540264 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[169] Loss:0.21831371648295317 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[169] Loss:0.9854497358202934 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[170] Loss:0.22643517825054005 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[170] Loss:0.9952045101672411 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[171] Loss:0.21665494072658475 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[171] Loss:0.991134337708354 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[172] Loss:0.22552072146208957 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[172] Loss:0.9995916571468115 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[173] Loss:0.22359914644039236 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[173] Loss:1.003426529839635 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[174] Loss:0.2242264681844972 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[174] Loss:1.0059840641915798 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[175] Loss:0.21533470402937382 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[175] Loss:1.0074241552501917 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[176] Loss:0.20957307392382063 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[176] Loss:1.0057538498193026 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[177] Loss:0.2247861777432263 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[177] Loss:1.0091293513774873 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[178] Loss:0.2070648327935487 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[178] Loss:1.0125629488378762 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[179] Loss:0.22041380054724868 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[179] Loss:1.022934728860855 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[180] Loss:0.22082748377579264 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[180] Loss:1.0174220856279135 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[181] Loss:0.21910033482708968 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[181] Loss:1.0337083037942647 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[182] Loss:0.22321640183508862 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[182] Loss:1.0364243231713772 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[183] Loss:0.22175944010086823 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[183] Loss:1.0336801212280988 | ACC: 84.3750%(540/640)\n",
            "Training Epoch[184] Loss:0.21763473469763994 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[184] Loss:1.0062532983720303 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[185] Loss:0.23354599639424123 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[185] Loss:1.029347189515829 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[186] Loss:0.21357184209045954 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[186] Loss:1.0252781484276057 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[187] Loss:0.2195938818040304 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[187] Loss:1.0205173227936029 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[188] Loss:0.2216621562401997 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[188] Loss:1.0314911976456642 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[189] Loss:0.2239098329009721 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[189] Loss:1.0340657941997051 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[190] Loss:0.22679957484797342 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[190] Loss:1.0432049959897995 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[191] Loss:0.21882459716289304 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[191] Loss:1.045146743580699 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[192] Loss:0.22774474590551108 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[192] Loss:1.0273687127977609 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[193] Loss:0.23485821356007364 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[193] Loss:1.0457965955138206 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[194] Loss:0.2271564292241237 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[194] Loss:1.0534779708832502 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[195] Loss:0.2201859453052748 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[195] Loss:1.0412953462451697 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[196] Loss:0.22230125777423382 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[196] Loss:1.0467082101851701 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[197] Loss:0.21921568288234994 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[197] Loss:1.0538884215056896 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[198] Loss:0.23304562813427765 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[198] Loss:1.0631336834281684 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[199] Loss:0.2365631433785893 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[199] Loss:1.0648472677916287 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[200] Loss:0.23501773465250153 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[200] Loss:1.0734791435301303 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[201] Loss:0.22224720925441943 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[201] Loss:1.0763295419514178 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[202] Loss:0.2206027544743847 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[202] Loss:1.0589943949133158 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[203] Loss:0.2230692339799134 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[203] Loss:1.0724674105644225 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[204] Loss:0.23606754618231207 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[204] Loss:1.0644646838307381 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[205] Loss:0.23193191904283594 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[205] Loss:1.0596629671752453 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[206] Loss:0.23752901802072302 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[206] Loss:1.0773662008345126 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[207] Loss:0.2281410540745128 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[207] Loss:1.0846178298816085 | ACC: 86.0938%(551/640)\n",
            "Training Epoch[208] Loss:0.23295665037585422 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[208] Loss:1.0797860831022263 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[209] Loss:0.23472014427534305 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[209] Loss:1.088988783210516 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[210] Loss:0.23149827819725033 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[210] Loss:1.0877327658236027 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[211] Loss:0.2471191473014187 | ACC: 93.8000%(469/500)\n",
            "Testing Epoch[211] Loss:1.103702524676919 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[212] Loss:0.22997892837156542 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[212] Loss:1.0932453740388155 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[213] Loss:0.24250902513449546 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[213] Loss:1.0925170682370662 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[214] Loss:0.24438582939910702 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[214] Loss:1.0963689863681794 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[215] Loss:0.23961559306189884 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[215] Loss:1.091558427363634 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[216] Loss:0.2384382514137542 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[216] Loss:1.0789089264348148 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[217] Loss:0.23690145352156833 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[217] Loss:1.090731054916978 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[218] Loss:0.2494787679897854 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[218] Loss:1.0979821033775807 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[219] Loss:0.24652667265036143 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[219] Loss:1.106719746440649 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[220] Loss:0.23761470479075797 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[220] Loss:1.1147387444972991 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[221] Loss:0.236536442200304 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[221] Loss:1.109265616349876 | ACC: 86.0938%(551/640)\n",
            "Training Epoch[222] Loss:0.23839587661495898 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[222] Loss:1.1152180299162864 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[223] Loss:0.2407903866842389 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[223] Loss:1.119559806957841 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[224] Loss:0.24681634989974555 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[224] Loss:1.1151688128709794 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[225] Loss:0.22770251601468772 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[225] Loss:1.1195571705698968 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[226] Loss:0.24671592788945418 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[226] Loss:1.109820830449462 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[227] Loss:0.24166221055202186 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[227] Loss:1.1202666107565165 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[228] Loss:0.24412260021199472 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[228] Loss:1.112871976941824 | ACC: 86.4062%(553/640)\n",
            "Training Epoch[229] Loss:0.249444649052748 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[229] Loss:1.1371170476078987 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[230] Loss:0.24209650489501655 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[230] Loss:1.1157500024884939 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[231] Loss:0.2523299844469875 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[231] Loss:1.132294305972755 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[232] Loss:0.24269849057600368 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[232] Loss:1.135994577035308 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[233] Loss:0.24989068799186498 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[233] Loss:1.1530787523835897 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[234] Loss:0.2529202322330093 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[234] Loss:1.135415915772319 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[235] Loss:0.2436512654167018 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[235] Loss:1.129979319870472 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[236] Loss:0.24416307426872663 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[236] Loss:1.151556328497827 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[237] Loss:0.24399207919486798 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[237] Loss:1.150056795589626 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[238] Loss:0.2522304376907414 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[238] Loss:1.1583036076277495 | ACC: 86.0938%(551/640)\n",
            "Training Epoch[239] Loss:0.24631932290503755 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[239] Loss:1.1330380633473396 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[240] Loss:0.24523168707673904 | ACC: 93.8000%(469/500)\n",
            "Testing Epoch[240] Loss:1.1724457774311303 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[241] Loss:0.2520330081169959 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[241] Loss:1.1649667181074619 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[242] Loss:0.25419247122772504 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[242] Loss:1.1473775137215854 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[243] Loss:0.2485970146371983 | ACC: 95.0000%(475/500)\n",
            "Testing Epoch[243] Loss:1.171054169535637 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[244] Loss:0.2488963448413415 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[244] Loss:1.1557312533259392 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[245] Loss:0.2618638853309676 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[245] Loss:1.161692313104868 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[246] Loss:0.2637618594308151 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[246] Loss:1.1819590663537383 | ACC: 86.0938%(551/640)\n",
            "Training Epoch[247] Loss:0.24974763152567903 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[247] Loss:1.1650278065353632 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[248] Loss:0.2512721747625619 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[248] Loss:1.1642899798229336 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[249] Loss:0.2654308767814655 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[249] Loss:1.165260013192892 | ACC: 86.0938%(551/640)\n",
            "Training Epoch[250] Loss:0.2543278882658342 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[250] Loss:1.1766150422394275 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[251] Loss:0.2611814486735966 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[251] Loss:1.1868927642703055 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[252] Loss:0.259961139265215 | ACC: 93.8000%(469/500)\n",
            "Testing Epoch[252] Loss:1.1809457160532475 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[253] Loss:0.262286982673686 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[253] Loss:1.1802469417452812 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[254] Loss:0.26506844513642136 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[254] Loss:1.1976614389568567 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[255] Loss:0.2642228138502105 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[255] Loss:1.1891692075878382 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[256] Loss:0.26005615942995064 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[256] Loss:1.1882430324330926 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[257] Loss:0.2490074256711523 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[257] Loss:1.174255765043199 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[258] Loss:0.2665729546279181 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[258] Loss:1.199543895944953 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[259] Loss:0.25548112679098267 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[259] Loss:1.2079840509220958 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[260] Loss:0.2535445887479 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[260] Loss:1.197800325602293 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[261] Loss:0.26460407081322046 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[261] Loss:1.203248143941164 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[262] Loss:0.256879016145831 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[262] Loss:1.202774800732732 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[263] Loss:0.2595776926027611 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[263] Loss:1.2110327761620283 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[264] Loss:0.2652616326813586 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[264] Loss:1.2138960218057036 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[265] Loss:0.2651810453244252 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[265] Loss:1.200754552334547 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[266] Loss:0.265082237921888 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[266] Loss:1.2103543017059564 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[267] Loss:0.2634131316008279 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[267] Loss:1.220612681657076 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[268] Loss:0.27061524815508164 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[268] Loss:1.2184492520987988 | ACC: 86.0938%(551/640)\n",
            "Training Epoch[269] Loss:0.27454439801658737 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[269] Loss:1.2150489784777165 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[270] Loss:0.2708537474973127 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[270] Loss:1.233929355815053 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[271] Loss:0.2715753762895474 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[271] Loss:1.2204348634928466 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[272] Loss:0.2652648849034449 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[272] Loss:1.2264341183006764 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[273] Loss:0.27186273524421267 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[273] Loss:1.2309251088649034 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[274] Loss:0.2738438199812663 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[274] Loss:1.2308598566800355 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[275] Loss:0.2718294410442468 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[275] Loss:1.2290793545544147 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[276] Loss:0.254771514897584 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[276] Loss:1.228380487859249 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[277] Loss:0.2702931951498613 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[277] Loss:1.2519635986536741 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[278] Loss:0.2746748038480291 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[278] Loss:1.2519219590350985 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[279] Loss:0.2682461525546387 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[279] Loss:1.2524281527847052 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[280] Loss:0.27088820358767407 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[280] Loss:1.2631367169320584 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[281] Loss:0.2682748162187636 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[281] Loss:1.255944688618183 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[282] Loss:0.2719579667027574 | ACC: 93.8000%(469/500)\n",
            "Testing Epoch[282] Loss:1.2531568832695483 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[283] Loss:0.2626816883275751 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[283] Loss:1.2665766827762126 | ACC: 85.1562%(545/640)\n",
            "Training Epoch[284] Loss:0.2795735242543742 | ACC: 94.0000%(470/500)\n",
            "Testing Epoch[284] Loss:1.262853741645813 | ACC: 85.0000%(544/640)\n",
            "Training Epoch[285] Loss:0.27002688983338885 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[285] Loss:1.262574010156095 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[286] Loss:0.2772167193106725 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[286] Loss:1.252166971936822 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[287] Loss:0.27323143937974237 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[287] Loss:1.2573033913969993 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[288] Loss:0.26126797319739126 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[288] Loss:1.2628291986882687 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[289] Loss:0.2740352997498121 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[289] Loss:1.260588045977056 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[290] Loss:0.2702643249067478 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[290] Loss:1.2746866766363383 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[291] Loss:0.26397681421076413 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[291] Loss:1.2676039738580585 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[292] Loss:0.2733841822919203 | ACC: 94.6000%(473/500)\n",
            "Testing Epoch[292] Loss:1.275690977461636 | ACC: 85.3125%(546/640)\n",
            "Training Epoch[293] Loss:0.2791692517712363 | ACC: 94.2000%(471/500)\n",
            "Testing Epoch[293] Loss:1.274769184924662 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[294] Loss:0.27985589115996845 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[294] Loss:1.2636034365743398 | ACC: 85.6250%(548/640)\n",
            "Training Epoch[295] Loss:0.2648205706718727 | ACC: 95.2000%(476/500)\n",
            "Testing Epoch[295] Loss:1.288314944319427 | ACC: 85.9375%(550/640)\n",
            "Training Epoch[296] Loss:0.2735723545483779 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[296] Loss:1.2875356638804079 | ACC: 85.4688%(547/640)\n",
            "Training Epoch[297] Loss:0.26879986353742424 | ACC: 94.8000%(474/500)\n",
            "Testing Epoch[297] Loss:1.2868562672287225 | ACC: 85.7812%(549/640)\n",
            "Training Epoch[298] Loss:0.27642901645594975 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[298] Loss:1.282802851498127 | ACC: 84.8438%(543/640)\n",
            "Training Epoch[299] Loss:0.2832757761789253 | ACC: 94.4000%(472/500)\n",
            "Testing Epoch[299] Loss:1.2902178019285202 | ACC: 85.9375%(550/640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 300 epoch second+mature mirna\n",
        "print('max val_acc_record ', max(val_acc_record))\n",
        "print('min val_loss_record ', min(val_loss_record))\n",
        "\n",
        "print('min test_loss_record ', min(test_loss_record))\n",
        "print('max test_acc_record ', max(test_acc_record))\n",
        "# max val_acc_record  84.6\n",
        "# min val_loss_record  0.5722110476344824\n",
        "# min test_loss_record  0.7427837997674942\n",
        "# max test_acc_record  75.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1109a343-015f-4406-ef3c-1e67833542d0",
        "id": "1JKVQUsR54Ys"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max val_acc_record  95.8\n",
            "min val_loss_record  0.16377647849731147\n",
            "min test_loss_record  0.4974013239145279\n",
            "max test_acc_record  86.40625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cmWmPVi054Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_class, '/content/drive/MyDrive/miRNA/last_data/classification_result/gen_gcn_gru_300.pt')\n",
        "torch.save(model_class.state_dict(), '/content/drive/MyDrive/miRNA/last_data/classification_result/gen_gcn_gru_300_state.pt')"
      ],
      "metadata": {
        "id": "VaUDHMYX54Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_acc_list_gen_gcn_gru_300.npy', val_acc_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/val_loss_list_gen_gcn_gru_300.npy', val_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_loss_list_gen_gcn_gru_300.npy', test_loss_record, allow_pickle=True)\n",
        "np.save('/content/drive/MyDrive/miRNA/last_data/classification_result/test_acc_list_gen_gcn_gru_300.npy', test_acc_record, allow_pickle=True)\n"
      ],
      "metadata": {
        "id": "GA0Oa5-F54Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1cc2de-946c-4874-8a94-937abc0c8d91",
        "id": "LqsMTaNF54Yt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch_geometric.loader.dataloader.DataLoader at 0x7f445224c110>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = Classificaion_module().cuda()\n",
        "model_class.load_state_dict(torch.load('/content/drive/MyDrive/miRNA/last_data/classification_result/gen_gcn_gru_300_state.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ed6cbe-882f-4b13-c4b9-d2993a1bda52",
        "id": "CJmmL13q54Yt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_roc():\n",
        "  \n",
        "  score_list=[]\n",
        "  score_one_list=[]\n",
        "  label_list=[]\n",
        "\n",
        "  \n",
        "  for idx, data in enumerate(test_dataloader):\n",
        "\n",
        "    label = data.label\n",
        "    data = data.cuda()\n",
        "    \n",
        "    digits = model_class(data)\n",
        "    _, predicted = torch.max(digits.data, 1)\n",
        "    score_temp=digits\n",
        "    score_list.extend(score_temp.detach().cpu().numpy())\n",
        "    score_temp=predicted\n",
        "    score_one_list.extend(score_temp.detach().cpu().numpy())\n",
        "    label_list.extend(label)\n",
        "  # correct = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   if label_list[i] == score_list[i]:\n",
        "  #     correct += 1\n",
        "  # print(correct/320)\n",
        "\n",
        "  score_array = np.array(score_list)\n",
        "  score_tensor = torch.tensor(score_one_list)\n",
        "  score_tensor = score_tensor.reshape(score_tensor.shape[0], 1)\n",
        "  score_one_hot = torch.zeros(score_tensor.shape[0], 2)\n",
        "  score_one_hot.scatter_(dim=1, index=score_tensor, value=1)\n",
        "  score_one_hot = np.array(score_one_hot)\n",
        "\n",
        "  label_tensor = torch.tensor(label_list)\n",
        "  label_tensor = label_tensor.reshape(label_tensor.shape[0], 1)\n",
        "  label_one_hot = torch.zeros(label_tensor.shape[0], 2)\n",
        "  label_one_hot.scatter_(dim=1, index=label_tensor, value=1)\n",
        "  label_one_hot = np.array(label_one_hot)\n",
        "  ns_c = 0\n",
        "  # for i in range(len(score_list)):\n",
        "  #   score_list\n",
        "  print('score_array shape', score_array.shape)\n",
        "  print('label_one_hot shape', label_one_hot.shape)\n",
        "  print(len(np.unique(label_tensor)))\n",
        "  # roc_curve, auc, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, recall_score, precision_score\n",
        "  print('roc_auc_score', roc_auc_score(label_one_hot, score_array, average=None))\n",
        "  print('recall_score', recall_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('precision_score', precision_score(label_one_hot, score_one_hot, average=None))\n",
        "  print('f1_score', f1_score(label_one_hot, score_one_hot, average=None))\n",
        "  # return 0\n",
        "\n",
        "  fpr_dict = dict()\n",
        "  tpr_dict = dict()\n",
        "  roc_auc_dict = dict()\n",
        "  # print(score_array.shape)\n",
        "  for i in range(2):\n",
        "    fpr_dict[i], tpr_dict[i], _ = roc_curve(label_one_hot[:, i], score_array[:, i])\n",
        "    roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n",
        "  \n",
        "  fpr_dict['micro'], tpr_dict['micro'], _ = roc_curve(label_one_hot.ravel(), score_array.ravel())\n",
        "  roc_auc_dict['micro'] = auc(fpr_dict['micro'], tpr_dict['micro'])\n",
        "\n",
        "  all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(2)]))\n",
        "  mean_tpr = np.zeros_like(all_fpr)\n",
        "  for i in range(2):\n",
        "    mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n",
        "\n",
        "  mean_tpr /= 2\n",
        "  fpr_dict['macro'] = all_fpr\n",
        "  tpr_dict['macro'] = mean_tpr\n",
        "  roc_auc_dict['macro'] = auc(fpr_dict['macro'], tpr_dict['macro'])\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  lw = 1.5\n",
        "  plt.plot(fpr_dict['micro'], tpr_dict['micro'], label='micro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['micro']), color='deeppink', linestyle=':', linewidth=4)\n",
        "  plt.plot(fpr_dict['macro'], tpr_dict['macro'], label='macro-average ROC curve (area={0:0.4f})'.format(roc_auc_dict['macro']), color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "  colors = cycle(['aqua', 'darkorange'])\n",
        "  labels = cycle(['down', 'up'])\n",
        "  # print(fpr_dict[i])\n",
        "  print(roc_auc_dict[i])\n",
        "  for i, color, label in zip(range(2), colors, labels):\n",
        "    plt.plot(fpr_dict[i], tpr_dict[i], lw=lw, color=color, label='ROC curve of class {0} (area={1:0.4f})'.format(label, roc_auc_dict[i]))\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--', lw=lw)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Genetics ROC')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.savefig('Genetics_GRU_GCN_classification_ROC_curve.jpg')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "S7y1k_ke54Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_roc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "a945fb42-c1bb-4bec-98e0-51835d4bb14d",
        "id": "_XBmEmaY54Yu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score_array shape (640, 2)\n",
            "label_one_hot shape (640, 2)\n",
            "2\n",
            "roc_auc_score [0.89157581 0.89107928]\n",
            "recall_score [0.89705882 0.7887931 ]\n",
            "precision_score [0.88192771 0.81333333]\n",
            "f1_score [0.88942892 0.80087527]\n",
            "0.8910792765382016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ilfg-Y0u54Yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}